,0
0,"RYAN TREICHLER: Hello, and
welcome to this presentationon how to use real time
streaming data to create a moreimmersive customer experience.I am Ryan Treichler, the
Director of Product Managementfor SAS Customer Intelligence.And I'm really excited to
talk to you about this.So before we spend
too much time talkingabout how to use
streaming event data,let's talk first about
what exactly that data isand why a marketer might
want to use that data.So when any user
interacts with a websiteor a mobile application,
there's a large amountof data that could be captured
based on those interactions.So there's things that
are passed nativelylike the IP address
for the user,and information about
the user's computer.But as they interact
with form fields,you can track all
those interactions.You can track navigation
through the website.Anything that the
user does essentiallyis something that
could be potentiallytrackable and usable
for marketing purposes.And in the case of
SAS, we have a tagthat is placed on the page.And this tag enables
Customer Intelligenceto capture information about
what the user is doing.So this can be configured
by the marketer.They can determine which
particular elementsare tracked, what things
are actually used.But it's a relatively
straightforward implementationto get set up.And it makes it so
that you can then startcapturing the streaming data.In it in addition to the
system collecting datathrough its own tags,
event data can be passedfrom other systems as well.So there is an API to pass event
data through an API gateway.And so there's a myriad
of different waysthat events could actually
be passed from other systemsinto this system.And then once you have
all of these events,once you have all
this information,then you can determine
what to do with that.And so one of the
things that youcan do with all these
events in the systemor all this data
in the system isyou can stream these events to
the agent or another system.And that other system could
be an analytics systemto make some decisions.So for example, you could
have a loan submission form.And the events from
that form could thenbe passed off through this agent
to an on-premises environmentwhere ESP or Intelligent
Decisioning couldlook at the elements
that are in the pageand use them to make
decisions about the user.So why is streaming
event data important?So when you look at
marketing and youlook at the construct
of marketing messages,one of the things that's
really, really importantis understanding the context
in which somebody is lookingat or viewing your message.And so by capturing
streaming event data,the organization can be aware
of the contextual information,about what the user has
done in that very session.And they can use that
contextual informationin order to run
an analytic modelor to execute something which
then can be used to personalizesomething for the end user.So how do we do this?I mentioned that
we have a tag thatintegrates with the website.We also have an
SDK that integrateswith their mobile applications.As data is captured by
Engage Digital or Discoverfrom the web and
mobile channels,some of that data, the
administrator in 360can determine to pass that
event data from those systemsthrough the event API into
their on-premises environmentwhere this data can be fed to
either Intelligent Decisioningor Event Stream Processing
for further decision purposes.So this enables the events
to be captured in 360.They can be streamed into
the customer's environment.And then in the
customer's environment,the customer can work in a
myriad of different toolsin order to determine what the
next best offer for the user isor how you want that
particular user to beto be handled going forward.So those are how things
are set up in the systemand what know streaming data is.So it's a good who, why,
and what of streaming data.Next we'll actually get into
the actual demo of lookingat this working on a live site.Now I'm going to begin
the actual demo of this.And so as we look at it, there's
a couple of different thingsthat I'm going to
be showing you.And so I want to
point those out to youbefore I start the actual
demo so that everyoneknows what we're looking at.So the first is, we've
got the website that I'mgoing to be navigating.And this is just a
SAS demo website.But it has been instrumented
on the general internet.And it's been instrumented with
the SAS Customer Intelligencetag to do the data
collection piece.And that's what
we'll be leveragingto capture the streaming data.On the right hand
side of the screen,I have a tool that's
just showing mein real time, the
events that arebeing captured by the SAS tag.So you can see what's actually
going on in the browser.It's a way of visualizing the
things that are occurring.But it's not really
part of the product.It just helps make the demo a
little bit more understandableso you can see the things
as they're occurring.And then lastly, I've
got some VA reportsthat can be used to see the
event data in real time.So just as we're
looking through that,you'll see stats about
what things are showing up,what things have
been calculated.And we can see some of the
more complex events thatare being tracked by the
system, just to get a littlemore information about that.So starting out, we've
got the SAS website up.And I'm just going to
refresh it so that itidentifies that I'm there.So you can see that
this has identified me.The identification doesn't
mean that it's actuallyreally recognized
me as a person basedon a deterministic identity.Right now, it's just
saying that it'sbeen able to issue me a cookie
and I have an identity evenas an unknown
visitor, so that I canbe referenced in the future.If I were to click
Log In, that couldassociate that unknown
visitor ID with some other IDsif there is a
deterministic identityfor the particular user.You can also see the different
things that have happened,so which pages I've visited so
far and what I'm looking at.So I'm going to navigate
to the loans page.You'll see as I navigate, those
events are being captured.And so these are
all different eventsthat are being gathered by
the system in real time.And an example of what could
be interesting with thisis so, we've got now
a loan application.So with this loan
application, wecan capture information,
not just aboutwhen the user submits.But as the user actually
enters the data in,we can capture that information
from each of the form fields.And if they were to go back
and forth and change things,we could capture that
information as well.So you go through and fill out
the information about this.So now we'll say it is--$5000.So I fill out all
this information.As I fill this out, you can
see all the field interactionsthat were captured.So all that data was
captured without meneeding to submit Enter.Just as I went
through in real time,we were capturing those
different elementsand passing them off.So now that we've got all that
data, if we click Apply Now,in this case, what
we're doing iswe're actually taking
all of the informationthat we've got in those four
form fields that had beenentered as it were entered.And passing them
off to IntelligentDecisioning to make a
decision about what to return.So in this case, it says,
hey, congratulations.We can offer you a loan
with a particular interestrate based on the information
that I had entered there.And so you can
see, we'll go hereto Form Field Interactions
for this particular reportfor this.And we can pull up and see
that I entered some informationinto a particular form.And if I go into the
Loan Calculation screen,you can see that it's got
my loan identity submitted.You can see that I
submitted some information.This is my identity
for what was submitted.You can see my income
that I entered.So you can see
that I had entered$5,000 in my income amount.You can see the--so all the information
that I enteredis actually showing up
there in these interactionsin this particular report
to identify, all right,all this data has
been captured by me.So to mimic a user
that was tryingto game the system
a little bit, I'vegone through and rerun or
reentered the informationinto these form fields
a couple more times,tweaking the monthly
income amount,moving it up and
down a little bitjust to see if that
had an impact on whatI got approved for.The system, however, remembers
what was entered in thereand uses that.And after they get
essentially one rejection,the user is told, hey, you
need to contact an advisorto actually understand
what's going onwith your loan application.So you can see in
the report viewerthe number of times that I've
entered the different amounts.And actually one
more just popped inbased on me making some changes.And as I went through
and made those changes,you can also see the
information about--in this case what's
important, whatthe minimum income
that I've enteredwas throughout all the
times that I went throughand what the max income
that I entered was.So in this case, the
minimum that I specifiedas my income was $1,000 and
the maximum that I specifiedwas $5,000.So the system keeps
track of all that.We have all that data
as part of the eventsthat are coming
out of the system.And that would enable you
to analyze that and usethat to make a decision, which
was exactly what was happeningwith this box right here.So whenever you saw
the flashing boxes,we were actually
leveraging the full setof data that was available
on the consumer, not justthe information in this form.And then using that total
view of the customerto determine what marketing
message to serve them there.So this is one way that you
could use streaming datato improve the
customer experience,to create a better customer
experience for the users.And to also protect
the organizationby looking at not just
the information thatwas successfully
entered into fields,but looking at all
the informationthat the user has
entered and lookingat users that are entering
information multipletimes into the same field.So thank you all very much for
listening to this presentation.Again, I'm Ryan
Treichler, the Directorof Product Management for
Customer Intelligence for SAS.That's my email address in case
you have any questions for me.I hope you enjoyed
this presentation.Thank you."
1,"Hello.My name is Eduardo
Riva, and I'm reallyhappy to be hosting
you, at least virtually,to the presentation of my global
forum paper Sas Grid Managerand SAS Viya, A
Strong Relationship.SAS Grid Manager has been
deployed in the past yearto countless production sites.At the same time,
SAS Viya adoptionis increasing
amongst our customersand many discover
that it perfectlyfits alongside the existing
SAS implementations.In existing environments, it
can be integrated and keptrunning at least until
major projects havebeen migrated over.Both SAS Viya and
SAS Grid Managerhave great distributed
computing capabilities,and some customers wonder
how the two compare.At the high level,
the questions weget the most from
SAS customers can besummarized in four categories.First, I SAS Viya
and SAS Grid Manager,how can and get the most value
from using them together?Second, I have SAS Viya.Can I get any
additional benefitsby implementing
SAS Grid Manager?Third, I have SAS Grid Manager.Should be moved us SAS Viya?And finally, I'm
starting a new project.Which platform should I use?SAS Viya or SAS Grid Manager?The common answer to
all these questionsis that SAS Grid Manager and
SAS Viya compliment each otherin providing a highly available
and scalable environment.In fact, starting
with SAS 9.4M5,SAS Viya and SAS 9 solutions
can play nicely together.You can access SAS
Viya CAS enginefrom SAS 9 thanks to the
capabilities available bothin SAS foundation
and many clientssuch SAS Studio, SAS Enterprise
Guide, SAS Enterprise Miner,and so on.By using both SAS 9 and
SAS Viya in parallel,you can continue to benefit
from any investment in SAS 9as you make use of SAS Viya
functionality and features.From within SAS 9 interfaces,
projects, and code,you can use the advanced
analytics and performanceenhancements that
SAS Viya provides.For example, you can see here
a screenshot of SAS Studio codesnippets that can help you
connecting to CAS server.Loading data into CAS memory,
submitting both traditional SAScode, and new Viya
analytic procedures.And finally, getting
back results.It is easy now to imagine
that the SAS 9 session couldbe running on a grid node.This way, you have
achieved a nice integrationwhere a job started
on a grid nodecan connect to a CAS server
and establish a new session.Execution is then
offloaded to CAS,which can accelerate the
processing by running itacross multiple complete
nodes on an MPP instance.CAS excels at running
individual jobson a subset of data in
parallel and in memoryin order to provide
maximum throughput.Why limit yourself to a single
job running on a grid node?SAS Grid Manager excels at
allocating multiple jobsacross a grid of machines
using sophisticated resourcemanagement algorithms.These maximize the consumption
of available resources.Together, SAS Grid
Manager and Sas SAS Viyacan provide an efficient
and highly availableenvironment that
ensures rapid results.Let's now address
a different topic.How do SAS Grid Manager
and SAS Grid Manageraddress capabilities
that they both have?Let's start from
workload management.SAS Grid Manager uses
cues to manage jobsboth to decide which ones
to start and on which hosts,and to manage jobs
already running.SAS Viya provides options to
limit resource utilizationsuch as CPU and memory.Although these capabilities
provide a basic formof prioritization and
resource management,SAS Viya, in the
current release,does not provide proper
workload management.Come to scalability
SAS Grid Managerusage of a clustered
shared file systemsimplifies adding or
removing grid nodes.With SAS Viya, you can
scale the CAS engineboth by adding additional worker
nodes to an existing instanceand by defining additional
CAS server instances.For both the Grid and Viya,
infrastructure servicescan be clustered to support
increasing numbers of users.These clustering
capabilities are notavailable with SAS
Viya on Windows.From a maintenance
point of view,it is easy to take a
grid node out of service.For example, to apply an
operating system patchwithout impacting any
grid functionality.Jobs are only dispatched to
the remaining online nodes.The same is true for
clustered Viya services.Individual members
can be taken offlineas long as each cluster
maintains a minimum quorum.With a distributed CAS
server, worker nodescan be stopped without
impacting running analysisand edit back live after
performing the maintenance.High availability has
always been a key capabilityof SAS Grid Manager.Both services and jobs
can be monitored and movedto surviving nodes
in case of failure.SAS Viya addresses
availability concernsby providing clustering
capabilities for our services.CAS server can maintain multiple
copies of data distributingthem on different workers.This way, if a worker
becomes unavailable,the controller can
instruct other workersto activate their local
copies and all the tablesremain available.Let's discuss
parallelization capabilitiesthrough an example.Assume you have a serial
sequence of steps.Let's say a data step followed
by three data preparationprocedures, two analytical
models, and finally, a reportwith the results.You can see on the
left side how youcan leverage Grid to reorganize
these steps since some of themcan run in parallel.When the re-organized
job is submittedto sub SAS Grid Manager,
it runs as many stepsas possible concurrently.The parallelized sequence
may terminate in the fractionof time of the original one.On the right side, you can see
how CAS tackes parallelizationdifferently.With a single node
CAS server, the jobscan only run sequentially
on all the data.If you scale to a
multi node MPP server,CAS splits the data evenly
in chunks distributedacross all the cluster nodes.The job can run on all
workers simultaneously.Each node can produce
results fasterbecause it has to analyze only
a subset of the original data.In the end, the CAS controller
collects and summarizesall intermediate results
before sending them backto the client.The previous example highlights
how SAS Grid Manager and SASViya parallelism using two
complementary approaches.The former uses
task parallelism.The latter, data parallelism.Task parallelism is when you
have concurrent executionof independent tasks on multiple
computing cores or hosts.Data parallelism leverages
the concurrent executionof the same task on each of
multiple computing course hostson different subsets of
the data to be analyzed.This table shows
the key differencesbetween the two approaches.For example, how task
parallelism is asynchronous,while data parallelism
requires that all sub tasksto be completed before
moving on to the next step.Also, task a parallelism
scales with the numberof independent concurrent steps
while data parallelism scalesaccording to how
data is distributedbetween the workers.Let's now review
some common use casesthat we have seen
with our customersintegrating SAS Grid
Manager and SAS Viya.The first is what they
call the evolution pattern.It is shown here with an example
where multiple users leverageSAS Enterprise Guide to submit
SAS code to a backend server.As the number of
clients grows, itmay become difficult to
avoid the resource contentionand prioritize the users
of the shared environment.In the second phase, the
single backend serveris migrated to SAS
Grid environment,which provides the required
workload managementcapabilities.To stay ahead of the competition
and leverage new capabilities,it is possible to
add the SAS Viyato the previous
environment bringingnew advanced algorithms and CAS
memory processing capability.Distributed in-memory
analytics are alsorequired to keep pace with
increasing amount of datato be ingested and analyzed.This evolutionary
approach can alsobe implemented for others
as solutions such as SASEnterprise Miner or even with
basic SAS parameters usingdata step, SQL, or
analytical procedures.SAS Grid Manager can
provide the workloadmanagement capabilities
required to avoid the resourcecontention and to prioritize
and reverse user base.SAS Viya can augment the
analytical capabilitiesby providing advanced algorithms
and accelerate the timeto result. Another
use case can befound in environments where
SAS Grid Manager and SASViya provide complementary
services in different stepsof a project.We can see here on the left,
SAS Viya feeding into SAS GridManager.For example, SAS data
preparation and SASused data mining and
machine learning, bothbased on SAS Viya, can
be used by analyststo prepare data and build
models in development.When the motors are ready to be
pushed as better jobs runningon the corporate data
warehouse, SAS Grid Managercan be used to orchestrate
the execution in productionproviding operational
batch workload management.On the right, SAS Grid
Manager feeds into SAS Viya.Traditional ETL
systems can lag behindin feeding the required data
into analyst environments.In these cases, data quality
and data preparation stepscan be run in a SAS
Grid environment.Grid can improve a scaling
and parallelizationto perform parallel
data manipulationfor a lot of users, a
lot of applications,and a lot of data.Grid jobs can include
both traditional SAS 9and CAS steps, all integrated
in a seamless flow.The last use case describes
managed analytics environment.SAS Grid Manager cannot directly
throttle SAS Viya or betterworkloads running
inside the CAS.However, you can
use SAS Grid Managerto control the workload
that gets submitted to CAS.For example, you
could submit all jobsthat access CAS to
a common grid queueand then limit the queue
to five simultaneous jobsat any given time.Obviously, this is a
very simple example.You can expand on it by creating
custom metrics for SAS GridManager.For example, a
custom script thatcould monitor the CPU
consumption of CAS workersor even the number of
existing CAS user sessions.SAS Grid Manager could
then be configuredto use this custom script
to dynamically adjustthe limits on the grid cues.In conclusion, if you are a
SAS Grid Manager customer,you can leverage the
new functionalitiesthat SAS Viya provides.But also, if you are
already using SAS Viya,you may benefit from SAS Grid
Manager workload managementcapabilities.Everyone can choose
the path that bestmeets their data needs using
the best from both worlds.Thank you for listening
to today's session,and feel free to reach out for
any questions you may have."
2,"Ladies and gentlemen, welcome
to SAS Global Forum 2020virtual conference.This is going to be a
presentation of parsingwhen using SAS when
the data's hidingin a non-standard format.Who am I?I am Andrew T. Kuligowski,
and I have been usingSAS for quite a long time.I am listed as an
independent consultant.In fact, I only take
assignments these daysif I am interested,
which means Iget to spend a lot of
time practicing allof this self-quarantine at
home, and I have been doing itfor a few months now just in
case I had to do it for real.I want to stress that this
is a very abbreviated look.Even by abbreviated
standards, thisis going to be an
abbreviated look.What is parsing?I spent a lot of time on the web
looking at various definitions,and some of them
came close, but Icoined this one for the
purposes of our presentation.Parse, the analysis of
a string of charactersand subsequent breakdown
of those charactersinto a group of components.What does all that mean?Well, let's take a
look and find out.We are going to have to do
some human analysis first.We have to determine, what in
our file is considered noise,and what is considered
useful data?A lot of people use
the word, signal.And I'm going to use
the word, signal, hereas well to represent the
data that we want to keep.However, I also want to point
out that we have identifiers.Identifiers help to determine
the difference between dataand noise.This is what's going to tell
us, where does our signal start,where does our signal end?We want to reject the noise.We want to keep the useful data.We will reject the
identifiers eventuallybut we can't do it yet.Here is a file.Ironically, this file
was done last yearfor a presentation in China.And it has to do
with medical recordsfor a group of patients.Of course, we had no idea
how timely this would be,but we decided to keep the
example for the purposesof this presentation.Taking a look at
this letter, we willsee that the beginning with
the salutation and the addressand the ending, this
is considered noise.There is no data in here that
we are going to store, report,analyze, whatever.So our routine is going
to be rejecting this.The part that I have highlighted
in yellow is useful data.This is where the
data exists, and thisis where the indicators exist.Let's take a look at what we are
going to do to find, identify,keep, and reject.If we look at the signal part
of the data, the useful data,we will see that we begin
with the string, Patient A.And we end with a blank line.So we are going to write a
routine that reads every line.It rejects the lines
until we get to Patient A,and then it keeps them until
we get to the blank line.Very simple routine.Let's take a look at
a couple of the tricksthat we are going to
use in this routine.Null input.I used to say that null input
was the fifth of four inputstatements.Some people agree,
some people disagree.It used to be the
only time you wantedto use null input was to
close off a trailing at sign.However, there is a
second benefit to that.It will populate and
temporary SAS fail--variable, rather, excuse me--called _INFILE_._INFILE_ will contain the line
of data that was just readby the input statement.The beauty of this
is that now wehave a string that
contains one line of data.We don't have to
define that string.We don't have to worry about
how long that string is.We don't have to worry about
anything because all of thishas occurred behind the scenes.Rather than writing
an input statementwhere we are keeping a
variable that we create,we're going to let
SAS do it for us.We're also going to use the
Index function to searchfor the string we want to keep.We are going to
search, quote unquote,a needle in a haystack.Index is going to return the
starting position of whereverit finds that string within the
full string called haystack.It will return a zero if
the string is not found.And as you can see
in the code, weare defining this
with an if statementusing SAS's internal 1-0
representation for true-false.Little concept.This is one of
the best practicesthat I have employed
for decades--I will admit it-- decades._IND.Any variable that I have in my
code that ends with _IND I knowis going to be a
true-false indicator.This way, when I am
debugging my code lateror if I have to enhance
it, it helps me understandwhat it was I'm doing.I do not know if any of you
have ever had to go to your bossand say, look, I know you
wanted me to fix this codebut I have no idea what
it is that they are doing.And then had to say further,
and I'm the one that wrote it.That's not a good thing.So anything you can do
to make your life easieris going to be of
a big help to you.And in fact, speaking of
debug, something went wrong.We only have one observation.Normally, I would
be sitting hereasking for input
from an audience.Since we're doing this
virtually, I'm going to cheat.I'll tell you what it is.There is a blank line right
after Patient A. We did notaccount for this in our code.Therefore, we told SAS to
start recording, basically,when we hit Patient
A and stop whenyou get to the blank line,
which was the next character.So we told SAS not to
bother to read anything.We can fix that.We are going to make a
few changes to the code.Where we have
RETAIN Output_IND=0,we are going to add
a second variable--RETAIN Blankline_Cnt, and we're
also going to set that to 0.Then, down towards the end of
the code where we're watchingfor the presence of a
blank line in INFILE,we are going to, instead
of just outputting the lineand changing the code
to say, stop recording,we are going to put a whole
if statement in there.We're going to keep
track of our blanklines.If the blankline count is 1,
we're going to increment it.Either way, we're going
to increment it actually.And if the blankline is 2,
then we're going to say,you know, we get the
second blankline.That's the point
where we are done.But set the indicator to 0
and let's stop recording.Hopefully, if we run this code
and we've done this correctly,we will now capture
the paragraphthat we were looking for.By the way, I do
want to point out,just as I have a
variable called _IND,I have a variable called _CNT._CNT for count is another
little internal thing I've donefor years in which I say, any
variable ending with this isa counter.So I know it is going to
contain whole numbers keepingsome kind of count or another.And when we run this thing,
I did not store the output,but trust me on it, it
looked a lot better.Here is that paragraph
again that we've captured.The next thing we
are going to dois we are going to convert it
to one observation per sentence.Technically, we could
probably write some codethat would do a lot of
parsing on these sentencesand not have to do this.But remember, we have to also
be able to understand our codeand debug our code,
and I found that it'smuch easier to do
that if we are dealingwith complete sentences.Therefore, what
we are going to dois we are going
to use the periodto denote the fact that we
have the end of a sentenceand we will begin another one.Simple.We have covered this
in English class goingback to when we were kids.Is it so simple though?Let's take a look.We're going to write some code.And in this code, you'll see
that what we are going to dois we're going to keep
track of partial sentences,and we're going to see if there
is a period in the output line.If there is a period
in the output line,we are going to make
a complete sentence.We are going to take
the partial sentence,and then we are going to
append to the piece of the codestarting at 1 and going
until the positionwhere we found the period.If it is not, then we
are going to append--well, actually, let's
take a step back.Now that we're done
there, we are alsogoing to say, take a look at the
code, and we're going to say,we have to start a
new partial sentence.Everything after the period
is the next partial sentence.And by the way, we
better take a lookand see if we found
the period again,just in case we have
a very short sentence.Now what we're going
to do is we aregoing to take the partial
sentence that we've got,we're going to append the
output line to it so we nowhave a longer partial sentence.Why do we use two
separate strings?Well that is, notice
that we are onlykeeping complete sentences.This way, we don't have to worry
about a bunch of observationscontaining partial strings.I told you it might
not be so simple.Normally in a
hands-on workshop Iwould let everybody discover the
fact that this code would notwork correctly.I'll tell you right now,
though, save some time,there there's a trick.English class taught
you about periods.Math class taught
you about decimals.The problem is, they're
the same character.If we do not allow for
decimals, then our codeis going to produce
some very wonky results.How are we going to do that?Simple.All we have to do is look for
a period followed by a space.This way we know
that we are dealingwith the end of the sentence.There's a couple of
people who say, yeah, but.Trust me on this.I will show you the code
is simple enough right now.There are some exceptions.We're not going to code
for them right now,but I will denote them later.Let's take a look at
some concepts here,some character functions
that we might need.Remember we talked
about index before?But we also have
a Find function.The Find function is
also going to searchfor a string, which
we're calling needlein a string, haystack.And just like index it's going
to return a one or a zero.Actually, it's going to
return a position or a zero--the position where the code
started if it finds needle,or a zero if it
doesn't find needle.So why would we want to
use Find instead of Index?There are some differences.Find can actually look
for multiple strings.Find can trim trailing blanks.And Find can also optionally
include and ignore the case.Trim.I mentioned Trim earlier.Trim removes blank
characters from the endof a specified string.And the double bar that's in
the code that I've mentionedis going to append the second
string to the first string.Why do we need Trim then?Because we have trailing blanks.The trailing blanks
are going to be--excuse me.We have a cat down here.Come on, guys,
leave me alone, OK?We have a double bar, and that's
going to do the appending.What's going to
happen, though, isthat it's going to append
where the blanks end off.The problem I have
with those blanksis we don't want to keep them.We want to have a
smooth transition.So we're going to have to trim
that first string to get ridof the trailing blanks.Somebody out there
who's probablypaying attention is raising
their hand saying, yeah,but what about all the
CAT functions that RickLansing used to tell us about?I do mention the CAT functions
in the extended versionof this talk.Hopefully, again, if we get
to do the 90 minutes someday,we will talk about
the CAT functions.Substring.I believe I mentioned
Substring earlier.Substring is going to take a
character value called string.We are going to take,
starting at a start position,and go for a length
of length, L--Len, we called it here.If you do not specify len,
then the Substring functionwill assume you
want every characterfrom start position to the end.And of course, if
you were to say,give me everything
from 1 to len,you'd basically be
saying an equivalentand you're going to be
taking some extra overhead.Don't do that.Substring, by the way, is
a very unique function.It can be specified on the
left-hand side of a characterassignment.So you can actually
use Substringto change part of a string.Normally, in most
civilized coding languages,this is a much more
complicated effortto have to change the
middle of a string.SAS allows us to do that
by employing the Substringfunction on the left-hand
side of the equal sign.And here are the results.You can see when we look
at this string right now,we have separated
everything into five rows.We have one column because
the only variable we keptis complete sentence.I mentioned before, there
were some exceptions.The exceptions that I have
are that the routine will nothandle the period if it's the
last character of a string.So if your sentence ends
at the end of a string,this code will not handle it.We will have to
fix that if we everdo a more complex
version of this code.For the sake of a
short presentation,though, we chose not to do it.Again, now that we have
everything put togetherin a string with
complete sentences,we can now begin to
actually parse out data.The complete
version of this talkwill actually show us how
to take each sentence, howto use indicators to
determine where variableand values start, where
variable and values end,and whether or not they are
present in each sentence,just in case somebody seems
to have forgotten one.All right.This, as I've mentioned,
is an abbreviated talk.We're not going to have
time to go into it,but what we would
have preferred to dois to give you a lesson on
many character functions.Each one of the variables
that we are going to read in Itried to use a
different techniqueso that we would get
exposure to multiple versionsand multiple character functions
and multiple ways to use them.In conclusion,
sometimes your datais going to come in a
non-standard format.It might be necessary
to even findthe data within that format.You can't clean the data until
you know what the data is.You can't process the data
until you've cleaned it.I would strongly suggest that
before you begin a processlike this that you have an
understanding of characterfunctions.Character functions
are the tool kitwhich is going to save your
professional life, if you haveto write something like this.However, the internal browser--I always keep Chrome, I'm
using Chrome for the moment--keep it up and
running and pointingat the SAS documentation
for the character functionswhenever I do this
because sometimes Iwant to use an option that
I'm not too familiar withor sometimes I won't even
remember what the characterfunction is that
I'm trying to do,and I will see if there
is maybe somethingthat's already written
rather than my havingto go dig for it.One other thing
I'll add there isthat if I can't find
a character function,lexjansen.com is a good
place to go looking.lexjansen.com is where all
of the SAS presentations,the SAS papers that have
been done at Global Forum,at the various
regional conferences,and many other user-based
SAS conferences can be found.I will search for what I am
trying to do there and seeif it can be found.Thank you for joining me
during this short, abbreviatedpresentation.If you'd like to
get a hold of me,you have questions,
please try to reach outat the email address
listed below.Thank you."
3,"Hi, everybody.My name is Zeke Torres.And this presentation is part
of the SAS Global Forum 2020topics titled Customized Output
with the SAS Config File.I live in Chicago.I am a SAS developer.I've worked with SAS
for over 25 years.I'm a dad.I'm into big data.I work with local user
groups, regional groups allacross the globe and have a
great time not only talkingabout SAS, but networking and
meeting other individuals whoare passionate about
solving big data issuesand analytic issues in general.So our goal today and what
we want to do and accomplishis make our lives
a little bit easierby improving the information
we get from our SAS s and listoutput.Typically, what
happens is we end upgetting a variety of files
from when we run our SAS jobs.They can become pretty tedious
and difficult to work with.It's important because
our logs tell usabout what happened in
the work that we did.They help us debug things.They help us optimize
our code and our processto improve the performance.And we get a history
of what happened.But oftentimes, it
becomes pretty toughwhen we're trying
to determine whoran something, what they did.Things get overwritten.And if you're working on
different projects thatare kind of the same or share
the same SAS code naming,things get overwritten as well.So we basically end up having
more work and more effortin order to get some of our
jobs or our projects done.So our typical LOG
and LST ends upbecoming a series of detective
work type of questions.Who ran something?Did they run it for
a production purpose,or were they testing something
or evaluating something?When they ran some job and
we do have a LOG or a LST,was this for a specific project
or a portion of a project?And then there's a
component of time.Well, when did this log occur?Even if we have the file date
and time stamp on the operatingsystem, it doesn't
necessarily meanthat that's when it happened.And so, typically,
somebody is goingto end up asking,
why didn't we savethis information somewhere?And especially, if you're using
Git or some type of versioncontrol, either
individually or as a team,the LOG and LST
become very useful,but we're hindered by
the fact that they're notnamed correctly.So to me, an ideal scenario
is that the LOG and LSTwould have a time
and date stamp,but in a more useful way.It would also be ideal if the
LOG and list had the person'sID, the person who ran it,
the person who executedthat job so we can see
what they were attemptingto do and identify who that
person is in case we wantedto narrow down our search for
questions about what was beingdone.And then what was their intent?Did they run us for production?Did they run this
to test something,or did they run this to
review somebody else's work?And of course, the actual
name of the code, right?That's already a default
with the SAS LOG and LST,but we're missing
some of the facts.So some other ones that could
be added if you were ambitiousis you could probably add
which server it ran onor some of the data facts.If you had a specific
job of a SAS thatran on monthly data versus
quarterly data, maybethat's a fact we can
add to this process.And so I won't get into
that level of detail,but at least you'll be able
to see what that looks like.And so this is a rendition
of that ideal scenario,where, here, with my
cursor, what I'm showingis that we've appended
in a sequence.Starting from the
left, the year,the month, the day of the
month, and then separatedby a consistent underscore,
the hour and the minute,followed by another underscore,
which then gives us the user,and then whether or not
that user ran somethingin production or in test.And then the actual name of the
original .sas was the targetof this process.And so what that does is
it gives us the abilityto organize our
work a lot better,but not our work as in
the .sas, more of our workas in the results
of what happened.So let's look at what we
need to do to set this up.And I've done this
in Windows and Linux,and I've done this
in, for example,a configuration of a
SAS Studio edition,but it did take some work.So I'm going to publish
that separately so that itis encapsulated
cleanly, as far as whatwe can do with SAS Studio.But the ideal scenario is that
this will run in Batch Mode.And if you're not familiar
with Batch Mode, what we'll dois we'll end up having
links in the paper thatdescribe how to get that done.So the paper does
document the detailsthat I'm describing here.So you're going to have to
become familiar with SYSPARMS.The SAS paper that I've
submitted to this processalso describes that.And you can have a little bit
of macros and data null steps.But it won't be that intense
because the code in the paperand on the GitHub repo for the
SAS Global Forum conferenceis already
functioning and works.And in any of this,
when you do set this up,I highly recommend that you
make the appropriate backupsof any files that you've got
just so that you have a wayto put things back in case
you find that you're nothappy with the setup
or that you wantto evaluate the
configuration differently,you have a way to start.So I encourage and I'm saying
this for full disclaimerthat you must back things
up before you startconfiguring and changing them.So the overview
concepts is we'regoing to adjust the
configuration file.That allows us to modify
the PROC PRINTTO values.The PROC PRINTTO function in
SAS helps us direct and redirectwhat the .log or the
.lst come out as.And so what we're
trying to do iswe're trying to capture that
process as early as possiblewhen we hit Submit.SYSPARM allows us to
modify and capture valueslike the user ID, the date,
the time, and other factsthat we want to push into the
new values of the PROC PRINTTO.And then the macro that
I've described in the papercombines all that for
us so that we end upwith a cohesive
new name to that.So one of the reasons
why this is usefuland why I've adopted this
method is because evenif I'm working individually--but more often I'm
working in the team--it becomes a lot easier to
create a historical trackingof what jobs I ran to get to
the current state of my work.I also share those results
by pushing them and sharingthose LOGs and LSTs with
my other team members.And so they now have a
chronological and a succinctand standardized way to find
those facts, specificallyuseful when we use Git as a
team in our team wide usage.So this year's paper--
because this is a follow upto 2019's paper--this year's paper in
2020, what I've coveredis a recommendation of
how a team could structuretheir folders or projects.And so in any
project that we have,we're typically going
to start somewhereon a laptop, or a desktop,
or a server that'sgoing to hold our projects.And so under that, we're going
to have subsequent folderswith many projects.And what those levels really
aren't important in this part.The next one is.Under a specific project--in this case, we're
saying project A--I've found it very useful
to have a code folder.That code folder
will hold syntax.That code folder is meant to
hold our SAS code and only SAScode in a subfolder, our R code
and only R code in a subfolder,and the same goes for
Python and the same for SQL.But there's a new
folder or a folderthat I strive to have team
members collaborate on,and it's a universal
syntax folder.And so the purpose of those
folders are when we're dealingwith a specific syntax
like SAS, or R, or Python,then those should hold only
.sas or only .r, or only .py,or .sql code.SAS code, ideally, will
only have SAS syntax.But if there's ever anything
like an if statement,or a where statement, or
something that could be usefulas a .txt because it holds
values like state abbreviationsequal to the state long
name, then those should goin the universal folder because
files like that or syntax likethat is really agnostic.It could be something that
our R language could useor our Python
language could use.So it's really a
universal componentthat any language could use.And so if we start to
structure our code that way,our SAS code is SAS-centric,
our Python code as Py-centric,but we still have the
need to actually putour output somewhere.So that wouldn't be
in the code folder.Our output would
be somewhere else,and our source data
would be somewhere else.This layout, this configuration
of folders, works well with Gitbecause our Git would be at the
highest level, and our logs,and our output,
and our source datawould be symbolically linked.They wouldn't actually be
hard-coded or hold dataat that level.We could symlink
to different paths.And with .gitignore, we
could protect the integrityof the repository and only
focus on protecting our versioncontrol methods for
the code that we have.So in that sense, a developer
can work on that project,have a log output
and source datasymbolically linked whether
it's on their laptopor on a production server,
but their code actuallyis part of that git repository.And so as they start
working with this processand creating SAS
logs and other outputthat we want our team
to track, that workstarts to go and get pushed
up to our team repository.Whether it's on
GitHub or GitLab,a developer
individually can workon either their SAS work or
their Py work, or their R work.And whatever the results are--for this presentation, we're
saying a SAS LOG or a SAS LST,but Py has similar
types of outputthat could be
generated as well as R,or they can be configured
by those team members whowant to emulate that same
type of log in list mentality.But because it is actually
an agnostic component--the result of a process--well, that would go
into the team logfolder that could then
be shared by the teamto understand what took place.So I would love to see and
follow up with everybodywhat they think of this
idea, and how you customizedthis idea, and how you
created similar features notonly with SAS, but
maybe with Python, or R,or SQL just to find ways to
actually integrate it betterwith Git.I'm looking to
collaborate with peoplewho are using Git, and
SAS, and other languagesto actually improve our
coding and project process.Because at the end of the day,
if we are spending more timehunting our code quality
issues versus actually solvingthe problems, then
we're probablydoing it the wrong way.So I'm eager to hear
what people think.The code on GitHub is
functional and works.If anybody encounters
any issues,I'd love to tackle it and
solve it or maybe improve it.So I hope this really
helps everybody.You can reach me at my email.This is my LinkedIn
contact as well.I'm busy with things with the
Windy City SAS Users Group.That's wcsug.com.Look me up there, and I hope
to see everybody very soonat user conference events
for SAS and other conferencesas well.Please help your local,
regional, and SAS Globalconference teams to
have a great experience.And everybody stay
safe, and see you soon."
4,"Hello and welcome.My name is Biljana,
and this super demofocuses on extraction of
facts and sentiment keywordsfrom social media messages.In text analytics,
concepts are setsof terms that are grouped
together conceptually.For example, one
concept could bethe names of divisions
in your company,such as operations, research
and development, or marketing.Facts are matches of terms based
on a particular relationshipbetween the concepts.For example, in
the sentence ""Janehas a great job
in marketing,"" youcan match the terms
""Jane"" and ""marketing""with a relationship of
employee in a division.Sentiment keywords
are terms whichdenote a positive or
negative relationship,such as ""great job,""
or ""terrible events.""The question we are
trying to answeris what SAS conference
presenters were most praisedin social media and how?We will use data from
social media messagesfrom four SAS conferences.We will use this data
in SAS Visual TextAnalytics for three main tasks.The first one is extending
predefined concepts.Predefined concepts are groups
of rules included with VisualText Analytics out of the box.They allow users to tap into the
knowledge of the SAS linguistsfor identifying named
and numerical entities.The second one is using
automatic rule generation.Visual Text Analytics
can help youcreate custom rules using
automatic concept and fact rulegeneration.The third one is filtering
with global rules.When you want to extract a
particular match in most casesbut also have some exceptions,
you can write a set of rulesto cover most of the cases
where you want to match and thenspecify the conditions for the
exceptions in a filter rule.Filter rules are
global, which meansthat wherever they are placed
in the model or hierarchy,they remove matches
so the matches are notpart of the final
output of the model.We will then visualize
some of the extracted textin Visual Analytics.Because of time constraints,
let me briefly review the stepsI've done before I
launch into the demo.I created a new Text Analytics
project in Model Studioand imported the deduped
English-only data.The pipeline includes data, text
parsing, and concepts nodes.We need to use the
predefined concepts,so we check the box for
including them in the modeland run the pipeline
to get a baseline.One of the best practices
in building a concept modelis to first build a
hierarchy and modify itthrough the exploration
of the data.If you look at this
hierarchy, you'llnotice that there are only
concepts for positive words.While I first started by looking
at both positive and negativewords, exploring the data
showed that most of the tweetswere in fact positive.Additionally, negation, such
as ""not great,"" for example,was generally absent.So I focused the
model on extractingonly positive sentiment.If you recall the three tasks
you will see in the demo,the first one is extending
predefined concepts,highlighted here.We are referring to them in the
custom concept customPerson.The second task is
automatic generationof concept rules, which you'll
see in the posAdj concept,and fact rules, which you'll
see in the posPerson concept.The third task is removing
ambiguity, which you willsee in the global rule concept.Now let's switch to the
project in SAS Model Studio.I have already created
a project and includedpredefined concepts.Next, we'll open
the concepts nodeand explore the matches
for the predefined conceptsas well as create
some custom concepts.There are over 200 matches
for the nlpPerson predefinedconcept.However, by
definition, it does notinclude Twitter
handles, which followthe pattern of the at sign
followed by letters, numbers,and underscores.To capture Twitter
handles in additionto the nlpPerson matches, we
create two additional customconcepts, the at symbol
concept has only one rule,capturing the at sign.The rules for the
personHandle conceptcan be created by
highlighting termsin the documents,
right click, and selectto add this term to the rules.In the interest of time, I
will add additional rulesthat I've already identified by
pasting them into the window.There are 900 matches.Reviewing the first few looks
exactly like we expected.Next, we'll create the
customPerson concept, whichwill take advantage of the
predefined concept nlpPersonand enrich its matches by
using the two other customconcepts we created,
atSymbol and personHandle.In this concept, we will also
add international names thatwere not captured as matches
with the out-of-the-box rules.I found these through
exploration of the data.The number of
matched documents isover 1,100, which is about
1/4 of the total numberof documents.In this way, we have been able
to incorporate the matchesfrom the predefined
nlpPerson conceptand add matches from our
custom concepts to it.Next, we'll build
custom conceptsfor positive sentiment.For the positive
adjective concept,we'll take advantage
of assisted rulecreation using textual
elements and the filter.Typing in a positive
sentiment term such as ""good""in the filter and then
expanding that termto see all of the different
terms that are stemming to itcan help us evaluate
whether we wantall of these terms in our rule.If so, we can rely on
a few system optionsto write the rule for us.In the interest of time, I
will paste in some other rulesthat I have created
based on my exploration.The matches look
good, so we willcontinue building our model.Add a new concept,
positive phrase,that will build on the matches
from the posAdj concept.In the interest of time, I
will paste some of the rulesthat I've already created
through my explorationof the data.We'll run the node to make sure
that the matches are expected.At first glance, examining
the matches, they look good.However, as I
continued to explore,I found several matches for
the phrase ""best practices.""That is not actually
a positive phrase.It's just jargon.So we need to figure out how
to remove it from our model,specifically from our
concept for positive phrases.Create a new concept
housing global rules.We created the new concept
called globalRule and addeda new remove item
rule that removesmatches from the
positive phrase conceptwhen they are aligned with
the terms ""best practices.""You can see that there are no
longer matches for the phrase""best practices""
in this concept.However, one observation
is still selected.This observation contains
the text ""best practices,""but it is not a match.The next concept we
build is positive person.For building the fact
rules in this concept,we'll use automatic
rule generation.In the fact rule,
we want to capturethe relationship between
persons and positive phrases.So we will use those
two custom conceptsto auto-generate a suggestion
for the new fact rule.We first run the node to
generate the fact rule.The fact rule has been
placed in the sandbox.Let's click on that tab
to see what it looks like.In this case, the system
generated one rule.Sometimes it can
generate more than one.To see the matches,
let's run the sandbox.The matches look as
expected, so we will nowcopy this rule from the sandbox
into the Concept Editing tab.In the Concept
Editing tab, we canget rid of the generic
argument names,such as Concept 1 and
Concept 2, and replace themwith something that's more
appropriate for our content.We'll run the node again
to see the matches.Let's see the matches for
each argument in the factfor the first observation.We can see that
the Twitter handle@DrJosh is labeled as person,
and ""Great discussion""is labeled as pos, which
means positive phrase.Up until this
point, we have seenhow to extend predefined
concepts with the nlpPersonand customPerson concepts.We've seen automatic rule
generation for conceptssuch as positive
adjectives as well as factsfor the customPerson concept.And we've also seen filtering
with the globalRule concept.Now let's look at some
visualizations for the resultsfrom this model.We'll export this
output table so that wecan use it in Visual Analytics.We will add three objects.The first one is a treemap
to display the person fact.Since most of the
observations are missing,we will filter
them out so that wecan get a better view of
the ones that are relevant.In the same way, we'll add
two more objects, a word cloudfor the positive
phrases, and a tablewith the message text, the
keywords matched by the model,and the message sender.We will also filter out missing
documents from the word cloud.Next we want to add
some interactivity.When I click on a
particular person,I want the other
objects to update.And now for the insights.We can see that the one that
was most frequently mentionedin a positive light was Oliver
Schabenberger, the COO of SAS.This is not a surprise since he
was able to attend and presentat all of the conferences that
we were including in our data.Let's look at the specific
positive phrases that wereused to describe his speeches.Looking at the
word cloud, we cansee that some of the phrases
that were most associatedwith Oliver were ""great start,""
""great talk,"" and ""inspiringspeech.""Second most frequently mentioned
person was Giles Hutchins.He was one of the presenters
on the main stage for oneof the conferences.Some phrases that were
associated with himwere ""inspiring speech""
and ""great time.""In fact, the top five presenters
were featured presentersat the conferences,
so they probablyhad larger audiences than
the breakout sessions,allowing these presenters
more opportunitiesto be mentioned in
social media messages.I hope you enjoyed
this super demo thatshowed SAS Visual Text
Analytics in actionwith social media data.Thank you for watching."
5,"[MUSIC PLAYING]DJ PENIX: Welcome to the SAS
Global Forum 2020 presentation""Unlock the Business Value
of IoT with Analytics.""My name is DJ Penix,
and I'm the presidentof Pinnacle Solutions.Today, we live in a very unusual
time with social distancingand regional and
national restrictionson work essential guidelines.With the advancement of
IoT, or Internet of Things,businesses must be
proactive and aheadof the curve with
utilizing technologies thatkeep them profitable,
keep them competitive,and, in some cases,
simply keep the doorsopen to running their business.We wanted to show
you a very simple usecase of building an
IoT device and showhow it can be integrated
with the powerful SASAnalytics for IoT solution.Our presentation
today will hopefullyinspire you to think about
how IoT and analytics canhelp your business
thrive in today'sdifficult global economy.Thank you.The Internet of Things
is an emerging topicof technical, social, and
economic significance.Consumer products, durable
goods, cars and trucks,industrial and utility
components, sensors,and other everyday
objects are beingcombined with internet
connectivity and powerful dataanalytic capabilities
that promiseto transform the way we
work, live, and play.According to the
internetsociety.org,projections for the impact of
IoT on the internet and economyare impressive, with some
anticipating as many as 100billion connected IoT devices
and a global economic impactof more than $11 trillion
by the year 2025.In many ways,
manufacturers were alreadydoing IoT-like initiatives
over 50 years ago,with the utilization
of Programmable LogicControllers, or PLCs.But they were really
restricted by the limitationsof the computers
and boards that werebuilt into their equipment.Many companies today
still have equipmentthat predates PLCs, or
even predates World War II.These assets may
still be incrediblyvaluable to the organization.However, it might be deemed too
expensive to replace or to evenretrofit the machines
to update themwith modern sensor technology.And even if an
organization is currentlyset up for data collection
mechanisms via their PLCs,they may be outdated,
or they may not collecttoday's current sensor metrics.Also, many systems
are often set upto be batched for
data transformation,stored in traditional
relational databases,and then scheduled for
analysis at a later time.To make things worse, these
siloed data repositoriesare often not integrated
with other key business dataassets, such as their supplier,
customer, marketing, R&D,warranty, or any other
corporate databases.Today, we have a
much better way--a better way to get
the data real time,not close to real time; a
better way to get the dataintegrated with other corporate
assets; and a better wayto integrate real-time
machine learningand artificial intelligence
into your systems.Many people think that IoT
is only for manufacturers.That is simply not the case.There are many relevant
use cases and applicationsof IoT across multiple
industry verticals.For example, connected
retail and hospitalityare building applications to
connect their guests real timeto offer better experiences and
to ensure higher return rates.Connected insurance is
using image classificationto process claims
faster, more efficiently,and to give their customers
a significantly betterexperience.Connected health care
and life sciencesare using IoT to
offer predictive staffalerting so that
high-risk patients canbe helped sooner, or even
before an incident occurs.And there are many
other use casesin all kinds of industries.I would challenge
you to look closelyat your business and
your competition.What are the ways
that they are usingIoT to keep their
colleagues, their customers,their employees, their
executives, and eventheir shareholders informed
about the key processesand metrics within
the organization?This is an example data
and analytics workflowfor many typical organizations.The items in blue are
likely components that youhave within your organization.Traditional enterprise
assets of data inputsare fed through a staging
area, where ETL processesbuild your factory data models.From there, you can do data
selection and exploration,which eventually filter
out into analysis,which may include visualization,
decisioning, and alerting.One of the benefits of
SAS Analytics for IoTis that we can integrate
real-time streaming of datawithin your existing processes.Reflected in green,
the IoT componentsallow you to see data real
time in custom dashboards,as well as integrate
the output streamingdata within your existing
factory data model and dataexploration mechanisms.In our demo today,
we're going to show youan example of building
your own customInternet of Things device.Using a Raspberry Pi and
a Sense HAT accessory,we can detect
various measurements,such as vibrations, temperature,
pressure, and accelerometerxyz coordinates.We can stream the data
through the Wi-Fi network,send it up to the Amazon cloud,
process with SAS Event StreamProcessing, which
then can be displayedin custom real-time dashboards.The Raspberry Pi microcomputer
was initially released in 2012and has become a
popular go-to hardwaredevice for do-it-yourselfers
and techies alike.Our initial development was
on the Raspberry Pi 3 B+.If you search online at your
favorite electronics retailer,you can find the boards
for around $40 to $50.But you can also
find starter kitsthat include other components
and accessories to get yourunning quickly for around $85.The Raspberry Pi itself does
not have any built-in sensors,but there are available add-on
boards called Sense HATS.The Sense HAT we utilize
can be found for around $35,and it includes an LED
matrix display and sensorsfor an accelerometer, gyroscope,
magnetometer, barometer,temperature, and humidity.But there are many
other sensor boardsthat you can add on to detect
other things such as light,sound, GPS, a video camera
module, and even more.A sturdy case will keep
the components secure.Most starter kits
will include a case,but there are many
options available.This simple case has
a clear plastic windowso you can see the LED matrix
display, which can be usefulif you want your IoT
application to report feedbackto the end user.A simple micro USB cable
can power the device.Again, most starter kits
will provide the power cablesfor you.Finally, we purchased some
optional battery power sources.This is ideal for
carrying the Raspberry Pidevice to remote locations
or hard-to-reach places thatmight have electrical
power limitations.We have found that
batteries we purchasewill keep charge for only
around four to six hours.Still, this is a handy option
for some quick IoT experimentsand proof of concepts.Putting everything together,
we get our Shakebox IoT device.So here's our final
Shakebox device.You can see here, I've
got it in the case.I've got a clear plastic cover.I'll go ahead and attach that.I've also got, on the back, some
Velcro for the battery supply.I kind of like to
do that, just sothat it can kind of
stick together and bea little bit more mobile.I'm going to go ahead and plug
in the micro USB power supplyand then hit the
battery button here.And we'll go ahead and
boot up the device.You can see that there are some
LED flashing displays here.This is nice in the event
that you actually mightwant to put some user feedback.We actually did
some countdowns sothat we could tell when the
device was actually booting up.You can do other
messaging if it'sunable to connect to
the Wi-Fi and so forth.So I'm going to let this
boot up here a minute.And then I'm going to go ahead
and launch the custom webinterface that we developed.And again, I'm going
to hold this in my handhere while we're working.On the lower section
of the display,this is actually
Shakebox number 2,which one of our co-developers
actually has remotely.We can see that
there's some datastreaming in through there.On the top, you can
see on the dropdown,this is for Shakebox
1, which actuallyis the device that we have.And the system did
just come online.So you can see now that we're
getting real-time displayand data.I'm going to go ahead and
set this down now on my desk.I've got another
camera here that we candisplay so that you can see it.And then we'll
take a look at someof the features on
the dashboard itself.So when I set the Shakebox
down on its side here,you can see that things kind
of stabilize a little bit.Things become a
little bit more flat.I can actually twist
it and turn it,and you can see the pie
chart as it's rotatingso that we can see how
it's rolling and havingthe effect on that with
the twists and turns.On the left-hand side, we're
seeing approximately 180seconds of trend.That's nicer.If you want to see a longer
window, we can customize that.Whereas, in the middle,
it's more of a real time,on a second-by-second basis.You can see the effect of it
kind of coming through rapidly.So use case on this
for the vibrationmay actually be one around
putting a device, a sensordevice, on top of heavy
equipment or machinery,maybe in a
transportation vehicle.And you're trying to detect if
there's excessive vibration.That may either
damage some goods--and we'll talk a
little bit laterwhere it may actually give us
an early indication around maybesome machine failure.So some ideas around
predictive asset maintenance.On the right-hand
side here, we'reseeing some other metrics that
we have the ability to collect.We're doing some
calculated values.We're collecting
the min and the maxfor the altitude, the
barometric pressure.On the right here,
we actually don'thave the GPS module plugged
into this particular device,so we're not able to
see GPS statistics.But we have some
other devices thatwill allow us to show that.We've actually got some mins
and maxes, some ranges here.We can see at the
top the total numberof records that are being
collected, along with thosethat are exceeding
the green threshold,the yellow threshold,
and the red threshold.Again, we can determine
those to see how supervisorsmight want to be
alerted if we doget those excessive vibrations.And you can see,
as I kind of shakethat up now, our
counts are increasing.So that can give
us an indication.Of course, we can do
anything on the back endwith stats with SAS and
some analytics on thatas we need to, as well.The little heartbeats,
we have an abilityto determine whether
the device is actuallyconnecting to the ESP.Because again, even if
the machine is down,it's still important
for us to knowthat we've got connectivity.That way, the operators
know whether it's maybean issue with the IoT
device, or maybe it'san issue with the connection
stream, the internet,or something else along the way.So that's the custom dashboard.Again, this can be customized
for whatever sensor datathat you guys are collecting,
whether it's the RaspberryPi or other devices.I did also want to
mention and remindthat this custom interface is
actually developed and deployedup on Amazon cloud, and
so it does not have to be.This can be located at wherever
your SAS footprint is at,on a custom web hosted
environment or Amazon.We wanted to demonstrate
that it's actuallypossible to do in the
cloud, and it actuallycan work very well, indeed.We've got a metric here.We're just showing a little
above four records per second.Again, keep in mind
what's happening hereon the device itself is
that this has a builtin Wi-Fi on the Raspberry Pi.It's connecting
through the Wi-Fi.I'm actually working
from home remote today.It's going up
through the gateway,going up into Amazon cloud.We've got SAS Event Stream
Processor running upon the cloud.And then it is feeding up
into the custom dashboard.So what appears to
be a lot of movementis actually incredibly
responsive and in real time.So, again, this is significantly
better than the near real timetype batch analysis
that many organizationsare using in their manufacturing
or other processes.This is the previous slide I
showed earlier with an exampledata and analytics workflow.The orange boxes now show
some advanced capabilitiesof SAS Analytics for IoT
by integrating modelinginto our process.Once the models
are developed, wecan even deploy the code inside
the IoT device or gateway,thus allowing for
real-time modeling.Why would we want to do this?Well, here's one example.Using a SAS procedure
called Support Vector DataDescription, or SVDD, we
can address common use casessuch as fraud detection,
equipment health monitoring,and process control,
for example.Using this type of model,
we can identify outliersin our incoming streaming data.The above diagram
shows an illustrationof normal data points
bound within a model drivenby hypersphere, reflected
by white circle dots.The black dots outside
of the hypersphereare considered outliers
and are typicallyevents of strong interest
as the data stream flows infrom the IoT device.Here's an example
SVDD output chartthey can quickly and effectively
demonstrate how a turbofan degrades over its lifespan.The red reference
lines have not beendrawn based on a
hardcoded value,but rather determined by the
model creation of the SVDDprocedure.The SVDD threshold
would not onlybe different for each process
within an organization,but it could also be different
for every machine and evenwithin different spots and
part locations on the machine.This is certainly a situation
where a one size fitsall model would
not be appropriate,and SAS will allow you
to manage and scalethousands of different
models for each IoT device.So building upon our
initial Shakebox,let's go ahead and
build a Shakebox 2.0that utilizes the SVDD model.With the release of
the Raspberry Pi 4,we are now ready to explore
the features of installingthe SAS Event Stream Processing
application on the Shakeboxitself.We can also deploy a
custom SVDD vibration modelthat we described in
the previous slide.This is a common use case
for IoT edge processing.It eliminates the need
to pass the entire streamof raw data back to the
SAS server for analysis.Instead, it can run the model
real time on the Shakebox.This will greatly
reduce the amountof data transfer and increase
performance, especiallyif you need to scale
hundreds or even thousandsof devices in the field.You can find the Raspberry Pi
4 boards for around $45 to $50,but we utilize the same
Sense HAT board, as well asthe various charging
and battery options.We also purchased a more
industrial aluminum case.So here, you can see a
screenshot of the SAS ESPStudio.This is a
drag-and-drop interfacethat allows us to
hook and connectall of the various input streams
from sensors and other devices.And we also utilize this for
communication with the RabbitMQand processing all of the data.This sample workflow was
utilized for Shakebox version1, but you can also
see some other iconson the workflow where we
had actually implementedand deployed some of the
models, specifically usingfor the vibration of the SVDD.And we deployed that in
Shakebox version 2.0.The details of the
ESP Studio are not--there's not enough time
for today to go into that.That will be a topic of
a future presentation.But we do have some
details in the white paperthat we had submitted
there at the Global Forum.So feel free to reach
out, look for there,or reach out and
contact us if youneed some more details on that.One of the things that I
wanted to highlight here--I'm going to flip back
over to the dashboard.This is going to look very
similar to the dashboardthat you saw previously, but
there is one subtle difference.If you notice, in
the address bar,we've actually got an
address of 10.10.41.And what that is, is
that we're actuallyrunning the web services or
the web server on the Shakeboxdevice itself.Rather than reaching
out to Amazon cloudfor the web services,
we're running it hereon the Shakebox itself.I can flip over to
the camera here.And again, the Shakebox
is very similarto the previous
Shakebox that we had.It's got the more modern case.I actually hooked it up into
a physical cable this timeinstead of a battery,
but didn't need to.We could still use
the battery on that.But you can see the real-time
display on the dashboard,as well.Very similar on the results.But the difference
on here now isthat all of the processing
of the Raspberry Piis happening on
the device itself.And so as we get the data,
it's processing it internallythrough the ESP that
was deployed on the box.It's then sending all the data
and the information to the webserver, the serverless web,
on the Shakebox device itself,so it's all nice and clean.We could still, if
we wanted to, whenthere are significant events
such as thresholds thatare exceeded, we
could shoot those upto either the server
or the home baseor to some other alert
messaging, text message, so on.But what this means is
we could significantlyreduce the amount
of data throughputthat needs to be
sent to the cloud.Because the device itself
could be doing all of the work.It doesn't have to be on
the Raspberry Pi device.In fact, as the more devices
that you get deployed out,it's often a strategy to
deploy ESP on the edgeat the gateway side of
things, so the router side.So it can handle it, and all of
the computer processing powerthat those devices may have,
we can steer all of the datathrough there, still deploy all
the ESP and the various models.So you can see, from
here, we can actuallydo pretty significant
scalabilitywith being able to deploy
ESP either on the device,on the gateway, or
on the server itself.In many cases,
customers may wantto do a combination
of all of the above.The last thing that I
wanted to point out herewas that the models, and now
the thresholds for the minimumsand the maximums for each
of the various groups,are actually now determined
by the model itself.So what we had done is we had
set the Shakebox on the table,and we simulated what normal
process and normal run was.And then we
simulated a vibrationfor the particular location
that the box was in.And then we could
simulate what'sout of the normal and
then really kind of definewhat those thresholds would be.Again, where this is important
is because if we wereto set this device on, say, a
tractor trailer versus maybesetting it up on a
compressor, the modelswould be very, very different.Because they would
determine whatis normal for each of the
various different scenarios.And so that is
why it's importantthat we can use a modeling
technique versus setting uphardcoded values
for the thresholds.So that's the big difference
here with Shakebox 2.0.I hope this
demonstration showed youthe capabilities and
power of utilizingSAS Analytics for IoT.While we demonstrated a simple
use case with the Raspberry Pi,you can start building
your solutionswith just about any sensor or
device available in the market.I want to thank you again
for watching this video.Please feel free to
contact me or my other teammembers for more information.We'd be happy to discuss how
you can unlock your businessvalue of IoT analytics.[MUSIC PLAYING]"
6,"I'd like to welcome everybody to
my presentation, Slinging Hash,the hashing functions
available in SAS.And I'm Rick Livingston, and
I'm happy to be here todayto tell you about
these functions.I'm going to first give
you a quick introductionto the concept of hashing before
I tell you about the functions.Hashing techniques take a
message, a stream of data,texturing, whatever it may be.Any kind of data of any
kind of length, and thenperform computations and create
what is known as a digest,reducing it down
to a small size.Various hashing
techniques have includedthe legacy cyclical
redundancy check CRC, whichcreates a four-byte digest.Been around for a long time
but not terribly secure.MD5 came along a
little bit later.It creates an eight-byte digest.Then SHA256 seems to be
somewhat the standard today.And it's a 32-byte digest.There are various
variants of SHA,like SHA1, SHA384 and SHA512,
but 256 is the one mostpeople tend to use these days.There's another
component HMAC, whichtakes an additional secret key
as part of the computation,and it's a public
domain algorithm.But I want to point out that
hashing is not encryption.Don't think of it in
any way as encryption.Because typically
with encryption,you are expecting
to create a streamthat you can turn around and
then decrypt in some fashionand get back to the original
stream, the original message.But hashing does
not work that way.It is not reversible.It's intended just to create a
hash that you may then comparewith another hash later on.So the original
hashing functionsthat have been available in
SAS for a number of yearsare these.There's MD5 and SHA256
that I spoke about,and a pairing, a group of
functions, the CRC functions.One would just take a
message and give back a CRC.The other did what is
called a piecemeal process,and I'm going to explain more
about that with the new typeof hashing functions.So these are the ones that
existed before M and they stilldo exist.So here are the new functions
that are available in SAS.And this is as of 9.4m6,
so you'll want to checkyour &sys.vlog or look at the
top of your log to see whatversion of SAS you have in
case you're not certain.But that's the version
you have to be at in orderto use these functions.So we see HASHING
and HASHING_HMAC.And we see the piecemeal set
of INIT and PART and TERM.And then we see functions
that can use files.And I'm going to explain
about each one of thosein these three sets.So these are all new,
not available until 94m6.But certainly available then.Here's a very simple
example of use.This is where we're trying
several of the methods,and we just have a loop.And by the way, you can use
character strings in a do,as you see here.And then it will try the
various methods given,and it will use
the same message.And we will see the
resulting digest.The digest comes
out as we see here.There are four different
hashes, all fourare completely different.Because there are different
numbers of characters,different types of
algorithms being used.But all of those
hashes are computedby the hash in the
same hashing function,instead of separate functions.Another thing to point
out is it's all in Hex.All of these digests are
using Hex characters.This is not-- notice I
did not say dollar Hex.I just put out the
result of the digests.And those all come out in Hex.And that's a design
considerationthat I came up with when
implementing these functionsso that you don't run into
any binary data issues.Suppose you want to hash
something that's quite large.And that may be very typical.You may have an
arbitrarily large stream.In this particular case, let's
say we're dealing with a streamof one million X's.One million lowercase X's.And something we can
do is use a VARCHAR.Many of you may be
longtime SAS users,but you don't know
what a VARCHAR is.A VARCHAR is a varying
length character string.And that concept was introduced
in 94n5 of the SAS system.And it hasn't been
well publicized,partly because you cannot
currently save VARCHARSinto a SAS data set.Because you can't
persist them, theymay be of more limited use.But in this particular case,
a VARCHAR can be very useful.So I wanted to show you an
example using VARCHAR here.So we have the x1m variable
set equal to a single characterof x.And then we can use the
repeat function on it.And the repeat function
does know about VARCHARS.So in this case,
the repeat functionwill give us back a
stream of one millionlowercase X's into
the variable x1m.We can then run the
hashing function, whichalso can deal with VARCHARS.We can tell it we want to
use the SHAR256 method,and we give it the stream
of one million X's.And we get back our result.We already know the digest
of what it's supposed to be.So we have set our
digest variableequal to that and to see
whether or not they match.I also have taken
this opportunityto use the IFC function,
which is very handy.Which you can give
it a conditionas the first argument.And give it a character string
if the condition is true.And another character
string if it is not true.So when we run this, we will
see that status is matched.So indeed it did work.So you could build your message
completely with a VARCHAR,but it's got to be all within a
single data step, in which youdo so because you can't save
the VARCHAR to a SAS data set.But this is a very
handy method to useif it's available to you.When I say if it's available
to you, it certainly would be.Because these functions are
available in 94m6 and VARCHARScame along a 94m5 but you may
be in a context where you cannotuse VARCHARS for
whatever reason.If you need to work with the
message in a piecemeal fashion,because it's too long,
whatever the reason may be,we can use this
pairing of functions.a HASHING_INIT is called.You give it the method, and
it gives you back this handle.That is simply a numeric
value that will notbe meaningful to
you, but it's verymeaningful to these functions.Then we are looping through
the 1 million charactersby iteration of 10,000.So we go 10,000 characters
at a time and pass in repeatto generate the 10,000 X's.And we run hashing
part with that handle,and it just contributes
to the computationof the final digest.And you can do this as much
as you need, as many timesas you need.And when you've gotten
the complete message,you then call hashing_term,
again with that handle.And it will give you
back the final digest.So the digest is the same one
we used in previous examples,and we come up with
status matched.So if you needed to
work with it this way,those functions are
perfectly available to you.So if VARCHAR isn't
going to work for you,or you don't wish to use
it, whatever the reason,these piecemeal functions
are available to you.Now what if instead
the data is in a file.And indeed, very
often people areusing things like md5,
SHA256 on a set of filesto confirm that
the files containexactly what
they're supposed to,especially if you're
doing transfer.Or you're doing an
install, you wantto make sure that the data
was preserved correctly.You could then use a file.So say we have this file
of 1 million X's, and it'sin x1m.txt is the
name of our file,we can run the
hashing file function.Again we should get back
the same digest that we gotand the other examples.And indeed we do.So hashing file
is also available,and you can pass the
name of the file.There's a couple
of caveats that Iwill cover later on in this
presentation about dealingwith files.Now I mentioned HMAC.HMAC is a specialized
hashing technique.It-- well, it uses all the
different hashing methodsbut it also introduces
what we call a secret key.And there is a public
algorithm for HMAC.You may find you may be
asked to implement something,and it may use HMAC.And just wanted you to know
that HMAC is completelyavailable with the
hashing functions as well,with the suffix of HMAC.So here we call the
hashing HMAC functionand give it our secret key and
our text, which is the message.And we get a digest back.And then we also can do
it in a piecemeal fashion.Here in this particular case,
I do nothing but processa single character at a time.And we want to see if those
results come out the same.And indeed, we do get the same
digest when we run this way.Whether it be just using hashing
HMAC or using the piecemealfunction.So everything I've showed
you about piecemealworks for HMAC just as well.Now what if we want to
use a file with HMAC?We can absolutely do that.And we can-- instead we write
our data out to a file, using--we wrote my message
out to a file.And we used a file ref.And then we use
hashing HMAC file.Now there are a
couple of differencesthan what we have seen before.And this is just to show
you another approach.In the previous slides,
we used a path name,the actual name of the file.But here we are using a
file ref as our argument.Now a file ref is
permitted, but youhave to add that fourth
argument with a value of 4.It is flag that says this
thing is not a path name,this is a file ref.Because you could have a
file called simply myfile.And there would
be no way to tellthe difference between the two,
because they're both myfile.So the fourth
argument being fourindicates this is a file ref.The other thing that I want
to point out is i used recfm=flrecl=1 when I wrote this out.This is so that I only
wrote the exact characters,my blank message,
and nothing else.If I had omitted that, we'd
have had a carriage return linefeed put out as well.And then it wouldn't have
been just my message,it would have been my message
character return line feed,which would be a
different binary stream,and it would not give
us the same result.And I'm going to talk about
that a little bit morein a few slides.So the legacy functions that
I mentioned at the beginningof the presentation--there's MD5 and there's SHA256.Here's an example
of using those.They still exist though.As is typical with SAS
things, they don't--things don't tend
to ever go away.So because you've got
existing SAS code that youwant to continue running.So here we use the MD5
function, but we alsouse the hashing function
with the MD5 first argument.Likewise, we do SHA256,
and we do it with hashing.The distinction is that MD5
and Sha256 as the older legacyfunctions, return their
digests as binary data.The hashing function returns
the digest as hex characters,as a character string.So you could see that we have
to show MD5 bin and SHA binas using dollar hex, instead
of just displaying them.Because they would otherwise
come out as garbage on the log.And we certainly don't
want to see that.This was my decision to have
the hashing functions returneverything as character strings.Because you can run into
problems with binary data,such as put all, or an
error message comes out.And a put all occurs.Or if you are trying to transfer
from one platform to anotherwhere there's a
different encoding,or if you're trying
to use VARCHARs,they can have real
problems with binary.So I chose to make
sure everythingwas in the form
of character dataso there would be no problem.So we could see--although this slide is a little
bit small here, how it gotshrunk.We can see that the
resulting digests arethe same for MD5 and SHA256.So I want to give you an
example of encoding problemswith hashing that you
need to be aware of.And so, say we have gone to
the Wikipedia page for Beyonce.And we want to cut
and paste her name.And we want to create
this little SAS program.And we are going to
be hashing Beyonce,and Jay-Z is going to be
very upset about that.So we're going to be
hashing the character stringBeyonce using MD5.And we're going to come up
with the resulting hash.This should be quite
straightforward.We shouldn't see any issue,
and it should work the sameno matter where we are.However, if we do that cutting
and pasting while we'rerunning university
edition with SAS Studio,we're going to see
a different hashthan if we were running a
display manager on Windowsand we cut and
pasted and ran it.Now the reason for this is
because of what we see in red.The SAS Studio client
is running NUTF8.And the display manager is, at
least when I was testing it,was running with Windows
Latin one encoding.And what happens is that accent
over the e, the acute accent e,is represented differently in
UTF than it is in Latin one.In UTF8, it is Charlie 3 able
9, it's a two-byte value.And in Windows Latin
one, it is easy 9.It is a one-byte value.They are different.And if you pass a
different messageto the hashing technique,
it is going to give youa different digest then.This is what happened
with us here.If you wanted to
avoid this problem,if you think there's any
possibility of encoding issues,then what you can do is use
KCVT, and what that doesis, it will do a transcoding to
a different encoding for you.So if you-- it will take
your text, in this case,and trans code it to UTF8 so
that we would be guaranteedof having the same
message regardless of whatencoding we started with.If we're running
a UTF8 encoding,KCVT will have a no opt.Well, not exactly a no opt.
it will just simply copy.But if we're running
in Windows Latin one,it will go to transcode
that to something different.So if there's any possibility
that you will have encodingissues, use KCVT to avoid it.Otherwise you are going to
have issues with the hashesnot being consistent.Now I mentioned
before about textfiles, and said be careful when
you're working with text files.And here's an example of
one that could be a problem.If we started out
on Windows, and weran this little piece of code,
where we wrote two records out,record one and record two, and
we wrote them as text files.Now we have not used that
recfm=f lrecl=1 that I showedbefore.So they're written
out as text files.And then we use the--we use, and there
is a typo here.This should be
hashing file, I justrecognized as I'm
presenting this.This should be hashing file.And we pass a file
ref of my test.And what the result is
that we should get backis this particular digest
for those two records.However, if we transferred this
file using FTP, and we ran it,then ran it--FTPed it to UNIX
and ran it there,we would get a different
digest than we got on Windows.And why is that?We would have expected
it to be the same.No, it isn't.If we transfer it instead
in binary mode, yes.We would get the same
result. So why is that?The problem is that with
text files on Windows,Windows is going to write
out a carriage return linefeed, which is hex
characters of 0 dog 0 Abel.So on Windows we would have
the characters record 1followed by 0 dog 0 able, record
2 followed by 0 dog 0 Abel.We FTP the file to
UNIX, we will onlyhave long feed, the 0 Abel.So the very same file as
far as we're looking at.It looks the same, but it isn't.The 0 dog, the
carriage return, isomitted when you
have transferred itin text mode on UNIX.Or if you created the
file initially on UNIX,it would not have
that 0 dog there.And just like we've seen
in previous examples,the binary stream is different.So we're going to have
a different digest whenwe hash it.So you need to be aware
of that situation.So now that I've given
you some informationabout these functions,
how can you use themas long as you're running 94m6?I wanted to make sure you were
aware of a couple of things.When a developer
such as I was at SASuntil I retired-- when
a developer is writinga set of functions,
the developerwants to realize
that they will runin multiple types
of environments.In SAS there's what's called
the multi-vendor architectureenvironment, the MVA versus a
threaded kernel environment.You as a SAS user may
not be aware of this.It just happens for you,
somewhat behind the scenes.So if you're running the data
step, or you're running IMLor you're running functions
created with FCMP,you would be running
with MVA functions.And likewise, with the macro
facility, you would be.However, if you
are running in DS2or if you're running a DATA
step that will run in CAS,you are going to be
running with the TKversions of the functions.As a developer, it
was my responsibilityto ensure that these functions
were available for both MVAand TK and both worked at
the same and shared codewhen possible.So you can use them
without being concerned.The only limitation is
that with HASHING_FILE,if you are running that
in a CAS DATA step or DS2,you cannot use a file ref
because the threaded kernelenvironment has no
concept of the file refs,like there is an MVA.This is a minor limitation.I would not recommend the
use of the hashing filefunction in CAS_DATA
step anyway,due to the architectural
nature of running in CAVs.I also want to point out a
couple of very handy websites.If you are doing any kind
of work with hashing,you can go to hashemall.com.You can type in whatever you
need to hash or upload itas a file.And you can get the hashes
for a lot of the methods.All the ones that are--
the hashing functionsupport plus many others.Very handy site.And if you're wanting
to work with HMAC,I can recommend
freeformatter.comwhere you can enter your
secret key and the textthat you wish to hash with HMAC.So these are two
very handy sitesthat I used as well
when I was doingsome of the developmental work.So in conclusion, be aware that
in 94m6, the hashing functionsare available and that
they use character data,hex character data for
their digest being returned.You can work with files.You can work with VARCHARS.And you can process
piecemeal as well.As well as HMAC.So all of these functionalities
are available to youdepending on what kind of
specifications you may have.It's been my pleasure
to speak to you today.If you want to contact me,
here is my email address.And I'll be glad to
correspond with youabout this subject or others.Thank you very much
for your attention."
7,"Welcome, everyone, to adopting
SAS Viya into your Business,Data Success Tales from a
Central Bank's Perspective.I'm Ben Zenick, founder
and CTO at ZENCOS,where we've been partnering
with SAS to deliveractionable analytics to our
customers for almost 20 years.My background is in
data and analytics,where I lead teams of highly
experienced consultantsacross many
industries, includingfinancial services,
which my co-author LeahSahely is part of.Leah is the director of
the monetary and financialstatistics unit of ECCB's
statistics department,and project manager
and technical leadfor their statistical
enterprise solution.She holds a Bcom
in finance and BAin mathematics from St.
Mary's University in Canada,and an MSc in statistics
from the Universityof Western Ontario, Canada.She's a professional
statisticianand an Accreditation Committee
member of the AmericanStatistical Association.Without further ado, I'd
like to hand it over to Leah.LEAH SAHELY: Thank you, Ben.So the purpose of
the presentationis to give you some
background on ECCB, what we'retrying to achieve
from this solution,and the solution
that we implemented.So who is ECCB?The Eastern Caribbean
Central Bank, ECCB,is the monetary authority for
currency with a quasi currencyboard arrangement.There are eight member
countries, six of whichare independent, and
two are territoriesof the United Kingdom.So the Eastern
Caribbean Central Bankhas an agreement at
1983 that lays outthe purpose of the
ECCB, and it includesregulating the availability
of the money and credit,promoting and maintaining
monetary stabilityand sound financial structure.It provides credit and
exchange conditionsconducive to the balanced
growth and developmentof the economies of
the member countries,and it promotes
economic development.The ECCB has as its main
target the maintenanceof a competitive
fixed exchange rate.It has, in fact, maintained
such a fixed rateto the US dollar of 2.7 to
1 US over the last 43 years.The ECCB supervises
and regulatesthe commercial banks
in its member countriesand other deposit taking
institutions that arelicensed under the Banking Act.Now, over the last
10 years, there'sbeen exponential
growth in data demandsfor both financial
and economic data.And we've seen that
the data growth hasled to capacity issues, there's
been an increase in the datarequests from internal
and external clients,and there's been inefficient
processes for extractingpublicly accessible data.These insufficient
advanced technologythat we have been
antiquated, and there'sbeen a lot of manual processes.We also saw
insufficient advancesin the analytics and
reporting framework.So in order to
meet those demands,we have been engaged to a
long-standing partnershipwith SAS and ZENCOS, and started
with the implementation of SAS9.3, and in 2019, built
our first Linux deployment,where we went live with a
statistical enterprise systemversion 2.So in 2013, the
ECCB began exploringoptions for this new
statistical enterprise solutionto house data submitted
by various providers,such as the commercial
banks, statistical offices,and other government agencies.SAS was selected
as ECCB's choice,and their digital
transformation beganby automating manual
processes and expandingour statistical
reporting capability.So in order to meet the
demands of our customers,the solution was
designed, developed,and implemented as a statistical
system covering collection,processing, storage, and
dissemination of databy leveraging SAS's data
integration and businessintelligence platforms.But a solution is only as
good as the data collected,so before we can focus on
analytics and reporting,we first have to
tackle the complex datarequirements, which Ben
will discuss in more detail.BEN ZENICK: Thank you, Leah.So since paper is part of
the data management track,let's jump into some of the
solution-specific requirements,as it relates to managing all
of the disparate data providedby the entities that Leah
just reference to the ECCB.So the solution is built
on the intake of datafrom over 200 providers that are
made up of the various entitiesLeah previously mentioned,
as well as entitiessuch as trading
partners and bankingand non-banking
financial institutions.The providers are
responsible for submittingapproximately 75 different types
of forms into the solution.And so as these
forms are collected,they include data such as
balance sheet data, assetand liabilities, daily
liquidity statements, debt,loan performance, anti-money
laundering reports,among other types
of forms submittedby these external entities.So based on the volume and
frequency of the disparate dataand the disparate
submitters of the data,the system was built to
ensure quality data cominginto the system.This is done through an
interactive real-time datacollection system that is
intended to eliminate or reducethe amount of manual
intervention requiredto ensure quality
data is consumedand valid results
can be produced.In order to support the
collection, transformation,and the dissemination
of this data,a reusable scalable adaptive
management frameworkwas defined and
designed for the ECCB.The solution is built
on the various SASand open-source
technologies, whichwe'll talk about
and show as we gothrough the remainder
of this presentation.So as you look at
this diagram, itmay appear to be a little bit
nontraditional from a datamanagement or a data
management architecture.You see a lot of
SAS, and open-source,and different technologies
with the end usersinteracting with the system from
the user interface perspective.Well, one of the beauties of
the SAS platform is just that.With Viya, it allows you
to transform your datacapabilities through its
openness and by making it doand be what you want it to be.So in our case, we've
used it as a frameworkto enhance and modernize
our digital capabilitiesof collecting data from
our data providers.We wanted it to be a
system that can securelyintake different types of
data from these providersthrough a custom user
interface writtenin some of the more modern
user interface technologies,like Node.js, Angular, and
leveraging Python web services.These web services, as you
can see from this screen,from the user interface are
communicating to a back-end SASserver that's running DataFlux
REST for web services,providing data validation
and data quality checksas data is being
provided into the systemand uploaded into the system
by our external entities.Only system validated
data is allowed in,and once that
validation occurs, thereare more traditional data
management processes thatare running behind the scenes
using SAS's Data ManagementServer, as well as SAS's
DS2 and FedSQL processes--which we'll discuss
a little bit laterin the presentation why
that technology was selectedand the value that it provides.The end result of these
processes and architectureis the confidence in knowing
that the data available isof the highest quality for
reporting and analytics,and is consumable
through SAS Viyaand trusted by all
entities and providersthat are using this technology.As we'll discuss in the data
management processes in a bit,the end result is that,
through this process,we have to support
a very diverse usercommunity that Leah will now
tell us a little bit moreabout before we describe
in further detailshow the data now gets
to that user community.LEAH SAHELY: Thank you, Ben.Who use this statistical
enterprise system?So as a central bank,
we also use the data,because we would want to look
at the data that's coming inand make sure the
data's of high quality.So we have automated
data management processwhich helps that.We also have our
reporting using SAS Viyafor auditing and compliance.We can look at that.And then we, of course,
can do some estimation,and forecasting, and some
data analytics using SAS Viya.We also have the IMF.They're one of our main users.So we can hear more
about that from Ben,as we talk about this built-in
Python-based web service API,and he will go into that.And then we have the
external entities,such as the Ministry
of Finance, that wetry to provide data for--monthly reports in particular.And then we have a website.The ECCB has its own
website, where here youcan access some
predefined reports--the economic and
financial review.So these are the type
of data that we wouldwant to provide to the public.Now, our users can
create reports to dosurveillance and oversight.We can use the solution to
estimate and forecast data,perform validation and
analysis of certain dataprior to reconciling the data.We would manage the
data used for collectionand transformation
process, and wewould report to the IMF,
the government agencies,and the other stakeholders.So the central bank basically
does this type of work.Now, for our
external entities, wewould be responsible for
submitting the data--they would be sending
the data to the ECCB.So not only do they send data
to us through this solution--they can actually access
this data as well.So they have access
to their own data,so we're giving it back to them.And this is not exclusive of--they would have access to
compliance and auditingreports.They would have a
time-series report,so they would be able to have
data for a longer period, asopposed to the one period
that they would have uploaded.And they also have
access to a dashboardthat we would have created
in a report for themto be able to monitor
key indicators.For the public, we
would provide reportsthrough the ECCB website, as
well as in various frequencies,and as well as by the countries.So remember, we have
eight member countries,and we would have
reports availablefor each of those countries
for various frequencies.So we would have created some
dynamic data-driven reportsfor them.And so I'll let Ben explain
how the IMF, a critical partnerof ours--if you can elaborate
on how we provide themwith surveillance data.BEN ZENICK: So as the ECCB has
been modernizing their approachto data collection
and dissemination,the IMF has as well.There's processes
that the ECCB usedto have to go through to
provide the surveillancedata to the IMF, which
included creating Excelfiles or various
formatted text files,and uploading them through
a data upload web interfacethat the IMF provided,
making sure that itmet certain standards and
formats that the IMF needed.So as we've gone through
this digital transformationwith the ECCB, we've also
modified and modernizedthe way that the IMF now
interacts with the central bankin order to get
the data that theyrequire for their
oversight and surveillance.This includes using their
integrated collection system,which is built upon
the statistical dataand meditate exchange
standards that are out there.And that's an
international standard.The web services
are designed wherethe IMF can request various
envoys, whether it'sprogrammatically or
through a browser,to retrieve certain
data at all frequenciesby various countries
or all countriesby a specific frequency.And so the ECCB has set up
an Apache service in the DMZthat the IMF can make
secure requests to receiveand retrieve data
based on their formatthat they require
for providing resultsto them for their
surveillance and oversight.So how does all this
data get into the system,and how do we ensure that
the data is of high quality,as we discussed, and
has been validatedprior to entering the system?So we developed as a front
end a data collection processthat these over 200
entities can submit upto the 75 different types of
both economic and financialdata through the
secure interfacethat is a custom-deployed
Viya application.So it's the application
we referenced earlier,written in some more modernized
web development technologies.And that interface
uses web servicesto communicate
between a back-end SASprocess and a front-end
user interfaceto ensure that the data
being submitted and selectedfor submission by a provider
from the user interfaceis consistent with not
only the data in the form,but data across forms.So for instance, we're
checking to make surethat the dates with in the form
match what the provider thinksthey're submitting.We make sure that there's
prerequisites-- if they exist--that they've been loaded prior
to a particular form beingloaded by a provider.We determine whether forms have
already been submitted or not.We also make sure that the
form that's being submittedis the version as
defined by ECCB,meaning columns and indicators
available to a particular formexist in that form.So we do comparison
matches basedon different versions, which are
determined based on date rangesthat that version
is applicable to--check things like the
number of columns are--exist that are supposed to, the
correct indicators or metricsexist in the forms
that are supposedto, whether a particular
indicator can have missing dataor not, whether a particular
indicator allows negative dataor not.And then we do things like
ensure that, within a form,different indicators
are consistent,or across forms, those
indicators are consistent.I'll talk a little bit more
about that in a minute.As well as variances--so from one period
to another, we'rechecking to make sure that
the variance between a valuefor one indicator doesn't exceed
a threshold as defined by ECCB.And as a user goes
through this process,I think one of the important
things we'll talk aboutis just that interactive
feedback and whether or not,as I'm editing the
user interface, what'shappening on the back end.So I make a submission.The back-end web services
are called in real timeso that the end user's getting
and receiving real timefeedback either related to
a specific form or a metricwithin that form to ensure
that the upload and the datavalidation is passing
what's expected by ECCB.So the user interface returns
a pass/fail based on the stepthat the user's going through
at a particular time of upload.For instance, if a user
submits a form that'sinconsistent within the
form or across another form,the UI will actually
stop the processuntil that issue is resolved.So in this particular
scenario, a data quality checkfailed because the comparison,
as you can see between,the left-hand side and
the right-hand side valueswere not equal to each other.And so that process stopped.And it will keep track
of an audit issueslike this so we know, what's
the progress of data that'sbeing collected, how
many times have datafailed as they've been
trying to enter the system,so we can continually
improve the systemand try to identify where a
recurrence of particular issueshappened, so we can
coach our providersto submit data one
time, and not haveto have them go through
iterations of datanot making it into the system.Another example that would
not stop the process,but would it make a user
actually interface and providedata back into the system,
is a variance check.So this does require
intervention,but does not fail or stop
and halt the upload process.So as a user's entering the data
and they're uploading the data,the system's going to
check whether or notdata from one period for
a particular indicatoris with in a specified range, as
you can see on the screen here,and whether that criteria
is within that thresholdfor that particular indicator.And if it doesn't
meet the threshold,that data must be marked
as a variance exception,and an explanation
needs to be providedprior to the data being
uploaded into the system.One of the things that this
data can then be leveragedfor is other analytics,
like text analyticsand reporting on whether--what kind of trends are
occurring during submissions.So it's not until
all of these checkshave actually passed
that data is actuallyentered into the system
for analysis and thensubsequent batch processing.A key component to the
system is the abilityand this interaction and
communication with an end userto let them know that the data
has been successfully uploaded.It provides confidence in
the data in the system,because they know that
they've gone through allof the checks that are
required to ensure quality dataand valid data coming
into the system.So once all this data has
been passed and enteredin the system, there's
additional processesthat occur to prepare
it for disseminationto the stakeholders.All that's accomplished
through data transformation.So data transformation
incorporates this reusable setof methods that are used during
that data collection process.But those methods
are now expandedto allow us to not only do those
real time checks we just saw,but to do some data aggregation
and data consolidation.So the system is taking
these 70,000 indicatorsthat are uploaded through these
various economic and financialforms, consolidating them across
potentially three formulasper indicator across up to
five frequencies over a rangeof three years, potentially,
depending on the transformationprocess that's being
loaded per entity.So there's a lot of combinations
of data that need to be done,and we've created as part
of our data transformationand ETL process a system
that can dynamicallygenerate these queries and allow
us to take advantage and reusecomponents both on the fly as
well as through standard ETLprocesses when batch
jobs are running.And so these batch
drives, as a result,are going to take and use this
standardized code for not onlyallowing us to load data
into a data warehouse,but also allow system
administrators and statisticsdepartment users
to validate dataas they go through
building out formulas.So they can take these--the same programs and they can
use them to alter, or create,or modify formulas and
test them to ensurethat they're
introducing results thatare expected into the system.This is going from anything
from what is a quote,unquote, ""simple calculation""--which isn't simple,
obviously, but you'retaking various sets
of data, you'reensuring that the data has--is being included--
but all the wayfrom the appropriate
entities thathave submitted the data to
complex calculations thatactually have
different aggregationmethods within the calculation.Indicators may look
at different periods,but it's the same piece of
reusable code that does this.So the benefits of this data
management architecture--or some of the benefits-- are
program one's used many times,as well as having
this dynamicallygenerated formula builder that
can be executed at runtime,allowing for changes over
time and the inclusionof only applicable data and
available data to the system.So in summary, what this system
has allowed the ECCB to do ismodernize their processes
of collecting dataand the disseminating analysis
to not only their internalstakeholders, but also
their member countries,by leveraging SAS and
open-source technologiesto create a data-driven
manageable system that empowersthe ECCB to be able to make
changes and support the systemas they--as it grows over time.This is allowing them to be
more effective in the supportof their customers,
as well as the dataproviders within their region.So with that said, I'd like to
thank you all for attending,or watching this webinar.And if you have any
questions, pleasefeel free to follow up
with myself at this emailaddress, or Leah as well.And we look forward to
hearing from you all.Thank you very much, and hope
everybody has a good day.LEAH SAHELY: Thank you."
8,"GREG KRAUS: Hi, my
name is Greg Kraus.I'm a developer on the
accessibility team at SAS?And today, I want to talk to
you about creating reportsthat comply with section
508 using SAS 9.4,and how planning is the
most important step.A lot of times when people
give me reports and ask me,how do I make this accessible?A lot of times, there have
been a lot of decisionsalready made that make
making the report accessiblevery difficult. And so I want
to talk to you about someof those things
you need to thinkabout beforehand as you're
starting to think about howto create your report in order
to make the process of makingit an accessible
report that meetssection 508 guidelines easier.So I want to talk to you
about four different aspectsto plan for when thinking about
accessibility in your reports.I want to talk about the
file format that you choose.I want to talk about
how you use tablesand how you structure them.I want to talk about
how you describe graphs.And I want to also
talk about using colorto convey information.So first, let's talk about
accessibility and file formats.So some file formats have
better accessibility featuresthan others.One thing you have
to look at is.How well does a
particular file formatwork with assistive technologies
like screen readers?Does a screen reader
have the abilityto read a Word document, or a
PDF document, or a web page?How well do each of these
different file formatswork with assistive
technologies?And then looking at some
specifics of reports,how well do these
different file formatshandle some of the trickier
things like table headers?And in each of
these file formats,can graphs be made
accessible in these formats?And finally, can you actually
define good document structurein these different formats,
things like headingsfor your page, or if you have
a bulleted list, that you cansay, this is a bulleted list.So when we're looking at
accessibility in file formats,I want to group this into
three different groups.The first group I'm
going to call thingsthat have strong accessibility.And in this group,
I've put HTML5 and PDF.Both of these file formats
have a lot of supportin there to work really well
with assistive technologieslike screen readers.They let you do things like
define good document structure.You can define
tables with headers,with multiple rows of headers,
and columns of headers,and all kinds of things.You can describe your
graphs and your images.So if you have a choice of
what file format to use,I would always look to an
HTML5 or PDF file format first.And I'll show you a bit later
why HTML5 has a few moreadvantages over PDF.There's a second group of file
formats I want to talk about.And some of these file formats
are a bit more specialized.Some of them, like Word, are
a more generalized format.But some of them like Excel,
and CSV files, and PowerPointare kind of specialized.Now, file formats in this
group can be made accessible.But they're often limited
in what they can do.So we're going to
look a little bitat how some of
these file formatsare limiting in what you can do.If you only use them
within those limitations,they can be very accessible.But if you try to
push them beyond whatthey're really capable of doing,
you can run into some problems.Then there's another
bucket of file formatsthey just have weak to
no accessibility, thingslike the RTF files or just
the listing output from SAS.These just lack the
very basics of whatit requires to be accessible.For a person using
a screen reader,oftentimes, there's
very little informationthey can actually get out
of these types of formatsbecause there's just no good
structure to the documents.So let's look a
little bit at HTML5.So HTML5-- some of the features
of this format is it reallysupports all the
features a user needsin terms of document
structure, tables, graphics,and all kinds of stuff.So when looking at this
from the ODS Destination,one of the really nice
things about the HTML5 outputdestination is it can
integrate with the SAS GraphicsAccelerator, which is
a really powerful wayto convey graphs in ways
other than visual formats.And we're going to look a
little bit at the GraphicsAccelerator a little bit later.One of the other really
great things about HTML5is it's the most
flexible option wehave in terms of creating
accessible layouts,highly customized reports.You can use CSS.You can use JavaScript.You can do all kinds of stuff
and still make it accessible.So let's look a
little bit at PDF.So the PDF file format does
support all the accessibilityfeatures users need in terms of
document structure, headings,tables, and graphs, and
all that kind of stuff.But there are some problems that
start creeping in with PDFs.So if you end up making a
PDF if that isn't accessiblebecause you've tried to
make you report it in a waythat SAS was not able to
fully make accessible--and there's a lot
of ways you cando that because of the
flexibility of the platform.If you've done
that, you then haveto go in and remediate the
accessibility of the report.And doing that is an
extremely tedious process,to go through and
manually fix a documentfor accessibility errors.And you will find there are a
couple limits to what you candesign excessively with PDF.You can still do quite a bit.But you'll start to
find some limits.One of the great things about
the SAS ODS Destination,though, for PDF is
that you can createPDFs files which requires zero
remediation work afterwards.So let's look at some of
the more specialized formatslike the Word format.So Word supports most of the
accessibility features usersneed.It can do things like
good document structure,headings, tables,
graphs, and so forth.You can start to run into
some problems, though.If your tables start to
get relatively complex,sometimes the complexity
of the table--like in terms of what
headers go with what cells--aren't always communicated
to screen reader users.You'll often find some
problems, especiallyif you get multiple
rows of headersor multiple columns of headers.Sometimes it can
start to fall down.But for simple tables,
it does relatively well.So with SAS with
ODS Destination,you can create Words
documents whichrequire zero remediation work.However, you'll find that
some reports might actuallybe more accessible if you
went ahead and just made themas PDF documents.Let's look at one other format.Let's look at Microsoft Excel.So Excel documents can
be made very accessiblewhen your goal is to
simply publish raw data.So some of the problems with
trying to publish to Excelis that the layout possibilities
are pretty limited.If you need to have a bunch
of graphs, a bunch of prose,and a bunch of other
things like that,you can quickly
run into problemstrying to make that accessible.Excel works really well from
an accessibility perspectivewhen all you're doing
is providing raw data.So it's perfectly
accessible for that.Once you start
going beyond that,it starts to become
less accessible.So you can create
Excel documentswhich are accessible
given that you're onlypresenting tabular data.So again, looking at these
kind of three buckets of fileformats, the ones with
the best accessibilityare HTML5 and PDF.Ones with moderate
accessibility--again, these can be
accessible if you use themwithin their limitations,
things like Word, Excel, CSV,and PowerPoint.And then the weaker
accessibilityare things like
RFTs and listings.Let's look a little
bit at table structure.So tables of data can
be made accessiblein several of these
different file formats.But some table layouts can
cause accessibility problems.And what I've found is that
almost all table structureproblems can be
reduced to one of threeproblems, the first
one being blank lines.So here, I've got a
very simple table.And the examples I'm
going to show you hereare relatively simple.But you can imagine
these same instancesin much more complicated tables.So this first problem
of blank linesis a problem because when
someone like a screen readeruser comes to this
table, the screen readeris going to report to them how
many rows are in this table.And they're going to be
reading down through the tableand hearing data,
and all of a sudden,they're just going
to hear blank.And they're going to
want to know, well,did something just mess up?Am I not reading
something in that cell?They don't really know that
it's really just a spacercell at that point.They're expecting to hear
more data at that point.So the number of rows
they're expectingdoesn't quite match up
with what the screenreader is telling them.Another problem are putting
non-data cells within a table.So what do I mean by putting
non-data cells in a table?So if we look at this table,
and if we look at this cellhere for the $26,990, that cell
has headers which describe it.It is described by the Acura and
the TSX 4-door, and the MSRP.So I know how that
cell is labeled.Similarly, down here
in this other cell,I can see that it's
labeled by Buickand the Regal GS
4-door and invoice.So those types of
cells are data cells.And they have headers
associated with them,so I can figure out exactly what
this piece of data is about.But what about all
these other cellsI threw in here where
I was just saying,all prices are in US dollars?I was putting this in here
as just an extra pieceof information.But if I'm reading through this
document as a screen readeruser, I'm going to read down.And I'll be reading about
the various Acura cars.And all of a sudden, I'm going
to hear all prices are USD.That table cell has nothing to
do with any of the headers thatare already defined in here.It's really just an extra
piece of informationthat's thrown into the table.So the bottom line is if a table
cell does not belong and is notkind of owned by one
of the header cellsand described by a
header cell, it probablyshould not be in the table.Because it's not really
data at that point.The last group
problems that I findare headers not being defined.So in this case, I've
got these subtotal lines.And just looking at this,
you might think, well,why is this a problem?Because the subtotals
come at the bottom of eachbreak in the region here between
Africa, and Asia, and Canada.The problem is if I'm
reading through this table,I can be reading
you're in Nairobi.And here it's $16,000.And then I'll read down and
hear subtotal, $119,000.And I know I'm still
in the Africa region.So why is that a problem?Well, you can probably intuit
what that subtotal means there.However, let's say
I was reading this,I was explaining the
table, and I was startingto read from the bottom up.And so I can read
that I'm in Asia.And I read that
Seoul is $60,000.I push up arrow.And I read that Asia
and Bangkok is $19,000.And then if I push up
again, all of a sudden,I'm going to hear
subtotal, $119,000 whenthe last thing I heard was about
Asia, and I hear subtotal now.So this is a piece of
information that is data,but the headers are not
defined properly for this cell.And so I, as a
user, don't actuallyknow what that $119,000, or
that $62,000, or that $385,000actually refers to.Which subtotal does
that belong to?So how do we solve
these problems?So there's multiple
ways to solve these.I'm just going to show you
one example of how to solveeach of these problems.So if you're having a bunch
of blank lines in your table,it's probably because
you're wantingto try to create some separation
between parts of the data.Well, one way to
do that is actuallyto create separate tables.So that's one solution,
to give your data bit morespacing between them but
make it fully accessible.For the non-data cells,
this basic solutionis to move the non-data
cells out of the table.So in this case, I just
took that repeated reminderin there, all prices are
USD, and I just moved that upand made it part of
the table caption.And so it's no longer
part of my table.I'm going to read it as part of
the description of the table,but it's not going to be
intermixed with all the data.Over here were the headers
that were not defined.So what I did here is instead
of putting the subtotalin that subsidiary
column, I madea new label in the region column
called Africa Subtotal, or AsiaSubtotal, or Canada Subtotal.So now as I read
through that table,I will hear explicitly
that $119,000 belongsto the Africa subtotal
and the $62,000belongs to Asia subtotal.I don't have to guess which
subtotal I'm actually reading.So now let's talk some about
how you described graphsand what's possible in some
of these different fileformats and approaches
you might take.So looking at this
graph, there'squite a bit of
information in here.So I've got a bunch
of shoe sales reportsfrom lots of different regions
of the country broken downby eight different products
with a stacked bar chart.So there's a lot of
information in here.How would I go about describing
this graph to someonewho can't see the graph?Well, there's a couple
of ways that youcould go about doing this.So the kind of
tried-and-true methodis, well, you can
provide tables of data.So here, I've got a printout
of some of the tablesfor that data.And you can do this, and it will
technically meet compliance.So what I've done is
I took all those bars,and I simply presented
the raw data behind it.So now if I were a screen
reader user, I could go through,and I could actually
read through this tableand figure out what every single
piece of information in thereis.Now, there's some
downsides to doing this.While this might technically
meet accessibility compliance,it's not necessary the most
usable thing for users.So here, if I had a
really big table--this is really only
about 40% of the datathat's populating that table.So the data is much bigger
than what I'm showing.And so if you can see the graph,
we can glance at this graphand understand
pretty quickly whohad the most sales, what
products were sellingthe best in different regions.We can tell that just
pretty much at a glance.If I give someone
a bunch of tables,it's not quite as easy
to figure that out.So while this meets
compliance, it'snot the most usable experience.And I'll show you another option
in a minute that can make thisa far more usable experience.Another option to make
a graph accessibleis you could
describe it in prose.And so here, I've
got a sample thingI might put in the
body of my reportsaying the Middle
East, United States,and Western Europe were
our highest regional saleswhile Asia was our lowest.While all of that is
true about that graph,that graph is conveying so
much more information than whatI said in that sentence.And so I can do that.But am I really conveying
all the informationthat's in that graph?I would say that this
option probably would notmeet compliance in many cases.If all you put was
this one sentenceI put up in this example,
this wouldn't be compliantbecause there's
lots of information.I'm not conveying.You might able to do this
for some simpler graphsand meet compliance.But again, it still can be
very tedious to try to reallyunderstand the data behind it.So let me show you
a third option.And this option I'm
going to show you herewill only work in the
SAS HTML5 destination.So one thing we can do is
provide accessible alternativesfor graphs.So I created this
graph with SAS.And you'll notice down in
the bottom right-hand corner,there's a little button
called Accelerate.And when I click on that
button, what it's going to dois it's going to take
all that data that'screating that graph.And it's going to send it over
to a free Chrome extensionthat we provide called the
SAS Graphics Accelerator.And when we click
on that button,it will launch the SAS
Graphics Accelerator.And it will let someone explore
this graph through sound.And they can actually hear
the height of these bars.And they can actually dig
into all the different groupswithin there.So when I click it, it launches
the SAS Graphics Accelerator.And I can explore
this through sound.So let's listen to a little
demo here of just readinga couple of the bars
and then digging downinto a couple of the groups
for one of these bars.[ALERT NOISE]SPEAKER 2: Africa,
$2,342,588 total.Asia, $460,231 total.Canada, $4,255,712 total.Canada, $989,350, women's dress.Canada, $410,807,
women's casual.GREG KRAUS: So when you're
looking at describing graphs,you have a couple of options.You can do what I just
showed with the SAS GraphicsAccelerator.You can provide that
alternative presentation.And that will work in
the HTML5 destination.Or other solutions are you
could use tables of dataor potentially write
prose to describe it.And you can do that in HTML5.You can do that in PDF.And you can do that in Word
for relatively simple tables.Well, let's look
now at the issueof designing reports
that only use colorto convey information.So here I've got a
stacked bar chart.And I've got a table that
has some traffic lightinginformation in there with
red, green, and yellow.And so these colors
are telling me thingsboth in the stack bar chart and
in the traffic lighting table.But what if I can't
perceive color?How will I understand
the informationbehind the graph in the table?So for example, on
the left here, I'vegot that graph where
I'm only using colors.But if I can't see
color, I createda version of this where the
colors have been stripped out.It's all grayscale.And it's really hard,
next to impossible,to figure out what each
of those stacked barsactually refers to.So when you're
designing with color,you also need to
think about, howcan I design so I'm
not just using color?So here, I've shown an
example on the SG plotprocedure where I can
actually add patternsas well to my colors.So I get the same color
scheme, but I alsohave a pattern added to it
so that if I can't perceivethe color, you'll notice
on the graph on the rightwhere I, again, have
made it grayscale,the patterns are still there.And so if I can't
perceive the color,I can still actually
read this bar graphand understand what it's telling
me and what each of the groupsare.So here's that example, again,
of the traffic lighted table.And so I was using
red, green, and yellowto indicate different
sales levels.It's a high-performing store,
a medium-performing store,a low-performing, I guess
in this case, product.And so how do I convey that
if I can't distinguish color?And so what I've done
here is I've added an iconto the traffic lighting.So for my
high-performing stores,I added the triple dollar sign.For the
medium-performing stores,I added the double dollar sign.And for the low-performing,
I added a single dollar sign.So now if I can't perceive
the color between red, green,and yellow, I can see the
icons and understand them.Additionally, if I'm
a screen reader userand I can't see the
colors or the iconsat all, what I've also done--
you can't really see this here,but on those images
that I inserted,I inserted alt text, or
alternative text on top of it.And so when a screen
reader reads that 823,they're going to hear $823.And they're going to
hear low-performingbecause I said one dollar
sign equals low-performing.And they'll get
to the next line.They're going to hear $14,775.And they're going to
hear medium-performingbecause I made the double dollar
sign the medium-performing,the description from for that.So hopefully, this will
give you some thingsto think about as you're
creating your reports,things to think about
before you start.What file formats are
you going to choose?How are you going to lay
out your tables to make surethat they're accessible?How are you going to go
about describing your graphsso people can understand them?And how are you using
color in your reportsto make sure that everyone
can perceive the information?So we do have a resource
available onlinethat's called Creating
Accessible SAS 9.4 Output UsingODS and ODS Graphics.You can find it on
our support site.And it goes into
a lot more detailabout how to use the ODS
procedures to create accessibleoutput for HTML5 and for PDF.So you can get a lot more
information going there.Thank you for attending
my virtual session.And if you ever
have any questions,you can email me at my
contact information,Greg.Kraus@SAS.com.That's G-R-E-G dot
K-R-A-U-S at SAS.com.Thank you."
9,"SQL-- the language of
databases and so much more.This tutorial is for
anyone wishing to learnSQL in a step-by-step approach.Hi, I'm Charu, an instructor
with SAS institute.Here's a little
background on me.You can also find
me on blogs.sas.com.On the agenda today,
I'll cover five topics.We'll first learn the syntax
order in which you can submitqueries to SQL, and
then we'll summarizedata using the Boolean.Thirdly, we'll manage metadata
using DICTIONARY Tables,and then we'll learn
to join tables.Lastly, we'll internalize
the logical orderin which SQL processes queries.Let's begin with section 1.Here we're going to get
an overview of PROC SQL.We initiate the SQL procedure
with a very simple PROC SQLstatement, and we terminate
it with a QUIT statement.In between, we can have multiple
statements within the step.Two things SQL always
needs which are mandatoryis, number 1, what do you want?And number 2-- where
do you want it from?The ""what do you want?""is the SELECT clause.And on the SELECT
clause we'll belisting the columns and
the order in which wewish to display them.The ""where do you want it from?""is the FROM clause, on which
we specify the FROM statement,and we can query up to 256
tables on the FROM clause.Here is the entire
SQL syntax order.Now, sometimes we know
many languages, both spokenand computer, and it
may be hard to keeptrack of the order, the
grammar, the logic--so I have a mnemonic
for you here.And here is my mnemonic--So Few Workers Go Home On Time.Isn't that easy?You have the SELECT,
and then the FROM.The two statements,
SELECT and FROMare not enclosed in
triangular brackets,indicating they are mandatory.Anything else is optional.So let's go ahead and
begin our demonstrationto examine the syntax order.I've invoked SAS, and I'm
using the Windowing Environmentas my interface of choice.Now, I'd like to open
up a program thatwill assign libraries.But even before I do
that-- a little shortcutI'd like to share.If I head over to View and
open up my Favorite Folders,I should see my
folder structure.I'm going to navigate
to this C drive, Data--or rather, C drive and HOW, and
within that I have the folderI wish.And within that, further
down, I have all the foldersfor this demonstration.I'll open up the Exercises
folder and on the right side,I'll see a listing of all the
programs-- just going to resizethis here a little bit.So now in future, if I wish to
open up a program to demo it,all I need to do is
double-click and nothave to go through the
file open, et cetera.So I launch this program, Press
F3 to execute, check the log,verify that the library
was successfully assigned--I have a little warning above
which is about my licenseabout expire in May, so I
need to renew the license.I'll open up Libraries, and
there is my SGF2020 library.I'll go to View,
Detail View-- so Ican see the data
sets that I will beusing for this demonstration.Next I'd like to explore
the syntax order.I'll open up the
first program here.Here what I'm trying to
do is gather informationfrom the employee
information data set,and I'd like to get these
three columns in this order--Employee_ID,
Employee_gender, and Salary.So I list them on the SELECT.I go ahead and execute with the
F3, and in the Results Viewer,I will indeed see these three
columns and all the rowsin that table.That was pretty straightforward.Now let's enhance
this a little further.Let's open up a second program.Here what I'm trying to do
in the business scenariois gather information about all
the employees who are female,and I'd like to get these three
columns listed in this order,and I'd also like to
order the data by salaryin descending order.One thing that'll be
very clear from this codeis the closeness to the
English language in SQL.The order by, the where,
the from pretty muchdo what the word is saying here.So the SELECT is going
to gather all the columnsin the order of syntax.The next statement is the FROM.It's going to extract
information from this table.The WHERE clause is
going to filter the rowsand subset them for the
condition that I requested.And the ORDER BY
is going to ordersalary in descending order.I'll hit F3,
execute, and I'll getthe information I requested.And we notice that salary is
displayed in descending order,just as we requested,
and gender is all-female.If you go further
down, you'll seeit has only filtered for
the female employees.For the next piece of
code that I'll open up,I have a question for you.And the question is,
will this code work?When you see the
order of sequence,if you recall the SELECT
and then the FROM,was it the ORDER BY?No, it was a WHERE clause
that appeared next,so we'll see what happens
when we submit codewhere we haven't respected
the syntax order of SQL.Nothing happens--
there is no result.And if I look at
the log, there isa note that says I
have a syntax error,and I don't expect
what you gave me,and I'm going to ignore you.And the SAS system stopped
processing this step.So it didn't go any further.It just came to a complete stop.The fix we will
make is right here.We'll just switch the order in
which the WHERE and the ORDERBY appear, so that our requests
to SQL appear in the orderSQL would like to see them--the WHERE clause, and
then the ORDER BY.Will this code work?Absolutely.You'll see results,
and you'll seethe data displayed in the
order that you requested.As a SQL coder who is maybe just
starting their SQL learning,it would be useful to
learn of an option.And this option appears
in our next program--an option called NOEXEC,
which will explicitlycheck for syntax errors
without executing the code,so NOEXEC stands exactly
what it says there.I'll execute this code,
and again, no results.I'll head over to
the log, and the logwill have a note that
says, statement notexecuted due to
the NOEXEC option.That's a handy option to
use, and the next timewe execute code-- now
that we know that thereis no error in this piece of
code, the next time we executeit, we'll just remove
the NOEXEC optionso that the code
will indeed execute.Hands down, summarizing
data using the Boolean gateand PROC SQL has to be my
all-time favorite technique.When I fell in love with its
elegance, I captioned my blog--number one best programming
technique for 2012.That's how impressive
this technique is.The Boolean is simply the
digital computing world's wayof converting
everything to 0s and 1s.Simply put, a yes is
a 1 and a no is a 0.Let's go ahead to the
demonstration for Section 2.This section is about
summarizing using the Boolean,so there are really two
concepts here-- summarizingand the Boolean.Before we get to
the Boolean, let'slearn to summarize in SQL.In my first piece of code,
I have a business scenario.I've been requested
to get a report thatshows average salary by gender,
so that's what I'll do here.And on the PROC SQL
statement, I alsosee an option which
says NUMBER, and thisis going to make sure that
the output of the resultswill have the raw numbers.By default, they don't print,
and this will ensure it.On the SELECT statement
as we saw in section 1,I list all the columns I
want, employee gender beingone of them.And then the second column
is an average of salary.I'm using the average
function on salary,as average indicates
that this is an alias.This is how I allude
to this new column.The FROM clause lists the
table, and the WHERE clauseis going to filter a subset of
the data for any condition Irequest.Here my condition is, give
me those employees whosetermination data is
missing, which meansthese are active employees.I'll execute this
code using my F3 key.And what I see in the result
and what I expected to seeare quite different.What I was really expecting
to see is two rows of data--one for male and one
for female, and each rowindicating the average.As is [INAUDIBLE] to the data,
thanks to the option number,I can see the row number--I can see there's 308
rows in this table.But I haven't got just two rows.This has given me every
single row on the table.In addition, it's
given me the averagethat doesn't look like it's
different for the two genders.So that means I need
to do something else.Before that something else,
let's go back to the log.Let's see what the log told me.The log says something very
interesting-- the queryrequires remerging
summary statistics backwith the original data.Hmm, what does that mean?Let's explore the
results again, and let'ssee what SQL did here.Oh, by the way, before
we look at the results,let's look at the code.We had what SQL calls
a Detail Column Gender,and we also had on the select
what SQL called a SummaryColumn, average of salary.And SQL says, whatever you
give me on this select,I'm forced to give you that.So it's forced to give me
this detail employee gender.Now it gave me the
average, all right.It just didn't give me
the average I wanted.It gave me the average
for the entire table,and this is the
note in the log--requires remerging
summary statistics backwith the original data.The results-- what did SQL do?With each individual row, it
went and stacked the averagefor the entire table.This is for the entire
organization, not just brokenby gender.So to break it down
by gender, we'llintroduce a new
statement, the GROUP BY.The GROUP BY statement
will guaranteethat I now have the data broken
into male and female gender.Something to be aware
of about the GROUP BY,and to know what column would
be a good candidate, any columnthat is a detail column
qualifies excellentlyfor the GROUP BY.You typically wouldn't say,
""group by average salary.""You wouldn't put a summary
column on the GROUP BY.I'll execute this code, and
I'm very happy with the resultsthat have been written--one group for female, one
group for male, and the averagefor each group.Let's try to take this
knowledge a little furtherin a second business scenario.In this scenario, I like
to see the employee countby department.So I know from the
previous examplethat the GROUP BY statement
is needed to group data.I also know that Department
would be a good candidate.So on the SELECT I'm going
to list the columns I need--Department, certainly.I've been asked for employee
count by department.So By Department is going to
be my grouping department.That is one column.My second column
gives me the count.This is a function, count(*),
whose role is to countthe number of rows that
a query will provide.So with this code,
I should expectto see the number of rows for
each department, and voila--we do.We see the number of
rows for each department.Incidentally, the rows
are not in order by count.We didn't expect it--We didn't require it.But we'll do so in
the next example.I'd like to add a little
further to this piece of code.I'd like to now provide
a result where I'm onlygoing to get those departments
that have 25 or more employees.In addition, I'd like to see
the results in descendingorder of employees.Here's the code
that's going to do it.We have the GROUP BY, and
then we have a HAVING.Why did we need the HAVING?Let's go back to
the previous code.Let's add the WHERE
clause, and if Iremember my knowledge of the
WHERE-- it goes after the FROM,I'm going to say where count
is greater than or equal to 25.Will this work?No Results indicates I
need to check the log,and the log comes back and
returns to me there's no--the following columns were
not found in the table.So let's expand on
that a little bit,and the role of
the WHERE clause.The WHERE is one of the
most powerful statementsin pretty much any
programming language,and SQL is no different.And the reason the
WHERE is so powerfulis, it's a preprocessor.It acts on existing data.What is it consider
existing data?Existing data is any column that
comes from a table on the FROM,so that's one thing to
remember about the WHERE.The second thing to
remember about the WHEREis that it acts on raw
data or individual rows.It doesn't act on
summary data, right,so that's what was the
problem in the log.So that means a
WHERE clause will notbe able to cut through
or filter my data.It's a difference between buying
apples in a grocery store--raw apples and how I cut them,
versus buying an apple pieand how I cut the apple pie.Here, we have an apple pie--think of the count
as an apple pie--it's completely processed.It no longer resembles
apple, and I want to go aheadand slice it.So the WHERE-- the knife
of the WHERE will not work,and that is why we have a
new statement called HAVING.The role of the HAVING
is to take the group dataand filter that group data.So this is going to work.I'm going to say,
having Count ge 25,and I'll order by count
in descending order.Let's submit this code.And these are the
employee countsin those departments with
at least 25 employees,and the results are in order
of count in descending order.So now we understand
how grouping works--how to filter or cut
through group data.Let's go ahead and begin
to understand our businessscenario for the Boolean.So here is my code, and the code
itself needs a little breakingdown, so let's break it down.So here's my business scenario.I'd like to create a
report that gives methe total number of managers,
the total number of employeesin each department--and I'd like also like to see
the manager-employee ratio.Now this itself is a
pretty complex ask.On the same row, for example,
let's look at row 1-- accounts.I'd like to see both the
managers and the employees.And if we start to think
about this, typically whatwe would get is either the
managers or the employees,but the ask is quite complex.So let's break it down further
to see how the Boolean willcome to our help here.Firstly, we'll try to figure out
if someone is a manager or not.We look at the job title
column to get this information.And how do we figure this out?We are going to use the
FIND function in a Booleanexpression and convert
everything to a 1if we find a manager and a
0 if we don't find the wordmanager in that job title.Here's how the FIND
function works,an example of the FIND
function on job title.Here is one row of data.This is the
administration manager.And on the FIND function,
I need to give ittwo things primarily--the string that
I'm interested in,which is job title, and
the substring within that,that I'm looking for,
which is manager.Now, data is not
always very clean.Sometimes the way I get it has
uppercase manager or lowercase.In order to
circumvent that, I'lladd the I modifier,
which says, I'mnot sure if manager is
uppercase or lowercase--please ignore case.How do we take this?And if we go back
to that, this isgoing to give me column position
16 is the first instanceof the word ""manager.""Well, that's not a 1 or a 0.How do I convert that into a 1?Very cleverly-- we'll
provide this statement,find Job_Title, ""manager""
i greater than 0.Is 16 greater than 0?Yes.What is a yes to the computer?A 1.If it doesn't find
the word ""manager,""SQL is going to return a 0.Is 0 greater than 0?No.What is a no for the Boolean?A false, right?So in this manner, we are
going to have a column whichwill have 0s and 1s.0s where it didn't find
the word ""manager""--1s where it found it.Let's head over to
the demonstrationto see how we apply
this to a code.Here we go.We have one column
where we have-- sorry,we have the SELECT statement,
where we have Departmentas the first column,
and then we haveone column that's
going to calculatethe column for managers,
another column that'sgoing to create for
me the employees,and the last column that's
going to divide the two.So the first manager column,
I have the FIND function.And that's going to
give me 0s and 1s.If I see greater
than 0, that's goingto give me all the
1s and that's goingto give me a list of managers.If I sum it up, then
I will see that Groupby Department, which we
will be grouping at the end.I'll do the same thing
for the employees.I'll go ahead and say, find
for me within job title,the word, ""manager,""
and that shouldbe a 0, which means no
instance of the word ""manager.""That's going to create
a column for employees.And wrap the entire FIND
function with the sum,so that'll give me the sum
of the number of employeesfor each department.I'm always remembering
the fact that Ihave a GROUP BY going on here.And then it's as easy as
dividing managers by employees.We are seeing the appearance
of a Calculated keyword.When is that used?The Calculated keyword is
used to reference a columnthat you have just built
in your PROC SQL query.These are not existing columns.These have been built by me.So use a Calculated keyword--calculated
Managers/calculated Employees.Now this column is
rather a long column,and I don't want to
give it a long name.So I provide a label
that is a shorter label--M/E Ratio, and then I
use a format=percent8.1.The FROM statement is then going
to specify the table from whichI need to extract the data,
and lastly, the GROUP BY.Amazingly elegant--
this solutionis going to give us
the results we need.With the help of
the Boolean, we'vemanaged to get the managers,
the employees-- twodifferent groups of data
together in one row,so we see a count
for each of themby department, as well as
the manager-employee ratio.Let's get started
with section 3.Now there's no magic
pill that will forgive usfor not doing our data.Know the Data must be the most
fundamental principle thatcannot be ignored.In fact, I'm going
to go out on a limbhere and say that
this is the only rule,primarily, that data
workers must know.Everything else is SAS.To help navigate through the
inherited, and sometimes messy,data my go-to suggestion
is DICTIONARY Tables.I love DICTIONARY Tables and
cannot imagine life withoutthem.I'm sure you'll feel the
same way once we revealthe value of DICTIONARY Tables.So let's proceed to the
demonstration for DICTIONARYTables.Since DICTIONARY Tables
is really new to usand we don't know what sits
in this dictionary table,we'll go and first
use a Describe Tablestatement to describe the
content of dictionary.tables.I can check the log
now, and in the logI see a description of the
metadata of this table.So now this makes it clear
that the column name's,called lib name,
is a library name,and obs is number of
physical observations,and var is a number
of variables.So if I wish to
query this table,I now can ask for
specific columns.Now, we've used-- in the
DICTIONARY Tables code,we've used dictionary.tables.And these views are
specific to SQL.If you want in your
remainder of the SAS worldto access DICTIONARY
Tables, then youwould head over to
the SASHELP Library,and within there you'll
find them listed.So as I scroll further down
in the SASHELP Library,anything that begins with the
letter v, which is probablytowards the bottom, is
indicative of a dictionarytable.So if I'm trying to
find the exact match,dictionary.tables,
then I'm lookingfor something called Vtable.So in the SASHELP Library,
we'll insert the letter Vbefore the table name, and then
strike out the S at the end.I'll move further in
my business scenario.Now that I know what the columns
are in this DICTIONARY Table,I can start to
analyze and requestall tables that have
an ID type column.But even before that, I like to
see all tables in the SASHELPLibrary, and I like
to explicitly listthe columns I need.This is a big help.Running the previous piece of
code with the described tablesgave me this intelligence.So once I submit this, I'll
get a listing of the tablesin the SASHELP Library and
the columns that I requested.I can now continue into
my code and requestjust those columns--just those tables that
have the ID column.So here is a program.Within the dictionary.columns
DICTIONARY Table,I'm interested in
the table name, whichis the column called memname,
and the name of the column,where this is the library.And I want to restrict my search
to only those columns whichcontain ID.Now, I'm not sure-- because my
data can sometimes be dirty,I'm not sure the ID is
lowercase or uppercase.So I add the upcase
function to limit my search.As I scroll through
the results, Isee that the letters ID appear
in all the column names,whether it's MSGID or it's
TABLID or it's SOURCEID.However, a limitation
of this techniqueis my having to explicitly ask
for the words ID-- letter ID.Now this was useful
because it tells mewhat ID type columns exist.But what if I want
to know all columnsthat have the same
name, and I don'tknow what those names are.And this is something
we learn to doin the next piece of code.We'll utilize technique from
the previous section here.I'll go ahead and query
dictionary.columnsand limit my search to
the SASHELP Library.I'll group the query by name--name, type, length, and memname
are listed on the SELECT--and then I'll go and
filter through and onlyask to filter the group
data with those rowswhere the count of
name is greater than 1.And that's pretty clever.So I'm saying, only
give me those names thatappear more than once.And voila-- here we go.The column called ACTUAL
appears in three tables--PRDSAL2, 3, PRDSALE.A column called AIR
appears in two tables.I've really removed
a hard coding,which I did in the previous
example, where I'd explicitlyask for ID.This time I made the
query more dynamicby asking the query to go
investigate DICTIONARY Tablesand find for me all columns
that had the same name.This is going to
hold us in good steadwhen we learn techniques in the
next section to join tables,and where we might not know what
other common columns by whichto do a join.In section 4, we are going
to learn to join tables usingconditions like Inner
Join and Reflexive Join .So first up, SQL uses joins to
combine tables horizontally.Requesting a join means
we want matching datafrom one row in one table
qith a corresponding rowin the second table.Typically we perform matches on
one or more columns in the twotables.A picture says
thousand words, so hereis a visual that
shows us the kindsof joins we can support in SQL.Inner joins are going to give
us the green highlighted piece--only the matching rows.Outer joins can give us either
a left outer join, whichis the matches, which is
the green piece, and thenthe yellow piece--
the non-matchesfrom the left table.A right join is going to
give me matches again--the green piece and the
blue piece-- which isnon-matches from
the right table.And a full join gives me
all matches and non-matches.So how do we write a join on
a query on the FROM clausewhen we have multiple tables?This is going to give
me a join, but thisis going to give me quite
an interesting join.This result is called
a Cartesian product.It's going to give me all
possible combinations.So let's see what
that looks like.The Cartesian product is barely
the desired result of a query.Here, it's taking row 1 from
table 1 to row 1 to table 2,row 1 to 2, row 1 to 3, row 2
to 1, row 2 to 2, row 2 to 3--and eventually we don't know
whether the first row, Smith,has an ID of 101 or 102.The IDs are not matching.So it's good to emphasize
that this is barely--rarely the desired
result of a query.And it's important to
note that this actuallycan happen accidentally if
we don't supply a condition.How would we supply a condition?How do we limit the rows
to exactly what we want?We'll do that in an
Inner Join Syntax.So let's proceed to the demo.Let's begin our first
exercise in the join section.I'm trying to combine
that customer's tablewith the transactions table.The customer's table has
one row for each customerand the demographics and
the transactions tablehas their transaction
itself-- detailed information.I'd like to really
see, with this code,if I can get customer
[INAUDIBLE] transaction.I have a select*, and the
select* indicates requestingall columns--not specific columns,
but every single column.Submitting this code is
going to give me matchingthree rows with three rows.This gives me the infamous
Cartesian product.3 times 3-- the table
grew exponentially.In addition, we don't
know if row 1 for Smithbelongs to ID 101
or 102, and so weget all possible combinations.To limit our search
to matching rows only,we'll head over
and punch in codethat looks like this, still
combining the two data sets,still collecting all the columns
from the two tables listedon the FROM clause--the difference is the
addition of the WHERE clause.This is the same
WHERE clause thatbe used as a subsetting
statement or a filterin earlier sections.This time it also acts
like the statementthat provides a join condition.We are joining based
on equal values of ID.Now SQL has-- if you
look at the resultsfrom the previous segment,
SQL has a unique abilityto provide the same column with
the same name, multiple timesthrough multiple tables.The DATA step would only
give us one instance of ID--not so much SQL.So now in this new
piece of code wewant to ask for ID from
the customer's tableor an equal value of aiding the
transactions table for matches.However, since ID is
in both the tables,I need to qualify it by heading
it up with the table name.Submitting this code
will execute and give mematches only.So this was my inner join.It gave me the customer,
Blank, who made a purchase.This is only one customer.Now, I do notice that
ID appears twice,and if I head over to
my code this is why.The asterisk gave me
all columns and IDappeared from both the tables.To get only one
instance of ID I'mgoing to have to request
only for one ID columnand remove the select asterisk.So here we've asked--and the question that might
come up is, I have two IDs.I have ID from customers, I
have IDs from transactions--which one do I pick?And in the grand
scheme of things,it doesn't matter, because
this is an inner join,so it's a matching row.And so whether I see
customers that IDor transactions that ID, I would
expect to get the same result.I'll execute this code.And I also want to point out
that you don't have to qualifyName, Action, and Amount.These are columns that don't
sit on the same table--on multiple tables, rather.Customer ID is the
only column thatis coming from both
the data sets--a much nicer looking report--no instance of
duplicate ID columns--just one instance in the other
columns that I requested.In the code that
we just submitted,we had to type Customers,
and we had to type it again,and this can be
quite a lot of work.So to avoid that kind of
work, I can give aliasesfor each table that
are short and sweet.An alias can be as
short as one letteror can be multiple letters.So this time, when I use ID
from the customer's table,I qualify it with c.ID.Or if I use ID from
the transactions table,I qualify it with t.ID.The only caveat--I list the alias the first
time on the FROM clause.Subsequently, I can use
the aliases anywhere.So as programmers, you're
always interested in efficiencyof the computing world
of our processing,but here, I'd also
like to highlighta little bit of efficiency
in lesser typing work.Next, we learn how to join
a table back with itself.This is also called a
Reflexive Join, or a Self Join.In the business scenario,
the Chief Sales Officerwould like to see a report
with the names of all salesemployees, as well as the
employees' direct manager.In order to do this, we have to
build a query in this manner.We'll first need to
read the addressestable once-- we're actually
going to read it twice.The first time, we'll grab
the employee's ID and name.So here is the first row.John has an EMP_ID of 100.Next, we'd like to know
who's John's manager.In order to do that,
we need to head overto the Organization Table.This table has two columns--EMP_ID and MGR_ID.So in order to get John's
manager and the managerID for John, we'll have to
go to the Organization Table.And if you're trying
to think of the SELECT,we almost want to
think of select managerID, where addresses dot EMP_ID
equals organization dot EMP_ID.So now we've got the
MGR_ID for John's manager.Well, we don't know
the manager's name.In order to do that, we'll
head back to the addresses eto read it again.So this is the second read
of the addresses table.This time we'll go and find
the name for this MGR_ID.So in terms of the
query that we craft,we'll be saying select
EMP_Name as MGR_Name,where MGR_ID from
organization tableequals the addresses EMP_ID.This is quite an
interesting new concept.Where we're reading
a table twice.So we're going to list the
same table twice on the SELECT,on the FROM clause.In order to do that,
SQL is perfectly OKwith us listing the
table as many timesas we want on the
FROM clause, but itdoes tell us to
qualify the tableand provide different aliases.So the first time we read
the table as employee,we'll qualify it as e,
and the second time we'llgive it an alias of m.Let's head over to
the demonstration now.Here is the completed
syntax for a Self Join.Let's begin with
the FROM clause,and let's decode that
first, because here is wherethe Self Join really appears.We have the addresses table
appearing once as Employee,so we give it an alias of e.Then we have the addresses
table appearing as Manager,so we give it an alias of m.The organization table is
not repeated more than once.We still give it an alias to
make our life easy later on,when we need to use
columns on this tableso we don't have to type
the long organization word.On the WHERE clause, based
on this slide presentation,we saw how we were
combining different columnsfor the join condition.And also we had another
filter, departmentneeds to contain the
string called Sales.Once you've done
that, we can lookat the SELECT, where we
grab specific columnsfrom each table.We'd like to see EMP_ID and
EMP_Name from the Employeetable--also give that a label.We like to grab Employee_ID and
Employee_Name from the Managertable and give that a
label, as well as Country.Lastly, we order by Country, and
here's something interesting.We are ordering by country,
which is a column, as wellas column number 4.This is what that integer
says on the ORDER BY--give me column 4 on the
SELECT, which is this column.Why did we need an integer
position and not a column name?It's because we never
gave it an alias.So in order to use it,
we give a column number.Same thing applies for column 1.Let's go ahead and
submit this code.And here we are.For each employee, we see the
name as well as the managerthat they report to,
and the manager'sname, as well as the
country that they are from.In this section, we'll
explore PROC SQL Logical QueryProcessing Order.In the first section, I shared
with you PROC SQL syntax orderthat goes like this--the SELECT, the FROM, the
WHERE, the GROUP BY, the HAVING,the ORDER BY--this is the order in which
we provide our syntax.However, the logical query
processing order goes as such.The first is FROM
that is looked at,and then SQL looks
at the WHERE clause--next, the GROUP BY.After that, the HAVING--and the SELECT is the fifth,
and then finally the ORDER BY.I'd like to go through
a business scenarioto start to let us think like
SQL so that we can type in codethat is the most efficient.So here is my business scenario.I have each of the statements
in a PROC SQL query,and we'll explore them, one
by one, in the demonstration.So let's break this
down, bit by bit,and then see where they can
be potential problems, so wecan understand that and
prevent that going forward.The first thing I'd like
to do is select the columnsI want from the Country Table.And if I was to look at the
Country data set in the SGF2020lib, the Country data set has
employee IDs of individuals,the country they come
from, and the datein which they were hired.This is the data
set that is goingto act as the source
for my first query.And the first query
is focused onlyon the SELECT and the FROM.Well, I already know
that the SELECT is notthe first statement
that's executed,it's the first statement that
SQL needs for my syntax orderperspective.The FROM is the first statement
that SQL will execute.I'm going to create
a table of this queryso I have a Create Table
statement, telling SQLthat I'd like to save this
table and the SGF2020 libraryas logicalq.I also have a nice macro
here, called &sqlobs.You know, SQL really
doesn't give youthe number of rows in a query
like the DATA step does.This macro will enable me to get
the number of rows in the log.So I'm going to
submit this queryand take a look at the log.So the table is
created with 322 rows,and %put came back with 322.Next, I'd like to use a WHERE
class, which is the second,in terms of logical
query processing order.I'd like to process the data
and only filter for those rows--for those individuals who
were hired on or after 1stof January, 2009.So the SELECT has
the same columns,the FROM has the same table--the only difference is the
addition of the WHERE clause.I've already gone
through this before,and I know the number of
rows I'm going to receive.And this is nine rows of data.Is it possible for me to
make a mistake in coding?Yes, there is.I could just go
and supply a SELECTwith the FROM and the WHERE.And on the WHERE clause, if you
recall in an earlier section,we tried to use a column that
was not an existing column.We tried to use a
column on the SELECT.Now you and I know
that this SELECTis the fifth to be processed--
the fifth statement,the fifth action.The FROM is the first action
and the WHERE is the second.Now it becomes crystal clear why
the WHERE clause choked beforeon a column that we were
building on the SELECT.So to understand that this is
a no-no, let's submit this codeand examine the log.And the log returns
to me an errorthat I've seen before in
the summarizing chapter.The following columns
were not foundin the contributing
tables-- yearhired.So SQL is coming back
very clearly and saying,I don't have access
to this table.I only have access--the FROM is a first
thing I looked at.And then on the
WHERE clause, I'mgoing to filter the
data, definitely.It's a second
statement I'll execute.But I don't know what
yearhired is, and yearhiredis listed on the SELECT.SQL doesn't operate
with the SELECT yet.It has much more things to do.So something you could
do to circumvent thisis, instead of saying
yearhired, you could actuallytype in the entire function.You could say WHERE,
year, emphiredate--and that would
work like a charm.Let's head back to a code, and
let's add now the GROUP BY.I've modified my
WHERE clause to say,give me only those rows
where emphired date ison or after 1st
of January, 2009.In order to add the
GROUP BY clause,I need some kind of summarizing.Then there is value
in the GROUP BY.So in addition to country
and year of emphire date,I'm also going to
get a count(*).This is going to count
the number of rowsfor this grouping of data.The grouping that
I'm interested inis country, and within
that, yearhired.So this is going to give me,
as we execute this code--this gives me five
rows of five groups.So let's understand what SQL
is trying to tell us here.The first row--Australia.The yearhired is 2009.I actually had
three rows of datacollapsed into a single row.The second row of data,
this group data 2010,I had two individuals that were
hired in 2010 in Australia.In the United States, 2009
saw me hire one individual,in 2010, another individual,
and in 2011, we had two.Let's continue our
building blocksby adding the next statement,
which is the HAVING.This time I'd like to see which
are the years in which-- whichis the country and, within that,
the yearhired grouping in whichI had more than
one employee hired?So now that means I'm going
to reduce those rows--the United States, which
had only one individual.If you look at
our results, we'llvalidate that we'll strike
through these two rows of datanow, and we are only interested
in rows greater than 1.So I added that
the HAVING clause,and in terms of the
logical processing order,this is in perfect sync.The FROM is first, the
WHERE is the second,the GROUP BY is the third,
the HAVING is the fourth--so let me execute this code.And as discussed,
I see three rowsof data with the HAVING clause.Heading back here is
the completed code.I've also added an ORDER BY.So to recap, the
SELECT is the first--I'm sorry, the FROM
is the first, and thenthe WHERE, and then the GROUP
BY, the HAVING is the fourth,the SELECT is the fifth, and
the last one is the ORDER BY.ORDER BY is something that is
all going to order the data.So there isn't as much
heavy-duty processingas the other columns
at the other rows.Let's submit this code.And the only
difference, in fact,from the previous
result and this--there isn't a lot of
difference in the row data,but there is a difference in the
order in which the rows appear.Now we are seeing the
yearhired in descending order,so 2010 first, and then 2009.In this program I
had hoped to showcasethe value of beginning
to think like SQLin the logical query
processing orderso that we can make
our queries efficientand also be able to debug
if we find an error show up.So in this tutorial I
had hoped to showcasethe best strengths
of SQL and lay outthese strengths step-by-step.For any questions,
feel free to reach out.Thanks for watching."
10,"Hello.My name is Steven
Sober, and I ampart of the global
technology practice at SAS.My super demo is based on
the SAS Global Forum paperthat I and Brian
Kinnebrew co-authored.The paper title
is Best Practicesin Converting SAS
Code to LeverageSAS Cloud Analytic Services,
which we refer to as CAS.CAS is the in-memory
engine for SAS Viya.This super demo has
three components.First, we'll review
the GitHub repositoryof the code I will be demoing.Secondly, I will
run a few examplesfrom the repository
using SAS Studio.Lastly, I will share with you
information on the SAS Viyareadiness utility.Let's get started.The GitHub repository can
be found at this address.This address is documented in
our SAS Global Forum paper.Once you go to this
address, please take timeto read the Read Me file.There are some
important information.For example, all of the
programming examplesare for functional testing,
not performance testing.Also, this is the
required step that allof the other coding
samples are dependent upon.This step simply sets a
path to a macro variableI call data path.Once you run the
setup step, then youcan run any of the additional
coding examples in any sequenceyou want to.To clone the repository,
what you need to dois to back up in
the tree structureto the SAS Global Forum 2020
level, as you can see here.Then notice you have a clone
or download button over herein green.When you clone it,
you actually haveaccess to any of
the SAS Global Forumpapers where the author has
chosen, like me, to createa GitHub repository.To demo a few with
the programs locatedin the GitHub repository, I
will be using SAS Studio 5.2.SAS Studio 5.2 comes
with SAS Viya 3.5.One of my favorite
features of SAS Studio 5.2is the GitHub integration.More about that later.First, we will run the
setup program, which definesthe macro variable data path.The macro variable data
path must point to a paththat your SAS Viya
environment has access to.Once the setup job
completes successfully,then we can run any of their
programs in any sequence.As we can see by
looking at the errors,we have zero, which means
our job ran successfully,and our data path
macro has been defined.Let's take a look
at a program thatwould convert all
character data typeswith the length of 16 or
greater into VARCHAR data types.The reason one would want
to do this conversion isbecause CAS data is closely
coupled with its memoryrepresentation and
requires alignmentto 8-byte boundaries
for memory mapping.That means all variable lengths
must be a multiple of 8,and that means record links
and data sizes will increasewhen lifting data into CAS.To accomplish this, we will
leverage the procedure CASUTIL,and specifically the
statement VARCHAR conversion.Notice we have set that
variable to a length of 16.When we look at the DATA step
that generates the sourcedata to be lifted
into our CAS table,we notice we have
the link statement.And for variable A, it
has a length of 300.Variable B has a length of 15.Variable C has a length of 16.Notice we're
creating three outputrecords were the values
of each of the variablesvary in length.Let's go ahead and run
this code and then lookat the result window.In the result window, we can
now see the data structureof our CAS table.And sure enough,
for all variablesthat were 16 or greater, we've
converted them to VARCHAR.And notice on variable A, we
have a VARCHAR length the 135.That's because the longest
value in the sourcetable for variable
A was 135 in length.The next program we will
look at will parallel loadand compress a CAS table.To accomplish this, we
will use the procedure CAS.The statement that parallel
loads and compresses the CAStable is the INDEX statement.The INDEX statement
of the procedure CASalways runs in parallel.Now, the important options
on the index statementis the compress equals true
statement, which simply meanswe will compress the CAS
table, and the single passequals true statement.The single pass equals
true option meanseach thread will load a
record, and it will alsocompress that record.The default value for
single pass is false.That means that CAS table
will be loaded first,and then in a second step, we
will compress the loaded CAStable.Customers have
reported back to methat this technique
has improved runtimes of loading CAS tables.When we review the SAS log, we
will see our compression ratio.By reviewing the
log we can now seewhat our compression ratio is.In this instance, it was
a compression ratio of 5.Here's the original
data size and thenthe compressed data size.Next, let's review
that GitHub integrationthat comes with SAS Studio 5.2.For example, let's say we
wanted to add a statement here,which will display the value of
our macro variable data path.Let's run the program.And that's we can see in
the SAS log on line 92,we have our new PUT statement,
and right underneath thatthe actual value of the
macro variable data path.I'm happy with that change.Let's save our program.Now that we've
saved the program,using the interface
to GitHub, I noticeI have one change to
my GitHub repository.If I click on it, we will
see what the change is.Well, here, notice I've added
a new statement in green,%put at data path.Yep, I'm happy with this change.The next thing we want to
do is to stage this programand add a comment
before we push itback to our GitHub repository.To do that, simply
click on this.And now you'll notice
in the staged areawe have our program.I'm going to add a comment.That way when we push our
change back to the master,they can decide if
they want to mergethat change into the
actual repository, soa very simple comment.Now we'll commit our change.So this will refresh
the interface.And notice up here, on Push
Master, we have one change.So let's go ahead
and click on it,and now we've actually
pushed that changeto our GitHub repository.And if we go into
the History here,we can see that we've
added a PUT statement.And sure enough, here's my
change, added a PUT statement.And if I click on
my Setup program,we now have in our GitHub
repository the new changesto our program.We covered quite a few
things in the demo.Let's refresh our memory.Remember, for the
setup.sas program,you must run this
program before yourun any of the other programs
in the GitHub repository.It's line 10 you need to modify
so the macro variable datapath points to a path that
SAS Viya has access to.With the how to
convert character datatypes into VARCHAR data
types when lifting the CAStable into CAS, as you
can see, on line 43,it is the VARCHAR
conversion statementthat allows us to convert
all of the character datafrom a source table
into a CAS table.And when we reviewed the
results after running,we validated the variables
that were 16 or greaterhad been converted to VARCHAR.With the how to parallel load
and compress a CAS table,the important statement
begins on line 27, the INDEXstatement.Again, INDEX will
always run in parallelon all threads of
all CAS worker notes.The next important
statement is on line 28,single pass equals true.That means in one pass of
loading that CAS table,we will have a
compressed CAS table.And then on line 29,
it's the compressequals true that tells the CAS
procedure to compress the tablewe're loading into CAS.When we ran that program
and reviewed the SAS log,we saw the original data size,
the compressed data size,and the ratio of
that compression.Now I'd like to shift
gears and give yousome information on the SAS Viya
Readiness Assessment utilityfrom SAS.The utility does not execute any
code ran through the utility.It simply scans
each word and scoresthat word based on it
being CAS enabled or not.You point the utility
to a given directory,and then the utility will pass
all files with an extensionof .sas in that directory,
as well as all subdirectoriesof that path.You have two options.The most popular is SAS will
come on site with the utilityand work with you to install it,
configure it, run it, and thenexplain the findings.The second option would be you
would send your code to SAS.We would run it
through the utilityand then schedule time
to present our findings.Whichever option you
want to use, simply askyour SAS account team to
contact myself, StevenSober, or my colleague,
Brian Kinnebrew,and we will assist
the SAS team to runthe utility against your code.We're going to review a
couple of the reports producedby the utility.This is the frequency
report, and wecan see in the frequency
report we ran the utilityagainst 1,501 SAS jobs.And those jobs are actually
from the sample programsthat are delivered
with SAS foundationfrom Base, CORE, our ETS
product, and our STAT product.The next thing to notice
are the proceduresthat are CAS enabled.That means these procedures
will run in CAS, providedthe source data and the
target data to the procedureare CAS tables.The next thing to notice
is candidate for CAS,like for DATA step and
these analytical proceduresand PROC APPEND.Candidate means you
might be able to refactorthe code for the procedures
into a CAS-enabled procedure,or you could study CASLE,
the language of CAS,and run your code as CASLE code.Regardless of which
way you want to go,you may be able to convert
a lot of proceduresyou use to run in CAS.Now, with data step,
the majority of codeis CAS enabled.But there are certain
functions and formatsthat are not supported,
and the utilitywill point those
statements out to us.Also, for a lot of the
procedures, not all of them,we have mapping suggestions.So look down here at
this one, PROC LOGISTIC.PROC LOGISTIC only runs on
the SAS Viya compute server.You can refactor that code to
the procedure PROC LOGSELECT,and it will run in CAS,
provided the source and targettables are in CAS.In addition to the
frequency report,there's the detail reports.For each program you
run through the utility,you will have a file.Let's look at one
of these files.The name of the program
we're looking atis up here on line 1.For the Flags column,
to understand the flag,review line 3.The way I use this report is
by focusing on the Restrictioncolumn.And any time you
see the word ""CAS,""that means it's found a
statement that will notrun in CAS.The key word will give
you a hint on whatthe actual statement is.This is an example
of a title statementand it only runs in SAS
Viya's computer server.If we scan down
here to LOGISTIC,it got flagged as
well because that onlyruns in the compute server.But we know, based upon
the frequency report,we can refactor that
code into LOGSELECT,and that will run in CAS.Thank you for attending
my super demo."
11,"CHRIS REPLOGLE: Welcome
to Getting From GovernancePractice to Data Awareness.I'm Chris Replogle.I've been with SAS Institute
for 19 years doing softwaredevelopment in areas of
data management and metadatalineage.I'm a maintainer and contributor
to the open-source Egeriaproject.It's under ODPi management.ODPi is a Linux
Foundation organization--active participant in
the Open Governancework through ODPi/Egeria.I'm on their weekly call.And nice to meet you.So today we'll cover this.So where's your data from?Is it internal?Is it external?Do you lease your data?Is it user-provided data?Data you use in
your organizationcomes from many
sources, typically--internally-generated or
collected, other organizations,external entities, users,
consumers, customers,et cetera.These latter ones are usually
shared data, so data that'sshared with your organization.How do you manage
the sharing of data?How do we agree on what should
be shared with one another,a person, process,
or organization?How do we agree on that?Typically, we have a
data use agreement.A data use
agreement's a contractwe used for the exchange and
transfer of non-public datathat's used-- that's subject
to some kind of restrictionon the use.A data use agreement
dictates what we should shareand how we should share it--so the data lifecycle,
onboarding the data,when and how do we receive
or store this data,how often do we get the
new version of the data?Maintenance-- how do we keep it
secure and validate the data?Along with maintenance,
we have retention.How long should
we keep the data?And then we deal with disposal--
archiving, and destruction,and transfer of the data.How long should we keep
it on request, and thenhow long should we
keep informationabout that data on request--the metadata about that data?We also need to deal with
data compliance standardsor regulations that
govern the data--data protection,
data management,and security safeguards that
also go along with that data.Data sharing implies
a need for governance.Shared data is informative
to an organization thereforemost valuable to/within
the organizationis sharing from some
internal / external source.Typically, you
will share the databecause you want one
authoritative sourceof the data.You want to make sure
you have the right dataand the data you
get is quality data.Typically, that's why you share
data with other organizations.If you're the one that
collected or organized the data,then typically, you're the
expert or the verified correctsource for the data.Along with data, we need to
know, what is governance?Since you talked about sharing
we talk about the peopleand processes that act
upon information or data.So really, governance is
about the people and processeswe use to manage the
data-- how we onboard it,how we organize it, how we
maintain it, how we archive,destroy, or transfer that data.What is governance practice?Governance practice defines
how data's accessed, organized,categorized, and secured.That's how the users
interact, and whatprocedures, and policies, and
things we enable and act upon.What influences do we have
for governance practice?Typically, data has influences,
such as regulations, policies,compliance, profit for your
organization or company,and company secrets.We want to keep those secrets,
so maintaining a company secretis a good governance
practice, because youdon't want to give that away
because you may be giving awayyour profit.These, in turn,
define and influencethe standard we abide by right.Those dictate the standards.Oops-- I'm hitting
the up button.Now-- sorry-- back into it--data-- what are the rules
and policies, and whoare the stewards of the data?What enforcement should we
apply from those use agreements?The policies-- we
need to document--have documents that
describe and definehow it should be secured,
consumed, validated,and the thresholds or limits
that apply to that data--how much it can be shared,
how it can be shared,how it should be managed--all those things.Those rules are processes
that validate the datawithin those thresholds
and make sure we'rewithin the compliance standards
that have been defined.Stewards are the
persons or teamswho ensure the
standards, compliance,quality of the data.Assets are any object, element,
or structure that gives valueto the organization.It could be anything, such
as data sources, databases,file systems--those type of things--infrastructure, and
applications, APIs,and services that
interact or provideinformation, analytical
models, processes,and intellectual knowledge
within the organization--buildings and natural
resources, physical resourcesthat have a unique identity.Asset management--
what information shouldbe maintained for an asset?What do we maintain on an asset?We need to manage the
details of the asset.That basically entails the
metadata around that asset--the who, what, when, where,
and how of that asset--and then related events or
change in status of that asset.When was it updated?When was it changed?When was it created?When was it archived
and took out of service?All those things are
information we needto collect about our assets.Asset lifecycle, as we talked
about earlier about data--asset lifecycle--
we need to manage itthroughout its use
in the organization.We onboard an asset--we make it active.We protect that asset,
we start using asset,and we maintain
that asset, and thenwe decommissioned that asset.That doesn't mean we lose
knowledge of the asset.That means we are no longer
consuming that asset.Descriptive information is about
the physical, the design time,the operation and semantic
information around an assetor piece of information.Who?What's the responsible
party for that?What's their role-- the what.It's related to the policies,
rules, decisions, thresholds,and metrics of that.When was it created
updated or validated last?Where is its location--both physical location,
cyber location, or evenglobal location?How is it accessed,
secured, and managed,and why was it
created to change?This seems exactly like
metadata, doesn't it?I believe that's what this
is, business metadata.Business metadata's usually
contained in the glossary.It's a vocabulary
common languageused to communicate about or
relate to information or data.Oops-- I hit the
wrong button again.What makes a good glossary?Common data definitions
are used to definethe meaning for assets.We want to basically assign
a meaning to an asset.The owner, the organizational
content, and thenthe components of
a good glossary--we need to know those things.Who owns this glossary,
how's it organized, and thenwhat components are in there?The terms-- they're used to
describe a concept or idea.The categories
organize those termsand put them into logical
folders or organization.Classifications
define how it's used,whether it's term or glossary.In relationships, as
far as that comes,semantics asigned to assets
and semantic hierarchiesthey're used to give
terminology meaning.Terms are the building
blocks of a glossary.They describe a single
concept or idea.Concepts form a hierarchy.They allow businesses to
define or refine concepts.They narrow a concept
to define meaningor apply organization
to information.For example, transportation may
be broke down into rail, air,water, and automotive.Automotive may be broken further
down into trucks, bus, car.And I might break
cars down even furtherto electric vehicles, gas
vehicles, and diesel vehicles.Semantic assignment is basically
assigning a term to informationin order to refine the meaning
or understand that information.Assigning a term to information
enables the business userto even understand
IT infrastructure,to understand what the
table or column means.To be able to find
relevant information,I can actually use the
glossary to find somethingthat's relevant to what I need.Once I understand
what the meanings are,I'd be able to find the data
related to that meaning.And then lastly, understand
the meaning of informationrelated to the term.And that's basically
what that is.So the information or data
related to that term--I now can understand
those things.So if we look at
glossary categories,glossary categories--
they organize the terms.They can contain a
category or term,so you build a
hierarchy of those--categories that contain terms.Term can be in more
than one category thatallows you to organize a term
into multiple categories,yet not have a
duplicate of the term.Classifications add
information to further describehow something is used.We talked about it earlier.They had descriptive information
when they are assigned.They can be used to describe
how items relate also--for example,
classification simplecan be applied to
information thatimplies the access
should be controlled,or it's sensitive information.Ontologies and
taxonomies definedhow we organize those terms.You've heard those
a couple of times.An ontology identifies
and distinguishesconcepts in their relationships.A taxonomy formalizes
relationship hierarchiesamong concepts and
specifies the term.This prescribes a
structure in terminology.Term relationships--
term relationshipscan be semantic or structural.Terms have relationships
to each otheror define how they are
related or linked together.Example relationship types is
isA, hasA, synonym, antonym,and seeAlso.An apple is a fruit.An address has a postal code.Postal code can be a
synonym for ZIP code.Rural can be an
antonym for city.Social Security
Number, SSN, may alsobe relevant to government
identification,so you may want to see also
government identification.Glossary classifications
applied to the glossaryitself-- it means I can
classify the glossary.So what that does is
denotes the organizationor structure of the
content within a glossary.So if I had more
than one glossary,I could have a taxonomy in one.Another one's
categorized-- or classifiedas a canonical vocabulary.Taxonomy means the same term
is not present in more than oneof its categories.Canonical vocabulary
means the glossary onlycontains-- only includes
terms that have a unique name.Therefore, there is
only one definitionfor any single concept.Category classifications--
a categorycan be classified to elaborate
on the type of contentswithin that category.For example, subject
area means a categorydescribes an important topic
area for the organization.Typically, subject area owners--have owners and are
managed carefully.Basically, that
means that you canassign owners and
management responsibilitieswith that classification.Term classifications define
how the term is used.Activity description,
abstract concept, data value,context definitions, spine
object, spine attribute,object identifier--they allow you to
classify and organizeyour terms into
what they're doingor how they're being used.Common information models--
basically, it's a standard waythat we represent information.For example, we'll
cover common informationin our files next with the
cloud information model.Common information models
are covered in the bookby Mandy Chessell.I recommend reading
this if you get time.The cloud information
model is an open modela standards-based solution.Cloudinformationmodel.org--
it's open,and anyone can contribute
or collaborate.Go to GitHub, cloud
information model.There's multiple output
formats available--AML, JSON-LD-- examples.Egeria has native import
for cloud information model.I recommend looking at
that if you get a chance.Common data definitions allow us
to understand how to referenceand represent our data--everything from the glossaries
all the way to the database.So data classes-- these provide
features to organize your datainto logical types.They define or relate
information for data typedetection, such
as rules, passing,detecting expressions, regular
expressions for validation--denoting the preferred
implementation typesby technology, indicating
what's expectedin each representation,
then documentshow it should be used, and thus
allows for easy data validationand rule creation.Use of data classes for
defining things such as a SocialSecurity number and how you
want to organize that, a--address information
and how you wantto organize that,
how it should bestored in different
types of databases--whether it's in a database, or
in a CSV file, or in big data.What's the preferred
data format of those?That's where you
use a data class.Schemas are used to document
the structure of the data.Between schema type
and schema attributes,you have two elements that allow
you to organize some things.Schema type defines
elements and how--and allow reuse-- means
you can use the schematype for multiple assets.Schema attributes define
elements of the schema type.For example, a relational
table will be a schema type,whereas the columns
of that tablewould each be a
schema attribute.Assets and schemas
basically are related.The schemas are used to describe
the structure of the asset.Note, schema type can be
assigned to multiple assets,so implying the shared structure
with different data content.So I have the same data
in three different assets.I had three different
data locationsthat have the same schema, or
same organizational structure.They may not have the same data,
but they have the same schema.Connections-- they
store the informationnecessary to connect an asset,
like JDBC or ODBC connections.Connector type is what
the actual JDBC or ODBCis it describes the
connection method,or technology used to
make that connection.In a SAS example, you could
use access engines or TKTSconnections.Those are also examples
of connector types.A connection may have
connector type of ODBC or JDBCto a database
server, for example.And those are typical usages
out in the real world.Discovery's a process
that runs a set of actionsto describe an asset.In the case of data, you want to
run discovery and then catalogat least the following--the metadata, the types,
the ranges, the metrics,and the quality--for metadata, the
who, what, when,and where information
of the asset.The types-- I want to know,
what data class is it?Ranges-- the ranges
of information.What's the range of the
data or information present,the metrics and quality--what's the state?Is it missing values?Incorrect values?What's the minimum, the
mean, the max of the data?What's the quality
score of the data?Those are kind of
things we can discover,are the deep running
metadata of that data.Governance landscape is fitting
those practices togetherwith the information.So I've defined all the
data landscape on the right,but we have all the data--the governance practices
and processes that we alsoneed to connect to that data.Connecting the
landscape-- when youlook at it top to bottom,
information or datais connecting your
assets to your catalog.So I've got a
connection to an asset.The asset information
is defined by a schema.A schema can be related to
the data class for typing.Asset metadata is discovered,
giving us data annotationsto give us those metrics and get
extra information about data.Schema can have
semantic assignment,where a term is defined
to a schema attribute.Terms can Have topology or
ontology or data modelingterm-to-term relationships.Terms are in glossaries.That's how we
organize our terms.Governance and organizational
relationships-- basically,we want to define how an
organization relates to others.How we relate as an
organization to our consumersor our customers?We provide goods or services.They give us feedback or reward.Same thing for regulators
and stakeholders--they typically want to
look at the governanceto see if we have a healthy
culture, to see if we'remanaging our assets correctly.And then business partners want
to exchange goods or serviceswith our organizations,
and they will alsowant to recieve
reward or feedback.Maturity models-- where do
we put our governance focus?The levels of maturity in
your governance practicecan break down into one
of these categories.And this is kind of
where your focus is.Data awareness-- where is
the organization's data,and what does it contain?Governance awareness-- how
should data be governed?Embedded governance-- how
can governance be automated?I want to automatically govern
the data as it comes in.Business-driven governance--
how can the business leaderstake ownership of
data governance?And then data citizenship--
how can every employee systemand server get the data
they need every day?A catalog enables the collection
in management of asset data.So basically, we allow those
levels of though of maturityalong with your catalog,
because a catalogenables you to add information,
relate assets, assignresponsibilities, and
find relevant information.These capabilities
of the catalogallow you to uphold
and implementthe policies you designate
for your governance practices.In summary, the
knowledge of your assetsleads to more insight
in how relevanta collection of information is,
which gives you data awareness.Please visit
opengovernance.odpi.orgfor a lot of more information
and more detailed informationabout the topics
covered here today--egeria.odpi.org for information
on the Egeria project,and SAS's data
governance websitefor more information about
data governance through SAS.Thank you."
12,"Hi, and welcome to
today's session,Accelerating SAS Using
High-Performing FileSystems on Amazon Web Services.My name is Darryl
Osborne, and I'ma solutions architect at AWS.I'm a member of the Amazon
EFS and FSx service teams.And I work with customers
on their file servicesneeds, evangelizing AWS
native file servicesand sharing best practices on
running your fully-managed filesystems on the AWS.When running efficient,
high-performing SASapplications in the cloud is
critical to your business,deploying the right
storage architecturecould be the difference
between success and failure.However, running a
high-performing parallel filesystem is difficult and
often very expensiveto setup, run, and maintain.What if you could offload
this to someone else, someonewith expertise and with
virtually unlimited resources?Well, you can.This presentation is based on a
white paper I wrote that guidesyou through selecting
Amazon FSx for Lustrewhen running SAS Grid on AWS.It also discusses the
different types of file serversavailable on AWS and
which you are bestsuited to access these
high-performing file systems.Amazon Web Services is the
world's most comprehensiveand broadly adopted
cloud platform,offering over 175 featured
services from data centersglobally.Millions of customers, including
the fastest-growing startups,largest enterprises, and
leading government agenciesare using AWS to lower
costs, become more agile,and innovate faster.These customers want to spend
less time deploying, managing,and troubleshooting
infrastructure,and more time focusing
on their core business.There are several
papers available thatwill help you understand how
SAS performs I/O, as wellas the minimum I/O requirements
for SAS file systems.These papers outline the
recommended I/O metricsfor file systems that
support SAS deployments,and they identified
guidelines thatcan help you with provisioning
and tuning I/O characteristicsfor optimal SAS performance.The I/O throughput
requirements for SAS computehere are very simple.Each node should have
at least eight gigabytesof memory per physical core.For permanent SAS
data files, SASrequires a minimum
I/O throughputrate of 75 to 100 megabytes
per second per physical core.For SASWORK and
UTILLOC file systems,SAS requires a
minimum I/O throughputrate of 125 megabytes per
second per physical core.So the overall I/O
throughput shouldbe at minimum 100
to 125 megabytesper second per physical core.SASDATA stores persistent
data for SAS workloadsand resulting SAS output files.These file systems are
typically read-heavy,with around 80 to 20 or 60
to 40 read/write ratios.SASWORK is the scratch
working space for SAS jobs.It is used to perform
the working storageactivities for single-threaded
SAS procedures.These file systems
are typically balancedbetween read and write activity,
with a 50/50 read/write ratio.UTILLOC is the same type of
space for multi-threaded SASprocedures.So UTILLOC by default is
placed in the subdirectoryunderneath SASWORK.So it too is typically
balanced between read and writeactivity, with a 50/50
read/write ratio.We are proposing you use Amazon
FSx for Lustre persistent filesystems for SAS data,
SASWORK, and UTILLOC,which is able to meet and
exceed file system performancerecommendations from SAS when
accessed from certain AmazonEC2 instance families.This combination allows you to
simplify your SAS Grid designand deployment and reduce cost.Let me give you an example
price comparison for SAS Gridcompute.So if you have a requirement
of four physical coresand 64 gigabytes of
memory per compute node,we would recommend one of
these three instance types.Some of them have instant
store volumes attachedto them included in the price.And this pricing is
northern Virginia pricing.Say you also have a requirement
for SASWORK and UTILLOCper node of 600 gigabytes.Based on those,
we would eliminateone of those
instances because itdoesn't have enough instant
store volumes to satisfythat requirement.But the R5N also doesn't
have instant store volumes,but we would recommend
using FSx for Lustrefor SASWORK and UTILLOC.So we just need to allocate
600 gigabytes per compute nodeon the FSx for Lustre file
system to accommodate this,to offload SASWORK and
UTILLOC to FSx for Lustre.If we have 10 nodes
within our environment,the monthly price
would be roughly $6,600for the I3EN instances or $3,900
for the R5N's plus the 600gigabytes of FSx for
Lustre storage per node.That would give you a monthly
savings of $2,600 or 40.23%.Amazon FSx for Lustre is
a fully-managed versionof the Lustre
parallel file system.And Lustre file systems
provide throughput upto hundreds of gigabytes per
second and millions of IOPS.It provides consistent
submillisecond latenciesdue to the parallel
file system designwhen clients talk
directly to servers.There is in in-memory
caching, and there's alsoSSD-based storage.And when using the recommended
persistent file systems,storage volumes are
replicated to give youa high-durability
and high-availabilitywithin that availability zone.It supports hundreds
of thousandsof cores for even the most
massive scale-out computeworkloads.Each Amazon FSx for
Lustre file systemis comprised of file servers
that the clients communicatewith and a set of disks attached
to each file server thatstores your data.Each file server employs
a fast in-memory cacheto enhance performance for the
most frequently accessed data.When a client
accesses data that'sstored in the in-memory
cache, the file serverdoesn't need to
read it from disk,which reduces latency and
increases the total amountof throughput you can drive.The following diagram
illustrates the pathsof a write operation--read operation served from
disk and a read operationserved from in-memory cache.When you read data that is
stored on the file server'sin-memory cache, file
system performanceis determined by the
network throughput.When you write data
to your file systemor when you read
data that is notstored in the in-memory
cache, file system performanceis determined by the lower
of the network throughputand the disk throughput.Ultimately, FSx for
Lustre's capabilitiesmake it faster and cheaper
to process your data sets.You don't want your compute
bottlenecked by I/O. FSxfor Lustre is designed
so that doesn't happen.There is tight
integration with S3,which reduces overhead
of orchestrating movementback and forth between
S3 and FSx for Lustre.There are different
deployment typesfor short-term and
long-term workloads.And it is a fully
managed POSIX filesystem that is compatible
with all Linux workloads.And there are a number of ways
to use Amazon FSx for Lustreto reduce your overall TCO.Amazon FSx for Lustre
offers a choicebetween scratch and
persistent file systemsfor short-term and
long-term data processing.Scratch file systems are
ideal for temporary storageand shorter-term
processing of data.Data is not replicated
and does notpersist if a file server fails.Persistent file systems are
ideal for long-term storageand workloads.In persistent file
systems, data is replicatedand file servers are
replaced if they fail.Because of this
higher data durabilityand higher availability of
persistent file systems,we recommend
persistent file systemsfor all types of SAS Grid data,
including SASWORK, SASDATA,and UTILLOC.There is a lot of
information on this slide,but here are a
few key takeaways.Persistent file systems
have higher availabilityand durability than scratch.Metadata and storage
servers are automaticallyreplaced on failure,
and storage isreplicated within the
same availability zone.I'll talk more about performance
differences in the next slide.Both can achieve
millions of IOPSin sub-millisecond
latencies regardlessof the size of the file system.And encryption of data
at rest and in transitare available for
both file systems.And file systems can be 1.2
terabytes or 2.4 terabytes.And you can have
larger file systemsin increments of 2.4 terabytes.Here, I'm comparing the
performance characteristicsof scratch and
persistent file systems.The throughput that a
file system supportsis proportional to
its storage capacity.File systems scale to
hundreds of gigabytesper second of throughput
and millions of IOPS.It also supports
concurrent accessto the same file or
directory from thousandsof compute instances.Amazon FSx provides burst read
throughput using a network I/Ocredit mechanism to allocate
network bandwidth basedon average bandwidth usage.The file systems accrue
credits when their file systemusage is below their
baseline limits,and it can use these credits
when they perform network dataoperations.So just to recap, we do
recommend using persistent filesystems for FSx for Lustre for
all your SAS Grid libraries,including SASDATA,
SASWORK, and UTILLOC.Today, we're available
in 12 regions,and we continue to launch FSx
for Lustre in other AWS regionsaround the globe.So as an example,
on March 27th, welaunched Amazon FSx in
our Hong Kong region.So if any of you are
familiar with Lustreand have deployed
it, I hope you'llbe amazed at how
simple and quickyou can create an FSx for
Lustre file system is.So let's go to the
Amazon FSx console.So from the AWS console,
in the Find Services,we just type in FSx.From the FSx console, we
select Create File System.And from the Select
File System Type window,we select Amazon FSx for Lustre.Scroll to the bottom
and click on Next.The file system
name is optional.It's just a key value
pair for the resource.So here, we could just
type in SAS Grid demo.Deployment type can either
be scratch or persistent.So again, we recommend
persistent deployment type filesystems for using FSx
for Lustre for SAS Grid.For the storage
capacity, so I recommendselecting or determining
storage capacitybased on the higher of
these two characteristics.First, of course,
it's the amountof data you have to store.So that's very important.You need to have the
file system large enoughto store your data.The second is based on
the amount of throughputyou want to achieve.Because the throughput
of the file systemis proportional to the amount
of storage capacity you have,you'll also want
to set the storagecapacity to determine how much
throughput you want to achieve.So if you select the larger of
the two, you should be fine.So here in this
example, I'm goingto go ahead and select
input 100.8 terabytes.So these are increments in
increments of 2.4 terabytes.The throughput capacity
per unit of storage,which is per terabyte, I'm
going to go ahead and selectthe 200 megabytes per
second per terabyte.That gives me a total throughput
capacity-- aggregate throughputcapacity of 19,688
megabytes per second.As I scroll down, I
can select the VPCthat I want to create
my file system in,the security group that's
going to be attached to my filesystem, and the subnet where
the FSx for Lustre file systemwill reside.I can select the AWS
KMS encryption key usedto encrypt the data at rest.I can also select
the data repositorythat's going to be
used for integration.So this is optional.And this would be an
S3 bucket that willbe linked to the file system.So when the file
system is created,any objects in
the S3 bucket willbe created as files
and directorieswithin the file system.Now, and when the file
system is created,only the metadata is
created for those objects.Upon first access,
FSx for Lustrewill transparently
get the data from S3and copy it over
to the file systemand then deliver
it to the client.So after that, any subsequent
access to that datais going to be delivered
from FSx for Lustre.So it really
simplifies the movementof data between FSx
for Lustre and S3.Also, if you write new data or
make changes to existing files,you can export those
changes back to S3with a simple data
repository task, Export API,which we have.So again, the movement between
S3 and FSx in either directionis very much simplified
with FSx for Lustre.I'm going to leave
that as none for now.Next, we have the
maintenance preference.So this is the 30-minute
window each weekthat maintenance could be
performed on your file system.So we don't take advantage
of this every week,but as an example, if there
was a critical patch thatwas released for
your file system,then we would take advantage
of this 30-minute windowthat week in order to
patch that file system.So something to keep in mind.And you can go ahead and set
the start time of the 30-minutewindow--the 30-minute weekly window
here within the setting.I'll go ahead and click on Next.I can review all the different
attributes of my file systemand click Create File System.So within a few minutes, I
will have a 100.8-terabyte filesystem ready for me and ready
for my SAS Grid workload.So now that we've selected
our recommended filesystem for SASDATA,
SASWORK, and UTILLOC,we need to make sure that
we select an Amazon EC2instance for the
compute nodes in orderto achieve the file system
performance recommendations.With over 260 different
Amazon EC2 instancetypes to choose from,
you may be lookingfor guidance on which
instance families best alignwith these recommendations.Aligning these recommendations
with EC2 resourcecharacteristics is critical
in selecting the right EC2instance family or type.So which one do you choose?To help you with this selection,
I ran a series of highlyparallel network throughput
tests from individual EC2instances against a
100.8-terabyte FSx for Lustrefile system which had an
aggregate throughput capacityof 19.688 gigabytes per second.IOR was used to
generate write activityto the file system
using parallel threadsand direct I/O. So I was
bypassing I/O buffers.These tests were run in multiple
AWS regions-- so northernVirginia, Ohio,
Oregon, and Ireland.Using multiple EC2 instance
families, identified here,I ran these tests.So based on these tests,
I narrowed down the fieldto five instance families
that I recommend--the i3en, the m5n, the
m5dn, the r5n, and the r5dn.So those are the
instance familiesthat align with the recommended
minimum requirements for SASGrid, both for network
performance and memory.If we look closer to these
five instance families,I ran an IOR write test using
multiple threads against allthe instances in all
of these families,all using Amazon FSx for
Lustre as that back end storagesystem.So to achieve the maximum
throughput per instanceaccessing that persistent
FSx for Lustre file system,we recommend using
multiple threadsor run multiple jobs
per EC2 instance.We have to work within
the laws of physics.FSx for Lustre is a high
performance file systemaccessed over the network.There is added
latency when accessingstorage over the network
compared to local disks.So in order to achieve
the levels of throughputwith Amazon FSx, you need to use
multiple threads per instance.If your workload
is single threaded,you may consider using
instance familieswith instant store, local,
and VME disks for SASWORKand UTILLOC, as you can achieve
these levels of throughputto local and VME drives
when using single threads.Because you're not
in the businessof running large-scale,
cloud-optimized parallel filesystems, we recommend
using Amazon FSxfor Lustre persistent file
systems for all SAS Grid,including SASDATA,
SASWORK, and UTILLOC.This allows you to
focus more on runningyour business and the
SAS Grid applicationand less on managing the
high-performing file system.Your goal with selecting a file
system for your SAS deploymentis to make sure you get
consistent low latencies,high throughput,
and millions of IOPsso your SAS jobs complete
within the expected time frame.While there are
other storage optionsfor running SAS Grid on AWS,
like Amazon Elastic filesystem or Do It
Yourself's or the DIYfile systems using Amazon
EC2 and either instant storevolumes or Amazon Elastic
Block Store or EBS,or even third-party
storage solutions.These offerings add
cost and complexityand may impact
performance, availability,and data durability.We recommend Amazon
FSx for Lustrefor its ease of use, quick
deployment, performance,simplicity, availability,
and durability.We also recommend using m5n
and r5n Amazon EC2 instancefamilies for SAS
Grid compute nodeswhen accessing these SAS Grid
libraries hosted on Amazon FSxfor Lustre.Thanks for watching,
and have a great day."
13,"JOHN WATKINS: Welcome to
SAS Global Forum 2020.In this demonstration, entitled,
Application, Authentication,and Authorization--Identity Theft Protection
Across the Customer Journey.My name is John Watkins.And I'm an advisor
industry consultantand the global lead for identity
and digital fraud for SAS'sFraud and Security
Intelligence Division.With the explosion of fraud
into the digital space,and the shift to identity,
it's imperative to addressthe legitimacy of a
customer's identity upfrontand in real-time.We'll discuss the need for
both internal and externalcontextual data in
real-time and howSAS's new offering, SAS Identity
Enrichment and Assessmentcan help bring it all together.So in this ever-changing
environment,it's critical now to
make sure that you'reprotected right up front
in real-time validatingthat the identity presented
to you is authentic.Keep them out of your ecosystem.So how do you do that?As with any problem, it
starts with the data.Getting the right contextual
data at the right time rightupfront where you can
make the initial decision.So how we focus on
that is by leveraginga real-time cloud-based
environment thatprovides a broad and
flexible ecosystem to bringthat enriched contextual
information in real-timeto make those decisions.Now, this could be
internal information,or it could be information
from third-party vendorsacross a broad spectrum of
different types of information.And some of those
are represented here.The first piece may be
the digital identityto understand the device that's
being presented, whether it'sa laptop or a phone or
idevice, to understand how hasthat device been used before.How does that person
use that device?What is the reputation of that
device out on the internet?Second piece of it may
be around biometrics.What's going on on that device?What are the keystrokes?What are the mouse movements?Are their indicators
that this maybe some type of
robotic automationthat's coming in
approaching your company.The third piece of it is
around authentication.What do you know about
the personal informationthat's being presented to you?How do you go outside of the
typical bureau informationto understand the name,
address, the phone number?Does this really
match the person'strue personal information?What do you know about
the phone number?Does that cell phone really
belong to that person?How long have they had
their relationship?And potentially has that
cell phone been ported?Email information.Does that email
correlate to that person,or is this a burner email that
they've picked up recently?And then, finally, machine
learning is critical in this.Whether it's leveraging
SAS models or vendors thatspecialize in this
type of modeling,this is an open
inviting environmentthat companies can bring
either their own vendors.Or they can leverage SAS to
bring in that type of machinelearning.From a discovery standpoint,
the consolidated data modelallows for real-time
processing and allowsfor real-time machine learning
to take place consolidatedon one platform.And a broad variety of those
machine learning capabilitiesare available on the platform,
whether it's supervised,whether regression type
models or classification typemodels, unsupervised learning,
or NT resolution and networkanalytics.All is available in real-time
to help make those decisions.And then, finally, leveraging
the robust SAS fraudcapabilities that
have been honedon the transactional
fraud side, bringingthose real capabilities on
a proven scalable platform.Bringing that speed to market.Being able to develop
and deploy rulesin hours rather than days
in a cloud-based offeringthat can be delivered quickly
in weeks, rather than months.Allowing the customer
build to reduce frictionfor their customers and
reduce fraud losses.So let's take a look at how
we will put that into action.This is a web page
that representsa typical application form
for a financial institutionor an insurance company,
anyone who has a web presenceand are taking applications
on the internet.So it's capturing information,
typical personal informationlike your name, address
city, state, zip, email,social security number,
date of birth, et cetera.As you notice over
on the right, yousee information
that's streaming.This information is
being captured as soonas the web page loaded about
the device that loaded the webpage, as well as things
like my mouse movementand other digital
biometrics thatare occurring based off my
interaction with the device.So this is information that will
be retrieved from the web pageand will be sent
over to our servers.That information
will then be sent outto multiple different
data providersin real-time requesting
information being sent backabout the device about the
personal information broughtback into the SAS Identity
Management System.Then processed.Brought into one
consolidated datamodel and run through
a series rulesto make a risk-based
decision on whether or notthis is a good customer or not.So we're going to go ahead
and pre-populate a persona.This is going to be Mr. Joseph
Hudson at 1000 Ravenswood Drivein Raleigh, North Carolina.So let's go and
submit an applicationand see what we know
about Mr. Hudson.All right?So this came back as approved
with no significant riskconditions detected.So let's look at some
of the informationthat we got back from him.Let's start with our
digital identity provider.So understand, so this is
right outside of Raleighin Cary, North Carolina.He came back, his
device is a pass.This created a cookie
based device IDthat sits on his laptop, but
also set a fuzzy ID that'sbased off his device core.Brought all sorts of
information in that setupa non-cookie based ID.Trusted device.He's running a Chrome
browser with Windows 10.Screen resolution of 1920.Believe or not, this is
very predictive element,screen resolution.Bots don't really care
about screen resolution.All sorts of
information about howthis device has been seen in
information around the networkwe'll look over.This information is brought
back in real-time through JSON.So you look at the
response time here.All of them in the
200 millisecond range.A little bit of lag there
with the biocatch data.Let's take a look
what we had here.This is looking at
digital biometrics.Overall, pretty good rating.It's picking up that this was
a long session for my laptop.So I've been logged in a while.So it did pick that up.So slightly elevated
score based off my demohere, but nothing
to worry about.Let's look over at
secure, which gave usback information
about first we'llstart with the KYC information.Name, city, state, zip,
social all matched.Very strong correlation to
name and email, name and phonenumber.Very low email risk
and phone risk.So Mr. Hudson looks
like a good guy.So let's take a look at
a another persona here.This is going to be
Mr. Boris Hanson.And let's take a look
at what he brings us.Oh my, Mr. Boris declined here.So KYC fails.Let's jump right on
over here and lookand see what were some
of the issues here.So it looks like the
basic KYC matched here,but he had a 99 on
the fraud score.So his initial evaluation,
we've got some challenges here.Let's look deep into those.So his name and
email correlation,so his name and
email didn't match.His name and phone didn't match.So we'll dig deeper
into the email risk.So the registered name
doesn't match the input name.This is a high-risk email.It's an auto-generated, so this
was a burner email looks like.Also on the phone, the name
doesn't match the phone.And this phone
line was in servicefor less than seven days.So potentially,
this is a prepaidthat was just picked up.So from a risk standpoint,
this is probably someonethat you want to cue
out and validate.While his name and all of
his typical KYC informationwill match, he's trying
to hide something.His contact information
does not match.And this is a good
person that you'regoing to want to evaluate.So let's go back over here
and let's look one more timeat Mr. Hudson.So we'll bring him on.And let's pretend that we're
working from a spreadsheet.We're a crook that has
a list of his accountsthat he needs to work through.So he is copying and pasting
from that spreadsheet, whichis what crooks do.They sit there in a room
and work their spreadsheetthat their boss gave them.And they'll go through
and load these accounts.So we're going to
submit this application.Now, you notice this person
was approved earlier.Now, they're declined.Device biometrics high-risk,
expert keystrokes, anomalies.Let's look back over
here at biocatch.Let's actually look at the
JSON they sent us back here.Criminal behavior detected.Expert user.Low familiarity.Elevated risk score
based off of this.So based off our rules, we
detect these expert keystrokes.This is someone that we'd
want to review based offof their activity here.So as you see, the value
of the different datasources in real-time, the
broad variety of information,and the ability to
combine these to makemultiple risk-based
decisions right up front.So that concludes
our demonstrationI'd like to thank
everyone for their timeand hope you enjoyed
this discussionaround authentication
and identity fraudin this demonstration
of SAS identityenrichment and assessment.If you have any questions,
my email is above.And you could find more details
at sas.com/fraud under identityand digital fraud.Thank you and have
a wonderful day."
14,"SEBASTIAN CHARROT:
Hi, I'm Seb Charrot,and thanks for joining me.Today I'll be demo-ing SAS
Intelligence and InvestigationManagement.Specifically, I'll
be showing youhow it can be used to
operationalize intelligencein the police force, by
allowing seamless transferof information from
desk-based staffto officers in the
field and vise versa.But first, some orientation.This will be a
split screen demo.On the left hand side,
you'll see softwarefrom the point of view of a
desk-based analyst, someonesitting in a station,
gathering lots of information,and then pushing that out
to other users in the field.And on the right
hand side, you'llsee our mobile application
from the point of viewof a field-based
operational staffgoing about their
day-to-day business,accessing information
and pushingtheir own into the system.So without further ado,
let's start the demo.So we'll log into our
desktop software first.Now, this scenario is
that of a missing child.So for the sake of
argument, let's saythat the call came in and
was handled by our commandand control center,
and a two-man carwas dispatched to talk to
the missing child's mother.So this is us entering
the informationinto our intelligence system
to see if we can aid and gatherinformation which could
help the investigation.The child's name
is Emma Cameron.She is nine years old.We have a photo and
other informationabout her last
known whereabouts.She was meant to return home
at lunchtime but didn't arrive,and this was confirmed
by the schoolthat she did in fact leave.So we're entering
everything we can here.We're entering her last
known location, whichwas her school, Kirkland High.And we'll fill all this out.Last person to see her was
her teacher, Karen Rogers.And we'll enter the
school's address here,which we've plotted on a map,
as well as a witness statementfrom the last person to see her.So with that, the
missing report--the missing person report
is already in the system.Now our next step is to create
an investigation from this,so that we can
start actioning someof the investigation
we want to do,and tasking
individuals for that.So we'll fill out a
few necessary fieldsand we'll task it
to my mobile user.Now that's created
and the investigationhas already been tasked out.So on our mobile device,
we can log in now,and let's say we're one of
the members of the two-man carwho've been dispatched.I can take a look
at my tasks and Ican see that indeed, the
investigation which was createdhas been assigned to me.So I can draw onto that, take
a look at the investigationitself, which is understandably
threadbare given it's justbegun, but from
there I can launchto take a look at the
missing person report itself,which has a lot
more information.In fact, the first things
that come to my attentionare a couple of warnings
that are being highlighted.This is because there is
already existing informationin the system which is being
highlighted to me here.So there is a potential
history of domestic violence,and the missing person is
vulnerable given their age.And the rest of this
information is at my fingertipsand accessible to me, so I
can appraise myself of allthat while I am traveling to
the location of the interview.So in fact, I see the
neighborhood is Burleith.While we're en route, while
my partner is driving,I can actually just do a bit
of proactive searching myself,just to get a better
sense of the situationwe're heading into.So we'll perform a neighborhood
search for Burleith.This will search for information
reports raised recently relatedto that neighborhood.And in fact, I can
drill down even furtherand take a look at only the ones
that were raised this month.So we have a few reports.In fact, a couple
of them do seemto pertain to
drug-related activity,so that may be something
worth considering.These sorts of things
often are risk factorsfor children in the area.Back at the desktop,
we want to start doingsome desk-based research here.So firstly, we'll search for the
address, which is 4434 SpringLane, and it is in
the system so we'llplot that on a map to get
a better geographical senseof where we are.Now we'll do a 1 kilometer
radius search from that point,searching for really anything
that could be of interest.So this will be a deeper
search, a broader search,than we did from
our mobile phone,and I can see a
number of results.In fact, a couple of things
leap out immediately.We have two registered
sex offenderswithin a 1 kilometer radius
of the missing person's house.One is Suzanne Paris and
the other is Fred Banks.So those are two
individuals whichI'd be very keen on finding
out more information about.In fact, I plotted
the locations on.You can see how close they were
to the school and household.So I'm adding them
to a workspaceto make sure I don't
forget about themand don't lose track, and
later we'll actually link thatto the investigation so
that other users can seethose links that I'm making.Now the source of information
I'm looking at hereare information reports, four of
which are in the same location.In fact, if I overlay
the address on top of it,I can see that the
information reports pertainto the actual household.So there's a long history
here of information reports,ranging through some
fairly serious thingslike domestic abuse.So we'll also add that
to my list of thingsto keep track of, and we'll
link those later as well.In fact, if we plot
those on a timeline,we can get a sense for
the escalating natureof the abuse in the household,
obviously culminating nowin a missing child.So although I'm not getting
any concrete answers here,I am augmenting my
understanding of the situationand finding out potential
avenues of investigationthat we should be researching.And I can take it
into more depthif I wanted to, to look
at the meat of someof these information reports.This one relates to an assault
charge at that location,and I can see Janet Cameron,
who was the person who firstreported Emma's disappearance.So I could take a look at
Janet's record if I want.I can see the things
she's linked to.Probably, I am more
interested in finding outabout Paul, who
seems to have beenthe abuser in these situations.I'd like to take a look
at his record as well,see any information
we have on him,and also take a look
at any other objectsin the system that
are related to him.So I see that he is
the owner of a vehicle,and that maybe will come
in handy in the future.So back in the field,
from our mobile device,we'll take a look at the
missing person reportand see what the outcome of
some of that desk-based researchhas been.And so these have been
updated by staff periodically,and so I can see some
of the informationthat we've discovered here.I can see the news about the
sex offenders in the vicinity.I can see the history
of domestic abuse,and I can see some of
the neighborhood riskswhich I also uncovered from
some of my own research.And we can also see
that we've linkeda number of the
information reportswhich were discovered during
some of that research,as well as the two
known sex offenders.So I have that access
now to the information.So I'm fully
appraised, and in fact,that's the key thing
about the system, is,the seamless stream
of informationback and forth is keeping
everyone up to dateand able to do their best
work, to maximize their impact.Now we're at a
decision point here.We obviously have a couple
of people we want to talk to.So what we're doing here
is creating some tasksthat are spawning off of the
main investigation, whichcan be handled concurrently
while other interviews arehappening.So we'll be
assigning other staffto interview Suzanne and Fred
regarding Emma's disappearance.And those tasks actually will
appear to all mobile usersin the target group, which
in fact, my mobile user isa member of.So he can see the tasks
that's been sent out,with all the details
and instructionson what information
is being sought after.So let's assume that through
the course of our interview,we actually discover
some information that wewant to raise into the system
and feedback to our colleaguesin the station.So we'll create a
new update ourselvesand enter a discovery
we made regardinga confrontation last night
between Emma's parents.The argument was about
visitation rightsof the children.Moreover, we also
discovered a batchof letters, correspondence
between Paul and Emma,which Emma's mother
hadn't been aware of,and we were able to take
a photo of the evidenceand attach that as well.So we'll do that right now.And the benefit of
this is obvious.The only alternative would
have been to take a photoand later upload it back in
the station, whereas this way,it's available immediately to
the staff in the office, whocan then choose to
research that or derivethe next course of action.So indeed, if I refresh the
missing person investigationback in the station, I
can see these updates,which are crucial in such
a highly dynamic situationsuch as this, understanding the
latest information as soon aspossible, as close to
real time as possible.So Paul Cameron is
definitely becoming somebodythat we want to find out more
information about and reachout to as soon as possible.So we'll find his records,
which we were alreadylooking at earlier,
and we can takea look at all the
information that's there,but what I'm really
interested inis his vehicle which I
remember seeing earlier.So we have his license plate.What we're going to do is
take that number and searchin our system for
any number platerecognition that's within
the last day in that area.So we've done a 10 mile
radius from Emma's house,and we can see a couple of
hits on ANPR, one of whichseems to be very close
to our area of interest.In fact, we'll use our
same trick as earlier,do a one kilometer
search from that point,and validate my inkling
that it's actuallyvery close to all our other
activity, and in fact it is.So you can see at
the bottom left,we have the school in
question, and Emma's houseat the top right, and right
in between is the ANPR hit.In fact, if we zoom
in, it is exactlyat the point at which Emma's
route home would have been.So I'm going to
take a photo of thatand I'm going to add that
to our updates as well.So we have our ANPR
hits plotted here,and now what I
actually want to dois annotate this
to show the route,and then better inform my
colleagues in the field,and I'm actually going to ask
them to maybe walk that routesee if they can
find anything, seeif anything seems out of place.And on my mobile device,
if I reload the record,I'll see this latest
information with the routeand the ANPR hits as well.So let's assume that our
field-based officers takea look at that route
using this information,and in fact, they do discover
a couple of pieces of evidence.They find a bag, which
they later determinedthat belonged to Emma.So we'll take a
couple of photos of itin situ before we
touch the evidence,then we'll immediately add
that to the investigation,so that, again, our
colleagues can beappraised as soon as possible.Now, this is obviously a
very time-critical situation,and this latest
piece of evidenceonly serves to augment
that perception.It's becoming clear that
we very rapidly needto be in touch
with Paul Cameron,and it strikes me that we
only did a search for 10 milesthe last time we searched
for ANPR, and in fact,broadening that search does
reveal a third ANPR hit.So the trajectory is clear.It's heading north,
away from Emma's house.And so we really do need to
get in touch with Paul Cameronand work out if he has any
information that could help usin our investigation.So what we're doing here
is recording the factthat we did dispatch another
car to intercept Paul's carand see if he has any
information that could help us.And that is the end of our demo.I hope it's been illustrative
and demonstrating a coupleof key important facts.Without the station and their
ability to collate and parsehuge amounts of
information quickly,the officers in
the field would nothave been informed or
equipped to carry outthe responsibilities, and
without the informationcoming from the in-field
officers and the actions theywere able to take,
the station would nothave been able to make decisive
changes in their courseof action because they wouldn't
have had the informationor evidence in a timely manner.So decision making
has been improvedby allowing the seamless
flow of informationback and forth between
frontline and desk-based users.If you have any
questions, pleasedon't hesitate to get in touch.Thank you for
watching, and goodbye."
15,"JACKIE YANCHUCK: Hello, and
welcome to my SAS Global Forumpresentation, The Secret
to Successful CECL ModelImplementation.My name is Jackie Yanchuck.And I am a data scientist
at SAS Institute.In my previous role, I
was a Solutions Advisorin the Risk division at SAS.In that role, I
focused primarilyon Model Implementation
for financial institutions.I, along with my
co-author Monica Wang,have implemented a wide
range of CECL methodologiesfor various financial
institutions.In these
Implementations, we haveseen a variety of
different challengesarise that we have to face.This presentation covers
those challenges, as well asrecommendations on
ways to overcome them.Our recommendations cover both
the design and implementationphases of Model Development
and Model Implementationand also include some
best practices for writingModel Implementation code.The Implementation and
best practice sectionsare geared towards the SAS
Model Implementation Platformsolution.This solution has a wide variety
of benefits for CECL ModelImplementation that we
will discuss furtherin this presentation.So let's start with the
challenges that are oftenfaced in Model Implementation.These challenges typically
fall into the categoriesof either data gathering
and preparation or coding.Data quality checks are
essential to ensure data isready for use in CECL models.Most source data will
require some level of effortto impute missing values
or correct outliersand erroneous values.Additionally, different types
of data joins must be utilized.For example,
different portfoliosmust be joined vertically
for reporting and disclosurepurposes that may not have
been brought together before.Horizontal joins are needed to
bring together loan informationpotentially from
disparate sourcesystems along with economic
data and any other dataneeded for the Model
Implementation.Once data challenges
have been addressed,a large amount of
code must be written.This includes forecasting
logic, as well aslogic to perform sensitivity
analysis, attributionanalysis, and back testing.These pieces of logic need to
be written for each portfolio.And the resulting code is
often lengthy and repetitive.The code can also be
very resource intensiveif it's not optimized.Keep in mind it can potentially
run for millions of loansacross hundreds of horizons
and multiple scenarios.If optimization of the code
is done, that in and of itselfcan be a lengthy process.The coding challenges don't
end once the code is written.Auditing must occur on this code
by internal Model validationteams, external auditors,
or potentially both.And that lengthy,
repetitive codecan be time consuming for
the auditors to review.It will also be very difficult
for end users to utilizeand for any other users
maintaining and updatingthat process.Now that we discussed some
of the challenges thatwill arise in Model
Implementation, let'stalk about how to address
them starting with design.Many of the challenges
discussed in the previous slidecan and should be addressed in
the design phases of both ModelDevelopment and
Model Implementation.Decisions made in model
design have effectson Model Implementation.For example, data
preparation, model creation,and model execution code
should be separated outin Model Development.This is because only
data preparationcode and model execution
code will be utilizedin that production cycle.If these pieces are
intertwined, additional effortmust be taken to separate them.That effort would be
unnecessary if those pieces wereseparate in the first place.Additionally,
variable availabilityshould be considered
in Model Development.For example, variables
such as the occurrenceof a natural disaster may be
highly predictive for defaultprobability but may not be
available in the productioncycle.Process automation should
also be considered.Some data may require
manual preparationfor Model Development.It should be considered how
that manual preparation willbe automated for the
CECL production cycle.It might be necessary
to redesign or removethat data preparation component
so that it can be incorporatedinto the overall CECL process.Data sourcing may differ for
Model Development and ModelImplementation.If that's the case,
thought has to begiven to how the data will be
brought into the CECL cycleand if any processing
must be done to that dataso it aligns with the
Model Development data.SAS Model
Implementation Platformhas many benefits when it
comes to Model Implementationdesign and coding.It reduces the amount of code
that needs to be written.The only code that
needs to be writtenis to extract data from source
systems and forecasting logic.No code needs to be written
for combining input loandata with economic data or
combining different portfoliosoutputs.It has standardized templates
to run attribution, sensitivity,and back testing analyses.Code within Model
Implementation Platformhas the ability
to be locked downto any further changes while
also being fully transparent,which is perfect for auditing.Because of its reduced coding
effort and friendly userinterface, usability
and maintenancecan occur very easily.Now, while Model
Implementation Platformsimplifies many of the
CECL Model Implementationcomponents, there are
still important questionsto consider when performing
the Model Implementationdesign in order to best
utilize the solution.Model Implementation Platform
refers to individual equationsas model objects.A model group contains
model objects as well asthe code that uses the model
objects to calculate outputs.Now, there are two
types of model groups--scoring and evaluation.Aggregate level
methodologies canbe incorporated into
scoring model groups,whereas loan level
methodologies can be runusing evaluation model groups.This allows aggregate
level calculationsto execute for each
distinct aggregate levelvalue like customer
ID for greatercoding and runtime
efficiency than if theywere run for each loan.If portfolios have
similar methodologies,but individual models
have different variablesor equations, the same model
group can be used to run them.The code to perform
the segmentationcan be incorporated
into the model group.So each appropriate atomic
model is called for each loan.If a methodology uses
transition matrices,consider whether the
transition matrixentries are static
values over time,or each cell is an
equation that mustbe executed in each horizon.SAS Model
Implementation Platformsupports and streamlines
coding for both typesbut in different ways.To source additional data
besides snapshot loaninformation and
economic data, data setscalled risk data
objects are used.To incorporate
static lookup tables,utilize parameter matrices.To incorporate low
level informationthat changes over time
in a predefined manner,use value data sets.For greatest efficiency,
Model Implementation Platformseparates model variables
into three different types--static, variables that
don't change over time,such as origination
credit score,age index, variables
that change over timein a predefined manner,
such as a loan's age,and simulation,
variables that changeover time in an unknown
way such as monthssince last delinquency.Classifying variables
appropriatelytells Model Implementation
Platform when to calculate themso that their values are
only calculated as needed.Model Documentation
is also important.It should include information
on the methodology, as well asany complex variable
calculations.This expedites the
ramp-up on the CECL Modelsfor auditors and
users and new users.Finally, consider what
output variables willbe needed in different stages.For example, the
production cyclemay require fewer
output variablesthan testing or ad hoc analyses.The more variables
that are output,the longer the code
will take to run.So be sure you're specifying
within Model ImplementationPlatform which
variables should beoutput to best suit your needs.Once these Implementation
questions are answered,the code is ready to be written.When writing code within
Model Implementation Platform,these are some best
practices to follow.Efficiency is very important.While Model Implementation
Platform is itselfa very efficient
solution, since codewill be executing for
the life of every loanin your financial
institution's portfolio,it's important that the
code is written as efficientas it can be.Minimizing string
comparisons, the usageof the dynamic_array subroutine
and the INTNX and INTCKfunctions are all easy ways to
make your code more efficient.Model Implementation
Platform alsogenerates system variables you
can leverage to make your coderun more efficiently.For example, the
variable terminate_repwill stop the execution
for a given loanwhen its value is set to 1.This will eliminate
unnecessary calculationsas not all loans will
mature at the same time.One more tip is to read entire
rows or columns of parametermatrices rather than
individual valuesif multiple values from
the parameter matrixwill be needed.Other best practices are
important for ensuring expectedresults and streamlining code.Clearing arrays, resetting
temporary variables,and checking return
codes are allimportant to make sure your
code is functioning as expected.Model Implementation Platform
has a transition matrix objectwhich stores multiple
model objects.One trick is to use the
transition matrix objectto call multiple models at the
same time, even when the modelsare not transition models.Users can write functions in
Model Implementation Platformto call common pieces of
logic across model groups.These are also excellent
ways to streamline your code.Utilize value data sets
instead of cash flow datasets as cash flow data sets have
more stringent requirements.For additional details
on these best practices,please see our paper.As discussed, there is so much
to consider when implementingCECL methodologies.While Model Development
is very important,the Implementation
of CECL Modelsas part of the production cycle
should not be an afterthought.Model Implementation
requires appropriate timeand consideration to be
done in the most efficient,transparent, and
auditable way possibleas CECL goes into effect.Thank you so much for watching
my presentation today.For more in-depth
discussions of this topic,please see the paper that Monica
Wang and I wrote on this topic.And please feel free to
contact us with any questions."
16,"Hello, everyone.Thanks for listening to the SAS
Global Forum 2020 super demo.I'm Zohreh Asgharzadeh.And in this demo,
I'm going to showhow we monitor bee
activity access at SASbeehives using IoT
and computer vision.So let's first review the
motivation behind this work.One out of every
three bites of fooddepends on bee pollination,
most often honeybees.By 2050, we need to
produce 70% more foodto feed the growing
world population.As a matter of fact,
enjoying world food securitywill be impossible
without honeybees.On the other hand, the
death rate of bees in the UShas been on the rise.Last year, we hit a new record.We lost more than
40% of our beehives.So beekeepers need to inspect
hives more frequently.And a lot can happen
between two inspections,like death of a queen.So any non-invasive
tool that canhelp beekeepers learn about
possible problems in the hivecan be useful.So we have a very important
problem to tackle,which is saving bees.We have some beehives
inside this campus.And we have the power of
analytics to study these hives.Nothing can stop us.So as a result, a team
of bee enthusiastswere formed to study
SAS's beehives.But the focus in this demo
is the computer vision aspectof this project.The amount of flight
activity from hivescan tell us a lot about
foraging activity,since bees that are exiting
and returning to the hivesare typically foraging for
nectar, pollen, and water.A lot of flight activity
implies the hive is healthy.By monitoring bees in
flight, we are alsoable to notice and notify
the beekeepers about swarms.Swarms indicate the
hives are healthyand they're reproducing.One way to monitor the
flight activities of the beesare through monitoring
the count of the bees.But bees are tiny,
and they fly fast.Well, we have SAS event stream
processing and SAS streaminganalytics tools to
overcome this problem.And the great news is
no labeling is required.So we do this through
unsupervised learningprocesses.Now let's review
that process flowfor monitoring the count of bees
entering and exiting a hive.The input to this process is
the streaming image framesfrom a fixed camera at the hive.This process has two phases.In the first phase,
we use SAS Viyato train an are
RPCA model which isused for separating the
foreground, which is flyingbees, from the background.To do so, first
we crop the imagesto limit the area around the
hive that we want to study.Next, we resize the images
to a manageable size.Then we flatten the images
and fit it to the RPCA model.We store the ASTORE file
generated from this RPCA modeland use it in the next phase.In the second phase, we process
the streaming frames usingSAS Event Stream Processing.On each frame, we followed
the same pre-processing stepsas before.Using the ASTORE generated
in the previous phase,we extract the foreground of
flying bees from each frame.Then we use Connected Component
Analysis to locate the beeson each foreground image.Finally, we use ESP Object
Tracker and AccountingLogic to count the
incoming and exiting bees.So now I'm going to open a
Jupyter notebook to walk youthrough the phase-one
steps in SAS Viyato pre-process the images
and train an RPCA model.And at the end, I will
briefly show an ESP dashboardfor counting the incoming
and exiting bees.But you will get a full
demo of the ESP partin Tech Connect and
other SGF demos.So let's go through
first initial things.So we import this
swat package in Pythonto have access to
any CAS action setsand also some other
Python libraries.And then we connect
to CAS server.We provide the server name,
the port, the user name,and whatever else
that's requiredto the build connection.And we needed three action sets
in this demo, the robustPcaaction set, image action
set, and aStore action set.So this is the video that
that's actually being streamed.And we are going to use this
in this demo as an input.So you saw this with your
before in the previous slides.I'm going to play
it one more timeto just show you a
point that if you trackthis bee, this bee
on the top, and thenif I go play the
video link back,and then you can see
that if I stop the videoand if you have a frame,
it's hard to visually detectthis bee.And fortunately, for
training our RPCA model,we don't need any label data.So it's a totally
unsupervised method.So that's one of the
advantages of RPCAthat we don't need
anybody to manually labelthese images for the bees.So let's go back.And what we are
feeding to this processis basically frames,
frames that arecoming from the camera
located at a fixed locationnext to the hive.So first we have
to load the images.So we define a CAS
library first and point itto our images, location of
our images in this path.And then we load the
images, our training images,to this library.In this demo, we used the
first 500 frames from the videoto train the RPCA model.And then we have to do some
processing on the images,as I explained before.So the original dimension of
the frames are 720 by 1,280.So by cropping, we bring down
the dimension to 440 by 1,280.So we reduce the
dimension on the x-axisbecause we don't need the
whole frame for processing.We just need the part of frame.So we use the
processImage actionthe image action set and use
the function type GET PATCHand specify these dimensions.So if you look at the
results, our cropped imageswould look something like this.And if you compare it
with the original frames,you can see that the cropped
area is the middle area somehowso that we can count the number
of flying bees towards the hiveand out of the hive.So after the
cropping step, we doconvert the images to
grayscale because wedon't need the, basically,
extra input for color imagesto the RPCA.And grayscale images
would work just as fine.And also it brings down
the computational effot,especially this is
important for ESP,because ESP has to process
the images on the fly.So we use, again, the
processImage action.We use-- this time we use the
function type CONVERT COLOR.And it converted to
grayscale, color to gray.So this is the
result of convertingthe images to grayscale.And next, we resize the images.We don't need to work with
the high-resolution images.Again for the purpose of
the computational effort,we bring down the dimension from
both dimension x and dimensiony to half, so from 440 to
220 and from 1,280 to 640.And we use this using
the RESIZE function typein the processImage action.Next we have to fit these images
to an RPCA model as a table.So the rows of these tables
represent the images.So each row is associated
with one frame.And the columns are the
pixels in each frame.And so to do that, we need
to use the action that'scalled flattenImage.So this is a syntax
for flattenImageand the fitted images to this
flattenImage image table.And we provide the width
and height of the images.And it basically converts
them to that table.Next we fit the table
to an RPCA model.So I take the opportunity
to briefly introducethe RPCA model.So in a nutshell, RPCA model
decomposes an input tableinto a low-rank table and
sparse table to an optimization.So in our case, the input
table would be images.And the low-rank table
would be the background.And the sparse table would be
the foreground of flying bees.So the we fit flattened
table, flattened imagesto the RPCA model.And we did some
tuning and set oneof the parameters
of the RPCA model,which is the most important
parameter, lambdaWeight of 1.1.It's basically-- lambdaWeight
is a coefficient thatbalances how much sparse we
want the sparse metrics to beand how low-rank metrics
we want it to be.And then we set the results
into a saveState folderor ASTORE file.And we call it Store.Next we need to download
this ASTORE fileso that a ESP can use it.And we do this through
the download actionof the ASTORE action set.And we put it somewhere
so that the ESPcan grab this binary blob file
to the scoring of the images.So now I'm going to demo
you in SAS Viya the resultof the scoring of these images.And then later on I'm going
to show you the ESP dashboard.So we have to do the same
process on all the imageframes in this step.So we had about 2,400 images
in that original videothat you see.We did train on the
first 500 images.And now we score on all images.And you can see
that we have to dothe same pre-processing steps
for scoring images as well.And then we do the total scoring
through the call to the scorefunction of ASTORE action set.And we store the results.But to be able to
visually see the results,we have to condense the
images, which is basicallydoing the reverse processing
that we did under images.And also we need
to save the images.So this is basically the
foreground of the flying beesfrom that the original video.And as you can see, the
background is all removed.And when we have
images like this,the Object Tracker can
easily track the beesand then do the final counting.So here is the recording
of ESP dashboardin order to briefly
show you this dashboard.On the right, you can see that
the foreground of the flyingbees is displayed.And in the middle, there you
can see bees are detected.And Object Tracker
tracks their movementand then labels them as
incoming bees or departing bees,arrivals and departures.On the right, you
can see the countsof the arrivals and
departures of the beesover a pre-specified
period of time.I guess in this
case it's 3 seconds.And here the frames are
fed to the ESP systemat the rate of 10
images per second.At the end, I would
like to mentionthat there are other
aspects to this project,like analyzing the acoustic
data from the hives,that you might have
already heard or readabout in this SGF.Also, I would like to
thank so many collaboratorsof this project
and special thanksto Anya McGuirk and
Brad Klenz for helpingwith this super demo.I hope this
presentation motivatedyou to plant more flowers,
trees or shrubs to provide beeswith pollen and nectar.Thanks for watching."
17,"Hi.Casey Smith here,
with SAS R&D. TodayI'm going to show you
what's new in SAS Studio.We've got a lot of content to
cover so let's jump right in.Today I'm going to be
covering SAS Studio 5.2.5.2 shipped in
November with Viya 3.5so it's been available
for a few months.It is a minor version increment.Our previous version
was 5.1, but 5.2packs some major new
features, many of whichI'm going to demo today,
including the new queryinterface, the Git integration,
import data wizard,reintroduction of file system
navigation, the DATA stepdebugger, job integration,
both scheduling and backgroundsubmits, and last but not
least, quick filtering.There's a number of compelling
features that are new to 5.2that I won't get
to today, includingnew and enhanced tasks.So I highly recommend
checking outthe What's New in SAS
Studio 5.2 help topicin the User's Guide of
the online documentation.You can access that
from the Help Centerfrom within SAS Studio.So let's get straight
into the demo.Here I've got SAS Studio
5.2, and the first thingyou'll notice is this new
start page which welcomes youinto the application.In fact, I'll use that
to start a new query.When I click New Query, you'll
see a new query tab opens up.I can select the Add button
to select data sources.However, I prefer to multi
select tables in the librariespane on the left hand side,
and drag them into the query.Once I drag them in, you'll
see those tables are added,and then I can expand
them and select variablesthat I wish to add to my query.I'll select a few
from the first,and I'll select a few
from the second table.On the Join tab, you'll
notice that a join has alreadybeen identified
for me because itrecognized those variables with
the same names in both tables.I can choose from one of
the pre-defined join types,or if I wish to provide it
advanced join expression,I can do so in the Join
Expression Builder.On the Filter tab, I
can drag a variable overand specify a filter.I've got a number
of comparitors.I can type a value or I can
retrieve the distinct valuesfor this variable,
which I'll do.And apply that filter.And then if I wish to
sort by a variable,I could do so, or
provide output options,whether I want to create
a table, a view, and whereI wish the output to land.If I look at the
code, you'll noticeit's generating PROC SQL,
but the new query task alsosupports PROC FedSQL.So if I go to Options,
Preferences, Query, you'llnotice there's this
Generate FedSQL checkbox which I can
check, and then thatwill result in the query
generating PROQ FedSQL instead.Otherwise I run it
like any other task.I click the Run button, it
runs the code, generates a log,generates the output
data set, whichI can then view at the bottom.So it's a point and click
interface for querying.The next thing I
want to show youis the Git integration
that we added to 5.2.Git was in SAS Studio 3.8
and in Enterprise Guide.We've added very similar
functionality to 5.2.So here I've got blah.sas
program, which I'll open.You'll notice the
folder that it existsin is SAS Programs
has this weird icon.And if I hover over it, it says
version control repository.That icon indicates that
it is a Git repository.And you'll notice there's a
similar Git icon for a pane,for a new Git repositories pane,
and if I select that, you'llnotice the SAS Programs
Git repositories is alreadyopen in this pane.I've already cloned
this repository.I could clone a new one
from GitHub, Bitbucket,or any other remote
location, or Icould add an existing
one that's alreadybeen cloned that may
already live on my server.In this case, I'm going to
work with the SAS Programsthat I already have.So if I double click it, you'll
notice that a new SAS Program'srepository pane is opened.And it shows me that there are
no changes currently unstagedor staged, waiting
to be committed.If I click on the
History tab, itshows me all the
commits that havebeen made in this repository.You'll notice my master is
currently one commit aheadof the remote origin master.So if I go to this
program, blah.sas--and I'll make another change--from ""hello world"" to ""hello
Casey,"" and I save it.And then I go back to
my repository pane,you'll notice when I
select the Commit tab,it automatically detects
that I've made a changeto a program in my repository.If I select that
program, then I geta preview of the
differences that were made.You see where I changed
it to ""hello Casey?""I can stage those
changes, and thenI can enter a commit comment,
""changed to hello Casey,""commit those changes.And then if I look
at the history view,you'll notice that my
local master repositoryis now two commits ahead of
the remote origin repository--origin master.And if I select
it again, I stillhave the preview at the bottom.At this point, I could
push my changes to master,or I could pull from master
in case any of my colleagueshave made changes to
programs in this repository.I could also create
branches off of any commitif I wish to work out
of a feature branchto add a new feature
or fix a bug.And so this is really the Git
integration in a nutshell.It is a wonderful
way for organizingthe lifetime of programs--
changes to programs over timein a structured way, which
is wonderful when you'recollaborating with colleagues.Next let me show you
the new import wizard.There's a couple
ways to access it.One is from the new menu.You can click New, Import Data.I prefer to find the file
that I wish to importinside the Explorer pane.In this case, gas.xlsx, right
click, and say Import Data.It opens the new import wizard
with the data preselected,as well as some default
output location.If I look at the
generated code, you'llsee that we're
generating PROC Importto do the import, and
then PROC Contentsto report on the imported data.I'm just going to run
it like any other task.It generates a log, the
results of the PROC Contentsand the output data set
that actually gets created.So a very quick and
convenient way to import data.You may have also noticed while
I was showing that featurethat I was navigating
the server file system,and this is something
that was sorely missed.In 5.1 you were only able
to navigate the SAS Contentlocation.In 5.2 you are now able to
navigate both the server filesystem, as well as the
SAS Content location,and store content in either one.The next thing I want to show
you the data step debugger.The data step debugger
is a really neat featurethat existed in SAS Studio
3.8 and in Enterprise Guide.And it works very similar
now in Studio 5.2.Here I've got a SAS program
with several data steps.If I click the Debug
button in the toolbar,you'll see it highlights
the sections of code thatare debuggable, in
this case, data steps.If I click the bug next to
one of those data steps,it launches the
data step debugger,and then pauses on the
first line of execution.At this point I can start
stepping through the data step,or I can toggle
breakpoints, and continue.An execution will break
on those breakpoints.You'll notice on
the right hand side,I see the variables in the data
step and their current values.Any ones that are
colored red, thatindicates that they have changed
since the last time executionwas halted.Rather than doing breakpoints,
I could set a watchon a variable.In this case I'll
do running horses.And now if I
continue, execution isgoing to break every
time this running horsesvariable changes.And, of course, if I want to
enter more advanced debuggercommands, I can do so in the
command line at the bottom.So a very convenient way
for stepping through datastep code and debugging it.Next I want to show you
the job integration thatwas added in Studio 5.2.Jobs in the Viya
world are synonymouswith stored processes
in the V9 world.They are SAS programs
with optional parametersthat do work, and
are particularlyuseful for end
users who may not befamiliar with SAS programming.So let's start
with some examples.Here I've got some jobs.I'm going to open one
up called Hello World.And you'll see it
consists of code,in this case a simple DATA
step that outputs some HTML.And it also has
an HTML form whereit receives some input
that is used in that code.So let's just run it
to see what it does.When I run it, you'll
see it presentsme, the user, the HTML form.I'm going to change the
value and run this code.And now it is taking
that user inputand using it inside
of the SAS code.And you'll see it
simply substitutedin the HTML output that was
generated by that data step.But let's look at a
more advanced one.Here's another job that's
using dynamic prompts.You'll see we've got a couple
macro variables, make and type,and we're using those in a
PROC SQL and a PROC report.We still have a prompt,
but in this caseit's not an HTML prompt,
it's task prompt.And this is common task model
XML, the same format thatis used for writing custom
tasks and SAS Studio,so it's very extendable.If we run it, we'll see we
have the user interface,and it is dynamically
populating the vehicle makes.So if I select Mercedes Benz--and you'll notice now
it is also cascading,so based on my
first selection itis running another query
to populate the vehicletypes for Mercedes Benz.And so once that
has been populatedand I select two values, I
can then submit this job.It uses those
inputs to the code,and then returns the
results to the user.So again, it's a
very convenient wayfor creating interfaces
for end usersthat may not know SAS code.All they have to do
is specify the valuesand they get the results.Another really nice
thing about jobsis that they can be
scheduled very easily.So for example, if
I right click a job,I can click Schedule
Job, and it brings upan interface in which I
can choose a frequencyand a time in which
I want it to run.If I wanted to
create a new job, Icould do New Job,
Definition, or a form.When I click New
Job, you'll noticeI can specify the code that
I wish to parameterize.I can optionally
associate a form with itif I want there to
be a user interface.I can set job
properties, and I candefine parameters for the job.In addition to being
able to schedule jobs,you can also schedule programs.So for example, if
I've got a program.sas,I could right click and
say schedule as a job.And what we are
doing is actuallycreating a job with that
code behind the scenes.It is a non-parameterized
job in this case.And I specify a period or a time
in which I want that to run.Similarly, we have
a background submit.And when I click Background
Submit, we are creating a joband running it immediately,
rather than scheduling it.And when you run
a background job,it does show up in the
submission status pane.And then as soon as
it is done, you'llnotice I got a
toaster indicatingthat it was complete.I have now a program.html and a
program.log in the same folderas my program.sas file.I can then double click those to
open them and view the resultsor view the log.Last but not least,
I want to show youthe quick filter in data sets.So if I open a table and
right click any column header,now you'll notice there's
this Quick Filter menu item.And it looks very similar to the
filtering in the Query Builder.I've got a number
of comparitors.I can specify a value or
get the unique values,and once I select one
and I apply filter,you'll notice the data grid
is then filtered to recordsthat match that criteria.There is a breadcrumb,
and I can close that,remove that breadcrumb, and
then the filter goes away.So this is a quick rundown of
some of the new features in SASStudio 5.2.I hope you will have time
to play around with it.And I highly recommend,
again, going backto the What's New
topic in the UserGuide for more information.Have a wonderful day."
18,"ARTHUR LI: Hello, everyone.My name's Arthur Li.I'm a biostatistician working
at City of Hope National MedicalCenter.In this presentation, I'm going
to talk about SAS DATA Stepprogramming.The contents from
this presentationare based on four chapters in
the handbook of SAS DATA Stepprogramming, which are the
core components of the book.And I believe these
are the core conceptsto grasp if you want to be
proficient in SAS programminglanguage.These core components
include understandinghow the DATA Step works,
by-group processing, writingloops, and array processing.In the first section,
we will learnhow the DATA Step processes
during the compilationand the execution phase.A common problem
often facing beginningSAS programmers is that the
SAS data set that they createis not what they
intended to create.For example, there are
more or less observationsthat intended, or the value
of the newly created variableis not retained correctly.These types of mistakes
occur because new programmersoften focus exclusively
on language syntaxbut fail to understand how
the DATA Step actually works.A DATA step is
processed sequentiallyvia the compilation
and execution phases.The compilation
phase, each statementis scanned for syntax errors.If an error is found,
SAS will stop processing.The execution phase only begins
after the compilation phaseends.In the execution phase, the
DATA step works like a loop,repetitively executing
statement to read data valuesand creating observations
one at a time.Each loop is called
an iteration.We can refer to
this type of loopas the implicit loop, which
is different from the explicitloop by using the iterative
do, do while, or dountil statements.The SAS statements
in the DATA stepcan be categorized as
executable or declarative.The declarative
statements are usedto provide information to
SAS and only take effectduring the compilation phase.The declarative statements
such as length, format, label,or drop statements can be placed
in any order within the DATAstep.On the other hand,
the order in whichexecutable statements appear in
the DATA step matters greatly.Program statement 1 illustrates
how DATA step processing works.These programs read
raw data from the textfile, Example3_1.txt, and
creates one variable, BMI.Example3_1.txt contains
two observationsand three variables, including
name, height, and weight.Notice that the weight variable
for the first observationis entered as 12D, which
is a data entry error.Each variable in Example3_1 is
occupied in the fixed field,and values for these variables
are standard characteror numerical values.Therefore, we can use
column input methodto read this data set.For example, program 3.1
reads this external file,but program starts with the
infile statement, followedby the input statement.The infile statement
is used to identifythe location of
the external file,and the input
statement instructs SAShow to read each observation.Thus, we must place
the infile statementbefore the input
statement, because SASneeds to know where to find
the external file before itcan read it.After that, we can
calculate BMI variableby using the
assignment statement.Next, let's talk about the
DATA step compilation phase.Since we are reading
a raw data file,the input buffer is
created at the beginningof the compilation phase.The input buffer is
used to hold raw data.However, if we read a SAS data
set instead of raw data file,the input buffer
would not be created.SAS also creates the
Program Data Vector, or PDV,in the compilation phase.PDV is a memory area
on your computer,and SAS uses PDV to
build the new data setone observation at a time.Within the PDV, there are
two automatic variables,_N_ and _ERROR_._N_ equaling 1 indicates the
first observation is beingprocessed.And _N_ equaling indicates the
second observation is beingprocessed, and so on.The automatic variable, _ERROR_
is an indicator variable withvalues of 1 or 0._ERROR_ equaling 1 signals
the data error of currentlyprocessed observations,
such as reading the dataas an incorrect data type.During the
compilation phase, SASscans each statement
in the data step.When it starts to scan
an input statement, whichis reading the name, height,
and weight variables,SAS allocates one space for each
of these variables in the PDV.When SAS scans the
assignment statement,BMI is also added to the PDV.Some of the variables
in the PDV aremarked with D, which
stands for Dropped.The others are marked with
K, which stands for Kept.Only the variables
marked with K will bewritten to the output data set.Automatic variables are
always marked with D,so they are never written out.During a compilation
phase, SAS checksfor syntax errors such as
valid variable names, options,punctuation, misspelled
keywords, et cetera.SAS also identifies
the type and lengthsof the newly created variables.At the end of compilation
phase, the descriptor portionof the SAS data set is created,
which includes the data setname, the number
of observations,and the number of names and
attributes of variables.Once the compilation
and phase is completed,and there is no syntax
errors in the data step,SAS stars the execution phase.At the beginning of
the execution phase,the automatic variable
_N_ is initialized to 1,and _ERROR_ is initialized to
0 since there is no data entryerror yet.The non-automatic variables
are set to missing.Next, the INFILE statement
identifies the locationof the input file.The input statement
copies the first data lineinto the input buffer.At this point,
the pointer pointsto the beginning of
the input buffer,then the INPUT statement
starts to read the data valuesfrom the input buffer
according to the instructionsfrom the INPUT statement
and writes them to PDV.The value for the
name variable, whichis specified from
columns 1 to 7,is copied to the
name slot in the PDV.Then the input pointer
points at column 8.Next, SAS copies the
values from column 9 to 10from the input buffer to
the height slot in the PDV.Then the input pointer
points at column 11.The value for the
weight variableis entered as 12D, which is
an invalid numeric value.Therefore, the weight
variable is set to missing.The _ERROR_ is set
to 1 at this point.Meanwhile, an error message
is sent to the SAS log,indicating the location
of the data error.At this point, the input
pointer points to column 15.Next, the assignment
statement is executed,and BMI will remain
missing since operationson a missing value will
result a missing value.When the output
statement is executed,only the values from
PDV marked with Kare copied as a
single observationto the output data set, Ex3_1.Now the control reaches
the end of data step.There are two things
happening automatically.First, SAS system returns
to the beginning of the datastep to begin the
next iteration.Secondly, the values of
the variable in the PDVare set to missing.The automatic variable _N_
underscore is incremented to 2,and the _ERROR_ is set to zero.During the second iteration
of the data step execution,the second data line is
read into the input bufferby the input statement.And an input statement
correctly copies the contentsfrom the input
buffer to the PDV.BMI is correctly calculated.The OUTPUT statement copies
the contents from PDVto the output data set.When the SAS system reaches
the end of the data step,two things happens
automatically again.The SAS system again returns to
the beginning of the data step.The _N_ variable is
incremented to 3,and the rest of the
non-automatic variables are setto missing.SAS attempts to read an
observation from the input dataset, but it reaches the
end of our marker, whichmeans that there are no
more observations to read.When the end of file
marker is encountered,SAS goes to the next data
or proc step in the program.In the previous program, an
explicit output statementis used to tell SAS to write
the current observationfrom the PDV to a SAS data
set immediately at a pointwhere you placed the
explicit output statement,but not at the end
of the data step.If you remove the
explicit OUTPUT statement,every DATA step contains
an implicit OUTPUTat the end of the DATA step.The implicit OUTPUT tells
SAS to write observationsto the OUTPUT data set at
the end of the DATA step.If you place an explicit OUTPUT
statement in the DATA step,the explicit OUTPUT
statement will overridethe implicit OUTPUT statement.In other words, once an
explicit OUTPUT statementis used to write an observation
to an OUTPUT data set,there is no longer an
implicit OUTPUT statementat the end of the DATA step.The SAS system
adds an observationto the OUTPUT data set only
when an explicit OUTPUTstatement is executed.Furthermore, more than
one OUTPUT statementin the DATA step can be used.We will see an example of using
multiple OUTPUT statementslater in this chapter.Now let's talk about
the differencesbetween reading a raw data set
and reading a SAS data set.Remember that, when
we read the raw data,the data was transferred
to the input buffer first,then transferred to the PDV.And in the end, the data
was copied from the PDVto the OUTPUT data set.But when we read a SAS data
set by using the SET statement,there will be no more input
buffer data from the input dataset.It will be copied
directly to the PDV,and then the contents
from PDV will becopied to the output data set.There are also
other differences.When creating a SAS data
set based on a raw data set,SAS initializes each
variable value in the PDVto missing at the beginning of
each iteration of execution,except for the
automatic variables--variables that are named
in the RETAIN statement,variables that are created
by the SUM statement,data elements in the
_TEMPORARY_ array,and variables created in the
options of the FILE and INFILEstatement.But when SAS reads SAS data
set via the Set statement,SAS sets each variable
to missing in the PDVonly before the first
iteration of the execution.For variables that exist in both
the input and output data sets,these variables will retain
their values in the PDVuntil they are replaced by the
new values from the input dataset.For the newly created
variables, whichare not from the
input data set, SASwill set these newly created
variables to missing in the PDVat the beginning of every
iteration of the DATA stepexecution.For example, suppose that we
are reading the SAS DATA setexample 1.At the beginning of the
DATA Step execution phase,SAS sets each non-automatic
variable to missing in the PDV.Next, the SAS statement
copies the first observationfrom the input data to the PDV.BMI is calculated.The output statement copies
the contents from PDVto the output data set.SAS reached the end of the
first iteration of the DATA Stepexecution.Next, SAS returns to the
beginning of the DATA Stepto begin the second iteration.At this point, _N_ is
incremented to two variablesthat exist in both the input
and output data, such as name,height, and weight--will retain their
values in the PDV.The newly created
variable BMI isset to missing at this point.The SET statement copies the
second observation to the PDV.As you can see, the contents
for the name, height,and weight variables are
replaced with the new valuesfrom the second observation.Then BMI is calculated next.This is a very important
concept that weneed to understand before
we can move further.In conclusion, only the
newly created variablesare set to missing
at the beginningof each iteration of
the DATA Step executionwhen we read a SAS data set.Now let's see an example based
on a data set on this slide.There are only two variables,
ID and SCORE in this data.Suppose we would like to create
a new variable TOTAL thatis used to accumulate
the SCORE variable.In order to create an
accumulator variable TOTAL,we need to initialize TOTAL
to 0 at the first iterationof the execution.Then at each successive
iteration of the execution,add a value from the SCORE
variable to the TOTAL variable.Since TOTAL is a new
variable we want to create,TOTAL will be set to
missing in the PDVat the beginning of every
iteration of the execution.Thus, in order to accumulate
the TOTAL variable,we need to retain
the value of TOTALat the beginning of each
iteration of the DATA Stepexecution.In this situation, we need
to use the RETAIN statement.The RETAIN statement
will prevent the variablefrom being initialized each
time the DATA Step executes.In the RETAIN
statement, variableis the name of the variable
that we will want to retain.The value in the
numerical value thatis used to initialize
the variable onlyat a first iteration of
the DATA Step execution.If we do not specify
a initial value,the retained variable
is initializedas missing before the first
execution of the DATA Step.The program on this slide
uses the RETAIN statementto create a TOTAL variable.Let's go over the execution
phase of this programto better understand
the RETAIN statement.The execution phase
begins immediatelyafter the completion of
the compilation phase.At the beginning of
the execution phase,_N_ is set to 1, and _ERROR_
is set to 0 in the PDV.The variable ID and
SCORE are set to missing.The variable TOTAL
is initializedto 0 because of the
RETAIN statement.If we don't use the RETAIN
statement to initialize TOTALto 0, TOTAL will be set to
missing at the beginningof the execution phase.Next, the SET statement
copies the first observationfrom the DATA Set
SAS3_1 to the PDV.The RETAIN statement is a
compile time only statement.It does not execute during
the execution phase.TOTAL is calculated.The DATA Step execution reaches
the end of the first iterationof DATA Step.Since there is no
explicit OUTPUT statementin this program, the
implicit OUTPUT statementat the end of DATA Step
tells the SAS systemto write an observation
to the output data set.The SAS system returns to the
beginning of the DATA Stepto begin the second iteration.At the beginning of
the second iteration,_N_ is incremented to 2.The variables that are
read from the input dataset, including ID and SCORE,
retain their values in the PDV.The newly created variable
TOTAL is also retained,because the RETAIN
statement is used.Without using the
RETAIN statement,TOTAL will be set to
missing at this point.Next, the SET statement
copies the second observationfrom the input data to the PDV.Total is calculated.The SAS system reaches the
end of second iteration.The implicit OUTPUT copies
the content from PDVto the OUTPUT data.SAS returns to the
beginning of the DATA Stepto start a third iteration._N_ is incremented to 3.ID and SCORE are retained
from the previous iteration,because they are
from the input data.TOTAL is also retained because
of the RETAIN statement.The SET statement copied
the third observationfrom the input data to the PDV.TOTAL is again calculated.The implicit OUTPUT
generates the last recordfor the output data.The SUM statement is very
similar to the RETAINstatement.The SUM statement variable
is the numeric accumulatorvariable that is to be
created and is automaticallyset to zero at the beginning of
the first iteration of the DATAStep execution and is thus
retained in the followingiterations.EXPRESSION after
the plus operatorcan be any SAS expression.In a situation where EXPRESSION
is evaluated to be a missingvalue, it is treated as zero.Therefore, the
previous program canbe rewritten by using
the SUM statementinstead of using the
RETAIN statement.By using the SUM
statement, TOTALis initialized to 0 at the
beginning of the DATA Stepexecution.TOTAL also retains its
value at every iterationof the DATA Step execution.Many applications
require the DATA Stepto process only part
of the observation thatmeets the condition of
a specified expression.In this situation,
we need to usethe subsetting IF statement.The expression in the
subsetting IF statementcan be any valid SAS expression.If the expression is
true for the observation,SAS continues to execute
statements in the DATA Stepthat include the current
observation in the data set.The resulting SAS
data set containsa subset of the external
file or SAS data set.If the expression is false,
then no further statementsare processed for
that observation,and SAS immediately returns to
the beginning of next iterationof the DATA Step.That is to say, the
remaining program statementsin the DATA Step
are not executed,and the current
observation is notwritten to the output data set.For example, program
3.5 creates a dataset that contains the
observation in which the SCOREvariable is not missing.Sometimes it is more
efficient or easierto specify a condition for
excluding observations insteadof including observations
in the output data set.In this situation, you can
combine the IF THEN statementwith the DELETE statement.Program 3.6 provides
an alternative versionof program 3.5 by using IF
THEN and the DELETE statements.Sometimes we may
want to know whenreading the last observation
from the input data set.We can create a
temporary variableby using the END equal
option in the SET statementas a flag to signal when the
last observation is being read.The variable after
the keyword END equalis a temporary
variable that containsan end of file indicator.The variable is initialized to
0 at the beginning of the DATAStep iteration and
is set to 1 whenthe SET statement reads the
last observation from the inputdata.Since the variable is
the temporary variable,it is not added to
the output data set.Program 3.7 calculates
the TOTAL scoreand reads the total
number of observationsfrom the data set SAS 3_1.In this program, the
temporary variable LASTis used to indicate when to
read the last observationof the input data.The output data set only
contains the last processedobservation because of
using the subsettingIF statement in the program.Restructuring data is a common
task for a SAS programmer.The purpose of transformation
to a different formatis to suit the data
format requirementsfor different types of
statistical procedures.This type of data transformation
can be easily doneby using more advanced
programming techniques,such as array
processing describedin chapter 6 or the
transposed procedure describedin chapter 10.However, these can
also be accomplishedwithout the advanced
techniques for simple cases.Suppose that we are transforming
the data from the wide formatto the long format.Notice that data
in the long formathas a variable time
that distinguishesthe different measurements
for each subjectin the wide format.The original variables in
the wide format, S1 to S3becomes the variable
SCORE in the long format.Since only two observation need
to be read from the wide dataset, there will be only two
iterations for the data stepprocessing.That means we need to generate
the output up to three timesfor each iteration.In some iterations,
the output may notbe generated three times,
because the missingvalues in the variable
S1 to S3 would not beoutputted in the long data set.Program on this slide
illustrates the datatransformation by using
multiple OUTPUT statementsin one data step.Again, let's go through
the data step executionphase of this program.At the beginning of
the first iteration,_N_ is initialized to 1, and the
rest of the variables are setto missing.The SET statement copies
the first observationfrom the input data to the PDV.The time variable is
assigned with value 1.The SCORE variable is
assigned with the valuefrom the S1
variable, which is 3.Since the SCORE
variable is not missing,SAS copies the variable, that
are marked with K from the PDV,to the output data set.Notice that variable S1
to S3 are marked with D,because these three variables
are specified in the dropequal data set option
in the data statement.After the first
observation is generated,the time variable is
assigned with value 2.The score variable is assigned
with the value from the S2variable.Since the score
variable is not missing,the second record is generated
from the output statement.The time variable is
assigned with value 3.The score variable is assigned
with the value from the S3variable.Since the score
variable is not missing,the third record is generated
from the output data set.Since we used the
explicit OUTPUT statement,there is no more implicit
OUTPUT statement.When SAS reached the end of
the first iteration of the datastep, we've already
generated three recordsin the output data set.At the beginning of
the second iteration,_N_ is incremented to 2.ID, S1 to S3
variables are retainedfrom the previous iteration.The newly created variable TIME
and SCORE are set to missing.The SET statement copies
the second observationfrom the input data to the PDV.TIME is assigned
with value 1 again.SCORE is assigned with
the value from S1.Since score is not missing,
the fourth observationis generated from
the OUTPUT statement.TIME is assigned
with the value 2.SCORE is assigned with
the value from S2.Since SCORE variable
contains the missing value,no output is generated
from this statement.TIME is assigned with value
3, and SCORE is assignedwith the value from S3.Since SCORE is not missing,
then the fifth observationis generated.SAS reaches the end of DATA
Step of the second iterationand returns to the
beginning of DATA Stepto start a third iteration.Since there is no
more observationsto read in the
third iteration, SASgoes to the next
DATA or PROC step.This chapter, we are going to
learn about BY-Group processingmethod.Understanding
BY-Group processingis also closely
related to knowinghow DATA Step processes do
the DATA Step execution phase.The data set that were
used in previous chaptersonly contained one observation,
or one measurement,per subject.Sometimes, we also work with
data with multiple observationsfor each subject.This type of data often
results from repeated measuresfor each subject and is often
called longitudinal data.Applications that
involve longitudinal dataoften require identifying
the beginning or endof a measurement
for each subject.This can be accomplished by
using the BY-Group processingmethod.BY-Group processing is a method
of processing records from datasets that can be grouped
by the values of oneor more common variables.These grouping variables
are called the BY variable.The value of a BY variable
is called the BY value.A BY group refers to all
observations with the same BYvalue.To utilize BY-Group
processing, weneed to place a BY statement
with one or more BY variablesafter the SAS statement.The input data set also
needs to be previously sortedby the same variable.During the BY-Group
processing, SAScreates two temporary
indicator variablesfor each BY variable,
FIRST.VARIABLEand LAST.VARIABLE.Since FIRST.VARIABLE
and LAST.VARIABLEare temporary
variables, they are notsent to the output data set.Both, FIRST.VARIABLE
and LAST.VARIABLEare initialized to 1 at the
beginning of the DATA Stepexecution.Consider the SAS data
set on this slide, whichconsists of five observations
with the values of SCOREfor two subjects, A01 and A02.Suppose that we use
ID as the BY variable,then there will
be two BY groups,because there are two distinct
values for the ID variable.The FIRST.ID is
set to 1 in the PDVwhen SAS reads the first
observation in each BY groupand is set to zero
when reading the secondto the last observation
in each BY group.Similarly, LAST.ID
is set to 1 whenreading the last
observation in each BY groupand set to 0 when reading those
observations that are not last.When there are multiple
variables designatedas BY variables, a BY group
will be a group of records,with the same combination of the
values of these BY variables,with each BY group containing
a unique combination of valuesfor the BY variables.For example, if we use ID and
SCORE as the BY variables,then in addition to
FIRST.ID and LAST.ID,FIRST.SCORE and LAST.SCORE
will be created in the PDV.There will be four BY groups
based on unique combinationvalues of ID and SCORE.Suppose that we would
like to calculatethe total scores
for each subjectbased on the data
set on this slide.To create a variable, say,
total that is the total scorefor each subject, we
need to initializeTOTAL to zero when starting
to read the first observationof each subject.Then TOTAL can be
accumulated by addingthe values from
the SCORE variableto TOTAL for each observation.In the end, we can
output the total scorewhen reading the last
observation of each subject.Therefore, we need to
utilize BY group processingand use ID as the BY variable.Program 4.1 calculates the
TOTAL score for each subject.In this program, the variable
ID is used as the BY variable.Therefore, this data set
needs to be sorted firstby the ID variable.When the FIRST.ID equals
to 1, the variable TOTALis initialized to 0.Then TOTAL is accumulated
with the value from the SCOREvariable by using
the SUM statement.Then substantive IF
statement in the programis to control when
to output the data.That is when the
LAST.ID equals to 1.To understand this
program better,let's go through the
contents in the PDVduring the DATA Step execution.At the beginning of
the first iteration,_N_ is initialized to 1.Since the ID is the BY variable,
two automatic variablesFIRST.ID and LAST.ID
are created in the PDV.Both FIRST.ID and LAST.ID
are initialized to 1before the first iteration
of the DATA Step execution.ID and SCORE variables
are set to missing.TOTAL is set to 0
since TOTAL is createdby using the SUM statement.When the SET statement
executes, SAScopies the first observation
from the sorted SAS4_1data set to the PDV.Since this is the first
observation for A01,FIRST.ID is set to 1.The LAST.ID is set to
0 since this is notthe last observation.The BY statement is a
declarative statement.It doesn't execute during
the execution phase.Next, TOTAL is assigned
to zero, because thisis the first
observation for ID A01.In the IF THEN
statement, I wroteif FIRST.ID, which is equivalent
to writing IF.ID equals to 1.The SUM statement accumulates
the TOTAL variable.The subsetting IF
statement is evaluatedto be false, because
LAST.ID does not equal to 1.SAS immediately returned to
the beginning of the DATA step.That means that the
contents of the PDVare not sent to the
output data set.At the beginning of
the second iteration,_N_ is incremented to 2.Both FIRST.ID and LAST.ID
are retained because theseare automatic variables.ID and SCORE are retained
because these two variablesare from the input data set.Lastly, TOTAL is also retained,
because TOTAL is createdby using the SUM statement.As you can see, all these
variables in the PDVretained their values
at the beginningof the second iteration.However, the reason for
retaining the valuesare different.The SET statement copies the
second observation to the PDV.Since the second
observation is notthe first observation for
A01, the FIRST.ID is set to 0.This is also not the
last observation for A01.The LAST.ID is also set to 0.Since FIRST.ID does
not equals to 1,there is no execution for
the IF THEN statement.TOTAL is accumulated with the
value from the SCORE variable.Since LAST.ID does not equal
to 1 in the subsetting IFstatement, SAS again returns to
the beginning of the DATA step.At the beginning of
the third iteration,_N_ is incremented to 3.The rest of the
variables are retained.The SAS statement copies
the third observationsfrom the input data to the PDV.The FIRST.ID is set
to 0 since this is nota first observation
for A01, but LAST.IDis set to 1 since this is
the last observation for A0.Since FIRST.ID does not equal
to 1, there is no execution.TOTAL is calculated.Since LAST.ID equals
to 1, which is true,SAS continues to
execute the remainingstatements in the DATA step.Remember that this is the
rule for the subsetting IFstatement.Next, SAS reaches the end
of the third iteration.The implicit OUTPUT statement
copies the contents from PDVto the output data.Then SAS returns
to the beginningof the fourth iteration.At the beginning of
the fourth iteration,_N_ is incremented to 4.The rest of the
variables are retained.The SET statement copies
the fourth observationfrom the input data to the PDV.Since this is the first
observation for A01,FIRST.ID is set to
1, but this is notthe last observation for A01.The LAST.ID is set to 0.Since FIRST.ID equals
to 1, which is true,TOTAL is set to 0.As you can see, this is
a very important step.Without initializing
TOTAL to 0, whenreading the first
observation for each person,TOTAL will accumulate the
values from the previous subjectin the next step.Now TOTAL is calculated.Since LAST.ID does not equal
to 1 in the subsetting IFstatement, SAS immediately
returns to the beginningof the DATA step.At the beginning of
the fifth iteration,_N_ is incremented to 5.The rest of the
variables are retained.The SET statement copies
the fifth observationfrom the input data to the PDV.FIRST.ID is set to
0 since this is notthe first observation
for A02, but LAST.IDis set to 1 since this is
the last observation for A02.Since FIRST.ID does not equal
to 1, there is no execution.TOTAL is accumulated.Since LAST.ID equals
to 1, SAS continuesto execute the remaining
statements in the DATA step.The implicit OUTPUT copies
the contents from PDVto the output data set.SAS returns to the
beginning of the DATA stepto begin the sixth iteration.Since there is no more
observations to read,SAS goes to the next
DATA or PROC step.Let's see another example.Suppose that we would
like to calculatethe mean score for each person
based on the same data set.The solution for this
problem is similarto the previous program.We need to accumulate all
the scores for each subjectand create a counter
variable n to countthe number of observations
within each BY group.Both TOTAL and n need
to be initializedto 0 when FIRST.ID equals to 1.Then they can calculate
the mean scoreby dividing TOTAL by n
and output the resultwhen LAST.ID equals to 1.A common task in
examining a data setis checking when the
data set containsduplicate observations.For example, the first two
observations in this data setare identical.To identify duplicated
observations,we can use the BY-Group
processing method.Suppose that we would like
to create two data sets, onewith observations with
non-duplicated recordsand one containing observations
with duplicated records.Since duplicated records will
have the same value for boththe ID and SCORE variables,
both ID and SCORE variablewill be used as
the BY variables.And non-duplicated records is
the one where both FIRST.SCOREand LAST.SCORE equal to 1.Otherwise, it will be
duplicated records.Based on this idea,
we can write a programlike the one on this slide.Patients with repeated
measures over timeare often encountered
in the clinical field.For example, the data
set PATIENT on this slidecontains the
triglycerides measurementas smoking status for patients
for different time periods.Notice that some patients
only have one measurement,whereas others
were measured morethan once in different years.Suppose that we
would like to createa data set that contains the
most recent non-missing data.The resulting data set
will have three variables,which include patient
ID, TGL_NEW whichis the most recent
non-missing TGL,and SMOKE_NEW which is the
most recent non-missing smokevariable.A couple of issues
need to be consideredfor solving this problem.First of all, the most
reason non-missingdata for TGL and SMOKE occur
at different time points.For example, for patient A01,
the most recent non-missing TGLis 150 in 2007, but the most
recent non-missing SMOKEis Y in 2005.For A02, the most recent
non-missing TGL is 210 in 2006,and the most recent
non-missing SMOKEis N, which is in 2006 also.Another issue is that
some patients may onlyhave missing values for
either TGL or SMOKE.In this situation, we only
need to use the missing valuesin the resulting data
set for this variable.For example, TGL measurement
is missing for A03.For A04, the most recent
non-missing TGL is 190 in 2006,and the most recent
non-missing SMOKE is N in 2007.For A05, since there is
only one observation,we will take 180 from
TGL and the missingvalue from the SMOKE variable.Since we only need to keep
the most recent record,the data set has to be sorted
by the patient ID and VISITvariable in ascending order.However, when utilizing BY-Group
processing in the DATA step,we only need to use patient
ID as the BY variable.One idea for
solving this problemis that we initially assign
TGL_NEW and SMOKE_NEWto missing values when
reading the first observationfor each patient
from the sorted data.Then at each iteration
of the DATA step,we will assign the values
from TGL and SMOKE_NEWas SMOKE variables to TGL_NEW
SMOKE_NEW respectively,provided that TGL and
SMOKE are not missing.Since TGL_NEW and SMOKE_NEW
are newly created variables,we need to retain their values
by using the RETAIN statement.In the end, we will output
the values in the PDVwhen reading the last
observation of each patient.The solution for this problem
is illustrated on this slide.Let's go over the DATA
Step execution phaseto understand this
program better.At the beginning of the first
iteration, _N_ is set to 1.The SET statement copies
the first observationfrom the input data to the PDV.Since this is the first
observation for A01,FIRST.PATID is set to 1.But this is not the last
observation for A01.LAST.PATID is set to zero.Since FIRST.PATID equals to
1, both TGL_NEW and SMOKE_NEWare set to missing.Since TGL is missing,
there is no executable.Since SMOKE is not missing, the
value from the SMOKE variableis assigned to SMOKE_NEW.Since the LAST.ID
does not equal to 1,SAS returns to the
next iteration.At the beginning of
the second iteration,_N_ is incremented to 2.The rest of the
variables are retained.The SET statement copies
the second observationfrom the input data to the PDV.FIRST.PATID is set to
0 since this is notthe first observation for A01.LAST.PATID is set
to 1 since this isthe last observation for A01.Since FIRST.PATID
does not equals to 1,there is no execution.The value from TGL is
assigned to TGL_NEWsince TGL is not missing.Since SMOKE is missing,
there is no execution.Since LAST.PATID equals to 1
in the subsetting IF statement,SAS continues to
the next statement.SAS reaches the end
of the DATA Step.The implicit output
copies the variablesthat are marked with K in the
PDV to the output data set.Then SAS returns to
the next iteration.At the beginning of
the third iteration,_N_ is incremented to 3.The rest of the
variables are retained.The SET statement copies the
third observation to the PDV.Since this is the first
observation for A02,FIRST.PATID is set to 1.Since this is not the
last observation for A02,LAST.PATID is set to 0.Since FIRST.PATID equals to
1, both TGL_NEW and SMOKE_NEWare set to missing.As you can see, this
is a important step.Without initializing
these two variablesto missing when reading
the first observationof each person, both variables
will contain the valuesfrom the previous person.Since TGL is missing,
there is no execution.SMOKE is also missing.There is no execution.Since LAST.PATID equals
to zero, which is false,SAS returns to the
beginning of next iteration.At the beginning of
the fourth iteration,_N_ is incremented to 4.The rest of the
variables are retained.The SET statement copies the
first observation to the PDV.Both FIRST.PATID and
LAST.PATID are set to 0.Since FIRST.PATID does not equal
to 1, there is no execution.Since TGL is not missing,
TGL is assigned to TGL_NEW.Since SMOKE is not missing,
SMOKE is assigned to SMOKE_NEW.Since LAST.PATID
equals to zero, SASreturns to the next iteration.At the beginning of
the fifth iteration,_N_ is incremented to 5.The rest of the
variable are retained.The SET statement copied the
fifth observation to the PDV.Since FIRST.PATID does not equal
to 1, there is no execution.Since TGL is not
missing, TGL is assignedto TGL_NEW to replace
the old value.Since SMOKE is not
missing, SMOKE_NEWis also replaced with the
value from the SMOKE variable.Since LAST.PATID
equals to 1, SAScontinues to execute
the next statement.Now SAS reaches the
end of the DATA Step.The implicit output copies
the contents from PDVto the output data set.Now we have the second
observation generated.I'm not going further for
the rest of the iterations.I hope you understand this
program clearly at this point.Restructuring data sets from the
wide format to the long formatwas illustrated in chapter 3.We can also transform
the data setfrom the long format
to the wide formatby using the BY-Group
processing method.Since we are reading five
observations from the long dataset but only creating
two observations,it means that we are not
copying data from the PDVto the final data set
at each iteration.As a matter of
fact, we only needto generate one observation
once all the observationsfor each subject
have been processed.Therefore, we need to use the
BY-Group processing method.By using ID as the
BY variable, we onlyneed to output the data when
the LAST.ID equals to 1.We can use the multiple
IF THEN ELSE statementto assigned the values
from the SCORE variableto S1 to S3 variables.The newly created variables
S1 to S3 in the final data setneed to retain their value.Otherwise, S1 to S3
will be initializedto missing at the beginning of
each iteration of the DATA Stepprocessing.Notice that subject A02
is missing one observationfor TIME equaling 2.The value of S2 from the
previous subject, which is A01,will be copied to the data
set wide for the subjectA02 instead of a missing value
because S2 is being retained.To avoid this problem, we
need to initialize S1 to S3to missing when processing
the first observationfor each subject.Program 4.5 begins by
sorting the long data setby ID and TIME.Sorting the variable
TIME within each IDis important, because it ensures
that the horizontal orderof S1 to S3 in the wide data set
for each subject can be matchedcorrectly with is the
vertical order of the scorein the long data set.Let's go over the
DATA Step executionphase of this program.At the beginning of the first
iteration, _N_ on is set to 1.Both FIRST.ID and
LAST.ID are set to 1.The rest of the variables
are set to missing.The SET statement copies the
first observation to the PDV.FIRST.ID is set
to 1 since this isthe first observation for A01.LAST.ID is set to
0 since this is notthe last observation for A01.Both BY and RETAIN statements
are declarative statements.Since FIRST.ID equals to 1,
S1 to S3 are set to missing.Since TIME equals to 1, the
value from the SCORE variable,which is 3, is assigned to S1.Since LAST.ID does
not equal to 1,no further statements
are processed.SAS returns to the
beginning of the DATA Step.At the beginning of
the second iteration,_N_ is incremented to 2.Both FIRST.ID and LAST.ID
retained their valuessince they are
automatic variables.ID, time, and SCORE are
also retained because theyare from the input data.S1 to S3 are retained
because they are as specifiedin the RETAIN statement.Otherwise, these three
variables will beset to missing at this point.The SET statement copies the
second observation to the PDV.FIRST.ID is set to
0 since this is notthe first observation for A01.LAST.ID is also set
to 0 since this is notthe last observation for A01.Since FIRST.ID does not equal
to 1, there is not execution.Since TIME equals to 2, S2
is assigned with the valuefrom the SCORE variable.Since LAST.ID does
not equal to 1,SAS returns to the
beginning of the DATA step.At the beginning of
the third iteration,_N_ is incremented to 3.The rest of the
variables are retained.The SET statement copies the
third observation to the PDV.FIRST.ID is set
to 0, and LAST.IDis set to 1 since this is
the last observation for A01.Since FIRST.ID does not equal
to 1, there is no execution.Since TIME equals to 3, S3
is assigned with the valuefrom the SCORE variable.Since LAST.ID equals
to 1, SAS continuesto execute the next statement.Now SAS reaches the
end of the DATA step.The implicit output copies
the contents from PDVto the output data set.Then SAS returns to the
beginning of the DATA stepto begin the first iteration.At the beginning of
the fourth iteration,_N_ is incremented to 4.The rest of the
variables are retained.The SAS statement copied the
first observation to the PDV.FIRST.ID is set to 1 since
this is the first observationfor A02, and the LAST.ID is
set to 0 since this is notthe last observation for A02.Since FIRST.ID
equals to 1, S1 to S3are set to missing values.Since TIME equals to 1, S1
is assigned with the valuefrom the SCORE variable.Since LAST.ID does
not equal to 1,SAS returns to the
beginning of the DATA step.At the beginning of
the fifth observation,_N_ is incremented to 5.The rest of the
variables are retained.The SET statement copied the
fifth observation to the PDV.FIRST.ID is set to
zero, and LAST.IDis set to 1 since this is
the last observation for A02.Since FIRST.ID does not equal
to 1, there is no execution.Since TIME equals
to 3, S3 is assignedwith the value from SCORE.Since LAST.ID equals
to 1, SAS continuesto execute the next statement.SAS reaches the end
of the DATA step.The implicit output
copies the value from PDVto the output data set.This is the end of this chapter.As you can see, there
are many applicationsthat can utilize the
BY-Group processing method.In this chapter, we will
talk about loop structurein the DATA step.A loop is a basic logical
programming conceptwhere one or more statements
are executed repetitivelyuntil a predefined
condition is satisfied.You probably learned
the loop structuringin other programming languages.Compared to other
programming languages,loop processing is more
complex in SAS becauseof its unique implicit loop.In addition to
the implicit loop,there is the explicit
loop structurethat you can use
in the DATA Step.There are three sections
in this chapter.We only focus on
section 1, whichis Implicit and Explicit Loop.An implicit loop, which was
introduced in chapter 3,results when the DATA Step
repetitively reads data valuesfrom the input data set,
execute statements, and createsobservations for the output
data set one at a timeduring the execution phase.SAS stops reading
the input file whenit reaches the
end of file markerwhich is located at the
end of the input file.At this point, the
implicit loop ends.Suppose that we
would like to assigneach patient in a
group of patientsin a clinical trial,
where each patient hasa 50% chance of receiving
either the drug or placebo.For illustration purposes, only
four patients from the trialare used in this example.The data set is similar
to the one on this slide.To assign a patient with either
a drug or a placebo group,we can use the RANUNI function.The RANUNI function
is used to generatea random number that follows
uniform distribution between 0and 1.The seed in the RANUNI function
is a non-negative integer.The RANUNI function
generates a stringof numbers based on seed.When seed is set to 0,
which is the computer clock,the generated number
cannot be reproduced.However, when seed is
a non-zero integer,the generated number
can be reproduced.The program on this
slide reads databy using the SET statement,
then the GROUP variableis assigned either
D for Drug or Pfor Placebo, based on the
randomly generated number.Let's use this
program on this slideto review the implicit loop.At the beginning of the
DATA Step execution,_N_ is initialized to 1._ERROR_ is set to 0, and the
rest of the variables are setto missing.The SET statement copies
the first observationfrom the input data to the PDV.RANNUM is assigned with
a value that is generatedfrom the RANUNI function.Since the generated number
is not greater than 0.5,the GROUP variable
is assigned with P.At the end of the DATA
Step, the implicit outputcopies the variables that
are marked with K in the PDVto the output data set.Then SAS returns to the
beginning of the DATA Step.At the beginning of
the second iteration,_N_ is incremented to 2, ID
is retained since ID is fromthe input data set, GROUP
and RANNUM is set to missing.The SET statement copies
the second observationfrom the input data to the PDV.Let's keep a few iterations.Now at the end of
the fourth iteration,the implicit output generated
is the fourth observation.SAS again begins to the
beginning of the DATA Step.At the beginning of
the fifth iteration,_N_ is incremented to 5.SAS reaches the
end of file marker,which means that there are
no more observations to read.The execution
phase is completed,and SAS goes to the
next DATA or PROC step.In the previous
example, the patient IDis stored in the input data set.Suppose that if we don't have
a data set containing patientsID, we are asked to
assign four patientswith 50% chance of receiving
either the drug or placebo.Instead of creating an input
data set that stores ID,we can create the ID and assign
each ID to a group in the DATAStep at the same time.Here's the program
to assign each IDto a group in the DATA
Step without reading IDfrom the input data.In this program, we need
to assign ID with ID valuesfour times.Then it assigns the GROUP value
based on the generated number.There are four explicit
output statementsthat tell SAS to write
the current observationfrom the PDV to the SAS
DATA Set immediately.Not at the end of the DATA Step.However, without using
explicit output statements,you will only create one
observation for ID equal M1240.Notice that most of the
statements in this programare identical.To reduce the
amount of coding, wecan simply rewrite
the program by placingrepetitive statements
in the DO loop.In the iterative
DO loop, we mustspecify an index variable
that contains the valueof the current iteration.The loop will execute
along value 1 to value n.And the values can be
either character or numeric.Now we can rewrite
the previous programby using an iterative DO loop.We will use ID as
the index variable.These four ID values are
the values 1 to value 4that need to be specified
after the equal sign in the DOstatement.These four SAS
statements are identical,which need to be
placed within the loop.The program on the
right side of the slideutilizes an iterative DO loop to
simplify the previous program.Most of the time, we use an
iterative DO loop to iteratealong a sequence of integers.The loop will execute from the
start value to the stop value.The optional BY clause
specifies an incrementbetween start and stop.The default value for
the increment is 1.Start, stop, and increment
can be numbers, variables,or SAS expressions.These values are set upon entry
into the DO loop and cannot bemodified during the
processing of the DO loop.However, the index variable
can be changed within the loop.Suppose that we are using
1 to 4 as patients ID.We can rewrite the
previous programas the one on this slide.The program uses ID
as the index variable.The START value is 1,
and the STOP value is 4.We didn't specify
the increment valuesince the default
value of 1 is used.Now let's go through the
DATA Step execution phaseto understand this loop better.Since we didn't read
any input data set,there will be only one
iteration for the DATA Step._N_ will be 1 for the
entire execution phase.At the beginning of the first
iteration of the DO loop,ID is assigned with value 1.RANNUM is generated.Since RANNUM is not greater than
0.5, GROUP is assigned with P.The output statement
copies the variablesthat are marked with K in the
PDV to the output data set.SAS reached the
end of the DO loop,and returned to the
beginning of the DO loop,and started the next iteration.At the beginning of
the second iteration,ID is incremented to 2.Since 2 is less
than or equal to 4,the second iteration continues.RANNUM is generated again.Since RANNUM is
greater than 0.5,GROUP is assigned with value
D. The output statement copiesthe contents from PDV
to the output data set.Let's skip two iterations.Here's the contents
in the PDV at the endof the first iteration.And we have four
observations generated.SAS returns to the
beginning of the loopand starts the next iteration.At the beginning of
the fifth iteration,ID is incremented to 5.Since 5 is greater
than 4, the loop ends.There will be no implicit
output since the explicit OUTPUTstatement is used
in the DATA Step.Since we didn't read
any input data set,the DATA Step execution
ends at this point.Using an iterative
DO loop requiresthat we specify the number of
iterations for the DO loop.Sometimes we will need to
execute statements repetitivelyuntil a condition is met.In this situation, we need to
use either the DO WHILE or DOUNTIL statements.The DO WHILE loop,
the expressionis evaluated at the
top of the DO loop.The DO loop will not
execute if the expressionis evaluated to be false.Now we can rewrite
the previous programby using the DO WHILE loop.In the DO WHILE loop, we
need to specify a conditionto control when
the loop will stop.In this program, we compare
ID variable with value 4as the conditions for
stopping the loop.ID variable is created by
using the SUM statementwithin the loop.Again, let's go through the
DATA Step execution phase.At the beginning of the
DATA step execution,_N_ is initialized to 1.ID is set to 0,
because ID is createdby using the SUM statement.The rest of the variables
are set to missing.Since the ID is 0, which is
less than 4, the loop continues.ID is incremented to 1.RANNUM is generated.Since RANNUM is less than
0.5, the GROUP variableis assigned with value P.The OUTPUT statement
copies the variablesthat are marked with K in the
PDV to the output data set.SAS reaches the end
of the DO WHILE loop,and returns to the
beginning of the loop,and starts the next iteration.Since ID equals to 1,
which is less than 4,the loop continues.ID is incremented to 2.Let's skip a few iterations.Here's the contents
of the PDV at the endof the first iteration.At this point, ID equals to 4.There are four observations
being generated.SAS returns to the
beginning of the loopto begin the fifth iteration.However, ID is not less than 4.The loop stops.SAS reaches the end
of the DATA Step.The execution phase ends.We can also use the DO UNTIL
loop to execute the statementsconditionally.Unlike DO WHILE loop,
the DO UNTIL loopevaluates the condition
at the end of the loop.The DO UNTIL loop will not
continue for another iterationif the expression
is evaluated to betrue at the end of
the current loop.That means the DO UNTIL loop
always executes at least once.Now we rewrite the program again
by using the DO UNTIL loop.Let's compare the differences
between using the DO WHILEloop and the DO UNTIL loop.When using the DO
WHILE loop, the loopwill not continue
if the expressionis evaluated to be false.The condition of the
expression is evaluatedat the top of the loop.On the other hand, the
condition of the expressionis evaluated at the bottom
of the DO UNTIL loop.The loop will not continue
for another iterationif the expression is
evaluated to be true.A common application
for using loopsis placing a loop
within another loop.To continue with the
previous example,suppose that we would like to
assign 12 patients from threecancer centers, COH,
UCLA, or USC with foursubjects per center,
where each patient hasa 50% chance of receiving
either the drug or a placebo.A nested loop can be used
to solve this problem.In the outer loop, the
index variable centeris assigned with the values with
the name of the three cancercenters.For each iteration
of the outer loop,there is an inner loop
that is used to assigneach patient to a group.Since the outer loop
iterates three timesand inner loop
iterates four times,placing the output statement
inside the inner loopwill generate 12 observations.In the previous example,
all the observationswere created from one DATA
Step since the DATA Stepdidn't read any input data.The center variable gets its
value from the outer loop.Sometimes it is necessary
to use an explicit loopto create multiple observations
for each observation thatis read from an input data set.For example, suppose
the values for centerare stored in the SAS data set.For each center, you need to
assign four patients, whereeach patient has a 50%
chance of receivingthe drug or a placebo.In this situation, we need to
read the value for the centervariable via the implicit loop.Then for each center that
is being read into the PDV,we need to utilize an explicit
loop to create the ID and GROUPvariables.If you are interested in
learning more exampleson explicit loops, you can
read chapter 5 in the handbookof SAS DATA Step Programming.Utilizing an explicit loop
is commonly used togetherwith array processing, which we
will learn in the next section.In this chapter, we are going
to learn about array processing.This is also the last
section of this seminar.The example on this slide
illustrates a situationfor utilizing array processing.The data set SBP contains six
measurements of Systolic BloodPressure measurements.The missing values
are coded as 999.Suppose that we would
like to recode 999to the standard
numeric missing value.We can write a code like
the one in program 6.1.In this program, each
of the if statementsconverts the number 999
to a SAS missing value.These if statements
are almost identical.Only the names of the
variables are different.In this situation, we might
think to use a do loopto simplify this program.Remember the program in
the previous chapter.We have four almost identical
blocks in the program.The only difference is the
value of the ID variable.Then we can simplify
the program by enclosingthe identical statements
within a do loop.The do loop iterates along
the values of the ID variable.However, in this
program, the differenceof these six statements
is the variable namesinstead of the values
of a single variable.If these six variables can be
grouped into one single unit,we can loop along
these variables.This is the concept of
array processing in SAS.An array is a temporary
grouping of SAS variables.To group a list of
variables into array,we need to use the
ARRAY statement.The array name in
the array statementdoes not become part
of the output data.The array name must be a
legitimate SAS name and cannotbe the name of a SAS variable
in the same DATA Step.Furthermore, array name cannot
be used in the LABEL, FORMAT,DROP, KEEP, or
LENGTH statements.The subscript component
in the array statementdescribes the number or
arrangement of array elementsand can be specified
in different forms.The simple form for
subscript is to specifydimensional size of the array.The optional dollar
sign indicatesthat the elements in the
array are character elements.You do need to specify the
dollar sign if the arrayelement had been purposely
defined as character elements.If the length of array
elements have notbeen previously specified,
you can use the length optionin the ARRAY statement.The optional array
elements are the variablesto be included in
the array, whichmust be either numeric
or character variables.For example, we use the
ARRAY statement to group SBP1to SBP6 variables.We can also provide a range
of numbers as subscriptby providing the lower and
upper bounds of the arrayand separate them by column.An asterisk can also
be used as subscript.Using an asterisk will let
SAS determine a subscriptby counting the
variables in the array.When we specify the asterisk,
we must include array elements.We can enclose subscripts
with braces, brackets,or parentheses.If array elements
are not specified,the array elements will be
implied to the variableswith the names that contain
the array name and the numbersfrom 1 to n.For example, the first array
statement on this slideis equivalent to the second one.There are a couple of situations
for omitting the arrayelements.The first situation is that
SBP1 to SBP6 variables alreadyexist in the DATA Step.In the second situation, if SBP1
to SBP6 variable do not exist,they will be created by
using this ARRAY statement.Other ways to list array
elements is to use the keywords, _NUMERIC_,
_character_, and _ALL_,which are used to specify
all numeric, all character,or all the variables
in the DATA Step.If the keyword _ALL_ is used,
all the previously definedvariables must
have the same type.We can also say initial
values to the array elementswhen creating a
group of variablesby using the ARRAY statement.When any or all
elements in the arrayare assigned with
initial values,all elements in
the array will actas they are named in
the RETAIN statement.For example, the
first ARRAY statementcreates n1, n2, and
n3 DATA Step variablesby using the ARRAY statement
and initialized with the values1, 2, and 3, respectively.The second ARRAY statement
creates chr1, ch2,and chr3 variables assigned
with the values of A, B, and C.The dollar sign is necessary
because chr1, ch2, and chr3are not purposely
defined in the DATA Step.We can also use the keyword
_temporary_ as an array elementto create a temporary array.Using a temporary
array is usefulwhen you want to create an array
only for computing purposes.When referring to a
temporary data element,we refer to it by the array
name and its dimension.Since a temporary array only
contains constants as elements,they cannot be sent to variables
as the output data set.Also, the values
in temporary arraysare automatically
retained without beingreset missing at the beginning
of each iteration of the DATAStep execution.Furthermore, we can not use
asterisk with temporary arrays.After an array is defined, we
can reference an array elementin the DATA Step.The subscript component
is used to specifythe subscript of an array.The subscript must be within
the lower and upper boundsof the dimension of the array.Now we can rewrite our program
by using the array processingmethod.Notice that the
variables, SBP1 to SBP6are listed in the
ARRAY statement.The IF THEN statement is
enclosed in the iterative DOLOOP.The loop iterates from 1
to 6, and the variable iserves as an index
variable for the loop.During each iteration
of the DO LOOP,the array reference SBP array
i refers to each array element.Instead of listing SBP1
to SBP6 individually,we can use the
shorthand notationby inserting a hyphen
between SBP1 and SBP6.We can also omit the array
elements in the ARRAY statementsince SBP1 to SBP6 variables
existed in the DATA Step.Let's go over the
compilation executionphase of this program.During the compilation
phase, the PDV is created.The array name SBP array
and array references are notincluded in the PDV.Each variable SBP1
to SBP6 is referencedby the array reference.Syntax errors in
the array statementwill be detected during
the compilation phase.At the beginning of the
DATA Step execution,_N_ is initialized to 1.The rest of the variables
are set to missing.The SAS statements compute the
first observation to the PDV.The ARRAY statement is
a declarative statement.It doesn't execute during
the execution phase.At the beginning of the do
loop, the index variable iis set to 1.In the IF THEN statement,
the array referenceSBP array i is the same
as SBP array 1, whichrefers to SBP1 variable.Since SBP1 does not equal to
999, there is no execution.SAS reaches the
end of the do loop.At the beginning of the second
iteration of the do loop,i is incremented to 2.Since 2 is less than or equal
to 2, the loop continues.Since this is the second
iteration of the do loop,the array reference SBP
array i is the same as SBP2,which refers to SBP2 variable.Since SBP2 variable
does not equals to 999,there is no execution.Now let's skip the rest of
the iteration of the do loop.SAS reaches the end
of the DATA Step.The implicit output copies
the contents from PDVto the output data.Then SAS returns to the next
iteration of the DATA Step.At the beginning of the second
iteration of the DATA Step,_N_ is incremented to 2,
SBP1 to SBP6 are retained,because these variables
are from the input data.The i variable is
set to missing.The SAS statements copy
the second observationfrom the input data to the PDV.i is incremented to 1 at the
beginning of the do loop.In the IF THEN statement,
the array referenceSBP array i is the same
as SBP array 1, whichrefers to SBP1 variable.Since SBP1 equals to
999, SBP1 variableis assigned with
the missing value.SAS reaches the
end of the do loop.Let's skip the rest of the
iteration of the do loop.SAS reaches the end
of the DATA Step.The implicit output copies
the contents from PDVto the output data set.Now let's skip the
rest of the iterations.Hope you have the idea of
array processing at this point.Next, let's talk about some
useful array functions.Sometimes we won't know
the dimension of an array.These tend to be the case
when you used _numeric_,_character_, and _all_
as array elements.A handy workaround
is the DIM functionwhich supplies the actual
dimension to the DATA Step.The optional N in
the DIM functionis used to specify the
dimension of an array.If the N value is not
specified, the DIM functionwill return the number of
elements in the first dimensionof the array.The second format of the
DIM function, the bound Nis either a numerical constant
variable or SAS expressionthat is used to specify
the dimension for which youwant to know the
number of elementsin the specific dimension.Closely related to
the DIM functionare the the HBOUND and LBOUND
functions that return the upperand lower bounds of
an array respectively.Program 6.3 uses
the DIM functionto return the number of
elements in SBP array.Instead of using
the DIM function,you can also use
the HBOUND functionto return the upper bound
of SBP array, which is 6.Now let's see how to
use the N operator.The data set on this slide
is similar to the onethat we used in
the previous slide,except that this
data set containsthe data with the correct
numerical missing values.Suppose that we would
like to create a variablecalled MISS, which is used to
indicate whether SBP1 to SBP6contain missing values.This task can be
easily accomplishedby using the N operator.The N operator
introduced in chapter 1is used to determine whether
a variable's value isamong the list of character
or numerical values.We can use the N operator to
search for numeric or charactervalues in the array.Program 6.4 illustrates
the use of the N operatorwith SBP array to create
the MISS variable.We can pass an array onto most
functions with the OF operator.Suppose that we would like to
create two variables, SBP_MINand SBP_MAX that contain
the minimum and maximum SBPvalues for each person.We can use the MIN and MAX
function with the OF operatorto accomplish this task.Like in program 6.5,
we must use an asteriskas the subscript
within the function.Here's the printed output
from the previous program.Now let's see some examples
of using array processing.Suppose that the first
three measurementsof SBP in this data
set are recordedas pre treatment measurements
and the last threeare recorded as post
treatment measurement.Furthermore, suppose
that the average SBPvalues for the pre treatment
measurements are 140and that the average SBP
is 120 for the measurementsafter the treatments.Based on this data
set, we would liketo create a list of
variables, above 1 to above 6,which indicate whether each
measurement is above or belowthe average measurement.Here is the solution for solving
this problem by using the arrayprocessing method.In this program, the
first array statementis used to group the existing
variables, SBP1 to SBP6.Since SBP1 to SBP6
existed in the input data,there is no need to specify
these variables in the ARRAYstatement.The second array statement
creates six new DATA Stepvariables, above 1 to above 6.Since above 1 to
above 6 variablesdo not exist in the DATA
Step, this ARRAY statementwill create these variables.The third ARRAY statement
creates temporary data elementsthat are used for
comparison purposes.These array elements are
initialized with six values.The first three values
are the average SBP valuesfor the pre treatment
measurements.The last three values
are the average SBPfor the measurements
after the treatments.The IF THEN statement
within an iterativedo loop, to compare each
element in the SBP arraywith each element in
the threshold arrayto assign a value, to the
element in the above array.Let's see another example.Suppose we would
like to calculatethe product of a list
of numerical variables.We can use array processing
to accomplish this taskby grouping numeric
variables into our array.We need to make sure to
treat missing value as 1during the calculation.In the DATA Step, we can start
assigning the first elementof the array to the result.Next, we need to create
a loop to iteratefrom the second element to
the last element of the array.Within the loop, we
can multiply the resultwith each element of the array.Here's the program based on the
idea that we just discussed.Transforming a data set with
one observation per subjectto multiple observations
per subject or vise versawas introduced in
chapters 3 and 4.The solutions in the
previous chapterswere not efficient if we have
large numbers of variablesthat need to be transposed.A more effective approach is
to utilize array processing.For example, here's the
program to transposedata set wide to data set long.Now let's see how to
modify this programby using the array processing.First, we need to
group the variable S1to S3 into the array S.
Therefore, in the program,instead of using
variable names S1 to S3,we need to change them to array
reference S of 1 to S of 3.Then the statements
in these three blockscan be simplified by placing
them within the iterativedo loop by using the time
variable as the index variable.Here's the modified
version of the programby using the array processing.Now let's see how to transform
data from the long formatto the wide format.The program on this
slide is the onethat we learned in chapter 4.Now let's modify this program
by using the array processingmethod.Again, we need to group
S1 to S3 into an arrayby using an ARRAY statement.Then we need to use the array
reference S of 1 to S of 3in the program instead of
using S1 to S3 variable.When reading the first
observation of each subject--that is when the
FIRST.ID equals to 1--we can use an iterative do
loop to initialize each elementin the array to missing.Next, to modify multiple
if then else statements,we need to understand
the contents in the PDVat each iteration of
the DATA Step execution.For example, when SAS processes
the first observation,the time variable equals
to 1 at this point.When the IF THEN ELSE
statement executes, S of 1is assigned with a value
from the score variable.S of 1 is equivalent
to S of timeby using time variable as
the subscript in the arrayreference.When SAS processes the
second observation,the time variable equals
to 2 at this point.When the IF THEN ELSE
statement executes, S of 2is assigned with the value
from the score variable.So S of 2 is equivalent
to S of time also.Therefore, the multiple
IF THEN ELSE statementscan be rewritten as S
of time equals score.Here is the final
version of the codeby using the array processing.This example will
end this section.If you are interested
in learningmulti-dimensional array, you
can read chapter 6 of handbookof SAS DATA Step programming.Thank you for your time for
listening to this presentation.I will also like to thank the
organizers of SAS Global Forumto provide me this opportunity
to prevent this seminar.If you have any questions,
please feel free to contact me."
19,"Welcome.My name is Louise
Hadden, and I'mgoing to be talking to you today
about several different methodsof using like statements in SAS.As you can see,
I've been using SASsince the earth was
a primordial soup.And I really love SAS.And I hope that you learn to
love SAS Like just like I have.How do I love SAS?Let me count the ways.There are numerous instances
where LIKE or LIKE statementscan be used to SAS in statements
and procedures, and all of themare useful.This presentation and
the accompanying paperand the proceedings will walk
through such uses of LIKEas searches and joins
with that smooth LIKEoperator and the NOT LIKE
operator, the SOUNDS LIKEoperator.Using the LIKE condition
to perform pattern matchingthat create variables in PROC
SQL and PROC SQL CREATE TABLELIKE.SAS produces and provides
numerous time and angst-savingtechniques to make the SAS
coding with LIKE easier.Amongst those techniques
are the abilityto search and select
data using SAS functionsand operators in the data step.In PROC SQL, the ability
to join data sets based onmatches at various levels, the
ability to create variablesbased on selecting values
from other variables,and the ability to create
empty shells of existing datasets using SAS and other data.This paper and
presentation exploreshow LIKE is featured in each
one of these techniques.And it's suitable for
all SAS practitioners.I hope that like will become
part of your SAS toolbox too.SAS operators are
used to performa number of functions,
arithmetic calculations,comparing or selecting variable
values, or logical operations.Operators are usually
grouped as prefix.For example, a sign before
a variable or infix,which generally
perform an operationbetween two variables.Arithmetic operations,
using SAS operators,may include exponentiation,
multiplication, and addition,among others.Comparison operators
may include greater thanand equals among others.Logical or Boolean
operators such as operandsas the double bars or double
exclamation points, AND, and ORand serve the purpose of
grouping SAS operations.Some operations that are
performed by SAS operatorshave been formalized
in functions.A good example of this is
the concatenation operator,the double bars, and the
double exclamation points,and the more powerful
CAT function,which perform similar, but
not identical operations.LIKE operators are
most frequentlyutilized in the data set in
the PROC SQL via a data step.Smooth operators.Smooth operators, there is
a category of SAS operatorsthat act as comparison operators
under special circumstances.Generally, in WHERE statements
in PROC SQL and the datastep in DS2 and sub sitting IF
statements in the data step.These operators include the
LIKE operator and the SOUNDSlike operator as well as the
CONTAINS and SAME-AND operator.It's beyond the scope of
this short paper presentationto discuss all of
the smooth operators.They are definitely
worth a look.Character operators
are frequentlyused for pattern matching.That is evaluating whether a
variable value equals, does notequal, sounds like a
specified value or pattern.The LIKE operator is a case
sensitive character operatorthat employs two special
wildcard charactersto specify a pattern.The percent sign
indicates any numberof characters in a pattern,
while the underscore indicatesthe presence of a single
character per underscorein the pattern.The LIKE operator is
akin to the grep utilityavailable on Unix and Linux
systems in terms of its abilityto search strings
with wildcards.Other functions, such
as the PRX functionto the Perl Regular
Expressions in SASalso performs this function.Giving a table of
boys names, youcan see that we
have six names here.The first character
is either T or Band the second
character is always r.And they are varying length.So we're going to walk
through some examples of usageof the LIKE operator.So as you can see, the first
example has Like ""Tr__"".That will return everything
that begins with Trand has four letters in it.And a second example like
Tro with a percent signreturns anything
that begins with Troand have either any number of
characters following that Tro.So that will return
Troy and Troyewith an e, which are four and
five characters respectively.Like ""Try%t"" returns Tryst.If there were any
number of charactersin between the y and the
t, that would be returned.In Like ""_r%"" returns all
six names in the list,because the underscore
is a single character,and everything began with either
B or T, a single character,second letter r, and then after
that any of these charactersare returned by
the percent sign.Like ""Tr___"" will return
all five-letter names.The Like operator, which
we just described and ledto some examples of, searches
the actual spelling of operandsto make the comparison.The SOUNDS LIKE operator
uses phonetic valueto determine whether character
strings match a given pattern.As with the LIKE operator,
the SOUNDS LIKE operatoris useful for when there
are misspellings and similarsounding names in a
string to be compared.The SOUNDS LIKE operator is
denoted with a shortcut, ""=*"",and it must be used in a where
statement in a data step.SOUNDS LIKE is based on
SAS's SOUNDEX algorithm.Strings are encoded by retaining
the original first column,stripping all letters that are
or act as vowels, A, E, H, I O,U, W, Y, and then assigning
numbers to groups of letters.So 1 equals B, F, P, and V.
2 includes C, G, J, K, Q, F,X, and Z. 3 includes D and T. 4
includes L. 5 includes M and N.And 6 includes R. As we'll
see on the next slide,Tristan therefore becomes
T6235 as does Trystan, Trysten,Trystian, and Trystin.Here is the example of that.The link on the
page will get youto more information on
the SOUNDS LIKE operator.This is also in the
paper in the proceedings.So we'll go through a
couple examples of this.And looking at Tristn, the first
character is T. So we say that.Then remove all the
vowels and and lettersthat act as vowels like W and
Y. And then the second characteris 6, because r is equal 6.The third character is 2.The fourth character is 3.And the fifth character is 5.Looking at Tribble,
the first characteris T. We remove all the vowels.And we get Trbbl.The second character is 6.The third character is 1.And the fourth character is 4.OK.Wait.Weren't there five
characters in the stringonce we remove the vowels.There is a side by
side rule in SOUNDEXthat converts two letters
of the same group 1 to 6to a single letter.This makes sense when you listen
to tribble, we only hear one B.But wait.What about Pink Zinc?We remove the vowels.We get PnkZnc with no i's.The first character is P.
The second character is 5.The third character is 2.The fourth character is 2.The fifth character is five.And the sixth characters is 2.That's a compound word.There's a space there.You ignore the blank except
in the instance of twoof the same letter groups
that are separated by a blank.And that also makes
sense when we thinkabout how pink zinc sounds.You can say k and the
z are very distinct.So now we're going to talk about
joins with the LIKE operator.In the first example here,
we're selecting recordswith a like operator in PROC
SQL with a where statement.So the code that we're showing
on the screen right now,selects records from
SASHELP.ZIPCODE file thatare in the state
of Massachusettsand are for a city
that begin with SPR.Note that I'm using the
UPCASE function hereto standardize my variables
that I'm performingthe LIKE condition on.And the reason for that
is the LIKE operatoris case sensitive.And we don't necessarily
know what your variable is.So it's easiest to
standardize anythingthat you know using
the LIKE condition on.It just makes your life
a little bit easier.And this results in
a data set that onlyhas cases with the
city Springfieldand some various
geographic identifiers.And that code here joins
SASHELP.ZIPCODE with itselfwith a renamed T column.And this is just to demonstrate
that you can actuallyperform joins with a wild
card using the LIKE operator.And what this does is it's
taking some different variablesfrom the two different
tables in joining thembased on where a city
has the sames names,and then just selecting
""SPR"" with a wildcard percentsign, so anything
that starts with SPRin the state of Massachusetts.And because it's grabbing
the two different city namesfrom potentially the
two different files--in this case, it's
the same file--are equivalent.So I think you can see the
possibilities of performingin a wild card join with this.And it's really a
very useful technique.And as you can
see, we've grabbedon some different information
from joining the twosimilar files together
and producing a union.The LIKE condition.The LIKE operator is sometimes
referred to as a condition,generally in reference
to character comparisonswhere the prefix of a string
is specified in a search.LIKE conditions are
restricted to the data stepas the core modifier is
not supported in PROC SQL.The syntax for the LIKE
condition is where firstname=:and then the starting
part of the variable.The statement would select
all first names in the tablethat we saw above.And to accomplish the
same goal in PROC SQL,the LIKE operator can be
used with a trailing percentsign in a rare statement.Now we're going to talk about
using PROC SQL in conjunctionwith the CASE WHEN statement
and the LIKE operatorto create variables.The code that I'm
going to show youfeature using the LIKE operator
with four operators and CASEWHEN to create two
types of variables.This variable on this screen is
creating a categorical variablewith values such as cancer
diagnosis, arthritis diagnosisby using a CASE WHEN LIKE 14.So in specifying that DX codes
beginning with certain stringsare coded as specific
diagnosis groupsin a categorical
variable DX type.This is really handy
especially whenyou're using very large data
sets such as claims data setswith billions of records.This is a very efficient way
of sorting through and grabbingthe diagnoses codes
and categorizing them.In this case when
using the LIKE operatorto create a binary
variable, and in this case,variable starting with either.These are diagnosis codes
that are a ICD-9 and ICD-10.So as I encounter any diagnosis
code starting with this,it's going to trigger
a binary 1 valuein a variable called OVERDOSE.And again, this is a
really handy techniqueto use when you are
dealing with huge data setsand just creating
an easy binary.So note that these are
not complete examples.I'm just doing a very simplistic
example to show the technique.So the code snippet is for
demonstration purposes only.You're welcome to contact me
if you would like fuller code.Talk about PROC SQL
CREATE TABLE LIKE.There are several methods of
creating an empty data set.PROC SQL CREATE TABLE LIKE is
one of the most efficient ones.When creating a shell from
an existing data set isit automatically
used the metadatafrom the existing data set.The code below takes
advantage of the factthat OPTIONS DLCREATEDIR enables
the creation of a libraryor folder if one does
not already exist.DLCREATEDIR is benign and does
not overwrite existing foldersor data within those folders.And the CREATE TABLE
LIKE method not benignand will overwrite
an existing data set.So the program snippet that I'll
show you uses conditional logicand &SYSFUNC to determine if
the file to be written alreadyexists and does not
overwrite the file,instead producing an error
note in the log if it alreadyexists.So as I said OPTION
DLCREATEDIR allowsthe creation of a
physical SAS libraryif one does not already exist.So right there,
we have the base,and then I am able
to create foldersfor the previous and
new annual data set.So the OPTION DLCREATEDIR
is default on UNIX and Linuxsystems, but not on
a Windows system.And we may not have access to
it on a secured system at all.But it's a really handy
option, and I encouragefolks to take a look at that.So here is a conditional code.If the file exists, using
&SYSFUNC to tell whetherthe file exists.And if the file
doesn't exist, we'regoing to create the empty table
with metadata from the existingtable.As you can see it's CREATE
TABLE LIKE the previous table.And then I'm just describing
it to make sure that everythingworked directly.And then you see the %ELSE
%DO, I'm going to put an errorin if the file already exists
it's not going to overwrite it,and it's just going to
notify me in my log.And something to note here is
that as of the later versionsof SAS, you can have
open conditional codein your program.And this is an
example of using that.So in conclusion, I
think we've definitelydemonstrated the utility
of LIKE in SAS programming.And I encourage you to add the
various capabilities of LIKEto your repertoire.Like the SMOOTH LIKE operator
and conditions in both datasteps and PROC SQL.The SOUNDS LIKE operator
in the data step,CASE WHEN LIKE in PROC SQL, and
CREATE TABLE LIKE in PROC SQL.There's definitely a lot of
reasons to like LIKE wih SASprogramming.Thank you for listening."
20,"Hi, everyone.My name is Chris Hooks.Little bit of
background about me.I have a master's in economics
from Southern IllinoisUniversity.I've also worked on a variety
of projects, includingcommunity surveys, economic
impact studies, and lobbyingin support of the UN's 17
sustainable development goals.I've also worked as a
business advisor in Ghanawhile I was a Peace
Corps volunteerand a few other things.I've taught economics
and ran a few businesses.But enough about me.Onto the topic of today.And thank you all
for joining as well.I want to talk about an
astounding lack of data.I know the irony, because it's a
data conference and everything.But imagine for a second that
you are running a business.And this is a large
business, where you havemultiple individual divisions.And you only get an
annualized reportof the activities in each one
of those divisions after oneto two years.Now, question.How well could
you make decisionsregarding your business
with this type of data?And secondly, how
successful do youthink your business would be?Is it going to be
sustainable for the future?Or are you going to succumb
to poor decision makingand the results of those?Chances are, your
business is going to fail.Now, keep that in the
back of your mind,because we're not talking
about business data here.We're actually talking
about sustainable cities.And currently, in my
opinion, I should say,sustainable cities are possible
within our current framework.And why is that?Well, because cities lack
data and they lack capacity.They don't have data
analytics divisions--the majority of cities
don't, at least.And there is a solution though.Localizing the data
collection processes.And then second of
all, for the economyand developing a
sustainable city,let's begin by modeling the
cash flows within the community.Now, our goal is to, as part
of the UN's 17 sustainabledevelopment goals-- and I
believe it's number 11--is to have sustainable cities
and communities by, say, 2030.How on earth are we
going to do this?But first of all, because
if you search online,you can find a lot of
different informationabout sustainability and a
lot of different viewpoints.So let's start by,
first of all, definingwhat is a sustainable city.Now, this is a bit of
my own narrative here,but a sustainable
city is a city thattries to live in harmony
with nature and societyin a way that ensures its
viability for the future.That means that we're looking
at things such as housing,transportation, communications,
waste management, resources,environment, and
socioeconomic equity.Now, you might ask, what
is socioeconomic equity?Well, consider the situation in
the US-- and I speak from a USperspective, because I live
here and have pretty muchmy entire life.But we have millionaires-- and
I cannot do the Bernie voice.I would love to.But we also have a massive
proportion of our populationthat is homeless or near
homelessness, because theyquite frankly are struggling
to make ends meet,living from paycheck
to paycheck,and burying themselves in debt.And this is not a way
to have a sustainablesociety for the future.So we need to do
something about this.But the question
is, how do we dothis when our cities have little
access to timely local data?Now, for example, let's
look at some of the datathat cities do
actually have availableand pay special
attention to the years.Now, I picked Wake
County because Sasis located and headquartered
in Wake County.But if you want to
look at analyzed datafrom the Bureau of
Economic Analysis on GDPper sector per
individual region,you can go back to 2001.And the most recent is 2018.Now, last I checked, it's 2020.And so remember
that first slide,making decisions based upon
data that's one or two years oldand it's just annualized
to begin with?This is the kind of
data that our citieshave readily available to them.Let's move on to the
Bureau of Labor Statistics.Now, this is actually
relatively new.And this is kind of cool.But we can actually get
quarterly employment numbersand average pay, total
payroll per quarterper industry per county
in the United States.However, this is
also for Wake Countyand this is the most recent
and this is Q3 of 2019.And currently, we are basically
at the end of Q1 2020.So there's still a time like
it's better and more detailed,but it's still not that
great if you were goingto run a business off of this.Now, let's move on
to the Census Bureau.Now, this is the county
business patterns.You may go from 1946 all
the way up to, if you checkout the year here, it is 2017.Again, let me remind you.Look at the bottom right corner.This is the Sas
Global Forum in 2020.So what kind of data
are city managersusing, are city
economic plannersusing when they're
making decisionsfor economic development funds?Where are these funds going?So can you imagine
making business decisionswith this amount of data,
with this timeliness of data?It's probably not going to
end up very well for you.So if that's the case, if it
doesn't work for a business,how can our cities make
equitable decisions whenthey have such little data?This is a major problem.And on top of that, as per
my own personal experience--and I know, as a
statistician, thisis a little bit of
an anecdotal story.But the city managers
and the city leadersthat I have worked
with generallydon't have that great
of an understandingof the importance of data.They say, OK, data is important.But they don't actually
understand data.They don't understand
statistics.And it's not like I'm
trying to slam them.They just haven't been trained.And they don't have departments
or divisions of their cityor the city governments that
are focused on analyzinglocal economic data.And this is a problem.So here's another graphic.And I'm not sure
if you can see--you might have to pause the
video and zoom in a little bitif you can.But this is from a US
city open data census.And they call it the census,
but it only has 266 locationsand there's several thousand
cities in the United Statesalone.So it's not really a census.It's more of a sample.But number 10 is
Pittsburgh, Pennsylvania.And it has a 78% score.And what that means is
that 78% of the datasets that the city of Pittsburgh
has as a municipality areaccessible for the public.Now, accessible city
data is essential.Imagine you're going to park--well, I know we're not in DC.But imagine you wanted
to go park your car in DCand you wanted to look
for a parking spot.It's kind of hard.You can pull out your smartphone
and find a empty parking spot.But that all happens because
of open data and API accessto that data.So that brings me
to my next point.I propose a
Copernican revolutionin economic analysis.And think about
that for a second.Can you answer, what's
the GDP of your city?Now, if we went back a few
slides, you can get that data,but you're going to
have to go back to 2018.So what is it in 2019?Now, what about the
GDP of your state?Do you know?Chances are you probably don't.What about the national GDP?Yeah, people probably
can guess at it.They know if it's
doing good or bad.But really, what
does the national GDPhave to do with them
and their local area?How much of an impact does
the change in the national GDPhave on their city?How many people in
your city struggleto pay their bills every month?The GDP may be growing,
but if the numberof people in your
city strugglingto pay their bills every
month is also growing,then that does not seem to be
very sustainable in the future.So while most people
know the national,but not the state
or the local GDP.They might know
how many people--unless you have a really
innocuous peer groupand don't really get out and see
what's actually going on a dayto day basis in the city.You might not really realize
the full extent of what'sgoing on in the economy.And that might sound a
little bit confusing.But think about it.If your focus is on the state--or not the state or not
local, but on the nationalGDP and it's growing,
while at the same timethe number of citizens
in your city strugglingto pay their bills
every month is growing,there's kind of a
mismatch of information.Why is the economy of your
local city not more important?Now, let me take this
time to make a note.GDP is not the end all, be
all, most important metricof economic activity.I could say that I eat 20
kilos of food last week,but that doesn't tell
you much of anything.GDP is basically a
quantity measure.So I could have eaten 20
kilos of potato chips.And that would have been
really, really unhealthy for me.Likewise, I could have had 20
kilos of fruits and vegetablesand that would have
been much healthier.So GDP doesn't consider
things like income equality.That's an important factor,
because if people on the lowerspectrum of the end of
the socioeconomic spectrumdon't have enough cash
to pay their bills,they're struggling
to survive, theyare the foundation
of the economy.And if you have never looked
at a building with a crackedfoundation, hopefully
you didn't buy it,because it is bound to
have massive issues.So we need to shift our
focus away from managinga large heterogeneous
national economyand focus on our
local economies.Basically, what I'm saying is,
how do firms and individualsinteract within the community?We don't really
have data on this.As an economist, we talk
about rational agentsand representative
agents and such.But is that really how
people make decisions?People don't sit there
and do a calculus problemto go and buy groceries
or decide whether or notthey're going to
have a candy bar whenthey're at the gas station.It's just not how
people really work.Now, granted,
calculus and algebraare very important still.So how would you go about
managing a local economy?Well, I propose using an
investment portfolio approach.With a portfolio manager,
the portfolio manageris sitting there analyzing
data and making decisionson which stocks to buy,
which stocks to sell,because their goal is
to optimize or maximizethe return on the portfolio,
while at the same timeminimizing the risk.Now, if city managers
or cities lookat the local economy as the
city's economic portfolio,can't they do basically
the same thing?Now, they can't go and buy
and sell shares of stockin individual companies
and individual sectorsor subsectors of the economy.But they have other tools.So from a city's perspective,
it's more like a businessand you're looking
at several projects.And how do you choose between
all of these projects?You start this project,
terminate this project.How do you make those decisions?You're looking at the
incremental cash flowsforecasted into the future.And you're trying
to estimate, what isthe net present value of that?And by maximizing the
net present value,you're also maximizing
your value of your companyor your stock price.So what if cities kind of took
the same ideological frameworkand applied it?And case in point,
I have to say,portfolios are all
about diversification.You do not-- while you
can invest in one stockand possibly get a
really big return,you can also possibly get
a really, really big loss.So we want to diversify
our local economiesso that we don't end up
with another situation,such as Detroit.So from the idea of
targeting economic sectorswith a portfolio
framework in mind,economic development
funds can betargeted at developing
specific sectors, subsectors,targeted industries, et cetera.But those industries should not
remain dependent upon subsidiesin order to survive.Five years, you should be able
to phase out your subsidies.Maybe 10 years max.You don't wnat a
case of having your--imagine being a
parent or something--which I have no kids.But imagine having a kid
that grew up and neverwas able to go and take
care of themselves.And it wasn't for
lack of ability.They just had lack
of motivation.You don't want to have
a situation like that.Consider the energy industry
in the United States,consider the agriculture
industry in the United States.Both of these industries
are massively subsidized.Energy is an input to basically
every single manufacturingprocess.And it's also an input
to transportation.Agriculture-- I mean, everybody
needs food to survive.Why is it we have
massive subsidiesfor these areas of the economy?Now, that's a question that
you can ponder in your minds.So how does a city go about
determining where funds shouldgo and what areas to develop?That's a good question.But this is precisely
why we need better data,because without that data, we
cannot really go and say, well,this area is better because it's
going to generate more revenueand recirculate that cash
back into the local economy.I watched a documentary
several years ago.Cannot remember the name of it.But they were talking--they basically took
a couple of beaversand put them in
this little ravinewith a stream running
through out in the middle of,I think it was Nevada.And it was basically a
completely barren area.Now, those beavers went
and they built the dam.And that water backed up.And it pooled up and
it stayed in that areafor a longer period of time
before it finally left.And over the course
of a few years,it completely changed that
little micro environmentand brought life to the area.In the same sense,
cities can target fundsin order to recapture
cash that's alreadyexiting their communities
to kind of capturethat, recirculate it
through the economyagain, and then let
the cash flow out.For example, did you know
that the average plate of foodtravels an average
of about 1,500 milesfrom the farm to your plate?Now, food is a basic
necessity of life.1,500 miles away
from your community.What if that food
was grown locally?That would generate
a cash recirculationthat is sustainable and
necessary just for the very--in order to be able to
live, you have to eat.So this is essentially
a massive cash flow.But it's just exiting the city.And there's a lot of others,
but that's just one example.So in order to actually tackle
this issue of collecting data,you need to first understand
the data generating process.And essentially, you
can view the economyas a series of traits.We don't really live
in a barter economyI'm not going to
the grocery storeand saying, well, let me stock
shelves for two or three hoursso that I can get
some food to eat.No.We go and exchange our
time as labor for money.We loan out our funds,
our capital for money.And then we take that money
and exchange it for goods.Each one of those
transactions isa cash flow within the economy.And that cash flow
is also representedby either a flow of time
for me as labor, capital,or there's an associated
good or service.But we're not actually
tracking these transactions.We are, but it's
kind of piecemeal.So cities should invest, if
they want to have a sustainablecity, in data collection,
warehousing, and analysisat the local level.I think I skipped
something there.So in the process of
collecting this transactiondata on the economic
activities of the area,it puts cities in a
prime spot to facilitatethe growth of small and
medium-sized businesses.Now, I'm talking about
collecting informationon cash flows.So you go to the store
and you buy something.I can say, well, you can go on
and look at your bank recordsand see, OK, well,
I made a purchasefrom this merchant
this day and this wasthe amount of that purchase.I want to also know the
goods that you purchasedso that I can tie that
cash flow to that good,because then I'm looking at
the exchange of resourceswithin the local economy.And that might be a little--people might think that's
a little bit intrusive.But the truth is, the
data is already there.The company that you
purchased those goods from,if you're a rewards member,
they could definitelyhave really accurate data
on all of those transactionsthat you've made.Your bank has information on
where you spend your money.And all I'm saying is, let's
pair these two things togetherand utilize it to build
sustainable cities.I have more information
about data privacyissues in the paper,
so please check it out.I basically don't have time
to introduce that topic here.But the city can provide
essential servicesto businesses and individuals
to help them thrive.And as an example, let's
look at for a local business.One of the main reasons
that startups failis poor cash flow management.And many small
business owners do notunderstand how to manage
their cash flows effectively.They may understand how to take
care of the operational sideof their business.But when it comes to cash flow
management, they don't know.And your mom and
pop shop does nothave the financial clout to
fund an entire finance division.So what if the cities
that are alreadycollecting all of this
data and modeling the cashflows within the community
gave you back or gave backto these businesses information
and provided services,such as inventory management,
cash flow and salesforecast, capital
budgeting tools, payrollservices, employee scheduling,
hiring and job board services?All of these are
very, very feasibleand can be more
efficiently providedwhen that data is being
collected locally.What about for a citizen?How many people in the United
States or around the worldhave a very good knowledge
of personal finance?But what if I am tracking all
of your-- not to spy on youand see what you're buying,
but for economic benefitsfor the community as a whole?But I'm tracking
your transactionsand I can categorize
how much moneyyou spent on individual goods.I can take my smartphone
and setup time limitson specific applications.Why can I not set up
categorical spending limits,so when I go to make a
purchase of a basket of goods,well, some of those goods
that are in that basket mightflag saying, hey, you already
spent this much money.This was your budget.Are you sure you
want to spend this?Just make people think
about that for a second.I'm not saying stop
them from buying it.If they want to buy
it, it's a free countryand let them do what they want.But just put that
alert there and letthem make their own alerts.Receipt organization.No more paper receipts.They're all digitized.How many of you have lost a
paper receipt somewhere or itaccidentally got thrown
away and, oh, I needed that?What about financial
simulation tools?Say you want to buy
a house or a car.Product search.If I'm doing
inventory managementfor all of the
businesses locally,I absolutely know what is going
on in every single business,like what their inventory is.I'm not sharing it with
their competitors so much,but if you want to find
it, basically think of itlike a local Amazon.Need a job?You can have your profile
built in a very uniform way.And businesses that are looking
for employees with your skillsets would automatically
notified that, hey, you'relooking for a job.And you could turn
that feature on justlike you want to turn on
a setting on your phone.There's many, many more.But unfortunately, I don't
have the time to do that,because I can add a lot.But please read the paper for
a more thorough examination.And contact me.I'd love to hear your thoughts
and answer any questions.But the main point
for the futurereally is, in order to ensure
that our descendants havea viable world to live in, we
must act now to make it happen.There are many
competing projectsand special interest
groups out therepushing their own agendas.But what's the agenda of
the local populations?Do they even have a
coordinated agenda?If you want a better community,
a better economy, and a betterfuture for your
descendants, then youhave to plan strategically
and build it.Too often, our governments
make short-sighted reactionarydecisions to appease the
public and get re-elected.But if we did not learn
to plan for the long term,then our species is pretty
much, quite frankly,it's just doomed.And we can have a better future.Remember, we can
have a better future,but it is absolutely not going
to happen without planningand coordination.And better planning does not
happen in an absence of dataeither.So if we want a
better future for all,we really need to start
collecting better data today.Thank you very much for
taking the time to watch this.Sorry I couldn't
give this in person.It would have been
so much more fun.But I'm glad, and I hope
everybody's staying safe.And here's my
contact information.I'm currently a
student at NC State,and so when I'm no
longer a student,that email's not going to work.So there's a second one.Thank you all again,
and have a good day."
21,"Hi.My name is Andy Ravenna,
and I'm a technical trainerfor Sas Institute.Today, I'd like to
talk to you about howto perform end-to-end
modeling, includingmachine learning in Sas Viya.I'm going to start by giving
you a very brief introductioninto the analytics
lifecycle, and we'll alsotalk just a little bit
about Sas Model Studio.Then I want to discuss
with you the data that I'mgoing to be using for
the demonstration,then I'm going to jump right
into the demonstration.The analytics lifecycle
consist of three phases.There's the data phase, where
we get access to our dataand begin to
understand our data.There is the discovery phase,
where we begin to build modelson the data, and then finally,
in the deployment phase,we're going to pick
a champion modeland we're going to put
it into production.Model Studio, which is
included in Sas Viya,is an integrated
visual environmentthat gives us a whole
suite of analytic tools,and it allows us to perform
end-to-end data mining, text,and forecasting analysis.Model Studio is a
common interfacethat includes the following
Sas solutions, Sas visual datamining and machine learning,
Sas visual forecasting, and Sasvisual text analytics.The data that we're going to be
using during the demonstrationis taken from a large
financial service.They offer things
like home equity linesof credit, automobile loans,
and other credit instruments.The data has been anonymous
sized and transformed,and it includes a
campaign from the bankthat ran for six months.During the campaign,
they offeredboth direct and
indirect promotions.We have lots of descriptor
and explanatory dataon the customers
and their activity,and our primary target
variable is binary.It's simply an indication
as to whether or nota customer made a purchase
during the campaign.So in this demonstration,
what I'd like to dois really perform that
end-to-end analytics lifecycle,where we're going to start
by loading some data into SasViya.Then we're going to build
some models using pipelinesin Model Studio, and then we're
going to pick a champion model,we're going to put it into
production using Sas ModelManager.Let's go ahead and get started.The first thing that we want
to do is get a hold of the datathat we want to use
in the demonstrationand bring it into Sas Viya.So you see I've got a link here
from Sas Viya to Sas Drive.Sas Drive is basically,
what I think of,as the home page,
where we get startedwith all of the different
functionality that'savailable to us in Sas Viya.I'm going to log in
using the default login.You've probably got your own
log in back at your officeor wherever you have
access to Sas Viya,and I am going to opt into
the assume mobile groupsas a Sas administrator
just in caseI want to do any of those
high level activities.This is going to
land me directlyinto Sas Drive, which
is our home page.And what we're going to do is
go to the very upper left-handapplication menu, and what
you and I want to do todayis build some models.So when we select
build models, youcan see it's taking us
directly into Model Studio.And what we want to do
inside of Model Studiois we're going to start
up a brand new project.You can see that
there are alreadysome other projects that
are available to us,but we're going
to start our own.I'm going to call this
particular project Model StudioWorkshop, and we are going
to be creating data miningand machine learning objects,
so we're going to leave that--you can see the
other choices hereare if we wanted to perform
forecasting or text analytics.Now, I want to get
access to the data,so I'm going to click
on the Browse button,and it turns out that our
data is out on the hard drive.So what we're going to
need to do is import it,so I'm going to go
over to the Import tab.We're going to come down,
open up local files,and access a local file.And if I go on to my D drive,
I've got a folder here.All the way down this path,
that contains the datathat you and I
are interested in.We're going to open up the
banking partition table.Now, we're ready to
import this table.It's a CSV file, and
what's happening nowis this table consists of
just over a million rowsor a million observations,
as we like to say at Sas.And now, we're taking those
1 million rows, includingall of the columns of
explanatory information,and we're going to
load it up into data--load it up into memory.So now, it's loaded into memory
so it's ready to be used.I'm going to click on
OK, and the other thingthat I'm going to do is if I
scroll down in this new projectwindow, there's an
Advanced button.And since we're
interested in buildingseveral different
models, in orderto help us pick the
best model, we'regoing to use partitioning.So you can see there's
a selection wherewe can partition the
data, and by default, it'ssplit into three partitions.You and I are just going
to use two partitions.We're going to use
a training, whichis going to be 70% of the data
and validation, which is goingto be the remaining 30%.So we're not going to have
that third test partition.I'm going to go ahead and
save those project settings,then I'm going to click on Save,
and what's going to happen nowis Model Studio is going to
bring that data in and beginto build some meta data on it.So the project is just
getting ready to open.And when it does open, we're
going to land on the data page.And on this data
page, you can seethat it's starting to load
in all of the metadatathat it's created.We're going to see a
little message hereat the top that says you
must have a target variable.And so in order for us
to build our models,we need to identify
that target variable.It's b_target.It's a binary variable
between 0 and 1,where 1 is an indication that
a customer made a purchaseduring the campaign
season, 0 meansthey did not make a purchase.So I'm going to highlight
or select that variable,I'm going to come over
here to the rolls,and I'm going to identify
this as my target variable.And then as soon
as I do that, we'regoing to lose that
little message that says,hey, I need a target variable
in order to continue.Let's move from the Data tab
over to the Pipelines tabso we can start to build
our first pipeline.We're going to start
with the data node, whichrepresents our data.I'm going to right click this,
I'm going to add a child node.We're going to do some
data mining preprocessing,and the first thing
that we'd like to dois to perform some imputation.We're going to be looking at
a neural network model here,and neural networks really
don't have their own wayto automatically perform
any sort of imputation.It's a model similar to other
models that use complete caseanalysis, so we'd
like the softwareto impute any missing
values for us.After we perform
that amputation,we're going to add another
child node of data miningpreprocessing, and
here, we're goingto select the variable
selection node.So also, neural
networks don't haveany way of performing their
own variable selection.A lot of times, we have
many, many columns or inputsinto our model, and we
want to narrow those downto the most important.So here's what
we're going to do.For the variable
selection node, I'mgoing to come over to my
variable selection options,and I'm going to
change the combinationcriterion from selected
by at least oneto selected by a majority.Then we're going to
identify three methodsthat we're going to use.We're going to leave the fast
supervised selection method on.We're going to add
in linear regression,and I'm going to minimize that,
as well as decision trees.And so those three
methods will be looked at,and if-- by majority.So that means if two of those
three methods select an input,then it's going to be
included in our model.The next thing
that we want to dois add in our neural network.So underneath my
variable selection,I'm going to do a
right mouse click.I'm going to add a
child node, and we'regoing to move into some
supervised learning.I'm going to select
a neural network,I'm going to add that
onto my pipeline,and then we also want to
add in a competing model.So we're going to come back
up here to our data node.I'm going to do a right mouse
button click, add a child node,go down to supervised
learning, and we'regoing to select a forest.Forest have their own way of
performing both imputationand variable selection,
sort of automaticallyby looking at splits, so
they don't need those twopreprocessing modes.Let's run this pipeline.My model has finished running,
so let's start to investigatesome of the results.I'm going to start by taking a
look at our variable selectionnode, and we're going
to go into the results.You can see we have a
variable selection table,and if we expand
that, we can actuallysee which of these
variables are inputvariables have been rejected
and why they've been rejected.So the reason is a
combination of our criterion.So we're going to close the
variable selection table,and we're going to open up the
variable selection combinationsummary.This allows us to see exactly
why a variable was rejected.So we can see, for example,
we have a variable calledthe demographic of whether or
not a particular customer ownsa home or not.This was rejected by all three
of our methods, the fast,the linear regression,
and the decision tree,and so therefore,
it was rejected.Let me go ahead and close that
variable selection combinationsummary table,
and I'm also goingto close the results or output
from my variable selection.Next, let's take a look
at our neural network.So I'm going to come over
to my neural network.You can either right
click or you canselect the three little dots.We call that the snowman
or the overflow menu,and we're going to
go into our results.Now, the results for
the neural networkare going to show us a network
diagram for those of youthat are familiar with those.They're interesting to look at
but they really don't tell ushow to interpret our model.Let's take a look at
the iteration plot,which I'm going to expand.And we can see the
validation erroris decreasing as the neural
network begins to learn.In other words, the model is
improving its performance onthat validation data as our
neural network is trained.I'm going to close that,
and I'm going to move overto our Assessment tab.And you can see that we have
several different assessmentreports.I'm going to scroll
down, and I'mgoing to open up the model
fit statistics report.And we have several different
model fit statistics.Hopefully, you'll find
your favorite one.I've got the
misclassification rate,I've got the average
squared error,I've even got our KS value.So there are lots of
different model fit statisticsthat we can use to help
us assess our model.I'm going to close
that, and let's go aheadand close the neural
network results.Finally, let's take a look
at our forest results.So I'm going to click
on that overflow menu,go into the results here,
and what I'm going to dois take a look at
these error plot.You can see, we can
actually look at this plotby a couple of different ways.We can look at the
average squared erroror the misclassification
rate, but we can definitelysee that the error is
being reduced as weadd additional trees here.And so as you would
suspect, the training data,which is this blue
line, is alwaysgoing to improve if we continue
to add additional trees.Let me go ahead and close that.I now want to show you how easy
it is to integrate open sourcemodels into pipelines.And it just occurred to me.Why don't I take
this chrome windowand move it into
full screen mode?This gives us a little bit
more real estate to play with,and gives us a little better
view of Model Studio Workshop.I'm going to come over
to my imputation nodeand do a right click.We're going to add
in a child node.Underneath the
miscellaneous category,we're going to add an
open source code node.I'm also going to
right click so that Ican rename this open
source code node,and we're going to call
this our Python forest.And click on OK, then
I'm going to right clickon the Python forest, and
we're going to open it.And that's basically
where we're goingto be able to type in the code.So I'm going to come to
the end of line number 16and hit Enter.And it says, hey, put
your code after this line.So this is where I want to
begin to code my Python forest.Now, I am not a Python coder.So what I'm going
to do instead isI have a notepad that
is already opened here,and it's a text file.The Python forest code.text.I'm going to do a
right click on that,and then we're going to open
that up with the notepad.Then I'm going to do a Control-A
and a Control-C So that I cancopy that code, and we're going
to go back over here to a ModelStudio, and starting in line
17, I'm just going to doa Control-V. I'm
going to type in--paste in-- that forest code.Let's go ahead and save
that, and then let's close.Now, we've added the Python
code node to Model Studio,but now we also want
to tell Model Studiothat we like to use this
node such that it containsa predictive model, and
that way Model Studio willoutput the score data for us.So I'm going to do a right
mouse button click on that,and I'm going to say, let's move
this to supervised learning.And then you'll notice
that it actuallychanges the color of
that node so that itmatches my other models.That's pretty cool and
was relatively easy to do.One other thing
I'm going to do isI'm going to come down to
my model comparison node.I'm going to select
it so that it becomesthe active item in my
interface, and whatwe're going to do is the class
selection statistic we'regoing to change this from the
default set in the project'ssettings, and what
we'd like to dois misclassification
of our events.And so this is how my models
are now going to be compared.Let's go ahead and
run this pipeline.My Python forest has
finished running.So now, we've got three
models that we can compare.Let's go to our
model comparison nodeand take a look at
the results, and wecan see that based off of
the misclassification ratethat the forest is
the champion model.Let me come over to
the assessment pane,and you can see that we
also get ROC reports.Let's change this from
accuracy to receiver operatorcharacteristic
chart, and now, wecan see that in terms of
avoiding misclassification,if I mouse over here, it
looks like that forest alsohas the best ROC curve
of all my models.So probably is
misclassification,and if we looked at
the KS statistic,it would also probably
indicate that the forestmodel was the winner.Let me go ahead and
close my ROC reportsand close the results from
the model comparison node.Let's go over to
our Python forestand just investigate
what kind of resultswe get from running that Python
forest, and what's fascinatinghere is, if we scroll
down just a little bit,I can take a look
at my Python output,and that's going to
show me that it actuallybuilt a first model,
and the settings thatwere used to create that
forest, so that's useful.And the other neat thing is if
I look over on the assessmentpanel, you're going
to see that weget the same type
of assessment outputthat we do for all
the other modelsthat we created in Model Studio.So we get the lift
information, the ROC reports,as well as that model
fit statistics table.Let me go ahead and close that.Model Studio gives
us the abilityto build multiple
pipelines and thencompare across those pipelines.So while we're in this
discovery phase of the analyticslifecycle, let's add
an additional pipelinebeside the pipeline 1 tab
there's a little + button.We're going to call this
new pipeline, pipelinefrom template,
because we're goingto use one of the
default templatesthat comes with Model Studio.I'm gonna click on Browse,
and I'm going to scroll down,and I'm going to find the
intermediate template for classtargets, and these are a whole
bunch of default templatesthat come with Model
Studio, so theymake it very easy for us
to build these pipelines.I'm going to click
on OK, and you'regoing to see what comes inside
of that intermediate template.That intermediate
template consistsof a stepwise logistic, a
forward logistic regression,and a decision tree.I'm going to select the decision
tree so that I can get accessto the options.I'm going to open up
the splitting options,and we're going to take
that minimum leaf sizeand bump it up from five to 500.That's to get it to run
just a little bit faster.Let me go ahead and minimize
the splitting options,and let me also go into
the pruning options,and I'm going to change that
substring method from costcomplexity to reduced error.Now that I've made
those two changes,I'm going to run
this entire pipeline.My new pipeline is
finished running.Let's go in and take a
look at some of the resultsfrom the decision tree.I'm going to open up the
results for my decision tree,and what we're going to see is
an interactive tree diagram.You can actually go in and spend
time expanding the diagram,looking at the branches and
the leaf nodes of the treeand going in and taking a
look at all of the statistics.I'm going to come down to
the error pruning plot,let me scroll down
here, and also show youwhen we expand on
that, one of the thingsthat I love is that we do
get a little bit of text herethat basically tells us
using that validationpartition in order to prevent
over fitting on the selectedsub tree was a
total of 34 leaves,and we also have the
misclassification ratethat's there.That's some great
information thatlets you see exactly what
happened with the model.It's also very easy to
interpret as well, I think.Let me close the
error pruning plotand close the results
from the decision tree,and in this brand new
pipeline that we've created,I'm going to come down to
the model comparison resultsand investigate those, and
we'll see that from these threemodels, the stepwise
logistic, the forwardlogistic regression,
and the decision treethat based off of
the KS statistic,you can see that the
stepwise logistic regressionis the winner.So it's got the
highest KS value there.One thing that's
kind of interesting,though, is that if we were to
look at the misclassificationrate, it turns out that the
decision tree has the verybest misclassification rate.It's got the lowest in
this classification rate.So now, depending on
which one of these modelfit statistics we
would choose, wemight get a different champion.So what I'm going to do in the
next phase of us investigatingthese models is I'm
going to add the decisiontree as a champion to the model
so that it will be included.So now, I've built
multiple pipelines.Let's move from
the Pipelines tabover to the Pipeline
Comparison tab.Pipeline comparison
is going to allowme to compare
multiple pipelines,and then we're going to pick
the champion of the champions.Based off of the two
pipelines that I have,it looks like the forest is
the overall champion bottlewith the highest KS statistic.But if we wanted to dig into
a little bit more detail,I can select all of
the champion modelsfrom all of the pipelines
and compare them,and if I scroll down, I've got
a nice model fit statisticstable, and we can
see, for example,maybe we look at the
misclassification rate.We would want to look at the
validation misclassificationrate for my stepwise logistic,
as well as the validationdata for the forest,
and we can seethat the lowest
misclassificationrate is held by the forest.Let me go ahead and close
the Fit Statistics windowand close the Comparison window.You'll remember one of the
other things that I wanted to dowas I wanted to go
back to my pipelinesand originally,
when we were lookingat this particular
pipeline it lookedlike the decision tree had a
better misclassification rate.So let's take this
decision tree,and let's add it as a
challenge your model.And when I add that
as a challenger model,I can come back over to
my pipeline comparison,and you can see that now the
decision tree has been added.One of the other
really great featuresof Model Studio workshop is--or Model Studio and my
workshop that I'm doingis this Insights tab.And what we get is we get
a nice project summary.That projects summary
has automaticallygenerated text that explains
that champion model to us,and also the most
important variablesthat are used in that model.If we scroll down
just a little bit,you'll see that we also have the
most common variable selectedacross all of our models,
so that's really showing meall of those variables that are
important to the models thatare in our pipeline
comparison window,and you can actually click on
this little information buttonto get another nice
automatically generatedset of texts that can help
explain what's going onwith that particular output.There's also a
Project Notes window,and this is a good place
for me to go in and makeany notes to myself
about what I learnedabout this particular project.We're now ready to move into
our third and final phaseof the analytics lifecycle.We started in the data phase by
loading our data into memory,then we spent some time
in the discovery phase,where we created several
different models,and then we discovered a
champion model, or two,that we may want to
consider, and now wewant to take one or
more of those modelsand deploy them or put
them into production.So I'm going to do
that by switching overfrom my Insights tab.I'm going to come back over
here to pipeline comparisons,and as it turns out, I'm going
to be interested in the forestmodel and the decision
tree, and whatwe're going to do is with
both of those models,we're going to
register them by goingto this upper
right-hand overflow menuand saying that we want
to register the models.So we're currently
inside of Model Studio.And Model Studio will allow
us to register those modelswith Model Manager.What's happening
is some metadatais being created in the
background about these twomodels so that when we
move from Model Studiointo Model Manager, Model
Manager will be ready for usto take that and deploy it.So as soon as these two
have finished registeringand they just finished
being registered.I'm going to close this
window, and I'm nowgoing to go to the upper
left-hand corner applicationmenu and switch from build
models into Manage Models.Then I'm going to come
over to the project's icon,and you can see there is
my Model Studio workshop.When I open up that
Model Studio workshop,here are the two models that
we have registered and are nowavailable to us over
here in Model Manager.I'm going to come over
to the Properties tab,and in the Properties
tab, I'm goingto set a couple of options here.I'm going to set my
target event value to 1.I'm going to specify that
my target level is binary,and then I'm going to choose
the output event probabilityvariable as em_eventprobability.Let's save these changes
that I've created so far,and now, I'm going to move over
to the scoring tab, where we'regoing to be able to create
a brand new test so that wecan test our model in
the deployment phase.I'm going to call this
particular test scoringexample, and then we're going
to want to choose the model.The model that I'm going to
select is the forest model.Then we need to
choose an input table,so I'm going to
select an input table.It turns out we're going to
need to import the table that wewould like to score.So you'll remember that
the process of scoringis to take your champion
model that you've selectedand then apply it to brand
new data, either new patientsor new customers.In our banking data, it's
basically new customers.I'm going to go to
the local files,and I'm going to select--
we've got a bank score Excelspreadsheet that I can import.So we're going to import
these new customers.This table of
brand new customersinto this particular testing
item so that we can run a testand see how our
model is performing.I'm going to click on OK.I've got everything
filled out here,so I'm now ready
to click on Run.So that one test is running.Remember, this is the forest
model that we created,and you can see here that the
status is that it is stillrunning, so we'll
wait for the new datato process through
this forest scoring.And then as soon as it
finishes scoring, what we'll dois we'll take a
look at the results.OK, my model finished scoring.I'm going to take a
look at the results.We're actually going
to go into the output,and you can see here is
the predicted probabilitythat b_target is a 1.In other words, how likely
is it that this customer isto make a purchase.You can see that it's only--this customer is only about
8% likely to make a purchase,so they're actually being
coded as a nonpurchaser, whichmakes a lot of sense.All right, thank you
very much for joining metoday and exploring how to
perform end-to-end modelingwith Sas Viya.I look forward to
seeing you next time,and if you have any
questions, please leave themin the comments below."
22,"DUNCAN BAIN: Hello, everybody.Welcome to this presentation
as part of the virtual forum.We're very happy to have the
opportunity to present to youall, even though we
can't do that in person.So the title of
this presentationis Deploying Computer Vision by
Combining Deep Learning ActionSets with Open-Source
Technology, whichis a bit wordy.But hopefully, we'll
be able to explainwhat that means in practice
as we go through the slides.So the presentation today,
you hear from ScottishPower,myself, Duncan Bain.I lead the data science and
science campaign functionfor domestic customers
at ScottishPower.Joining me are my
colleagues, Jonny McElhinney,who is a data scientist
working for me under my team.And also, from
SAS, Haidar Altaie,who's been supporting us in
the delivery of this project.First of all, I'd
like to start offwith a little bit of thanks.This has been quite a
challenging project for us,it's got far and
certainly wouldn'thave gotten nearly
as far as it haswithout the help
from these people.So from ScottishPower,
I'd like to thankGail Miller, Jessica
Walkenhorst, Cara Tulley,and Monica Murphy.And from the SAS side, Scott
Bowler, Emma McDonald, JenniferMajor, and Prashant Chamarty.These guys have all been
instrumental in makingthis happen for us.And I'm sure you'll
understand whyas we go through
the presentation.So first of all,
about ScottishPower,who are ScottishPower?Well, most people think of
us, they think about this.So some wires are going
across the countryside.Or hopefully, maybe this.This is a picture of
our Whitelee Windfarm,it's one of the
largest in the country.So both of those
things are true.Our network business
delivers powerto millions of households
across the whole of the UK.And the renewables
business is partof that, generates 100% green
power from on an offshore windfarms.And we're quite proud
to say that for allof your online products, we
are generating 100% green powerfrom our wind farms.And that's what you
get out of the wiresat the end of the day.But that's not what
we're here to talk about.So I work in the energy
retail part of the business.We sell gas, electricity, and
other services to around 10%of the UK domestic market.So we have approximately
5 million servicesthat we supply, which is
about just under threemillion households in the UK.We also provide those services
to small and medium sizedenterprises.And also, we have some large
industrial and commercialclients as well.And those range
from customers whojust have multiple
small sites upto large factories and
industrial complexes.So that's a little bit of
background about ScottishPower.So to lead into the main
part of the presentation,I'm just going to give you one
or two quick stats about whatpeople contact us about.So around 30% of
all our contactsare either through
the web channelor through telephone or
email about meter readings,so if you wanted to give us a
meter reading, to resubmit one,to ask a question
about one that'scome out as part of a bill.And what about smart meters?So in the UK, as in
many jurisdictions,we are rolling out a
smart meter program.And with the aim of course
of eradicating those meterreading contacts and
bringing everyone'sbilling as up to
date as possible.But as it stands in the UK at
the moment, only about a thirdof our customers have
smart meters installed.And that's pretty true
across the whole of the UK.Some of the early
versions of smart metersrevert to dumb mode when
changing supplier becauseof differences in the
firmware and the supportthe different suppliers
were able to giveto those customers.And equally, some customers
are choosing not to have one.The way the regulation's
shaped in the UKis that, although we are
obliged by the governmentto fit a smart meter in every
home, as a domestic customer,you're not obliged to have one.And unlike our
colleagues in say Spainor some of our other
European businesses,a lot of metering equipment in
the UK is inside the property.So it's not like we can
just go in and changethat because we want to do so.And yet, there's still a lot of
people who cannot yet get onefor technical reasons.And they range from so
the mobile signal is notstrong enough for the
meter to communicate backwith its home base.All the gas and
electricity metersare too far apart, so they can't
communicate with each other.Or maybe, it's as simple
as you hid your ugly gasand electricity meters
inside a tiny cupboard.And that cupboard is
now too small to fita larger smart meter into.So just by means
of illustration,these are government
figures from the UKthat have been collected from
all the energy suppliers.And as you can see, the fact
that we have about a thirdof customers on smart
and 2/3 non-smartis pretty much in line
with the rest of the UK,with the exception of a few
kind of niche providers, who areonly have smart-only products.So what was the main
challenge of the project?So given that we have
30% of our contactsthat refer to meter
readings, could wedevelop our own
solution in-houseto meet that challenge?So with the aim of reducing
the need for customer contact,giving assistance to
some of our customerswho've got a visual or mobility
impairments or all of the morecomplex needs that
we want to give themsome extra support with.And also, improve the
overall customer experience.So key to this is we want
this to be a seamless customerexperience.We don't want to make
this an onerous thing.We want it to be easy
for the customer to do.Because if it's not,
they'll never do it.It's going to drive down
costs, because any contactwe can take out of the system
reduces our overall cost base.We're looking to leverage
the fact that we'redoing this in the
ScottishPower appto drive up penetration,
which opens up the abilityto use that as an
additional sales channel.It generates interest
in us as a company.It's good to be seen
as a company thatis doing new innovative things.And ultimately, we hope that's
going to increase sales.So there are some
challenges to this.Getting GPUs on premise is
an expensive investment.By and large, we're seeing
the new JSON site coming in.I'm very highly
proficient in Pythonbut less so in other
languages, such as that.And we already know
from all the projectswe've been involved
in that sometimes wecan see that there are
issues in deployment with allthe technologies that we've not
experienced when we've deployedsome of the more
traditional modelingout through the SAS platform
that we have on premise.So at this point, I'm going
to hand over to Haidar,and he'll tell you a little
bit more about this technology.HAIDAR ALTAIE: Thank
you very much, Duncan.So yes, we saw some of
the challenge that we had.And in line with all of
the challenges that Duncandiscussed, we came
up with a solutionto use SAS DLPy, which
assesses Viya's Deep LearningAPI for Python.And essentially, what this is
it's a high level deep learningAPI for Python.So it allows you to use APIs
to mimic the feel and nativelook of Python, but be able
to use the CAS actions.So the CAS action being the
SAS Cloud Analytics Serviceactions, this will allow
that in-memory platform.It will allow you to use
predefined deep learning modelsand deep learning action sets.And that was the key thing.You were able to use those
predefined models, which Jonnywill talk about
later on, and usethe strength of both platform.So you are able to leverage
what we have in SAS.But similarly, you can use open
source packages from Python.And you then have the ability
to import Keras, Caffe modelsand ONNX models, which we'll
talk about a little bit later.It does flow by
building a wrapper.So we've got a
SWAT package, whichallows you to essentially
communicate with the SAS CASserver through an API.So the DLPy covers a lot of
areas within computer vision.But similarly, it
does other things,such as recurrent neural
networks for things like text,speech to text, and
times table forecasting.But we will focus on
the computer visionelement for this.From an architecture
perspective, what we did is weused the APIs to call
the deep learningaction sets, as we mentioned.And then, we can use the
CAS analytical server, soCAS engine, to run
either on CPUs and GPUs.And this was one of the
key components for usto not have GPUs
running all the time,because they can get very
expensive, as Duncan mentioned.Once we have that, we can
use ONNX for the scoring.So ONNX is an open
neural network format,which allows SAS to interact
with other deep learningplatforms.And we'll talk about why
that was so importantfor this particular project.All of this was done
on an AWS Cloud.And the reason why
we went for thatis because AWS allowed us to
use elastic GPUs with AWS EC2.So how that work
is our applicationis normally run in
CPUs, as we'd expect.But then, when we needed to,
we can call a graphical API,and we call then
the elastic GPUs.And that will enable us
to use 4 different GPUs.And this was really
good, because CPUswas used for the majority
of the model built.But once we needed to do heavy
testing and heavy scoring,we can just use that graphical
API call to then use the GPUs.This prevents us from having
to use it all the timeand only really use
it when we needed to.So once we had the
platform in place,and when we had the
AWS instance in place,and we had all the
different components,Jonny was able to start with
his model building, whichhe will talk about now.JONNY MCELHINNEY: Thank
you, Duncan and Haidar,for introducing the project.I'm going to try and cover most
of the key points about howwe combined open source
technologies with SAS Viyafor the development of our
automatic meter reader SAS.I'm not going to go
into too much detailabout all the
technology, since we'vecondensed this presentation down
from 50 minutes to 20 minutes.But we have also submitted
a technical paperto complement this presentation.So I would encourage anyone
who wants a bit more technicaldetail to read our
paper, or pleaseto send us any questions.So the computer vision
meter reading projectwas originally my master's
dissertation projectlast summer.The outcome of my work was a
prototype delivered entirelyusing open source technology.It was programmed
in Python usingKeras as the deep
learning frameworkon the Google Colab platform
to make use of their free GPUresources.The next step was to transform
the open source prototypeinto a production-ready
system using SAS Viya.As Haidar mentioned,
the DLPy packageallowed me to use Python to
re-engineer the system on SASViya with elastic GPU support
when it came to trainingthe deep learning models.At the end of the development
phase in SAS Viya,we had produced three
deep learning modelsthat we could then deploy to
a prototype phone application.The SAS R&D team are
helping us out a lot with usby creating a test app using
the models that we developedand deployed in the ONNX format,
which, as Haidar described,as a neural network exchange
format that allowed usto convert the
pretty large modelfiles into much smaller
and mobile-friendly format,such as TensorFlowLite, so
that we can do all of the imageprocessing on the Edge device
in the hands of the customer,rather than transmitting
large images back and forthover the network.Well, let's take a step back
and talk about the actual meterreading problem.We are trying to give
our customers not yeton smart meters the ability to
take a photo of their energymeter and automatically
submit the reading for them.It's pretty easy for us
to look at this meterand record the reading.This is because humans
are acceptable at seeingand understanding text
in the natural world.But it gets repetitive.I know that because I'm
a ScottishPower customer.So myself and millions
of other customershave to submit
readings every monthfor the most accurate bills.And, as Duncan
mentioned, operatorshave to visit each household
at least once per yearto record the reading.So the easy task of readings
some numbers on the meterdisplay becomes
very time consumingand very expensive as well.So why don't we
just get a computerto do it automatically?Well, it turns out it's quite
difficult for lots of reasons.One of the biggest
difficulties isthat meter cases have lots of
readings of text and numbersthat a computer could
incorrectly identify.There's loads of other
considerations as well.Meter designs
aren't standardized,so there's dozens of
different styles of metersthat our customers could have.Reading displays can be anywhere
from four to eight digits long.Some meters have
digital screens asopposed to the physical
scrolling meters.And, as Duncan
mentioned, in the UK,electric meters are
often kept insidein a dark cupboard,
which means itmay be very difficult for a
human to understand the readingif they can't even
see the meter at all.On the other hand, UK
gas meters are oftenkept outside of
the house and aresubject to some pretty
measurable weather, especiallyhere in Scotland.So they can be
damaged or coveredin dark, which makes
it very hard to read.So how do we handle
all of these variables?Well, that brings us
into our solution.Three neural networks
used togetherto process a single image, which
means the output of one modelbecomes the input to the next.Models 1 and 2 are
object detector models,and model 3 is an image
classification model.Let's break this task
down into smaller stepsin much the same
way that a humanwould to read the
meter, that is to findthe counter in the image
and then classify each digitand build an output reading,
which can be submittedinto our customer database.The YOLO models that we
used for models 1 and 2are the same model that was
demonstrated on the mainstay2019 SAS Global Forum used
for liver cancer detectedfor medical imaging.Although, ours
doesn't quite helpsave lives in the same way.So to visualize the steps for
classifying our meter readings,we would ask the customer to
take a picture of their meter.The only requirement
up to this pointis that a human would be
able to look at the pictureand be able to record
the reading as well.So the first model
as a single classobject detector model
to detect the counterfrom the input image.Next, from the input image
and put that separately sothat we can get rid of all
the unwanted image data.We do this with a
small margin of paddingto make sure that none of
the digits get cropped out.From there, we used another
single class object detectorto detect all the
digits from leftto right within
the counter image.After that, we have the simple
digit classification taskto label each digit from 0 to 9.Once we've classified
all individual digits,we can put them together
to create the output meterreading, which will be shown to
the customer for verification,and then be submitted
quickly and easily.It is important to note
that since this project wasoriginally a piece
of university work,we used a data set that
was produced for researchby a Brazilian university.So our prototype system
in this presentationwas trained using this
data as a proof of conceptbefore we created a
ScottishPower data setusing images supplied
by our own customersand the future for
our final version.So for the project, we decided
to build the first two modelsfrom the ground up in Viya.As Haidar mentioned, Viya
supports a Python user like mewith a DLPy package for
model building and trainingas well as the SWAT
package for creatinga connection to the Viya
Cloud Analytic services.So I could prepare the
2000 images and label filesfrom our meter data set
using native Python packages,like pandas and OpenCV, and
then load them into Viyawith the help of
the SWAT package.From there, SWAT lets
you use CAS actions,like dljoin to create
a retaining data setby joining a table of
all the training imageswith a table containing
the labels for each image.DLPy lets you build a deep
learning model layer by layer,only you can select a predefined
architecture like YOLOusing the built-in functions.Once the data has been
loaded, and the modelis ready to be
trained, we switchto GPU mode and train the
model using the DLPy model dotfit function, which
uses the DL trained CASaction under the hood.As Haidar mentioned, CAS
uses four GPUs in parallelto train on streams of--this was really helpful for us,
because training object modelsis very computationally
expensive.And they can take
a really long time.The end result was that we
could train our YOLO modelat over 450 seconds or
7 and a half minutes,averaging around about 7
seconds for each epoch.For comparison, Google
Colab took much longer.We trained the full-sized
YOLO architecturefor the open source prototype.And it took hours to finish,
whereas in Viya, I barelyhad time to go and grab a
coffee before it was finished.After we trained the
models, we could quicklyvisualize the model outputs
using the DLPy display objectdetection function
for both modelswith some unseen text images.This let us quickly verify
that the models werebehaving as expected
with very little effort.You can also
backscore all imagesin the test set
against their labelsto calculate the
accuracy of the modelsover large quantities
of test data.So now that we have our first
two models built in Viyathe next step is to load our
digital classification modelthat we've already trained
in Google Colab into Viyawithout needing to rebuild it.DLPy lets us load
some Keras modelsand directly in
their H-5 format.In our case, we converted
the model into ONNX formatand loaded it from there.We needed a bit of help
from the SAS R&D team here,because there were some
difficulties getting the modelloaded in the first place.But they were really
helpful, and wegot it to work in the end.Using the form from the
ONNX model function,we can convert
the original modelinto SAS compatible format.And that means we have all three
models from our open sourcepipeline loaded and ready
to use in Viya, wherewe can test or deploy them.So the last thing to do in Viya
is to test them all togetheras an end-to-end system.We can input an
unseen test imageand run it through each model
to make a reading prediction.From there, we can
create a mock-upof what a customer
would see when theysubmit an image for review.You can use the
CAS actions to makethe prediction for
each model, and thenhandle the image processing
using Python and OpenCV to passthe image through and
display a predicted readingfor the customer to verify.If the prediction is
wrong, then there'salso the option to
display model confidencescores to help debug
any cases where there'sbeen a wrong prediction.We can offer you these
confidence scoresto flag any prediction or
specific digits that the systemisn't too sure about and give
the customers the abilityto send us their image of
all the confidence dataso that we can
improve the system.Lastly, once we've done
our end-to-end testingof the whole system, and we're
happy with the performance,we can take all three of
our SAS compatible modelsand deploy them with
a single line of codeusing the deploy function, where
they can be put into productionin the most appropriate way.I'll now hand back
over to Haidarto talk a bit more
about the deploymentformats in a bit more detail.HAIDAR ALTAIE: Thank
you very much, Jonny.I think really great
work that was donein terms of the model building.And as Jonny
mentioned, deploymentis really quite a tricky
thing with open source a lotof the time.But what we're able to use
we can use the DLPy actionsto actually just deploy
in one line of code.So this was another aspect that
was really powerful about whywe wanted to use the DLPy.But once we had the
ONNX models to summarizethe entire project, with
SAS, the deployment end modelbuilding is very flexible.As we mentioned, you can use
the CPU or GPU for the scoring.And once we have that,
there are two waysto normally go about it.So deployment can
be done on ASTORE.So ASTORE is a format
exclusive to SAS,where we can convert
really, really big filesinto binary small files
in order to do some Edgedeployment or the
process that wetook in this particular
aspect, where we converted itwith ONNX.So once we had the
ONNX format, wecould just deploy that,
and then use this for stufflike mobile deployment,
either using ONNX runtime,or converting it to other
ones, such as TensorFlow,which is exactly what we've done
with this particular example.What we had was we
had the SAS model,which we converted to ONNX and
allowed TensorFlow to use itfor mobile deployment.But similarly, what
this allows us to do,since SAS is a verified computer
vision framework for ONNXsupport, we can actually
leverage other formats,convert it to ONNX, and use SAS,
and then push it to SAS ASTORE,for example, or from SAS
to ONNX to other ones.This format does really
allow you to be flexiblewithin different vendors
and different environmentsjust to get the very best
model available to you.Finally, once that
everything is done,and we've got the model in place
and so forth, we can almostdivide this into two sections.So from one aspect, in SAS
Viya on the server side,we can load those images.We can process it, train
the deep learning models,and generate the model, just
as Jonny just showed us.But once we have that,
we can ingest that modelinto the Edge, so
on a mobile phone.A customer would submit an
image, and then, on the Edge,we can load those images.And we can ingest some
of the previous modelsthat we've made with SAS Viya.Once we've got those images,
we do a pre-processingof those images, perhaps.So for example, as
we mentioned earlier,if it's not well lit
and stuff like that,or it might be a
little bit of dirt,some of the pre-processing
of that customer imagecould perhaps deal with that.And once we have both of that,
we can score it in-stream,so give it a probability
score, as Jonnyshowed, to understand
which numbers theyare to do their meter reading.And then, finally, we can do
some pre-processing as well.And even the
pre-processing couldbe stuff like identifying
the location, or time,and so forth.That's completely adaptable.But once we have that,
we can feed that backinto the control system,
so that we can actuallyget feedback on what the actual
numbers are that were read.And that can be then,
instead of a customerhaving to manually write
that, just by a picture,we have that available to them.So this project is something
that we've worked quite a biton, and we're continuously
working on it to improve it.And as Jonny mentioned,
using ScottishPower datain the future.But this was something
that took a lot of time.It took a lot of collaboration.And we were happy that we were
able to present it to you guys.Fully, thank you
very much, Phillip.If you've got any sort
of questions, pleasefeel free to email any of the
emails, either Duncan, Jonny,or myself.And we're more than
happy to support.And hopefully, you've
enjoyed this presentation.Thank you very much."
23,"DAVID ASERMELY: Hello, welcome
to SAS Global Forum 2020.My name is David
Asermely, and I'llbe presenting--
telling your model riskstory during COVID-19.In today's COVID-19
environment, the pressureon modeling activity
has increased,and things are moving quickly.Models are the lifeblood
of financial institutions.As a model risk
manager, you realizethat many of your
historical datasets and critical models fail
to represent the new reality.These changes
reduce your abilityto estimate critical
business questionsthat impact your company's
short-term profitand long-term
viability, includingliquidity, lending decisions,
fraud management, et cetera.New and modified models,
including advanced analyticsand machine learning, must
now be developed and deployedquickly.But with all the rigor
and governance requiredby the stakeholders,
including management, audit,and regulators, firms
that are able to quicklyadjust and ensure their models
are giving them the bestpossible results to inform
strategic decisions willhave a better chance of
getting through this period.And perhaps, even strengthen
their capabilitiesin the future.Models need to be trained and
built using historical data.Of course, in the
midst of this pandemic,we're seeing data
that is unprecedented.Many of the models
banks depend onto run their business are
simply not going to providethe insights they need.This graph of US jobs
claims is just one example.Others include the level
of stimulus and collapseof entire sectors
of the economy.Any institution that is unable
to quickly adjust their modelsand introduce new modeling
techniques to addressthese challenges is likely
to be at a disadvantage.The first priority
is to identifythe models that
are not performingand determine the best fix.This requires a full view and
understanding of your modelinventory, as well as
the data dependenciesand interdependencies
of all your models.From there, your
teams can identifythe most critical models for
redesign and replacement.Of course, this is
all taking placein an environment where
IT budgets are restrained.However, IT model
risk professionalsare under a lot of pressure.We've learned from our clients
that many expect model riskregulatory pressures to
increase as we move throughand emerge from this crisis.In a recent study by McKinsey--you can see the graphic
from that study here--many model functions, including
development, validation,and monitoring, that
were typically conductedquarterly or monthly are
now taking place weekly.This forces the model
lifecycle workflowto be condensed and
accelerated, a requirement thatis practically
impossible with itwithout an enterprise-wide
streamlined model governanceframework in place.Some firms are currently forced
to outsource model governanceand validation activities.This only adds to an
unsustainable level of cost.Another trend we are hearing
about from our customersis the need to adapt
advanced analytic techniqueslike machine learning.Since data going back
30 years is simplyinadequate in this environment,
machine learning oftenproved to be more
effective finding patternswithin very large amounts of
current and alternative datasources.But the challenge is to have
an infrastructure, includingmodel governance, that
includes continuous monitoringand retraining, and support
getting machine learning modelsinto production with sufficient
transparency, explainability,and governance.Earlier this year, we
did a survey with GARP.We asked people what they are
using to support model riskmanagement.Over 30% said they
were using SharePointor generic operational
risk tools.Surprisingly, this
was even the casein some of the
largest global banks.There are a number of
significant challengeswith this approach, all of
which are amplified now.Understanding which models
are the most critical,which are underperforming,
what interdependenciesexist across models--all this becomes very
difficult to assessin a compressed time-frame.This flowchart gives us a sense
of the complex series of stepsacross a range of
functions that haveto take place for one model
to be put into production.With hundreds or even
thousands of modelsgoing through this process
with a sense of urgencyand acceleration, things can
get out of hand very quickly.Unless you have a comprehensive,
integrated, and systematicapproach that addresses
business requirementsand regulatory guidelines.Another requirement for a
comprehensive model governancesolution that
addresses these needsis that it accommodates all
of your modeling solutions,including open source
and multiple vendor.An enterprise view
can only be achievedif you are able to link
to an aggregate datafrom all your modeling systems
across your organization.An integrated model
risk management solutionof this kind will
also enable youto automate a number of
steps in the workflowacross your multiple teams,
from developers, testers,implementers, and audit.One of the best tools we
found to help managementtrack these changes
through the model lifecycleis visualizations.Here is a visual
analytics examplefrom SAS Model Risk Manager.At a glance, we can see
and track performancewith this COVID-19 dashboard.These are the type
of tools that helpyou understand which models
are failing, what models havefaulty assumptions, what
models are currently being usedoutside of tested limitations.It helps you track
your economic forecastsand make sure that they
are being applied and usedby your models.And also help you
track model overlays.And again, many of those
are happening today.I want to thank you very
much for joining me today.If you have any
questions or would justlike to talk model risks,
please reach out to me directly.Thank you very
much, and stay safe."
24,"Hi, everybody.I am Dr. Aurora Peddycord-Liu.In this tutorial, we're going
to learn about SAS Viya and CAS.CAS stands for cloud
analytic server,and this is the
heart of SAS Viya.You're going to
learn how to talkto CAS through CAS-enabled
procedures, CASL,and open source language.There are a lot
of demonstrationsin this tutorial.We're going to see how
to load data using CASL,how to manipulate
data using CASL,and how to use different
machine-learning modelsand assess them using CASL.In the end, we are
also going to seehow to connect CASL with
open source languageand build a deep learning
convolutional neural network.In the very first
lesson, we are goingto learn about the basics
of SAS Viya, CAS, and CASL.Then we're going to see how
to manipulate with the basics.How to start a session,
how to use existing data,how to pass data,
and how to run SAS 9DATA steps and SQL in CAS.Next, we're going to learn
more about the componentin CASL language, the PROC CAS
syntax, data types, variables,action sets, and
self-defined action sets.Finally, we're
going to see how weuse this deep
learning action setsand how to use CAS
in open source.Let's get started.So what is SAS
Viya, CAS, and CASL?I like to start
with this sentence.In the new world,
it's not the big fishwhich eats the small fish.It's the fast fish which
eats the slow fish.Having a way to quickly
transform data into actionis essential to
today's business.And SAS Viya is a
good tool for that.It is an open, cloud-enabled,
in-memory analytics enginewith a number of
supporting services.The SAS Cloud Analytic
Service, the CAS,is at the heart of SAS Viya.So CAS is how the SAS
Viya does its calculation.SAS Viya has a
lot of advantages.It's really fast for big
data because, using CAS,it can provide scalable
distributed computing.It is open, and it's accessible
via SAS, Java, Python, R,Lua, and REST APIs.And it's cloud-enabled.It's capable of running
on remote machinesvia the internet.In other words, to make it less
dry, in the old way, the SAS 9,you kind of hired a single
person to run each process.And usually that
is your computer.It does all the work.But nowadays, we have
bigger data and more complexalgorithms.Using your computer
is not enough.Your computer may not have
enough memory or speed.In the SAS Viya world, you
hire a manager called CAS,and this CAS is not
only smarter, faster,but also has an entire
team to handle the work.So CASL and his/her team
retain the data in memory.It has a big brain.It can store all the
data in the memory.It can scale to
data of any size.You can just increase the amount
of where computer is workingother CAS or has more CAS.It is really fast because
every task you pass to CASis going to be done parallelly
by the workers entered in CAS.It's parallel computing.It also supports
node-to-node communication,or the workers in the task
team talk to each other.So this makes them
fault tolerant.If one fails, the others
can pick up the work.Of course, you can hire a
larger CAS team as you need,based on the size of your
data and the complexityof your tasks.So now you can see the
fundamental differencebetween SAS 9 and SAS Viya is
how the computation is done.That part is different.In SAS Viya, you
do it in the cloud,in the parallel
computing fashion.SAS Viya and SAS 9 can both
coexist on the same hardware,physical or virtual.So it's not A or B.
You can have them both.There is a little mindset shift.Back in the old time, when
your algorithm is simpleor data is not that big, we
can do all the calculations.Our results are
deterministic, whichmeans if you run
in multiple time,they're likely to stay the same.But nowadays, in SAS Viya,
because we distribute the workto a whole team, because
there is a lot of randomnessin algorithm, in
order to optimizefor large, complex data and
tasks, then in SAS Viya,the result can be
nondeterministic,which means the results
will follow a distribution,and you don't always get
the exact same result.But you get a very good result
following the distribution.So now we see the difference
between SAS 9 and SAS Viya,and that SAS Viya is more
suitable for big dataand more complex algorithm.Now it's CASL.What is CASL language?You can think of it as the
command you pass to CAS.So the CAS is clear about, am I
doing the work or is still youdoing the work?And it is CAS doing the work.He or she is able to do
so in a parallel fashion.CASL is a script language.It's using PROC CAS
to run CAS actions.You can run it in
many interfaces,and you can commonly use
to be written and submittedfrom the SAS studio
program editor.But in addition, there
are other languagesthat you can write that
convert to CASL and evokethe same process.We're going to see
how to do that.And those languages include
CAS-enabled SAS proceduresand open source language
interfaces for CAS.CASL is the back end code for
all SAS Viya applications.For those of you who have
experienced using SAS Viya,you know there's a lot of
tools in this big tool box,and the tools support
the whole data lifecycle,for how you store
and manage data,to how you discover
patterns, visualizations,and models in the data.And in the end, to how
you deploy your model.You manage your model,
manage your decisionsthrough Event Stream
Processing and all of these.But all of these tools, no
matter how easy they are--if it's point and click
or drag and drop--at the back end, everything
is translated into CASL.We strive for this
consistency, no matterif you talk to CAS through
all those point-and-clickinterfaces and SAS application,
or through open sourceinterface like Python
and Lua, or usingCAS-enabled procedures.All of these are
being translatedto the exact same CASL action.You can do it through
different interfaces,and you will get
the same results.How to use CASL
is really simple.The first step is you just need
to book a session with CAS.The second step is you use data
in CAS or load data into CAS.You give CAS your
data to do the work.And lastly, you run the PROC CAS
procedure with this CAS actionsin it.Just a bunch of syntax, so
you can tell CAS what to do.Throughout this tutorial,
there are demonstrations.And the tool we are using
mainly will be SAS Studio.In the end, you're also going to
use a little Jupyter Notebook.This is the SAS Studio
interface for those who haven'texperienced the SAS Studio.On the left is a
navigation pane,where you can see all the
data, the tasks, and folders.And there is already some
really fancy tasks beingstored in the navigation pane.And these tasks are
already writing codeto do machine-learning steps.On the right is a
work area, whereyou can write
program, where you canrun program, and see the logs.This also this click interface
that you can click on it.You can see in the center
of this screenshot,and the code will be
automatically generatedon the right.Are you ready to
start the next lesson?So now you've seen what is
SAS Viya, CAS, and CASL.This lesson is about how
to work with data in CAS.How are data in CAS different
from data in traditional SAS 9?The very first step
of doing anything CASis to start a CAS session.What we are doing when we
start a CAS session is let'suse SAS Studio as an example.When we open the SAS Studio,
we are starting a sessionin the SAS Programming Run-Time
Environment called SPRE.And the SPRE supports
all the SAS coding.it acts like a good
old SAS 9 session.And then inside this SAS
Studio when we alreadyhave the SPRE running,
we start a CAS session.And this CAS session can
execute CASL and CAS-enabledprocedures.The cool thing is that each
user gets his or her own SPREand CAS session
because a lot of time,your CAS server has to
serve a lot of clients.And you don't want those clients
to mess up with each other.So everyone gathers on
SPRE and CAS session,and each session
provides fault isolation,so you don't influence other
people that are using CAS.This also enables identification
and session-specific resourceallocation, like how many
worker nodes works for you.This is a syntax to
start a CAS session.It's really, really simple.You just need to say
CAS, gave it a name.There are some options.You can search more online.One of the commonly
used option is allocatea library in the CAS brain.We're going to talk more
about this myCasLib soon.To terminate the
CAS session is assimple as CAS coupled
with your session namethat you just created
in the start CAS sessionand terminate.Just two lines.Very, very simple.So now we know how to
start talking to this CAS,how to use data.The simple one is
how to use datathat already exists inside CAS.Data in SAS Viya are
different than data in SAS 9.The data left on
the CAS server isretained in memory for multiple
steps and multiple users.And it's distributed in memory
to multiple worker nodes.It is processed in parallel
across the worker nodes.It can be saved to disk
for permanent storageand is subject to Read
and Write access controls.Caslib, this one, is
an in-memory spaceto hold data and other access
information in SAS Viya.So you can imagine
this CAS serverhas a part of the brain
stored just for your data.This caslib is
living in the cloud.Data in SAS Viya and
CAS has two scopes.Some of the data
are session scope,which means they're only
visible to the CAS sessionwhere it's created.It's only visible to you,
who created this session,and it's deleted from memory as
soon as you end this session.This design makes sense because
imagine, we all write programsand we all create
a lot of data sets.And if CAS keep
remembering all of the datasets you created during
your calculation, thenit's going to explode.So by this way, having
this session scope,it makes sure data
in your sessiononly live in your session.Data also have a
global scope, so youcan use the promote
action to make a sessiondata in global scope.This is a data that's
stored in CAS brain,and everybody use
this CAS are able--is able to see the data.It's visible across
CAS sessions.You can start a session,
terminate the session.Start a session,
terminate the session,and the data will
always be there.It's invisible to any user who
can access the global scopecaslib.And once you terminate, the
CAS is not jogged from memory.Users and administrators
can decidewhich data you're going to
promote into global CAS,so you don't waste CAS
memories on those datasets that are not
important, that you justneed for your one session.To access data in CAS,
this is the syntax.The caslib all list
shows you all the dataalready exists in CAS.In order to talk
to this caslib, youneed to create a
SPRE library, whichis a library in your
SAS code, to referto the location of the caslib.So in this example, the
caslib is my caslib,and then you create this
library, my myWorkLib,which is basically a pointer
pointing to this caslib.The mhWorklLib is
the name you canuse to access data in caslib
and do your calculationmanipulations.Sometimes the data you want to
use have not existed in CAS.Nobody knows.Only you and your computer know.In those cases, you want
to pass your data into CAS.One thing you need to
know is because CASis a cloud computing.It is another computer
running in the cloud,so it can only work with
data it already knows.It can only work with data
that already exists in CAS,so that's how it can do these
distributed computing tasks.If your data is not in CAS,
SPRE, your SAS ProgrammingEnvironment on your computer,
is actually do the work.So we need the procedures to
load the data from your clientside to this caslib to put it in
your CAS manager's big memory.There's several
ways of doing so.The first one is DATA step.This is just the overview
of the DATA step,and all you need to do is that
you need to use the same DATAstep and options in
SPRE and then the libreference, the one,
the pointer youcreated that point to the
caslib as the output location.So this is two examples.You can see the
yellow highlight.You load the data from
your SAS Studio lib,deliver it in your SPRE, and
then when you output nextto the data, you're
outputting it in my caslib,the pointer you've created
that points toward the caslib.In the second example
here, as you can see,we're loading a CSV file.We did the same thing, the
infile, the input, the format.And just on the top, in
the yellow highlight,we make sure that the data
ends up in the my caslib.The second option is to use
the old, good PROC IMPORT.And we also just need to pay
attention to the yellow step,to make sure the out
statement is using,is connected to this
lib reference that'spointing towards our caslib.Below is an example.You can see the out is
mycaslib.myBigCasData.And this replace just
means if this data alreadyexists in CAS, then replace
it with a new version.The third option, and
this is the PROC syntaxdesigned specially just for CAS,
and this is the PROC CASUTIL.You can check more
online documentation.This PROC has lots of usage.In here, you can load data.You can load your data.You can specify what
is your output caslib.Specify the name of
your output data,and you can promote to global or
replace this existing data thatalready exists in the CAS.So here's an example.You have this data on your
desktop called myBigData.sas.You just inside this PROC
CASUTIL, load the data,specify your output,
caslib, specify your casout,and replace if this data
already exists in CAS.Very simple syntax.The CASUTIL has many usage.You use Load the Data
to load the SAS data.You use Load File to
load a Microsoft Excelor other supported file formats.You use Load CASDATA
for the SASHDAT file.And this is the file I'm
going to talk about soon.It is a file optimized
for distributed computing.If you have a really,
really large file,you can also use the option
here, the CASDATALIMIT,to specify how big is the size,
what's the size you allowedto be uploaded.You can have multiple
data set loadedusing one CASUTIL procedure.So like in this example, I'm
loading three different data.The first data I'm
using to replaceand existing data in CAS
that has the same name.The second one I
promoted at global scope,so everybody using
CAS can see my data.And the last one, it
doesn't have any options.This is just to load a data,
a local Microsoft Excel data,to CAS.So these are the stats
that you pass datafrom your computer to the
CAS cloud computing service.So the CAS cloud
computing servicecan do its parallel
computing tricksto make your thing work faster.The next part is how you
save data to the server disk.So even if we promote a data
to leave in CAS in its memory,it can still disappear.It just like every data and
memory on your computer.When a CAS server stop, when
your IT have to shut it down,the things in memory disappear.Or sometimes when
people accidentallysubmit a PROC
CASUTIL DROPTABLE isbeing dropped from the memory.If this data is really,
really important,you want it to
always exist in CAS.Then you can save it.You can store it on the disk, on
the hard memory, the disk part.However, you need to store
it in the SASHDAT format.The reason because
this is the formatoptimized for
distributed computing.If you store it in this format,
then it's much easier for CASto pick up it and load
it back to its memory.This is just a format.It stores a direct
representationof an in-memory table on disk.This format helps store
this representationthat is distributed
across the worker nodes.So when the CAS
reload the data, itcan quickly do the reload to
support the parallel reload.This type of format
cannot be used in SAS 9.It can be used only by SAS
High-Performance engines.The syntax is really simple.It's a PROC CASUTIL.You just save CAS
data, given the name.And of course, you can
have many more options.You can specify the
INCASLIB, OUTCASLIB,and what is a CASOUT,
what is a new name.All right, so now we've
learned how to pass data to CASand how to work
with data in CAS.In here, you're learning
how to run SAS 9 DATA stepand SQL in CAS.A lot of data
preparation work canbe done using just
those two steps,and I want to make
sure you know howto use it in a simple fashion.To run the DATA step
in CAS, a DATA stepwithout running in
CAS just executein a single thread in SPRE.In order to make
it run in CAS, youhave to make both the
include and output data setsCAS tables, which
is basically whatI highlighted in the yellow.Make sure in the set and in
the data here, both of themrefer to a CAS in
memory table, nota table in your local
library, but the referencethat you set that point
towards the caslib.There are tiny, small tricks.For example, there might be
differences in the output.In the demo, we'll
show an example.And sometimes certain
language elementcan prevent this
processing in CAS.These you can see
from documentation,but for the majority of the
cases, you are totally fine.If you do the yellow part,
then you will make surethe data is running in CAS.If you are not doing
the yellow part,if one of your yellow part is
only your work library data,then your data
will still be run.It is just going to run into
SPRE, in the SAS programmingenvironment, not in the CAS.To run SQL in CAS
is, again, you justneed those two yellow
parts, and also youneed to use another
procedure called PROC FEDSQL.PROC SQL executes in a
single threaded SPRE.Only PROC FEDSQL executes
in multiple threads in CAS.You need to also have this SES
reference to refer to the CASsession you created.And then you need to select
from this CAS in memory table.So this yellow part
are the part youneed to work on in order to
make this procedure run in CAS.Otherwise, it's just
going to run in SPRE.Here is a very useful
procedure I'm goingto introduce called MDSUMMARY.A lot of the time, when
we manipulate data.We just want to calculate really
basic things like frequency,average, mean, and max.In order to calculate them in
the parallel computing fashion,PROC MDSUMMARY is
the one to use.Below shows an example.This procedure computes
basic descriptive statisticsacross all observations or
within the specific groups.And this is a very
useful procedure.Then you also use other data
manipulation things inside CAS.There are other CAS-enabled
Base SAS procedures,like PROC COPY, PROC DATASETS,
PROC FEDSQL, PROC MEANS,and all of these.So not all the SAS
procedure are enabledin the parallel
computing fashion.So these are some of
them, and there's morein the online documentation.But you just need to be mindful
that in order for thingsto be done in the
parallel computing way,there are extra code
that we need to write.So don't automatically
just assumethat everything
you read in SAS 9will be run in the
same way in SASViya in the parallel fashion.Sometimes you need to run
non-CAS procedures with CAS.For example, this SQL plot.In here, if the data
is a CAS memory table--look at this yellow part--then even though this SQL
plot is ready in SPRE,the WHERE statement is
actually run in CAS.So this saves you
a lot of effort.Instead of passing the
whole big table from CASall the way back
to your computer,this WHERE statement make it
select and meaningful inputin CAS first, and then
put the smaller data backto your SPRE to
run the SQL plot.There's an option.Put MSGLEVEL equals to i.And this is
basically how you canprintout in the log, which
steps are executed in CAS.So now we're going to see
three little demonstrationson how to do some of the steps.We're going to see how to
start a CAS session and loaddata into CAS.This is the hardest part, right?Get data into CAS, and the
rest you can figure it out.The next one is how you load the
client-side data into a caslib.And the last one is
how we use this DATAstep in distributed computing.So let's go to SAS Studio in
SAS Viya and see how we do this.All right, so we are now
in SAS Studio in SAS Viya.We are going to first start
with how to start a CAS session,so we can talk to CAS.The good news is that
inside this tool,there's already snippets,
which are code peoplealready wrote for you to
use them more efficiently.One of the snippets inside
the SAS Viya Cloud AnalyticService and new CAS session.If you just double-click, I'm
going to change this layoutto be single.Then this is all the code you
need to start talking to CAS.Your session is
called My Sessions,and then you have this
space in CAS memory.You call it casuser.I already write some
code down for youfor a quicker demonstration.In here, we start as My
CAS Session, My Session CASSession.And then this lib main
statement is basicallycreate a reference to
this caslib CAS userand name this
reference myCaslib.So let's run the first
two lines of code.You'll see here, we
have active caslibs.Everything is successful.And when you see
this libraries here,you see you have this icon
of the library in the cloud.Everything in the
caslib will show here.Next, we're going to see how
we load the data into caslib.To do that again, there
are several snippets.Click on this Load
Data to caslib.Change the layout to single.You see, you already
have templatefor three different
ways of loading data.The first one is you specify
a path to the client file.It can be your
desktop or somewhere,and it's where you
store this data.And you output it in myCaslib.The second one is an example
where you load a base SAS DATAstep from an existing library.So for all the
library you see here,you can load a SAS DATA
from there to the caslib.The third example is how
you load the CAS datafrom a specific caslib
to another caslib.So you are moving this
data across caslibs.In the program I've
already written for you,we just load this file
from a workshop folder.The output CAS library is
the CAS user, this one.And we are, of course,
using the PROC CASUTIL.Let's run this code.And if we go to this
library, we open it,you see the data right there.So this is not the caslib.This is a reference to the
caslibs in your local library.Double-click on the data.You will see the data.This is the data we are
going to use to later viewthe machine learning models.Each row is a customer.This is a financial
service data.So these are information
about this customer'sfinancial activity and some of
the demographics, such as JOB.We're going to predict
this BAD, and thisis whether this customer
ends up paying back his loan.So the bank wants to
predict whether I shouldgive a loan to this customer
because it has informationof in the past whether a similar
customer has paid off this loanor didn't pay off his loan.So this is what we are
doing in the demonstrations.And this is all the steps
to start a CAS sessionand put the data into CAS.The next part of the demo is
how we run a DATA step in CAS.Here we have two blocks of code.The first one, you're only
running the DATA step in SPRE.Why?Because as you can
see here, insteadof having my caslib in the
set and my caslib in the data,in here, it's just work because
both of them are not caslib.So this DATA step is
being executed in SPRE.And here we used a
simple PROC IMPORTto load this data into our local
library, local work library.And we run this data.What it does is that
it's going to countthe numbers of office
workers, the numbers of peoplethat has dropped.It goes to office
in the data set.So let's run this.It's really simple.In the output data, it
tells you exactly thereare 948 office workers.And we're calculating
this through this code.If this job office then
we increase the numbersof office workers by one, and
we output the number, whenwe meet the end of
the file, and wekeep this numbers of office
worker to keep this variable.The next level code is doing
the same thing, but just in CAS.And you will see a difference.We use PROC CASUTIL
instead of PROC IMPORTto load the data into
the CAS user library.And then because mycaslib here
is pointed towards a CAS user,we're able to grab
the data from there.And we run the same DATA step.If end of the file, that outputs
the numbers of office workers.So let's run these.Something different happened.You can see it's quicker.But instead of one number,
we now have 16 numbers.Why is this office
worker become 16 numbersif we're doing the
same thing, right?The reason is because the
process is now distributed.Instead of one single
thread, one single worker,calculate all the
numbers, in here, in CAS,we have 16 parallel processes
to calculate workers.Each one of the process have
a proportion of the file.And here, we are calculating
it using the end of the file.So we got 16 members,
just represent howmany office workers
in each of the fileand each of the worker.So that's why we see
16 different numbers.If you sum them together,
it is the same resultas the previous step.And in the log, as
you can see, thereis some information of
these are being run.The data set is being run
in Cloud Analytics Service.So now you can see that the same
DATA step using the end of fileworked differently in
the two environments.The good news is that
this end of the fileis like one special
example to helpyou understand the difference.The majority of the procedures
will give you the same result.So in the previous two lessons,
we've learned what is CASand how to pass data to CAS.That's the hard part, right?So now we're moving to the
easy part, the syntax part.We're going to learn the basic
components of CASL, the CASlanguage.And in the end of
this lesson, we'regoing to see a very cool demo in
which we build multiple machinelearning models using
CASL and compare them.So let's start
with the PROC CAS.PROC CAS is how you
run the CASL language.We are at the step 3, include
CAS actions inside PROC CASto do real cool stuff.Using PROC CAS, we can have
variables, expressions,functions, and CAS actions.Let's start with data types
and expressions in CASL.This is like the basic of
every programming language.These are the lists of data
types that it supports.Most of them are very
similar to SAS 9.Two of the data types
I like to point out.The first one is array.This array is the same array in
many other computing languages.It's basically an
ordered list of values.You can imagine each
value set in a row,and there is an index.And machine can directly
query which valueit wants by giving the index.I want number 2, I want
number 3, I want number 4.This is how we define this
data structure in CASL.The syntax should
be very familiar.Please check the
online documentationfor more examples.In here, I'm just showing
you how to define itfrom table, from a data table.Then how to loop over the array,
which means for each person,I do this stuff.In here, for each product
name in the array, I train it.So I find myself most
usually use arrayto store long variable names.Sometimes you have a long
list of different variables.Instead of writing
them again and again,you store them in an
array, and then youcan look over them to do stuff.Another data structure that is a
little different is dictionary,and it's basically an
unordered key and value pairs.A key must be a string, and it
can be a really short string.And the value can be everything.It can be a whole
host of stuff--different data
structures or a function.The reason we have
dictionary data structureis that is really fast to assess
this large amount of value.It takes a long
time to search it.This is because it is stored
using a hash function,so the key can directly
being translatedinto the exact
location of the whole--where the whole value
is stored on the disk.This is how you would
define a dictionary in CASL.You declare a hash.You define a key.You define the data,
and you're done.And then you can look over
each element in the dictionary.There's more online
documentation for dictionary.CASL supports conversion
in CAS of data sets--of data types.For example, if you add
an integer to a double,then your y would
become a double.If you add a string to a
numeric, in the end result,the y would become a double.You can also do this
CAS, which meansyou can convert a value
explicitly to another data tab.Just use the brackets.For example, in
this example, we aretrying to add this
year, 2020, and youcan CAS 2020 to be a string.And this is very similar to
how you would do it in Java.The expressions in CASL.Arithmetic, the plus, minus,
multiplication, division.You can compare operators.You can have the
comparison operators.You have the logical
operators in between.You have others, for example,
this one, the two barsconcatenate character values.You can check online
for more documentation.And this is just a
very basic componentof this CASL language.It should be very
similar to SAS 9.Now this is the more
interesting part.Variables and functions.You can have variables and
functions inside PROC CAS.Variables are just like
variables in other computerlanguages.You have a name.You have an expression.The expression can be a value,
an expression, or a function.A CASL variable will always have
a name, a scope, and the datatype.They're commonly
used as parametersto a function or an action, in
an expression, or as an aliasto a function.So you can assign a
function to a variable.CASL variable has scope,
just like CASL data.They're variables created during
the execution of a function,created inside a function.And they are local
to the function.As soon as this
function returns,the variable disappears.And for the same
reason you can't justkeep creating variables and
then explode the memory of CAS,right?Some variables can be global.And in here, what
global means isthat it can exist anywhere
inside a single invocationof PROC CAS.So a PROC CAS can have
multiple functions,and this variable can be used
throughout this PROC CAS.Now we've seen variables.These are the functions.The functions are
just like functionsin every other
computing language.We have two types of functions--common and built-in.Common functions is common
to both CAS and SPRE,which is the basic SAS
environment before westart a CAS session.So to remind you, each user has
his or her own CAS and SPRE,and they are kind of
co-exist together.These common functions
are replaceableby user-defined function.And an example, we can see in
the chart are absolute value,datetime.You can see all the
common functionsby using this PROC
CAS; fnc; run.So this is the syntax to show
you other common functionsavailable.There are also
built-in functions,and these functions are unique
to CAS because they're designedto support parallel computing.It cannot be replaced.Examples are
readPath and addRow.And you can see all
these built-in functionsusing the PROC
CAS, functionlist,and run this [INAUDIBLE]Using CASL function
is really simple.This is how you would
define your CAS function,and this is how
you would run it.You can define your own CASL
functions inside PROC CASas well.In here to define
it, first you needto put it inside a PROC CAS.And second, you say it's a
function, give it a name,give it a list of
arguments in the brackets.You write down
your function body,whether it's a calculation
or train or whatever,and then you end.This is how you
define a function.To use the function,
you can just My Functionand have the argument in it.What the function return will
be stored in this output.Very simple syntax.You can run and define multiple
functions using one PROC CASstep.In this example, we are just
forming Fahrenheit to Celsius.And where input
just temperature,then you perform
this calculation.And then here you
return the Celsius,and you end the function.And now you have an array of
four different temperaturesin Fahrenheit.You loop over the array.So for each value in the array,
we use this FtoC functionand return the value
to the Celsius.Then you can print
out the Celsius.This is how you
define [INAUDIBLE]..Need to remind you that,
just like everything else,function also has scope.The predefined function,
the system-defined function,will always exist within CAS.But for user-defined
function, theydisappear as soon as the
current PROC CAS session ends.What you can do
to reuse functionis you can store your
function in a fileand use the include statement
to run this file in PROC CAS.You write down
this function, youdon't want it to be disappeared
when the session ends.So you save it as
myFunctions.sas.You save it in the file,
exactly look like this.And then when you need to
use it inside PROC CAS,you just need to say
this include statement.Give it a path that leads
to this myFunctions.sas.And then it's there.It's just like you copy all
of the things into this CAS.And the output, you
can use in the same wayyou would use a function.So now is the time
for a demo to show youhow you use this function.So now we are back to the
SAS Studio in SAS Viya,and we have this Fahrenheit to
Celsius function prewritten.This is how the function
looks like, really simple.In the PROC CAS, the function,
the name, the input variablesperform this calculation
and return the Celsius.And the function
runs this PROC CAS.To use it, in here,
all we need isto include the address
to this function.Then we have this input of
an array of four values.Loop over it.Transform each of the
Fahrenheit into Celsiusand print the Celsius.So let's click Run.It's done.So easy.And these are the Celsius.So this is how you would
use a predefined functionin SAS Viya.So now we've been
through the boring part.This is the exciting
part, the CAS action sets.This is where we can use
all these complex algorithmsin parallel computing fashion.CAS actions are the smallest
tasks that a user can directlycontrol.This is like the
small unit of commentwe can tell CAS,
tell CAS what to do.These are also the actions
that are generated and executedbehind the scene by all
CAS-enabled proceduresand others point and click
actions we use in applications.The actions are functionally
organized into groupscalled CAS action sets.There's online documentation
for all the actionsets and each action
and what they can doand how to use them.To run CAS action, we run it in
PROC CAS, and it's very simple.Inside PROC CAS, we
first load an actionset, just like loading
a package in our Python.Then this action
inside this action setdot which action you're using.And dash what are the
parameters of this action?And then you run it.That's all you need to
do to run an action.Here's an example.We have a simple action
set to do simple statisticscalculations.In here, we are generating a
Pearson correlation matrix.There are two ways of
doing such CAS action.One is through PROC
CAS, and the otheris using CAS-enabled procedures
PROC CORR And PROC COR is justbasically a record of the PROC
CAS that do the same thing.Inside of PROC CAS, we
see below the actionset simple, simple.correlation.We run this correlation action.The inputs are these
two variables pairedwith these two variables.And then the table,
the data is iris.And we can also
do a little wherestatement of only finding
correlations with speciesequals to seto.When we run the CAS-enabled
procedure PROC CORR,we're doing the
exact same thing.Data equals to mycas.iris.These are the variable.There's those two variables.I forgot to include
the where statement.But as you can see, this is
how you calculate correlationusing these two
methods, using PROC CASand the CAS-enabled procedure.But behind them is
all the CAS actions.Another example is
when we run an SQL.We talk about we can run SQL
in parallel computing usingthis PROC that is SQL.And refer to a session, and
do the same SQL proceduresas a PROC SQL.But then you are running
it at the back endif you're actually running
this PROC CAS on the top.To do that directly,
it's basicallyPROC CAS inside of it.You have this
fedsql action set--the execdirect action.And inside the parameter
behind this slashis a query of this SQL.Those codes will
yield the same result.This is another example in
which we build decision tree.Below the decision tree
action set and the dtreeTrain.Inside of it, we say what
are the input in our models,what is the data-- the table--and what is the
target-- is the pricewe are trying to
predict or model.So we run, we quit.And this is equivalent to this
CAS-enabled procedure PROCTREESPLIT where we specify
the data to input the target.And this is also equivalent to
this structuring Model Studio.So one of the node is
this decision tree.So when you drag this one
the green node onto the ModelStudio interface,
you are actuallyexecuting the PROC CAS
here, or the PROC-enabled--the CAS-enabled procedure here.This is how you would
use the CAS action set.And as you can
see, there there'sso many chances inside
this action set for youto perform machine
learning, statisticsanalysis, and other things.If you are not content
yet, you can alsodefine your own action set.And this is the syntax.You just use
builtin.defineActionSet action.Then you give your
action set a name.And you can have
multiple actions.For this example,
we have one action,and we name it listFiles.We give a description of what it
does, and then in the parents,we define what are
the input parameters,and what's their type, and
whether they are required.Inside the definition
is where we write outthis whole function.And send_response is
where we declare whatis the output of this action.Once we defined this action
set with several action,you need to know
there's also a scope,and it only exists in
the current CAS session.In order to make it
available to you forever--well, not forever, but for a
long time, you have to save it.In order to save it,
these are the syntax.You need to first translate
this action set to a table.It's as easy as
here's the action set,and here's a CAS of the table.Then just like how we store
data onto the disk usingthe SASHDAT file format, we
also need to do the same thing.We use table dot save
and save this tableinto the SASHDAT format
so that later, whenCAS needs to use
it, it can quicklyload it in a parallel fashion.After you save it, when
you use it next time,it's basically used the
builtin action set table.And here you transform this
action set from SASHDATback to a table.After that, you can
directly use it.So this is about
CAS action actionset-- the coolest thing in CAS.So it's time to see an example
of how use CAS actions to buildmultiple machine learning
models to create,score, and access several
machine learning models.So now we're in the
SAS Viya SAS Studio.The first couple lines are--
we've already done them before.This is just to
start a CAS session,and to load your data into CAS.Then we're going to build this
many machine learning models.We have data from a financial
service of each customer'sfinancial activity,
their demographic,and they want to
model the relationshipbetween these features, and
whether this customer pay offtheir loan are not.So the very first step is we are
going to do some preprocessing.And I'm telling you this
is a toy example data.It's not as nasty
as real world data,so here's only two simple steps.The first one is to
replace missing value.Because for many math formula
based machine learning models,like regression
or neural network,if there is any value missing
in the whole observation,this whole observation
is being thrown away--not used in building model
because math formula cannothandle missing value.So here we include
missing value.We use the data preprocess
action set and impute action.And we replace all the
continuous variable thatare missing with the median.And if it's nominal, we
replace it with the mode.And we tell the machine
these are the variables,and the output data after each
imputation is called in-data.The next step is we're
doing the sampling.In here, we are just
using the sampling actionset srs random sample to
split them into 70% and 30%.And this would cause a new
column up here on the data,and this is the
partition indicator.If the number is 0,
it's validation data.If it's one, it's training data.So let's run the
two PROC CAS here.Things are fine.These are the output.And we have this in-data--this indicator--
partition indicator.Here are the
partition indicator,and you can see this data
is living in the cloud.Next, we do the feature
selection procedure.We all know we shouldn't
throw all featuresinto machine learning model.So here we are using the
variance reduce action set--this supervised
feature selection.We tell it these
are the features.This is the target
we try to predict.And we want features
that explain90% of the variance in data.We are only using
the training databecause we're using
supervised feature selection.The validation data just blindly
takes whatever the trainingdata selected as feature so that
we're not exposing a validationdata to the supervised learning
in the feature selection part.So we ran this, and
I'm going to tell youbecause this is a
toy educational data,all the features are selected.Next we are going to
train multiple machinelearning models.We start with a
logistic regression,and it's just this
action set regression.We start with logistic action,
what is the table input, whatare the class variables.What are the rest of
the variables thateffects for the regression
model, and is itthe binomial distribution, and
the link function is logic.We store or train the model,
which is this trained mathformula, into lr model.Then after that,
you're going to scorea model, which is applying
this learned formulaonto the validation data.And in here, I
would rather call ittest data, be it we are not
kind of using it for validation.We're doing a very
simple model approach.So this is basically the data
we haven't used in the training.I would call it a test data set.We score using the lr
model we learned before,and then the scored data
is saved as lr scored.Then the very last
action is we run the codeby rename this predict
the column which storewhat is a predictive value.We rename it to
be P_BAD so we canmake it consistent with the
column output by other model.This is also an example of how
we use multiple actions in onePROC CAS.Let's run this.Here you can see these are the
details for logistic regressionmodel.We can see the
fit statistics, wecan see the parameter
estimates, and whether theyare significant.We can see this task timing--how long does it take to
do each step in this model.And now we know our model
is stored as LR_MODELand our scored data
is this LR_SCORED.And if you
double-click on it, youwill see this is the real
value whether it's bador not, whether it is
of that mode or not.And this is the predicted
posterior probability.Next, we're going to do the same
thing using a decision tree.It's the same procedure--
just different syntax.We use the decision tree action
set, the dtreeTrain action.We use the indata,
the training data,what is the target, what
are the input, whichinput are nominal data.Then we save the
model in the casout,and then again we use the
dtree score action to scoreor save the model
on the test data.And we save our predicted
value into this dt_scored.So let's run it and
view this decision tree.You probably have realized that
things are running so fast.This is the merit of parallel
computing in SAS Viya,and why SAS Viya is really ideal
for big data and complex datatasks.In here, we have this tree.It shows how many leaves and
a bunch of other information.And now we have this DT_SCORED,
which is the same table--what is the Bad?What is the real value, and
what is the predicted posteriorprobability of that node.Then we are doing the same.We are building a
gradient boosting model,and we're building a
neural network model.You can see it's all of the same
syntax below this decision treeaction set, and use
the GB, gradientboosting tree training.Then we score it.And same in neural network,
we use this neural net actionset, and use the annTrain
and annScore to trainingand score our model.There's detailed
documentation onlineof all of this action set and
what parameters you can use.So let's run these two CAS =es
to train your gradient boostingmodel with 1,000 train, and to
train a neural network modelwith 150 hidden neurons
in one hidden layer.These two model will
take a little longer,especially the gradient
boosting because we are nottraining one tree, but training
a sequence of 1,000 trees.Each tree predicts
the residue, whichis the difference
between the real valueand the predictions
from the previous trees.But it's done really quick.You can see here more
information about the gradientboosting tree.All these trees.And this is a neural network.And the cool thing is that you
see there's so many tables,and you just have the
directory right here.So you want to say,
hey, I want to seethe iteration history of my
network, you just click here.And here we just make
sure that the lossfunction is kind of
going down slowly,but it keeps going down.So now we have all
of these scored data.It's time to compare
the model performance.In here, we're using
the percentile actionset and assess action to create
assessment for each model.Let's run this and see
what is the output.Click Run.And now let's open
one of the assess.Actually, let's first combine
those assessment tablesbecause it's right next to here.And this is basically--
for each assessed table,we have a new column telling
it which is the model.And then in here, it shows
a very simple dataStepthat we combine all of
these assessed data sets.And you can see we just use
this dataStep action set,and runCode action.We run here, and then
are going to see what'sinside this assess table.Now we have this one assess
table containing assess tablesfrom all four models.So let's open this ASSESS_ROC
which is in my test lab.So here, you can see we're
doing the cutoff, whichmeans for all the
data, we are insteadof having this cutoff
above 0.5, yes, 0.5, no,we try different cutouts.And this different
cutoff, we are calculatingtrue positive, false positive,
false negative, sensitivity,specificity, all of these.So we are going
to use [INAUDIBLE]to create an ROC plot.And as you can see, just
to explain what this cutoffvalue means, if cutoff is 0,
it means everything above 0is yes-- everything
below 0 is no.So that's why the true
and false positiveall very high, but there's
no true or false negative.And on the contrary,
if the cutoff is 1,this says we don't have a 1.If it's 99, then that
means above 0.99 is yes,below 0.99 is no for the
posterior probability.That's why there's so little
true and false positive, and sohigh negatives.Usually, the result
the model givesyou is when cutoff is 0.5.This is usually the
classification result thatis given to you by the models.But here, we calculate all
of them to do the ROC plot.So now with this table,
we can do ROC plot.And in here, we're
using PROC as a plot.And let me tell you, this
part is not run in CAS.This is running SPRE or
traditional SAS because we'vealready done all the heavy
lifting of training modelsand calculating results.So now we have a really small
table which is just 100 rowsrepresenting different cutoffs.We use that-- we plotted
this in the sgplot.To see what the
data is in mycaslib.And then we are just
plotting false positive rate,true positive rate.And then do all the
same plot as in SAS 9.Let's run it.We wait for these
plots to render.Here is the colorful plot.So we know for ROC curve,
the higher the better.So this is telling me
gradient boosting--this example model is doing
better than the other models.So this is an example of using
different machine learningmodels and comparing it
using SAS Viya using the CASlanguage.You can see it is very simple.It's just loading action
set, and using the actions,and putting in parameters.There is a lot of
action sets onlinethat are supported by SAS Viya.So definitely look online
and search for documentation,and explore what are all
the other fancy thingsyou can do using SAS Viya.And the good thing
is you can do allof them really quick
using parallel computingin the cloud.Last lesson, we now reached
this exciting part--the most exciting
part where we'regoing to build our deep
learning convolutionneural network using
SAS Viya CAS language.We're also going to
learn how we connectSAS Viya with open source.To build deep learning, you
need deep learning action set.So I do not have time to explain
to you what is deep learning,and how to use it, and
all the bunch of stuff.But we do we have a
tutorial on introductionto deep learning
and computer vision.Check it out on YouTube.We also have course
specially designedfor you to understand
deep learning, especiallyconvolution neural network
and recurrent neural network.And we also have a
certification for deep learning.So feel free to
explore more online.The deep learning action set
provides high flexibilityof network structure.If you have a little
background in deep learning,you know building
architecture isone of the most time consuming
and complicated stuff.So we need to test action to
allow such flexibility to builddifferent types
of neural network,like deep neural network,
convolutional neural networkfor image data, recurrent neural
network for sequential datapattern, or a
combination of them.Because of the complexity,
there's no CAS enabled action--CAS enabled procedure
for deep learning.You have to do so
using PROC CAS.So these are real valuable
deep learning action sets.Please check online for more
documentation, of course.And here I'm going to go through
the simplest step of buildinga convolutional neural network,
which has this PROC CAS syntax.At the beginning, we
need to find a wayto transform image data,
transform the pixelsinto numbers.So we need to load access
sets image and table.Next, we need to load the
deep learning action set.Then we need to create
an empty deep learningmodel, basically just
saying what type is it,and give it a name.And how we build
this model is weare adding layers one by one.So you have multiple
add layers statementsfor different types
of layers-- input,convolution, connected, output.And there's many
different layer types,and feel free to explore
the online documentationabout what those types
mean, and how to use them.And after you build the
model and add layer here,we're just filling
out this math formula.DlTrain is how we
train the models, howwe load the parameters into
this very, very long, big mathformula.After we train this data,
we have the DlScore justlike how we use this in other
machine learning models.We score the fitted model,
we score this formulayou learned on new data.So now before we see the
demo that combines those two,we're going to see how to
use CAS in open source.And here, just for
you to understand,that using CAS in open
source is not pure magic.It is science.SAS Viya is designed
to be open, whichmeans it's designed
to have interface talkto other open source languages,
such as Lua, Python, and R.And all it does is that you need
to download this little packagecalled SWAT.And you can think of
SWAT as a translator.This SWAT, you can write
R or Python programthat connects to a CAS server.Load data into CAS, analyze
large in-memory data setsquickly and efficiently
using CAS actions,and work with the
results of your analysisusing familiar data
preparation techniques,or like image plotting
techniques in open sourcelanguage.So it's a collaboration
between the two.Letting CAS do all
the heavy lifting,and let the open source to do
all the customized fidgeting.And this is another way I like
to explain this SWAT package.So the CAS enabled procedure,
like proc, tree split,and all of those that
we can run in CAS,it's basically like you're
telling CAS make me a cake.And Lua and Python are
the same thing, justin different languages.So for example, Lua was invented
in Brazil, so it's Portuguese.And this is a language.And Python is by the Dutch.And the SWAT Package is a
linear translator translating itinto what exactly it means--
into the CASL language.And CASL language is made
of a CAS action, whichis a basic step of
preheat the oven,mix ingredients, put
in mold, put in oven,blah, blah, blah, and frosting.And CAS enabled procedure
and all the open source thingis basically a wrapper
or shortcut of tellingCASL to do these CAS actions.To use CAS in open
source, there'sjust these five simple steps.Step number one,
load the package,then you connect to the CAS.You give CAS the data.You let CAS to do all
the heavy calculation,and you get data from CAS.That's all you need.To download SWAT package,
SAS has a GitHub page,and that's where
you download it.Please review some of the system
requirements, prerequisites,and some examples there.Next, you just load
the SWAT packageinto R or Python using
different syntax.This is the easy part.And this is where you
do the connection.And once we establish
this connection,it's our playground.A connection object-- conn--represents a connection and a
session between the client--that is your R or Python--and the server-- the CAS.You just need to
follow the syntaxto specify the hostname,
the port, the username, the password,
and a bunch of stuffso you don't randomly come back
to somebody else CAS server.Once you have this
connection, you'rejust able to talk
through this connection.The next step is you
give your data to CAS.And you do so by creating
an in-memory table whichis a CAS table object.And these are the
two examples of howyou load a CSV file from
the client side to the CAS--pass it to the CAS, and the
CAS creates the CAS tableobject for it.And this castbl is basically
a reference to this CAS tableobject in CAS.Then with this CAS
table-- castbl reference.You can kind of start
some tiny explorationusing dimension, the names.And these are enabled by SWAT.And now we have our reference.You can run algorithms in CAS.All the algorithms will be
executed on the CAS tableinside of CAS memory.And then the trick is to
let CAS do the heavy liftingto take advantage of
the speed and efficiencyof the distributed environment.You can do small stuff,
like manage data,explore variables on the
client side, [INAUDIBLE]for variables, and create
models on those computationalexpensive kind of tasks.You do it on the
server on the CAS side.So now that CAS already done all
this big calculation for you,you want this result.
So this is the last stepof getting data from CAS.Some CAS tables,
especially the small ones,are automatically
returned to the client.Then data is returned
to the client.The client side CAS table
is called casDataFrame.It's just a type of big object.It's a copy of in-memory data.So this is not a reference,
this is real datanow moved to the client side.And if it's tiny, it just
gets automatically returned.For example, column means.There are all the data that
CAS have, but have not yetreturned them.And what you need
to do is you needto use defCasTable or
the CAS table action--not action-- the wrapper--depends on which
language you're using.And this enables you to create
a reference to the CASTable.So lr_model-- linear regression
model build in the CAS memory,you can have this resolved that
refers to this table in CAS.The last one is the hardest one.In this case, you don't want
to just create a reference.You want to copy the
whole big CAS table backto your client side.This is basically transforming
the object from a CASin CAS to a data frame--CAS data frame object
on client side.And to do that, you use
this to CAS data frameor to frame function.It depends on which
language you are using.And this will take a
long time because youare loading all the data from
CAS server back to your client.You are copying the data.So unless you need to, you
should avoid doing that.You should let CAS finish all
the calculations until you havea small table result and only
have the small table resultpassed back to your client.So there are some
functions you canuse to figure out where the data
is located if you get confused.And these are the examples.If it's CASTable
is on the server.Otherwise, you have
it on the server,and there is a reference
on the client side.These are some of the efficient
coding practice of usingCAS and SAS in open source.The first efficient coding
practice is create functions.Avoid repeating code
again and again.And this is like a coding
practice in other computinglanguages as well.You create functions to
automate duplicated parts,and substitute parameters
to call it again and again.Next, efficient
coding techniques,and this is also the same
in a lot of other computinglanguages, is looping.You loop to avoid repetitive
coding, simulation,sensitivity analysis, scoring
many models, and stufflike that.Create a loop to do that.So now we're going to see
the most exciting part whichwe are going to build our
convolution neural networkto classify fashion data.We are going to deal with
image data using SAS Viya CAS.And we're going to call SAS
Viya CAS in Jupiter notebookusing Python.So let Python call CAS.Let CAS do the work, and
return result to Python.So now we're inside Jupiter
notebook, another interfacethat can talk to SAS CAS.In here, we're going to
use Python to talk to CAS.CAS is still doing all
the work, but we are justusing Python to talk to it.In here, we're doing a
computer vision task.We are classifying images.We have an image data set.Let me show you.Here, the fashion image data.We have a lot of black and
white photos of clothing,and each category of clothing
is in their own folder.So class 0, these
are all the t-shirts.Class 1, these
are all the pants.And we are trying to
build a deep learningconvolutional neural network
to classify those images usingSAS Viya CAS.And the very first step
is to load the packages--load this SWAT package which
is our translator from Pythonto CAS.Click Run, load it.Then we're going to make
a connection to CAS.We're going to create
this connection object,and it's basically
using SWAT without CAS,and tell what is the server,
what is the port, the username, and password.We can run.After that, they're
going to add a caslib.We need to store
[INAUDIBLE] datainto this big brain of CAS,
just like in SAS Studio.So this is the path to
our folders of the image.And then we just run it.So now it's telling you the
cloud analytic service--the CAS added this caslib mycl.The next step is we
load images whichare essential step because
we are transforming imagesinto numbers, into data.We use this image action set.And here using the
conn.builtin to help.It's going to show you all the
actions possible in this image.And every time you run
this CAS action or loadaction set, make sure you
have this conn in front of it,because this is the connection
SWAT has setup for you.These are everything
you can do with image--all the actions.And here you load the images.You start this conn, the
image, the action set,and load image--the action.This is your caslib.This is the path
which is the folderinside the path you've given,
where all the images arestored.Labels are at level 1, so
there's next layer folderof class 1, class 0.And the CASL the
data is going to besaved in this fashion table.The input data is fashion.So let's run.So all the 15,000 images
are loaded into CAS.And these are just some actions
to summarize the image--show the column information.And also here, we're loading
the familiar action set sample,and we are using it to run
this simple CAS action--frequent CAS-- simple
CAS action set--frequent CAS action,
and calculate what'sthe frequency of each class.In everything, you need to
have this conn in front of it.This is the object doing all
the translation into CAS.So let's run this, run
this, run, and run.So again, we can
use the built-insto help to see all the
possible actions inside sample.In the last step, we
calculate the frequency.So we have a total of 10
classes, and in each class,there are 1,500 photos.Next, we do the same step as
in every machine learning,which is partition the data.And this time, we are doing
it into training, validation,and the test.Here, we first load
this action setsampling so we can do random
action set random sampling justlike how we did it
in SAS Studio wherewe build the other
machine learning models.Then we use the same
SRS random samplingto create three partitions.Let's run this.Hey, we have a partition data.Next, do the same step
of shuffling the data.It's very, very important
in image related tasks,because you don't
want your eyes to seethe same image or the same class
of image for a long, long time,and then move onto another
for a long, long, long time.You want to see one of
each in an alternative way.And that really helps
your convolution neuralnetwork learn.So shuffling the data
is an important stepfor image related tasks.Let's run this.Shuffle.Next, it's going to build this
convolutional neural network.We're following all the
steps in this slide.Below the action set using conn,
the deep learning action set.Let's run it.And it's going to
show after it'sdone all the possible actions
you can use in deep learningaction set.Then we are going
to build the model.The model's name is cnn, so
this is just an empty shell.And here we're adding each
layer onto this empty Model.We start with an input
layer, which here is justwhat is the input size--the size of the image.And we do a standard deviation.Next after this input layer
transform image into data,we are adding a
convolutional layer.We don't have time to go
deep into what it's doing,but it's basically ways of
creating different featuremaps.We are using 10 filters--we're going to create 10
different feature maps.And each filter
is of size 5 by 5.And every time it
creates a new feature,it is going to move by one.After convolutional layer,
we create a lot of features.We add this pooling
layer, which islike a way of summarizing
local features.Adding those numbers together
or take the max here.It's having to size 2 by 2,
and each time it moves by 2to create a new pooled feature.Then we add this
fully connected layer.In convolutional neural
network, the earlier layersare all about creating features
from the raw image pixel data.And the later layer here,
the fully connected layer,the output layer is
where we make decisions.We make the connection between
features and the outputjust like in the other models.So here we add this
fully connected layer.We're using 100 neurons.The last layer is the
output layer wherewe use this softmax function.So these last two
layers are exactlythe same as in a
traditional neural network.So all of these, we are
not training the model.Deep learning or neural network,
they are just math formula,and here we are just building
out this math formula.When we click run, this
math formula is built.And now we are
training the model.This is where we'll
take some time,because we are learning all
the parameters inside this mathformula.And being a convolutional
neural network,we have a lot of parameters.This is our training data,
this is our validation data,this is our target.And we save the best trained
weights in this trained weight.This optimizer is where
we train the model, whichis a mini batch
size how many datawe use to update the weight each
iteration, how many maxEpochsoutputs.And we're using the stochastic
gradient descent and usemomentum to also consider
the previous directionof the update.And we have a
learning rate of 0.01.So let's run this trained model.This is where we learn hundreds
of thousands of parameters.After we train this,
then we can score it.And it's done.It is really quick given the
large amount of parameterswe need to learn [INAUDIBLE].We are learning 197,250--a six-digit number of practice.And this is the
merit of SAS Viya.Imagine running this
in R or even in Pythonusing you're own
little computer.It would take a long time.But being able to use the
cloud to run this action in SASViya, SAS CAS, the CAS is going
to do all the work for youin the SAS end, and it's
going to give the result backto your little client machine,
and the program you're using.This is iteration history.The output, the trained
weights, into this CAS table.So now we learned all the
parameters in this formula,we're going to apply
this to our CAS data.This is data we've
never seen, so wecan see what is the real
prediction power of your model.So let's score it.The score part is really quick,
because the expensive partis learning the large
amount of parameters.But once it's learned, you
can just clock numbers in.That's really quick.We have a misclassification rate
of 15%, which is pretty good.So now we calculate our result,
and it's now in the CAS.The CAS already done
all the heavy lifting.We can bring this small
calculated result table backto our local client
side, and thenanalyze the results locally.So to do that, in here, this
is a confusion matrix whichis for each class what this
class is being classified as.So you can see the numbers on
the diagonal is really large,and these are the correct
classified instance.Then we see the familiar
pd Panda data frame,and this is-- we already
brought result table from CASback into work client side.We run this-- we printout
what is the classificationrate for each class.And as we can see, shirt
is the most difficult onebecause it's very easy to
classify shirt as number sixto be, let's say,
number 1 and number 3.Oh, it makes sense--
shirt, t-shirt, pull over,I can't classify them.No wonder machine can't.So this is where the machine
makes the most mistakes.And then we can use this
little plot inside Pythonto make it look pretty.Run this.We have this plot.Shirt has the highest
misclassification rate.You can also do a
bunch of little things.But in this example, of
course, we end the session.So in this example, it
kinds of showing youhow to call CAS inside Python.And it's so simple.It's all the syntax--just you need to use this conn.And just at the
beginning, you needto download this SWAT package--import this SWAT package,
and establish this connectionto CAS.And then using this conn, you
can just directly talk to CAS.Let CAS do all
the heavy lifting,all the heavy calculations
of that large amountof parameters, and pass the
result back to your client sidefor you to do all
the other fidgeting.Lastly, now we've seen
so much cool stuff.And by the time we reach here,
the most common question Ireceive is often, hey,
when do I use SAS Viya,and when do I use
other alternatives?Data science is a
really diverse field,and these are my opinions.SAS Viya CAS has a
lot of advantages.It's cloud enabled
and it's scalable.So when you have really, really,
really, really large data, youcan use CAS because you
can scale how many CASand CAS teams you're hiring.It has the speed of parallel
computing, ideal for big dataand expensive algorithms
like deep learning.It has layers of product
just for garbled dataand to make data so pure.So you have really
sensitive data and youwant reliable performance--SAS Viya is the choice.Even though today,
we only see the m ,there is a lot of
SAS Viya products,and they are designed for
this loop to transform datainto actions.So there's products
for data management,there are products
for creating plots,for creating BI dashboard,
for building models.And there are also products
for managing modelsto deploying models,
and managing decisions.So SAS Viya gives you this
tool box of a lot of tools.And when you want multiple
steps in using the data,SAS Viya is an easy,
ideal tool to use.And one of my last
favorite thingsis that it removes this
programming barrier.There is a lot of
excellent scientiststhat just don't know
programming yet.And for them to run analysis,
to understand the data,the SAS Viya, there's a lot
of point and click interface--removes this
programming barrier.Whereas the alternatives,
their advantage is freeAnd sometimes, they have
new technology and morefunctionalities just because
there are so many peoplearound the world developing it.It's also support some
object oriented programming,and you can create
more detailed features.So for me, when to use
SAS Viya and open sourceor other alternatives given
these pros and cons isthat I would often
use SAS Viya whenI have a huge amount of
data flowing consistently--live streaming data,
huge transaction datathat's flowing consistently.And they're often sensitive.You want to use them to make
reliable, big decisions.It also helps when you
want quick transformationfrom data to insights.You don't want a lot of
programmers to work on it,and take a long time.You want to set up this
automated procedureto transform data into insights.And of course,
when your workforceis full of excellent scientists,
but may not know programming,this is an ideal tool.In this tutorial, we saw the
SAS programming interface,but there's a lot
of other productsthat you can do it from the
drag-and-click, point-and-clickkind of interface.It's very easy.So there are also
some scenarios whenI will use the alternative.For example, when you
don't have enough resourceto investigate in
infrastructures to set up CAS.Or when you're developing and
researching new algorithm,then you can join
the other peoplewho are doing so all
around the world.We can have all the
algorithm inside of CAS--we don't have enough work force.And when you need heavy
programming for really,really massive
feature creation--sometimes, you need object
oriented programmingto do some weird
feature creation--then you can do that as well.And for me, by
having SAS Viya open,you really are combining a
lot of advantages together.You can use your
little alternativesto research your algorithm,
to compare it with CAS,and you can do your
feature creationinside this alternative.And then when you
need heavy lifting,you can go back to
SAS Viya and letthem do their part of the work.So running CAS
open source reallycombines advantage of both.This is the end of this
very long tutorial.My name is Dr.
Aurora Peddycord-Liu.I wish you learned something
you, become more confidentusing SAS, and CAS, and
other CAS languages.Stay sassy, and have fun
learning more about SAS."
25,"Hi, I'm Ari Zitin.And this is our education
tutorial what is SAS Viya.Today, I'll be discussing with
you the SAS Viya architecture,and specifically how
that relates to the wayyou access data and how
data moves and is processed.I'm assuming that you have no
prior knowledge of SAS Viya.And the objective is to give
you a basis and startingpoint for going forward with
some foundational understandingof SAS Viya.We'll start with
the architecture.SAS Viya in a full deployment
is a multi-machine distributedplatform.Here's an example in the slide.Each box here is an individual
computing unit, eithera separate computer or a
virtual machine in it's cloud--in a cloud deployment.Each machine has its own disk
storage that is hard drive,its own memory that is RAM,
and a CPU or multiple CPUsfor each of these machines.They're all going
to be connectedvia some sort of
networked communication,probably ethernet.Focusing first on the server
controller and the serverworkers.These machines together
provide a distributedmulti-machine data
processing environment,and we'll call this environment
SAS Cloud Analytics Services.We'll often call
it CAS for short.The CAS server would be all
of these machines combined.Here, we show three
CAS server workers,but you can have as
many as you need,depending on the volume
of data and the complexityof processing required.You can also scale up.So if you find that you
don't have enough CASworkers for the
processing that you need,you can always add more.In CAS, multiple
tables can be loaded.These tables are retained
in-memory, not just availableon disk on the hard drive.Each table is
distributed in-memoryacross all the workers, and
is processed in parallelwith various CAS
processing actions.In addition to the
in-memory data,a permanent copy of the
data can be stored on disk.CAS supports parallel loading
from disk into memory and alsoparallel saving of the data
from memory back to disk.The purpose of this
multi-machine parallelprocessing is to allow you to
process large volumes of dataand perform very complex
resource intense processingon big data quickly.The idea with in-memory data
is that once the data is loadedfrom disk, it remains
available in-memoryover time for repeated use by
one user or by multiple users.This speeds things up because
loading data into memorytakes a long time relative
to processing dataonce it's in-memory.CAS is a shared server,
so multiple userscan actually use the
same in-memory datafor different processing tasks.SAS programmers
using SAS Viya caninteract with CAS or the CAS
server from a SAS sessionseparate from CAS, called SPRE,
or SAS Programming RuntimeEnvironment.Each user starts up a
user session in SPRE.SPRE is where your SAS programs
are initially submitted.You can use SAS studio as
the programming interface.Think of SPRE as where you're
good old SAS session is.You can execute the same type
of SAS code in that SPRE sessionthat you may have
been doing for years.In fact, SAS 9.4M5, or
later, or a newer version,can also be used
in place of SPRE.This allows you to integrate
existing SAS 9 implementationswith SAS Viya.Now, of course, if you
don't have an existing SAS 9implementation, the
SPRE environmentcomes with Viya on its own.Users access a SPRE session,
and the CAS session,in fact, from a browser
on a remote client machinegenerally using a
SAS Studio interface.First you start a
SAS Studio sessionon the SPRE server machine.At this point, you can
execute traditional SAScode, your good old DATA
steps and SAS procedures or--excuse me, on your SPRE
session without any use of CAS.But from the SAS
Studio session, youcan also start a CAS
session on the CAS server.The CAS session is a
multi-machine session.You'll have your own
session controller,which is essentially similar
to the CAS controller.And you'll also
have session workersfor performing
parallel processingon distributed in-memory data.Depending on your
administrator settings,you might have access to
all of the workers connectedto your count server,
or you might onlybe given a subset
of them to free upresources for everyone else.Each user gets their own SPRE
session and their own CASsession.In this demonstration,
we'll explore the SAS studioenvironment, start a local SAS
session on the SPRE machine,and then also start a CAS
session on the SAS Viya server.We'll start by
opening Google Chrome.And under SAS Viya bookmark bar,
we'll navigate to SAS Studio.You can see it takes us to
this address server/SAS studio.We'll sign in using
our user name Lynn.And we don't want to treat
Lynn as an administrator,so we'll click no to not
sign in as administrator.Once we've entered
SAS Studio, wecan see there's some
options here on the left.We're looking at
the Explorer tab.We might also want to
look at the Libraries tabto show some of the SAS
9 libraries loaded in.So it looks like we've already
started our SAS sessionon the SPRE machine.Under Explorer, we can
see this is basicallyserver files and folders
on the operating system.So this object here server is
in fact the SAS Viya server.Its server.demo.SAS.com.But it has an alias here.And under server, we see
we have this home folder.And this home folder
points to Lynn's folderon the operating system
of the SAS Viya server.So Lynn's account has
some sort of accessand has been given some folders.And here are the folders
that Lynn has access to.So we're not going to
see all of the rootfolders on the operating system,
but just the folders that Lynnhas access to here.Let's go ahead and
look at these foldersin a different interface just
to verify that what we'retalking about makes sense.I'll minimize SAS
Studio in Google Chromeand I'll open up WinSCP.I'll sign in as
lynn.server.demo.sas.com.My password is already
saved in WinSCP.WinSCP is a tool that allows
me to copy folder-- filesor folders from the local
machine-- the local clientto the back end server.Right now, I'm getting just
permission denied errorjust because it's
pointing to a folderthat Lynn doesn't
have access to.So click OK.I'll just go back to home.And I'll actually just go
back to the root directoryhere by clicking the slash.So here's the root
directory we cansee on the operating system.I'll make this full screen
to see a little bit.Here's the client system here
over our windows computer here.And this is the Linux computer.To look at what we were
seeing in SAS studio,I'll go to home Lynn.And now, we can see
there's these fourobjects, SAS user.viya, demo
one, demo two, and sales.csv.Just if I go back to
SAS studio real quick,we can see those are the four
objects in Lynn's home folder.So since we've
signed in as Lynn,we only have access to the files
that Lynn would have accessto on the operating system.But we could see that
there are certainlymore files on the
operating system.Other users, and then
just the root directory.I'll close WinSCP.I'll terminate the session.Actually, I'll just minimize it.We'll leave the
session open in casewe want to come back to it.I'll open up mRemoteNG,
which is really an SSH toolto SSH over to the server.Once mRemoteNG is
open, I'll doubleclick Lynn to open up a
connection to the SAS Viyaserver as the user Lynn.If I type in PWT, this will
print my working directory.I'll make this full
screen a little bit.My working directory
is /home/lynn.So that's that same location
that we were pointing toon the SAS Viya server.I could also just
look at everythingon the operating system--ls -l /-- to look at the home
directory on the operatingsystem.So this is what we don't
have access to in SAS studiobecause they're not living
in Lynn's home folder.But if we look ls -l /home/lynn,
we see the four foldersin Lynn's folder.So you can see
there's more foldersthat we don't have access to
in the SAS studio interface.We'll go back to SAS studio.I'll open up this
code demo1.sas.This has a bunch of content for
the demo for the first section.We'll just execute
the first few rows.The very first rows,
options and CAS sessions,are just to terminate
previously existing sessionsand set some simple options.But the next code-- we'll
go ahead and run this codeand look at it.We're assigning a live name to
a local SAS-- in the local SASenvironment.So this is part of
our SPRE session.This is our SAS session
on the SPRE machine.If we just look at the log real
quick, I like to point out,it gives me an error that
says, session-- my sessionnot recognized.When I tried to end
the CAS session,it tells me there's no CAS
session open because youhaven't started one.This was just in case we'd
already run this code.When we assign the live name,
we get live name my sounds,was successfully
assigned as follows.It's just a base SAS
engine, base SAS library.So this is all part
of our SPRE session,and it lives here on
the compute server.So this is all just
part-- excuse me,as part of our SAS session
on the SPRE machine.When I list all the libraries
in the SPRE session--oh, before, I do that.Here it tells me a list of
libraries in the SPRE session.I won't look through
all these libraries.But if I click over
here on Libraries,it matches the SAS libraries.Maps GFK.Maps.And now if I refresh
this, we'll seethat my SAS library
that I just createdgets added to this list.We will get some results.We also called this PROC
MEANS on a local SAS table.So this is just all
happening in the SAS session.So this is all in
the SPRE machine.We calculate all these
means, all of these summarystatistics, on a
single machine, whichis the SPRE machine in
the SAS Viya server.So this computation is not being
distributed because we haven'tmade use of CAS yet at all.So far, we're just using
the SPRE environment.I'll go ahead and
start a CAS sessionand list all of the CAS
libraries available.So I'll run these lines of code.CAS my session.We have some session options.Just the timeout options
and the default Caslib.We'll talk about Caslib shortly.As well as listing
some informationabout different Caslibs.We'll start by just
looking at the results.We can see the session My
Session successfully connectedto Cloud Analytic Services.So this is the SAS Viya server.This is the CAS
server controller,server.demo.sas.com.We see a CAS statement.And we see this session, My
Session, has been created.And we default to the
CAS library CAS user.When we listed all of
these CAS libraries,this Caslib_all_list, we see
a bunch of CAS libraries.I'll just highlight a few.Here's the CAS user
CAS library, so it'sspecific to the user Lynn.It points to this location
on the operating system.I find this tends to
be very useful if youwant to put files in there
on the operating systemor pull files out of there
on the operating system.It's nice to know where
that path is locatedbecause it's sort of hidden
a little bit in the operatingsystem.We can also see--I'll scroll down.We'll look at the
public CAS library.This is sort of
the Caslib that'sshared between all users.We get this description
accessible to all users.And we can see here's
another location wherethe path is located.So if we wanted to
move files to there,again, these are where
these Caslibs are actuallylocated on the back end server.So you can see, even
though we have accessto this server folder,
Lynn's home folder,by default, that's not
where her Caslib is.So we might want to put
another personal library thereif we were so interested.For now, we'll just
be loading filesinto Lynn's CAS user Caslib.And we'll talk about
what those Caslibs arein the next section.In this next
section, I'll explainCaslibs and how to use them.Caslibs represent a way to
reference an in-memory spacefor loading and accessing
one or more in-memory tables.Each Caslib is also associated
with a data source locationon disk, where data can
be permanently stored.So to repeat that
again, each Caslibare a way to identify a
set of in-memory tables.And each Caslib is also
associated with a disk locationfor storing a copy
of tables on disk.The disk data source can be in
the form of SAS data sets, CSVfiles, Excel worksheets,
database tables, SAS HDATfiles, and others.Sometimes these objects
can even be images or audiodata for doing deep learning.Caslibs also include
authorization information,such as user IDs and
permissions to control accessto data and other resources.Just like traditional
SAS libraries,multiple Caslibs
can and typicallyare defined for
each CAS session.Each with unique data
source information,but also specific authorizations
for different users and groups.This is important because CAS
is a shared server environment.So you need to be
able to controlwho has access to
various data sources,and what type of permissions
they have for that data.In the demo I just did, I
started a SAS studio sessionon SPRE.And I started a CAS
session on the CAS server.In the next demo,
I'll show you howto access data stored
on disk and move itinto memory on the CAS server.And you need to be able to do
that because you cannot use CASdata in a SAS procedure and
doing any processing untilit's-- and do any processing
until it's been loaded upinto memory.Recall that in-memory tables
are automatically distributedacross all worker nodes.So how do we load
tables into memory?One method is PROC CASUTIL.With PROC CASUTIL,
one of your choicesis to move data stored on
the SPRE machine into CAS.That is represented
here as the foldericon with the little
purple squareson the yellow SPRE machine.So data is stored there in
the SPRE machine on disk.And that data is first moved
from SPRE to the CAS sessioncontroller machine.From there, the
session controllerdistributes the data across
all CAS workers in-memory.To do this, you can either
use a load data equalsstatement and name a SAS data
set from your SAS libraryin your SPRE session,
or you can usethe load file equal statement
and name a file from the SPREmachine file system.These can be CSV files,
a worksheet from an Excelworkbook, and a few others.With a load data
equals, you specifya SAS data set in the form
of saslibref.datasetname.With the load file equals,
you specify the file systemfile path and name.You can also load data from disk
on the CAS session controllerinto the session workers.This data source
is now representedin the diagram as the folder
with the purple squareson the session
controller machine.The CAS session controller
process manages this operation.To do this, you'd use the load
CAS data equals statement.On the load CAS data
statement, you simplyname the file and the Caslib
that points to that file.You don't specify
the file systemwhen you name the file because
the Caslib definition alreadydefines that physical location.So you're loading data from
the Caslib, which specifiesa location on the server.In some cases, data can
be loaded in paralleldirectly to each CAS worker.This can improve the
performance of the loadfor large volumes of data.Keep in mind that no
matter how we load datainto in memory cards
tables, whateverthe origin of that data, it's
all directed from the programcode we submit from
our SAS Studio sessionrunning on the SPRE machine.Here's an example
of a valid loadCAS data statement for
parallel load to CAS workers.This requires the
use of a file storedin SAS HDAT format, which
is a SAS format designedspecifically for
parallel loads and saves.Think of it as a distributed
form of a SAS data setdistributed across
these session workers,or the worker nodes
in the CAS server.Once data is in
memory, it becomesavailable for use in DATA
steps or PROC steps thatare executed on the CAS
server through a CAS session.PROC MD summary is a procedure
in your SAS studio session.You submit PROC MD
summary, and you name itin-memory CAS table as input.SPRE will notice in the request
to run this PROC to the sessioncontroller process,
which will coordinatethe execution of
the PROC in parallelin each of the worker nodes on
the distributed in-memory data.Here, I've shown you how to
name that in-memory tableon the data equals
option of the procedure.It is a two part name in the
form of caslib.tablename.Just like you name
your SAS data setsin SAS libraries
with a two part name.But in this case, CAS user
is a Caslib rather thana SAS library.Non-CAS procedures
and other processesthat do not run in CAS can
also use CAS in-memory tablesas input.In such cases, the
data is moved from CASback to the SPRE machine for
processing in a SPRE session.PROC PRINT, for instance,
is such a procedurethat not only-- that
executes-- that onlyexecutes in SPRE and not in CAS.But this is not practical
for large volumes of data,but is often used for
subsets of data or summaryresults produced by CAS
analytical procedures.In this demonstration,
we'll look at some optionsfor adding data to Caslibs
and executing SAS proceduresagainst data that's in
memory in the CAS server.In the last
demonstration, we endedby listing out some of
the caslibs that we had.We explored just a few.We looked at CAS user Lynn.We saw that it
was a path Caslib,so it points to some location
on the operating system.We also looked at
the public caslib,and saw that it also
was a path caslib,so it points to some location
on the operating system.Why don't we look at
some of the files thatare located in those caslibs.So we'll submit this
PROC CASUTIL code,and we'll just list files
in disk for CAS useras well as tables in
memory for CAS user.And of course, the same
for the public caslib.So we're going to look at
files living in the hard drivelocation as well as tables
in memory for both the CASlibraries.I'm going to change
that option a little bitjust so we can see the results
a little bit more easily.Let's look at some
of the results here.We see files on disk for
that CAS user CAS library.And we see here's some
information about the CAS userCAS library.All the information we
saw on the log earlier.Here's all of the files in
that location on the operatingsystem.So we see we have sales.xlxs,
which we saw in the foldera little bit earlier.But it's actually this
is a different location.Some other tables.Just to point out, there's some
SAS HDAT files, a SAS 7BDATfile, different
kinds of data files.If we go down to
tables in memory,we see that although it gives
you information about the CASuser CAS library, it doesn't
list any files in memoryor any tables in
memory below here.So if we go to the log,
we can see this note here.And I'll just click
the note up here.We could scroll through
the log to find it.No tables are available
in caslib CAS user landof Cloud Analytic Services.So there aren't any
tables in memory here yet.If we look down to the
public CAS library,we get more information
on the CAS libraryas well as all of the files
living in the operating system.Here's some different files.Some of these were
created by other users.You could see group SAS.Oh, I guess owner CAS.So these were created
by different users.And once again, there's
no tables in memory.We go over to the log.No tables are available in
caslib public of Cloud AnalyticServices.So so far, we just
saw some files livingon the operating system,
but no tables in memory yetin either of these
CAS libraries.Let's load a table into memory.We'll run out a CASUTIL
to load casdatasales.xlxs.So this is a file
that was alreadyon the operating
system in the CAS userfolder on the operating system.So we're basically just loading
that file from the operatingsystem into memory.Just execute this real quick.And we see the results.Well, at the very end, we listed
all of the tables in memoryin the CAS user CAS library.And we see that the
sales table is nowloaded into memory in
the CAS user CAS library.And we see that it's source is
sales.xlxs in the CAS user CASlibrary, which is where
we loaded it from.If we went to the log,
we could see that it--we could see how long
that it took to load it,and we could see that it
processed the request.I guess, I was going to
point out-- here we go.Cloud Analytic Services
made the file sales.xlxsavailable as table sales.So it's now loaded it
up into memory for us.I'll go back to the code.So now that we have
the table in memory,we feel like we should
be able to execute a SASprocedure against this table.So let's run it, and we'll see
that this doesn't quite work.And I'll explain when
we look at the errorwhy it doesn't quite work.So we call PROC MD
summary, which we knowis a CAS procedure, so it
can execute on CAS data.And we use datacasuser.sales.So let's run this.But we don't get any results.We get an error that says lib
ref CAS user is not assigned.So although we have a CAS
library named CAS user,and we know it exists
on the CAS server,are local SPRE session right
here doesn't know anythingabout CAS user caslib.So if I click libraries here, we
see all these SAS 9 libraries.So we see all these libraries
on the SPRE machine,but we don't see any
libraries on the CAS server.Because we have to tell
SPRE about these librariesbefore we can use them.So let's go back
and do that and seeif we can get this
Proc MD summarycode to run with no errors.So I'll go back to the code tab.I'll scroll down.Here's the LIBNAME statement.I'm going to assign a
little caslib to allowSPRE to access the data.I sort of think about
this as telling SPREthat we have this caslib
and we want to use it,and here's where it is.So we'll run this code, we'll
run the LIBNAME statement,and then we'll rerun the
PROC MD summary code,and we'll see that this time,
we get the results that we want.So we see libref CAS
user was successfullyassigned as follows.And just note that immediately
it refreshes our Libraries tab,and we see this CAS
user CAS library here.We also get the PROC MD summary
that executes successfully,and it produces
some output data.So we create this data
table salary summariesthat contains the
summary information.And notice this data
table is an output tablethat's actually been added
to the CAS user library.So if I open up the CAS user
library here on the left,I can see in addition
to the sales tablethat I loaded into memory
with the PROC CASUTIL,here's the salary
summaries tablecreated by PROC MD summary.I'll go back to the Code tab.So that's a good
lesson that you justwant to make sure
you have accessto your CAS library
in the SPRE sessionbefore you execute SAS
procedures or CAS enabledprocedures to call
that CAS library.Now, we'll go ahead
and use a procedurethat will only run in SPRE,
such this PROC PRINT procedure.PROC PRINT doesn't-- isn't
a CAS enabled procedure,which means it won't run
in the CAS environment.We still get results
even though we call it--so just pointing back to the
code because I didn't highlightthis--we call this print procedure
against a table livingin the CAS user library, and
we're getting some results herelocally in the SPRE environment.But if we go to
the log, we can seethat nowhere does it talk about
using Cloud Analytic Services.So this procedure
gets executed locallyin SPRE, which means that
basically this data getsbrought locally to SPRE to be
executed on the single machineenvironment.I'll go back to the code.We'll just look at a couple
of different examples of waysto get access to tables
in the CAS environment,essentially, to get tables
into the CAS server.We can load a table from a
SPRE library into a CAS libraryso that it's then loaded into
memory on the CAS server.So we'll use PROC CASUTIL to
do this using the load datastatement.We can see that the
data has been loaded.Here is now our tables in
the CAS user CAS library.And the table we just loaded
was the employee's table loadedinto the CAS user CAS library.But this time, we loaded it
into the CAS user CAS libraryfrom the My SAS library.We had this employee's table
in the my SAS library livingin the SPRE machine.If we go to the log, we can see
that the table mysas.employeeswas added to the
CAS user library.I'll go back to the code.So although we could move
tables from SPRE, essentially,from the SPRE environment
into the CAS environmentusing PROC CASUTIL,
we could alsodo it using a couple of
different procedures.We could use it as a table
created using the DATA step.So in this DATA step
code, we have a table,the set statement calls a
table in my SAS library.But it outputs a CAS table, so
it loads the data into memory.Or we could use a procedure
like PROC SQL, whose output canbe specified as a CAS library--as a table in a CALS library.So the create table
statement, insteadof specifying a SAS library,
we can specify CAS library.And this PROC SQL code, will
just like this DATA step code,load our output data into CAS.So I'll run both
these procedures.And we can see in the log the
data set CAS users employeeswas loaded into memory.Employees 2 as well as
CAS users employee 3.So the DATA step code loaded
casusers.employees2 into memoryon the CAS server.And the SQL, this PROC SQL
loaded casusers.employees3--well, created it, and
loaded it into memory.So if we go-- if we look
back at the CAS user library,we now have employees
2 and employees 3as in-memory data tables.We can look at the output data
to see that these are the CAStables that we've now loaded.CAS user, CAS user.Go back to the code.So a bunch of different
ways to load data from--basically, from your SPRE
environment up into CAS,or from your CAS user library.So we've looked at ways
to either load datafrom your local SAS environment,
your local SPRE environment, upto the CAS server.Or from living on the CAS
server up into memory.What if we have a file
on the SPRE machine,but it's not in any sort
of SAS library or anything?So it's not in a local
SPRE library, like my SAS.It's just on the hard drive
of the local SPRE machine.We can load those tables
into the CAS server as well.So we'll use the load file
option in PROC CASUTILto load a couple
of different files.In this case, these are
actually from the server.So these are files
on the Linux server.But they're not located
in any CAS library.So we're loading them
up into the CAS libraryfrom some location
on the server.So this could be
data on the serverthat we don't have a
library pointing to.I'll go ahead and
execute this code.So we can see the
CASUTIL procedureshows all the tables loaded.And we now see we have this my
Orion and sale from that CSV.So if we go back to
the log, scroll up,we see load file Orion.xlxs,
and we output it as my Orion.So we have a table
casuser.myorionthat we created from this
file on the operating system.And we also load this other
file, it's a CSV file.And we have to specify
some import optionsfor the CSV file.Basically, the CSV
encoding, and the factthat we want to get the column
names from the CSV file.And we output it in the CAS
user library as sales from CSV.And we can go ahead
and double click this.So I'll just double click one
of these tables in the CAS userCAS library to look
at the in-memory data.So now this essentially gives
us a view of the in-memory data.And of course, it's not going
to show us all the data,so it only specifies
some of the rows.I guess these are all the
rows in the data, actually.But it allows us to only display
a subset of the rows in casethe data table is too big.So it doesn't try and bring all
the data from the CAS serverover into this
graphical interface.But this allows us
to explore our data.And we've seen in this demo
a bunch of different waysto get access to our
data in the CAS server.So we can load it from the
SPRE machine up into memoryusing a SAS library.We can load it from the
CAS machine up into memoryusing a CAS library.Or we could load data from the
operating system up into memoryskipping the
libraries altogether.So a bunch of different options.The best way I sort of think
about it is, generally,whenever I want to
use a new option,I generally have
my favorite way Iload data, which often is
putting it in the CAS libraryand then loading it into memory.And if I want to use any
of the other methods,I might just look up a
table in the documentationto list all the different ways
that I can get data into CAS.And in fact, since
I've taken this course,instead of using
the documentation,I might just use
the course notes.In this section, I'm going
to talk more about caslibs.There's more to the
story on caslibs.Additional caslibs can be added
during a session by the user.To do this, you do need to be a
user who's authorized to do so.This is done with
a caslib statement.You choose a name for your
caslib, here it's my CAS,and then you specify
the physical locationfor the disk copy of the
CAS tables in the caslib.Here, we specify a
file system location./workship/casserveloc.This location, in this
case, is a directoryon the CAS server machine.When you define caslibs
during a session,they're typically defined
only for the durationof that session.So you therefore need to
redefine them in each sessionthat you want to use them.In contrast, global caslibs are
predefined and are availableevery time you
start up a session.So the caslibs we saw
earlier, includingCAS user and the public
caslib, are predefined.Permissions are used to
control who has access to eachof these global caslibs.In CAS, we also have two
types of in-memory tables.Un-promoted tables,
when loaded into memory,are only available in
memory in the sessionof the user that loads them.When that session ends, the
tables are dropped from memory.Promoted tables when
loaded in memoryremain in memory even
after the session ends.That means that any
user will have accessto that in memory table
in subsequent sessions,assuming they have the
proper permissions.You can only promote tables
that are in global caslibs.Because of course, if the
session caslibs exist and theyhave tables in
them, those caslibswill disappear when
the session ends,and the promotion
wouldn't mean anything.In this diagram, a
SAS Studio sessionhas already been
started on SPRE,and a CAS session has been
started on the CAS server.A table has been
loaded into memorybut the table is not promoted.So this table loaded into
memory are these purple dots,and it's not been promoted.So it's depicted as being
in the session only.Then, the CAS session is
ended with a my session--a CAS my session
terminate statement.One way to do that is to
submit the session statement.The session goes away,
and the in-memory tableis also removed from memory.So the data is essentially
deleted unless it'sbeen safe to the hard drive.The table could still be
loaded again in another sessionif there's a stored
disk copy to reload.And this would be that SAS
HDAT file we saw earlier.But an in-memory
table can also bepromoted so that
it's not eliminatedwhen the session is eliminated.So if we're in a-- if
the in-memory tablesin this global caslib,
we can promote it.Here, the promoted table is
being as depicted in-memoryon the worker notes.It's not simply on
the session nodes,but it's actually here
on the worker nodesindicating that although it's
available to the session,it's promoted.So then, when we
cancel the sessionor close the session by
using a terminate statement,the in-memory table
exists on the worker nodesnot just in the session.So it persists in memory.This table is
retained in memory,even though the
session is removed.Then, when a user starts
a new CAS session,since the table is
still in-memory,they can still access
this global tablefrom a global caslib.In fact, each
session that startswill have access to
this global table,assuming that the
session owners, whoeverstarts the session,
has permissionto look at this table.So global caslibs are
available across allthe different sessions.However, only promoted
tables remain in memoryafter the sessions
are completed.Of course, you
can't promote tablesthat aren't in global caslibs.In this demonstration,
we'll lookat caslibs, the difference
between session scopeand global scope
caslibs as well asthe difference between promoted
and non-promoted tablesin caslibs.So first, we'll just
create a new caslibnamed my CAS that points to this
location, workshop/CASServLoc.And we'll list all
caslibs in the concession.So we see we just added
the caslib my CAS,and it's now the active caslib.And if we look at
it, we see that itis a local caslib, which
means that its session scope.So if we look at some of the
other caslibs like public,it's not a local caslib.Or even CAS user,
which although itis a personal caslib
for our account Lynn,it's not a local caslib, which
means it's not session scope.So it'll persist
beyond this session.Currently, the my CAS
caslib would be removedif the session were to end.Let's go back to the code.Let's look at some files in
our caslib, files and tables.There are currently
no in-memory tables.So we could go to
the log and see.No tables available in caslib my
CAS of Cloud Analytic Services.However, there are a few
files in the operating systemlocation.It looks like we have
the products Excel file,and the profit SAS data set.So let's go ahead and load
some of these into memory.So we'll run some code.We'll run some PROC CASUTIL code
to load some data into memory.In this case, we'll load profit
into memory in the my CASlibrary.And we'll load products into
memory in the my CAS libraryas well.However, we will promote
the products file.So we'll see if that's
a possibility here.Go ahead and run.Spoiler alert, it probably
won't work because it'sa session scope caslib.So execute this.It tells me the error
stopped-- the action don'tstop due to errors.Error.You cannot promote an output
table to a session localcaslib.So what's happened here
is our caslib, my CAS,in session scope, which means
when we end our session,it gets removed.But we tried to
promote a CAS library--excuse me, an in-memory
table in this library saying,when the session ends, please
keep this table in memory.But the entire CAS
library is goingto be removed from
memory, so we can'tpromote a table in this caslib.We'll go back to the
code and see waysthat we can promote tables.So in this case, we'll
load data from a SASdata set into our
CAS user CAS libraryand we will promote that data.We'll load a SAS data
set from our public CASlibrary, which is already living
in the CAS library folder.We'll load that one into
memory, but we won't promote it.And we'll also load some
data into the public libraryfrom a path.So we're loading data
in three different ways.This is loading data
from a SAS data setusing the load data statement.This is loading data from a
caslib server location usingthe load CAS data statement.And the final one is
just loading a filefrom the CAS server operating
system using the load filestatement.Incidentally, the final
table we do want to promote.So we will see how that works.We'll run this code.We see that the SAS help
table was successfullyadded to the CAS user
and the CAS library.We see that the goalies data
set was added to the CAS librarypublic.And we see that the table orgdim
was added to the CAS librarypublic as well.We'll have to explore to
see the promoted tables.So let's display the
tables in memory,now that we've
loaded some tables.First, we'll look in
the public CAS library.We have two tables,
goalies and orgdim.If we scroll over to the right--we don't even have
to scroll that far.But we see that goalies
is not a promoted table,but orgdim is a promoted table.And just looking back
at the code real quick,here we loaded goalies--the goalies table into
the public CAS library,and we chose not to include
the promote statement.But here, the
orgdim file, we didinclude this promote
statement right here.So we're loading
this data into memoryand then promoting
it so it persistsoutside of this session.Just looking back
at the results,we also have the same results
for the CAS user CAS library.It's got a little bit more data
because we've been consistentlyloading data into it.But we see none of the
tables are promotedexcept for this one table, 1986
baseball data, the baseballtable.So if I go back
to the code, whenwe loaded the data into
the CAS user caslibfrom sashelp.baseball,
we includedthat promote statement.Since CAS user caslib
and the public caslibare both global CAS libraries,
they're not session scope.They'll persist after our
CAS session has ended.We are allowed to
promote tables into them.So if we look here, we
now have two tables thathave promoted-- been promoted.The baseball table
in the CAS userCAS library, and the orgdim
table in the public CASlibrary.All of the other loads that
we've done were not promoted.So their only
session scope, whichmeans when we end
our session, they'lldisappear out of memory.We'll go ahead and
experiment with thisby ending our session,
starting a new session,and then executing the list
tables in the PROC CASUTIL listtables functionality to
see which tables are stillin memory.So I'll run this next fragment.We see that our session
successfully terminated.And I just look at
the log deletionof the session was successful.If we look at the results
in our public CAS library,the only table in memory
is the orgdim table,the one that we promoted.We scroll down to the
CAS user CAS library,it's even more dramatic.All of our data tables,
except for the baseball table,were removed from memory.Now, in theory, there
could be a concernthat these tables
are deleted and lost.But of course, we loaded them
from the hard drive anyway.So those tables are still
present on the hard drive.They're just not available
in memory for doingprocedures and processing.We talked about loading
tables into memory.But if you create
tables in memory,how do you save them so you
have a permanent disk copy?Saving data is
important if you needit to be available over time.In-memory data will disappear if
the server fails unexpectedly.You may not be able to hold all
tables in memory at one time.Or you may want to export the
data to another application.One way is the save
statement in PROC CASUTIL.When saving to files
on the CAS server,you can use CAS data equals
and name a file system file.This slide lists the file
format supported for that.sas7bdat, CSV, Excel files,
JMP files, SPSS files.But keep in mind that you can
also save the database tables.That's not covered here.This is an example of
saving to an Excel workbook.This, like most formats, will
be stored in the file systemon the CAS server machine.For SAS HDAT files,
you can save thoseto a shared network location
accessible to each CAS worker.This save can be done in
parallel from each CAS worker.So we see in this slide, the
little in-memory data tableliving on the
session workers getspushed into a hard drive space
associated with that sessionworker.Because it's in parallel,
it can save fasterand load up into memory faster.Here is an example of a
valid saved CAS data equalsfor distributed SAS HDAT files.And we'll see some
more examples of thisin the following demonstration.In this demonstration, we'll
create an in-memory tableusing a CAS enabled
procedure, and then we'llsave that in-memory table on the
hard drive of the CAS server.We'll start by just connecting
to the CAS user caslib,and then submit
a PROC MD summaryprocedure to create
an output CAStable, casuser.statsbyposition,
which is basically summariesof the casuser.baseball table.After we do that, we'll use
the PROC CASUTIL statementto save this CAS table,
stats by position,in the caslib CAS user.And we'll use the
file type auto,which means we'll let it choose
the best type to save the file.Since it is a distributed
in memory CAS table,the file type when you use auto
will be the SAS HDAT file type.And we'll replace it in case
there is any files alreadyon the operating system.We'll then list the files
in the CAS user caslibto see what sort
of files we have.I'd like to point
out here, we didn'tspecify that we want to
save this stats by positionin the CAS user caslib.But since that's
the caslib wherethe in-memory table
is living, that'swhere it defaults to saving.If we'd want it to save it on
the hard drive in a positionor in a location corresponding
to a different CAS library,we could have specified that
with the out caslib statement.We'll go ahead
and run this code.Here we see the
CASUTIL procedure.We asked for it to list files
from the CASUTIL procedure.And importantly, the one
that we wanted to highlightwas statsbyposition.sashdat.And we can see it was
last modified-- well,this is the day
I'm recording it.But for me, this
is the newest day.So it's last modified
most recently.So this is the table
that we just saved.And we chose no
encryption methodbecause we didn't
specify anything.We could go to the log
real quick just to see.We connect to the CAS library.And then we save this file.Cloud Analytic
Services saved the filestatsbyposition.sashdat
in caslib CAS user.And this would be a distributed
file on the CAS server.So it would be saved across
the different worker nodes.So that when we load,
we can load in parallel.Finally, a few words
about where SAS programsexecute in SAS Viya.There are in fact to places
where your SAS code nowexecutes.Things either execute
in the CAS session,or they execute in
the SPRE sessionthat is the SAS programming
runtime environment.This matters because it
can affect performance.It can affect where data has to
be moved in order to process.And only CAS processes
execute in parallelin a multi-machine environment.Some general statements
are the following.DATA step code, PROC FEDSQL
code, and some SAS Foundationprocedures might be
able to execute in CASif CAS tables are used.But certain language
elements, if used,result in downloading the CAS
table to SPRE for processing.A simple example is if
we use DATA step codewith the lag function, which
looks one row behind, thatwill guarantee that the table is
moved to the SPRE environment.Because that lag
function doesn't reallywork in a distributed
environment.Any procedures executing in
SPRE using CAS tables as inputcan pass WHERE clauses to CAS.So what that means is
that the CAS environmentdoes the WHERE clause
and subsets the data,and then bring some subset
to SPRE for analysis.Finally, there are
new SAS proceduresthat only execute in CAS.These are procedures
essentially designedfor CAS-- for the CAS server.We'll start with a DATA step.One requirement for DATA
step execution in CASis that both the input data
set and the output data setmust be CAS in-memory tables.But certain DATA step
language elements,if present in your code,
will prevent the DATA stepfrom executing in CAS.For example, that lag
function I mentioned.The DATA step always executes
in a single thread in SPRE.In CAS, the DATA step executes
in threads in parallelacross the CAS worker nodes.Certain DATA step
language elementsprevent CAS
processing, and causethe CAS table to be
returned to the SPRE sessionfor processing.Documentation for
using DATA step in CASis available in great detail.There's a whole section on
DATA step programming for CAS.DS2 or DATA step 2 code can
execute in CAS as long as youuse the sess ref equals,
or session reference equalsoption, and name
your CAS session,and you use CAS tables for
both the input and the output.A DS2 data program only executes
on a single worker node.However, DS2
threads will executeacross the worker nodes.So just like in
Base SAS, you needto execute DS2 threads
in order to accomplishparallel processing.Essentially, you specify
the distributed executionacross different
threads in your code.As far as executing
SQL code in CAS,you need to use PROC FEDSQL,
with a sess ref equals option,and if you select from accounts
table, it will execute in CAS.PROC SQL on its own
does not execute in CAS,and will only execute in SPRE.Thus, if you want the
distributed processing,you'll want to use
the FEDSQL procedure.When using a non-CAS enabled
procedure, in other words,a procedure that only executes
in the SPRE environment,if you use a CAS table as input
and use a WHERE statement,the WHERE subsetting
will be executed in CAS,and only the subset
will be returnedto SPRE for further processing.If you set the option
message level equals I,you will see information
in the log verifying this.In the documentation, you
can see a list of Base SASprocedures that are CAS enabled,
and will execute in CAS if youuse a CAS table as input.I have listed them here
and on the next slide.So we've got PROC APPEND,
PROC CONTENTS, PROC COPY,PROC DATASETS, PROC DELETE,
PROC DS2, PROC FEDSQL.So a lot of data
manipulation procedures.PROC FORMAT, PROC MEANS,
PROC REPORT, PROC SUMMARY,PROC TABULATE, and
PROC TRANSPOSE.There's also a
collection of CAS actionsthat we can execute
using PROC CAS,but that's not going to be
the focus of this course.However, I like to
mention that thereare more options available in
the CAS server using the CAS Lprogramming language.And we have another
education tutorialby my co-worker Aurora
on that very subject.For SAS fire.There are a very large number
of procedures that are designedspecifically to execute in CAS.Depending on the SAS
products you licensed,you will be able to use these.And you can see them
all fully documentedin specific sections for
each type of SAS license.Here, I'm showing a list
of these document sections.So there are many procedures
under each of these categories.For example, under
SAS Visual Data Miningand Machine Learning
Procedures, there'sall kinds of predictive modeling
procedures, like decisiontrees, gradient boosting,
and neural networks.There's even deep
learning models.To finish, I'll just leave
this recommendation slide up.There's just some
basic considerationswith respect to performance
tips and considerationswhen developing code in
the SAS Viya environment.We want to avoid processing
large CAS tables in SPRE.When we have big
distributed tables,we want to leave them
on the CAS serverso that we don't have to
bring them across the network,and so we can process them
in parallel rather thanon the single SPRE machine.In general, we want to use CAS
to generate summary resultsso then we can return those
summary results to SPREand potentially analyze
or go into more detailreporting or creating
dashboards with those results.We also potentially
want to use optionsmessage level equals I in
order to better understandwhether processing is
occurring on the CAS server,or if we're getting our data
kicked back over to SPRE.I think of this as sort
of a learning option,but really an
option that we wantto use for learning and
better understanding.And whenever we
use SQL queries, wewant to use FEDSQL
instead of PROC SQLin order to execute
in-memory on CAS tables.In this demonstration,
we'll be exploringwhere our processing
is being executed,whether it's in the
SPRE environmentor in the CAS environment.And we'll check out a few
different procedures, DATAstep, DATA step 2, and SQL.We've just opened our demo2.sas,
which is in Lynn's home folder.And we'll go ahead and just
execute these first few lines.What we're doing here is just
loading some data into memory.We use PROC CASUTIL to load
the sales.xlxs file into memoryin the CAS user caslib.Then, we use PROC MD summary to
create some summary statisticsfrom the sales data table,
the in-memory table.And we output that as
casuser.salarysummaries.So I'll go ahead and run this.We see that Cloud Analytic
Services processed the request.We've loaded that data
up into memory in CAS.And now, we'll execute a
few different data steps.So first, we'll start
with a DATA step thatexecutes-- that starts
with the CAS table,but outputs a table to work.new.So we're loading data
from a CAS library,but we're writing data
using this data setto a SPRE library.So I'll execute this.We notice on the log it doesn't
say that it was executedon the Cloud Analytic Services.So this table was actually
output in the SPRE environment.And if we go over
to our libraries,we can open up the
work library and seethis table new is really a SPRE
table on the SPRE environment.We'll now do the same thing.But instead of outputting our
table in the work library,we'll put it in the
CAS user caslib.Now we can see, we're
running the DATA stepin Cloud Analytic Services.It's this note we
get in the log.And now, we've output our data--and of course, it's in
the CAS user caslib,we have this data table
new in the CAS user caslib.We're going to change the
options for the message levelsjust so we can see a
little bit more results.We'll try and run this
first DATA step 2 code.We're outputting
data in CAS user,and we're reading
data from CAS user.So it feels like it
should work, but we'regoing to end up
getting an error.The PROC DS2 expects a
sess ref equals optionto specify the CAS session.So we automatically-- or
we started a CAS sessionat the very beginning of
this code named My Session.And we need to
feed this session.I actually submitted
in the other code filethat we ran earlier when
we started our CAS session.And we need to feed
this session to PROC DS2so that it knows
which CAS sessionit's trying to execute in.So when we add the
sess ref equals option,we have all the other same code.Everything's the same.So our data is being
loaded from a CAS libraryinto a CAS library.So go ahead and execute
this new DS2 code.And when we've used the
sess ref equals option,we no longer get an error.So it executed successfully.And we use PROC DS2.It doesn't give
us any informationabout the CAS being
executed on the CAS server,partially because
it's multi-threaded.We'll now try this
next statement,PROC DS2 with a sess
ref equals option.But we're trying to write
our data to a SPRE library.So we're reading
from a CAS library,but we're writing
to a SPRE library.We'll try and run this.We'll see that it does not work.So in this case,
the problem is isthat we're trying
to write our dataall in one thread from a CAS
library into a SPRE library.But these threads are really
on different machines.The CAS machine and
the SPRE machineare potentially going to be
actually different computers,even if it's all sort of folded
into the same environment,they're going to be separate
virtual machines from the pointof view of the software.So in order to get
this to work, in orderto get PROC DS2 to
work successfullywriting data tables
to SPRE, we haveto use two different threads.So we basically create
a PROC DS2 programwhere we read in the
data into one thread,and then we use a
different threadto write the data in
the SPRE environment.And of course, these
two threads aregoing to be different
because theycorrespond to different machines
in the server environment.And I'll go ahead and make sure
I turn on the message leveloptions a little bit so that
I can see that it's actuallybeing executed in CAS.Note, DS2 running on
CAS due to sess ref.So this was the option that
we were missing earliersince we didn't have this
message all set to equal I.So if you want to see these
messages about where processingis occurring, I'll encourage
this options messagelevel equals I.This CAS action
completed successfully.It executed DS2.But it did output the
data into the work libraryat the end of the day rather
than into the CAS library.We'll now explore some SQL code.So we'll just run PROC SQL.We're using PROC SQL
from accounts table,but by default, it's going to
execute in the SPRE environmentwith one exception.So here, we get
these results backand they're printed here,
which sort of clearlytells us that they're
printing at least in the SPREenvironment.But we know PROC SQL is going to
execute in the SPRE environmentbecause it's not a
CAS enabled procedure.So let's look at the log.We turn this message level on
so we could see this message.Note, the WHERE clause is being
processed by Cloud AnalyticServices.So even though PROC SQL
is executing in the SPREenvironment, this subsetting
WHERE that we're selectingis actually executing in CAS.So the only data we're passing
back to the SPRE environmentis the subset data collected
by the WHERE clause.We'll now look at
some PROC FEDSQL code.In this case, we are
reading from a CAS library.So we feel like it should
work, but we'll double check.And we're going to turn
the message all downjust a little bit here.Message level equals n.We get an error here
because PROC FEDSQL expectsa sess ref equals option.So even though we're
using a CAS table,FEDSQL doesn't really know--
the procedure doesn't reallyknow where to find that CAS
table because it doesn'thave a CAS session specified.Notice I like to point
this out, and thisis one of the
reasons we're goingthrough this demonstration,
it doesn't tell usthe exact error, which is you
need to state a session ref.It tells us it can't
find table casuser.salesbecause this FEDSQL
procedure doesn'tknow about our CAS session.So we will now give it
information about our CASsession by specifying
a session ref equalsmy session, that's the
concession we started earlier.When I run this code,
I get note, FEDSQLrunning on the CAS server.So we get these slight
summary results passed backto basically the
local SPRE environmentjust to print them
out in SAS Studio.But the CAS environment
does all the processingto create this table.And if we go to the log,
we do see that FEDSQLis running on the CAS server.And we get four rows returned
to SPRE, which are basicallythese summary results
that we're analyzing--that we're looking
at graphicallyin the local environment.Now, we'll look at some
CAS enabled procedures,or we'll look at the difference
between CAS enabled proceduresand the non-CAS
enabled procedures.PROC MEANS is a CAS
enabled procedure.So our output is a CAS table,
and our input is a CAS table.And we'll run this.We've got this message
level equals I still,so we're going to
see some messages.We see it-- so we get our
results for PROC MEANS.But if we look at the
log, we see this note,note the CAS
aggregation.aggregateaction will be used to perform
the initial summarization.So in fact, it is executing this
procedure on the CAS server.But the CAS server seems to have
a different name for generatingMEANS.It's this
aggregation.aggragate action.For more information
on CAS actions,I'll refer you to my
co-worker Aurora's courseon the CAS L
programming language.It's another one of these
education tutorials associatedwith SAS global forum 2020.And we can see this data
set, casuser.salarysummaryhas been written out into
our CAS library still.Next, will run PROC SG plot.We'll execute this
on a CAS table,but it's not a CAS
enabled procedure.It just does local plotting.So I'll run this code, We
see the WHERE clause is beingprocessed by Cloud
Analytic Services,but nothing else happens in CAS.So it basically just
pulls the data from CASbased on the WHERE
subset, and thenplots these results locally.So we see here's our local plot.And the only thing
that executes in CASis just this WHERE subset.Before we finish
this chapter, I justwant to point out a
few more resourcesfor documentation on
programming in CASand using DATA step in CAS.We'll go to
support.sas.com/documentation.So sort of our home dot page.We'll navigate to
programming SAS Viya--SAS 9 4 in Viya.We'll be focusing on
SAS Viya, of course.On this bar on the left, we'll
open up DATA step programming.And we'll go to DATA step
programming for CAS, whichhas some specific information
that is useful for executingthe DATA step in CAS.We'll open up DATA step basics.It gives you some
tips and tricksfor running DATA step in CAS.Some of the information that
we've presented in this course.But it goes into
some more detailabout what is supported
in the DATA stepin the CAS environment.So if you're a DATA
step programmer,if you use SAS 9 a lot and
you've programmed a lot in DATAstep and you're
transitioning to CAS,I would think this
documentation page willbe very useful at learning what
can transition over into CASand what you're going to
need to change to make surethat your DATA step executes,
essentially, more efficiently.That it takes advantage of
the distributed computingprocesses we have.I'll also scroll up under
SAS Viya programmingand look at introduction
to SAS Viya programming.This just includes a bunch
of different informationabout procedures
and SAS programmingthat will execute in SAS
Viya, execute on CAS.In the next demonstration,
in the next sequenceof demonstrations
in chapter 2, we'llbe looking at some SAS visual
data mining and machinelearning procedures.But whatever you need to
do in SAS Viya, whateversort of programming task
you need to accomplish,you can check out the
documentation for ithere on this page.And his focus is on what's
happening in the CAS serverand how to program for CAS.In this lesson,
we'll explore someof the interfaces you can use
to control SAS Viya and the CASserver, Cloud Analytic Services.In this case, we'll be doing
a bunch of different examplesbuilding decision
trees-- well, really,collections of decision
trees, random forest models,using a bunch of
different interfaces.We'll start by
briefly over viewinga variety of SAS Viya
application interfacesthat you can use.Then, I'll demonstrate how
to accomplish decision treemodeling, while
build random forests,using a number of
these interfaces.SAS Viya contains a
server process called CAS,or Cloud Analytic Services.CAS is a distributed
multi-machine systemfor parallel processing
of distributed data.This architecture allows you
to perform complex computationson large volumes
of data quickly.But to access and direct
processing on the CAS server,you need to use some kind of
client application interface.Both programming and point and
click Application interfacesare available.One of the point and click
applications I'll demonstrateis the web application
interface of Model Studio.This will allow you to build
predictive modeling pipelinesquickly and easily.I'll be showing you two other
point and click interfacesas well.Here's a snippet of code
from a Python programI'll demonstrate.SAS provides a downloadable
set of Python methods for CASthat you can use as
part of your Python codeto connect to SAS Viya, submit
processes that execute in Viya,and view and download results
for further processingwithin your Python
client application.This uses this
Python package calledSAS SWAT, the SAS scripting
wrapper for analytics transfer.We'll talk about that in more
detail in the demonstration.Programming interfaces
for SAS Viyainclude SAS procedures
and the DATA step code,Python, R, Lua, Java,
and a REST interfacethat uses an HTTP protocol.No matter which client
interface you use,including the point and click
ones, the request you specifyare converted into
a new SAS languagecalled the CASL language,
which includes CAS actions.You can also use CASL
directly, and that'sthe syntax shown in the
bottom right on this slide.CASL has an object
oriented syntaxsimilar to Python R and
other such languages.What I like to emphasize
here is that, regardlessof how you submit your
programming, whether you usePython R, SAS procedures, or
directly use the CASL language,it all gets translated
into the same CAS actions.So the functionality you get
in Python and the functionalityyou get in the CASL language
are basically identical.Now, I want to describe decision
trees for a couple of minutesand then demonstrate
building some decisiontree models using a number of
different SAS Viya interfaces.Since I'm using
it as my example,I'll first give you a whirlwind
overview of what decision treemodeling is.If you're new to
decision trees, you'relikely not to be able to take
in all of the details here.But please don't concern
yourself with that.Just keep in mind that
the main objectiveis to give you a chance to see
the wide choice of applicationinterfaces in SAS Viya.And this is simply
one objective.SAS Viya has broad capabilities.With statistical
modeling in general,the objective is to predict
new cases or outcomesbased on measurable inputs.Statistical models are
created using past datawith known outcomes.But if you create
a complex modelto maximally fit the
past data, it usuallydoes not predict new cases
as well as a simpler model.Because the extra
complexity is explainingrandom effects in
the original data.So the model creation
process usuallyincludes techniques to
remove some of the complexityso that the model generalizes
better to new data.First, let's look
at how decision treemodels, once they've
been created,are used to predict cases.Consider a data set
with two input variablesand a binary
outcome that we wishto predict based on the values
of those input variables, x1and x2.The binary target
outcome is representedby two colors, yellow and blue.For a more concrete
example, considereach dot is a bank loan
to a customer and yellowindicates that they
defaulted on the loanand blue indicates that they
didn't default on the loan.x1 might be the debt to
income ratio of the person whotook the loan, and x2 might
represent annual income.And we want to predict
how debt to income ratioand annual income of a
customer affect whether or notthey default on a loan.In real life
decision tree models,you'll typically have a large
number of predictor variables.Two is enough to illustrate
how it works here, and makes itso that we can plot it in two
dimensions a little bit easier.To predict an outcome,
decision tree modelscreate rules that are applied
in a step wise order thatcan be represented like
an upside down treeas shown here on the left.The rules create separate
areas in the plotthat maximally concentrate
yellow outcomes in some areasand blue outcomes
in other areas.Here, the first
rule splits our plotinto two areas
along the x2 axis.One area above x2 equal
0.63, and one areabelow x2 equal 0.63.A second rule is then applied
to each of those two areasthat splits those
areas further basedon the values of
the other input, x1.This leaves us with four
different areas in the grid,each with a different
proportion of each outcome.The proportions of each
outcome in each areabecome our prediction of the
likelihood of each outcomebased on the values
of x1 and x2.For example, for
a new observationwith values of x1 and x2
at the point indicatedby the red circle in the
plot, the decision tree modelpredicts that the color
has a 70% chance of beingyellow because 70% percent of
the data falling into that boxwhen we built the
model were yellow,or people who defaulted
on their loans.So how are these
prediction rulescreated for decision
tree models?To select useful incomes--to select useful
inputs and to come upwith the best
hierarchical set of rules,trees use a split
search algorithm.We'll now go into a
little bit of detailon the split search algorithm.Here, we assume a binary
target, but the methodsare available for other
kinds of predictions.For example, a continuous target
where we're just predictingsome kind of number as well.The split search
starts by selectingone of the input variables.So here we begin with x1.We look at the number of
yellow and blue to the leftand to the right of each
possible split point for x1.We find the split point for
x1 that maximally separatesthe two types of outcomes.Here, the value
x1 equals 0.52 isthe value that maximally
separates the blueand the yellow outcomes.Then, we repeat the
process independentlyfor x2 and all other inputs,
finding for each inputthe best split point
that maximally separatesthe blue and yellow outcomes.Here is the same white
line that shows usthe best split point for x2.And we see that it looks
like the best splitpoint is right around 0.63.So have 0.63 is the
best split point for x2.And we compare the best
split point for x2,and the best bullet
point for x1.After you determine the best
split for every single input,the algorithm compares those
best splits across all inputsto find the overall best split.Here, we determined
that the x2 splitterwas better at
separating the outcomesthan was the x1 splitter.This creates our
first decision rule.We split the points
according to the rulex2 is greater than 0.63,
or x2 is less than 0.63.So now, we repeat the process
for each subset on each sideof the initial split.Determine the split point
for x1 that maximallyseparates the two outcomes.In this bottom subset, so
values of x2 less than 0.63.Repeat the process for the
x2 input in the subset.And for the subset, compare
the best x1 and the best x2splits to determine
which of the twoachieves the maximal separation.In this case, it
was the split on x1.The data in the subset
area is then split againbased on the second rule.The process repeats in
each leaf until thereare no more splits that meet
a minimal measure of a--of effective separation.The resulting set of rules
is known as the maximal tree.However, it's likely
that the maximal treefails to generalize well on an
independent set of validationdata.Basically, the maximal tree
does really well at fitting datathat it already
knows the answer to.So these yellow and
blue points that we'retrying to build the model on.But when we go and
try and score datathat this model has
never seen and that itdoesn't know the
answer to, it doesn'tgenerate very good results.So we've selected useful
inputs and split rules,but now we need to do
some optimizing complexityto ensure we build a model
that works well on new data.When the split search
algorithm produces a modelbased on the training
data, the resultmay have many branches
that representthe most complicated
model possible basedon the chosen
algorithm parameters.Usually, this model
is too specificdue to random effects
of the training data,and does not generalize
well on new observations.To avoid potential over-fitting,
many predictive modelingprocedures offer some mechanism
for adjusting model complexityto remove random effects
of the training data.For decision trees, this
process is known as pruning.So the decision tree is created
by applying the split searchalgorithm to the training data.But we hold back a separate data
set, a separate subset of datawith known outcomes so we
know the values of the targetto test the performance
of the decision tree modelon an independent set of data.We call this the
validation data because weuse it to validate our model to
make sure that it works well.We create the most complex
model on the training data,but then we prune it
back in several stagesto create simpler models
with fewer splits.Then, we compare how well
each level of model complexityfits the validation
data that we held back.So these stars
indicate performanceon the validation data.The simplest model with the
best fit to validation datais selected.So we choose this model
here with three starsbecause it gives the best
performance on validation data.We choose it over the
more complex modelbecause we have a preference
for simple models.Basically, Occam's
Razor, where wewant to make the fewest
assumptions possible.For training data,
the model improveswith increasing complexity.However, it's typical
that for validation data,the model fit is
better initiallyas complexity increases, but at
some point starts to level offand decreased.This is because the
more complex modelsbegin to explain patterns that
are unique to the training datasubset and do not generalize
to the population of the samplethat they're meant to represent.This is an example of
over-fitting in the machinelearning terminology.An alternative approach
to creating a single treeand then pruning it is
to use ensemble methods.Ensemble methods
create a new modelby combining the predictions
from multiple models.The commonly observed
advantage of ensemble modelsis that the combined
model is betterthan the individual
models that compose it.Ensemble models
for decision treesare referred to as random
forests, or one exampleof ensemble models,
the naive exampleof ensemble models for decision
trees or random forests.A random forest model is a
collection of individual trees.Each tree in the collection is
created at the training stagefrom an independent
random sampleof a subset of
observations and variablesfrom the training data.To validate the forest
model, each treeis then applied to
predict outcomesfor each observation
in the validation data.The final predicted outcome is
based on the average predictionacross all trees.This is also how
the forest modelis used when it's deployed.Individual decision
trees in the ensembledo not need to be pruned
because the ensemble model tendsto be more generalizable because
it aggregates informationfrom a diverse set of
trees, and therefore,gives better predictions
than any specific tree.So now, we'll do
some demonstrationson how to build ensemble
forest models usingeach of these application
interfaces listed here.These are basically
the different toolsthat we'll be using to
build our predictive models.We can use point and
click tasks in SAS Studio,that's a graphical interface.We can use CASL
programming language,that's of course a
programming language.Python in SAS SWAT, another
programming language.We could use SAS Procedures,
which is sort of the old schoolSAS programming language.We could use SAS
Visual Statistics,a graphical interface,
or Model Studio,another graphical interface.Here's a description
of the variablesin the data set
that we'll be usingto build the forest model.It's a home equity line of
credit data set named HMEQfor home equity for short.The table contains one row
per loan for each customer.For each loan or
customer, BAD equals 1for loans that are in
default or delinquent,and BAD equals 0 if the
loan is in good status.The target is bad because
it's bad to default on a loan,but it's also bad for
binary applicant default.The value of BAD will be
the binary target variablewe will model with the
forest models using the restof the variables as input.So things like the debt to
income ratio or their homevalue or their years on the job.We'll get started
with the demos.So we'll have six different
demos demonstratingsix different graphical
interfaces for buildingrandom forest models.Our first demo is
building a decision treemodel, really a random forest
model, using SAS Studio Tasks.Will go to SAS Viya SAS Studio.We'll sign in as student.And we won't sign
in as administrator.Before we look at any
tasks and build our models,we have to load up the data.Here in our home folder
under SGF 20 VIY, the coursecode for this workshop, we'll
open up the loadhmeqdata.sas.This is just some code
to create a connection.It basically creates
a CAS session for us,and loads up some data
into our CAS user caslib.So if I run this
code, the outputjust tells me that I've
loaded up some data.And we can see if I
go over to libraries,here we have the
CAS user caslib,and there's this HMEQ data
table with all of the inputswe were expecting.We're now going to
look at some tasks.So I'll click this tasks icon.We see a number of
tasks available.Things like connecting to CAS.We'll be doing SAS
Viya machine learning.So I'll open this up.I'll open up
supervised learning.I was actually under
SAS Viya statistics.I wanted to show
a decision tree.So I'll open up this decision
tree task real quick.When I double click
this decision tree task,it opens up this new
object, decisiontree.ctk.It allows me to specify the
data set that I want to use.I'll use CAS user HMEQ.And I can specify whether I
want to include validationdata, what sort of targets
that I want to include,and then I can go
over to optionsand specify options associated
with the decision tree.We'll go ahead and close
this and load a forest taskthat we've already created
so we don't have to spenda lot of time clicking options.But I wanted to highlight
this graphical interface thatallows us to select options
for our predictive models.I'll go back to this explore.And notice we have
this foresttask.ctkfile in the SGF 20 VIY folder.So I'll double click
this forest task.This will open up a task
where I've already saved it.So these options were options
that I'd already saved on it.We're using the
CAS user data set.We'll be including
validation data.70% training, 30% validation.Our target is binary
applicant default.We'll use the rest
of our things as--all of our continuous
inputs as inputs,all of our continuous
variables as inputs,our two nominal variables
as nominal inputs.Under options, we'll be
building 1,000 trees.A couple of different
forest options.We'll leave these options as the
default. We'll go into output.We'll save the score data as
a CAS table named Score Data,and we'll actually call
it casuser.scoreddata.We'll go ahead and
click overwrite justin case there's already a table.We'll only include variables
used in the analysis.And we won't save the
model or the scoring model.Well, we'll go ahead
and save the model.I'll name it casuser.rf for
random forest underscore model.And include this overwrite data.The thing I wanted
to highlight before Irun this is that this
generates code on the back end.So even though we've
filled out our optionsin a graphical interface,
we specified various optionswe want associated
with the random forest,in fact, what we're
doing is we'reusing a graphical interface
that renders code.So it runs this
PROC FOREST code,partitions the data into
training and validation.Our targets, our
inputs, a coupleof different options
that we have,how we want our scored
output data to look.In addition, it'll plot some
fit statistics as well asvariable importance
using SGPLOT.So we'll run this.And then we'll
run all this code.This is a nice way to
learn the code, too.You can use the
graphical interfaceto select the options
that you want.But then, you can see
what sort of code actionsthat it'll create.What sort of SAS code
that it will generate.Now that it's finished, we
can look at the results.And I usually like to adjust
the way the pane looks.This is it.And under results, we'll
see some model information.1,000 trees.Four variables per
split for each tree.This is information about
the trees in the forest.We had about 4,000 training
data cases and about 1,800validation data cases.We see variable importance.Evidently, the most
important variablefor predicting
whether or not peoplewill default on their loans
is the debt to income ratio.The least important is the
reason that they want the loan.We see fit statistics
as a functionof the number of trees.But instead of reading this
table, scroll all the way down,we can look at the same
information in a plot.I'll actually re-balanced
the size of my objectsa little bit so the
plots more visible.All right, just reorganizing my
SAS Studio layout just a littlebit.We can see the
average squared erroras a function of
the number of trees.It's much lower on
the training data.The one that we're interested
in is the validation data.And we have this
extra sample calledthe out of bag sample, which
there's some details on thisif you want to look this
up about random forest.It's basically another
version of datathat the models
haven't seen before.We also have a plot of that
variable importance tablethat we looked at.These SGPLOT
procedures that we ranwere rendered automatically
by this forest task.Now that we've
built the model, wewant to do some
assessments of the model.So we'll go back to tasks.Real quick, before we go
back to tasks, the modelthat we specified-- or
really, the score datathat we generated, was a
data table casuser.scoredata.I'll be using that
in a little bit.We'll go to the tasks.Under SAS Viya evaluate
and implement models,we'll open up the assess task.This allows us to
assess score data.So our scored CAS table
is no longer casuser.hmeq.It's going to be
casuser.scoreddata.You can see that RF model
table we created in addition.For roles, we do have
a nominal target.And our target is BAD
binary applicant default.So I'll select BAD
for the target.The event level is
going to be one.We want to predict whether
people are going to default.That's a one.The posterior probability
of the target eventis the prediction
generated by our model.We didn't exactly
look at this, but Ihappen to know what it is.PBAD, the probability of BAD
classifying as binary applicantdefault. So I'll
select this one.We'll look at some options.We don't need to specify
any additional roles.I'll just on select lift
chart because we justwant to look at ROC chart.We'll go ahead and leave
the rest of these optionsas their default. I'm not really
worried about saving the outputinformation, although
I can if I want.So I'll go ahead and run this.Real quick before I run it, it
generates the assessment datausing PROC ASSESS, and
it plots the ROC curveusing PROC SGPLOT.And then, it makes sure to
delete the temporary datatables afterwards.We'll run this.Again, let me just re-balance
how much space I have here.We see these are the
assessment tables, I guess,that are generated.We don't plot the lift table.This is a table with information
on depth and cumulative lift.So it's a way to judge
how much better a model isthan a random guessing
model if you're onlyselecting a certain percentage
of the observations.And we have ROC information.When you select different
values for your probability cutoffs, what true positives, false
positives, false negatives,and true negatives do you get?Essentially, how does
your model performat different cutoff values?And as well as that, we
can plot the ROC curve.This is the ROC curve
on the entire data set.So we've looked at
all of the score data.When we built the
model, we split itinto training and
validation data.But this ROC data is
actually the full data set.So we might want to
just restrict thisto the validation data in order
to get an unbiased assessment.This is going to be a little
bit of an optimistic assessmentfor the ROC curve.That's pretty much all
we're interested in here.Some assessment
statistics for our model.We can pull out all sorts of
different assessment statisticsfrom these tables here.In particular, we can
look at the cutoff,and we can select-- we can look
at different misclassificationvalues for different
cutoff values.Evidently, just looking
at this data set,we have the lowest
misclassification rightaround here, which is
about the 0.5 cutoff.So if we say everyone whose
probability is above 0.5will default on their loan and
everyone whose probability isbelow 0.5 will not default on
their loan, that will give usthe best classifications with
about a 7% misclassificationrate.In our next demo, we'll explore
building the same random forestmodel using SAS
Visual Statistics.In this next
demonstration, we'llbuild a random forest model
using SAS Visual Statistics.We'll go up to this top
left show applications menuin SAS Studio.And go to explore and visualize.When I click it, you
can see it takes meto SAS Visual Analytics.Visual Statistics is
just a subcomponentof SAS Visual Analytics.There were some unsaved report.We'll just click
no, we don't wantto load someone else's work.We'll hope that they saved
their work if they cared.So we'll start by
creating a new report.And we'll start by
adding some data.So I'll click data here.This opens this
browse data menu.I could load up the
HMEQ data table,but it's already here in memory
because we've been using itthroughout this course.So I'll click it
and I'll select OK.In fact, we loaded into the
CAS user caslib with that SASStudio code that we submitted
a little bit earlierin the previous demonstration.We start out-- we can
go to objects here.If we scroll all the way
to the bottom in objects,here under SAS Visual
Statistics and SAS Visual DataMining and Machine Learning.Here's our machine
learning algorithms.We'll use the random
forest algorithm.So we'll grab this
forest object,drag and drop it
into the middle.It wants us to assign
some data in roles.So here on the roles
option, we wantto specify a target and
some input variables.Before we do that, let's
just do a little bitof data preparation.So I'll go over here to data.Notice that binary applicant
default is a binary variable.It has zeros and ones.Visual statistics
or Visual Analyticsautomatically identifies
this as numeric.We want this to be
treated as a category.So I'll open up the
properties here.And I'll change this
from measure to category.And you can see now,
BAD appears up hereas a categorical variable.In addition to
doing this, we wantto add a partition variable.So I'll click new
data item, partition.I want to generate a
partition based on sampling.But I don't want to do
a simple random sample.I want to do a
stratified sample.I want to stratify based on
the target binary applicantdefault. I want to
have equal proportionof the target in the training
data as in the validation data.For our training
data, we'll use 70%.And I may as well include
a random number seed.Well, I'll just let--I'll let the software choose
its own random numbers here,actually.I'll go ahead and click OK.Notice I now have a new
variable named partition.And I can now specify roles
for my random forests.Our target is binary
applicant default.Our predictor variables
are all of our infants.So I just click the
first one, hold shift,click the last one, select OK.And for partition, well,
there's only one option.I created the
partition variable.So it knows that this
is our partition.Notice when I add
the partition we nowget training and validation
data instead of just trainingand out of bag sample.I'm going to do a few changes
to some options on the forestjust so we can make sure
it matches the forest webuilt in SAS Studio.I'll specify 1,000 trees.It's retraining the model.But while it's doing that, I'll
also change the maximum levels.This is how deep the decision
tree is allowed to be.So I can hover over here.Specifies the maximum
depth of the tree.So we want a decision tree--
this is saying our decisiontree can't have more than
six different splits in it.Let's allow for slightly
more complex trees.So we'll specify this as 20.So each of our trees in
our random forest modelcan be up to 20 levels deep
with 20 different splits.And we'll leave the
rest of these settingsat their default.Takes a sec to train
these 1,000 trees.And we can see here we really
are training 1,000 trees.Because here on our error
plot, it goes out to 1,000.But we probably could have
only trained a couple hundredand we would've gotten
similar performance.Once again, we identify
that debt to income ratiohas the most important.It's the variable that's
most important in predictingthe target.And here, it's
defaulting to show usthe KS statistic, which is
sort of a separation on the ROCcurve from a random
guessing model to the modelthat we built. We could
change this to validationdata misclassification rate.So our model has about
a 9.5% misclassificationrate on the validation data.We can also change
this confusion matrixto show us ROC information.So we see here's the ROC
curve on the training data,and here's the ROC curve
on the validation data.It's really good to
separate these curves, whichis, of course, why our software
does it automatically for you.Because you really care
more about performanceon the validation data than
you do on the training data.It's good to make sure
you build a good model.But ultimately,
the validation datatells how well your
model's going to workon data it's never seen before.Once we built this model, we
can do a couple of neat things.One that I really like is
if we open this more option,we can duplicate
as another model.And if I hit Alt, it allows
me to duplicate on a new page.So if I click duplicate as
a new page on decision tree,it's going to build me on a
second page a decision treemodel using all of the
same roles and settingsas the random forest model.So now, and of course, I
could right click on thisto switch it to ROC.We see the decision model
looks a little bit weakerthan the random forest model.But one advantage is that we
have this interactive decisiontree to explore and even modify.So I could right click.It won't let me enter
the interactive modewith validation data.So I'd have to change
it a little bit.But I have some options to
explore this decision tree.We see most of our customers
are flowing over herebecause most of our customers
don't default on the loan.So most of the more classifying
as blue, which means they'renot defaulting on the
loan, which is good.Because as a bank, we
don't want to be giving outprimarily loans to people who
are going to default on them.We can save this report
using the Save button.So I'll just save this
as forest decision tree.Since as a forest model
and a decision tree model.I think a visual statistics is
really designed for buildingthings to show to people.So even though we built
these models here,I would generally
use these modelsto show to executive
leadership or a manager whatwork I've done, or
to convince someoneto make some sort
of business change.If I wanted to
actually build modelsthat I'm going to deploy
on some large new data set,I'll end up using
Model Studio, whichis the next graphical interface
that we'll be talking about.In this next demo, we'll
build a random forest modelusing SAS Model Studio,
a graphical pipelinetool for building and deploying
predictive modeling algorithms.We'll go to this
show applicationsmenu in the top left.Instead of explore and visualize
data like we did last time,we'll do build models.This takes us to Model Studio.And we'll go ahead and open
up our HMEQ forest project.We could create a new
project from scratch,but it'll be a little easier
to start with some work we'vealready done.And I'll explore these results
and show you what we've done.When I first opened
the project, Isee what I like to call the
Data tab, which is basicallyjust a bunch of metadata
that Model Studio knowsabout the variables in
the original data set.So here's the variable BAD,
binary applicant default.I've actually already
specified that it's a target,but it's now locked
for me because I'verun this project before.It also identifies that
it's a binary variable.Notice it identifies some
interval variables as well assome nominal variables.Some of these are
numeric, but they onlyhave 11 levels, for example.So the software has decided
that it's really betterto treat this numeric variable
with only 11 different numbersas a categorical variable.And thus, the metadata is
set to have it specifiedas a nominal variable.We'll go over to
the Pipelines tab.We have the option to create
multiple different pipelines.For now, we'll have a
very simple pipeline.We have data feeding into a
random forest model, whichgoes into model comparison.If I click on the left,
we see all kinds of nodes.We have data mining
preprocessing nodes.We can do things like
feature extraction,principal components analysis.We can do things like text
mining, generating singularvalue dimensions
from text columnsfor predictive modeling.And things like variable
selection and imputation,which are what I think of as
core predictive modeling tasks.I always like to highlight
there's this new node featuremachine, new in the software
version, that's very exciting.Because what it
basically does istries to figure out
the best featuresto extract for your
predictive modeling algorithm,and then generates
those features.So it goes through a bunch of
different types of featuresand generates the ones that
end up creating the bestpredictive model.We also have a bunch of
supervised learning algorithms.Here's decision tree.Here's the random forest
node that we're using.We also have gradient
boosting models,which are another ensemble
of decision trees.Simple linear and logistic
regression models,neural network models,
this very exciting node,model composer, which
basically buildsa bunch of different
models and compares themand selects the best one.So I always think the most
naive modeling algorithmwould be feature machine
to let the softwarefigure out the best features
followed by model composerto let the software figure
out the best models.That'll essentially
let the softwaredo its own experimentation to
figure out the best models.We have some post-processing
nodes to ensemble.So we can ensemble
models together.As well as some utility
nodes, miscellaneousexploring data, integrating
open source code,integrating SAS code,
saving and scoring data off.When we click the forest node--I'll go ahead and close
this nodes here on the left.We click the forest node,
we see some options.We see we have some
tree splitting options.So things like maximum
depth equals 20.We're going to
build 1,000 trees.I've set these to make sure
we're using the same settingsas we did when we built
our other forest models.We have a bunch of different
options that we can specify,just like we did in the
other graphical interfacewhen we used the
SAS Studio task,or when we used
Visual Statistics.Often, I find we have more
options here in Model Studiothan we do anywhere else
simply because it-- well,except for in code--simply because it surfaces
as many options as possible.This is the premiere
modeling toolfor building complex
predictive models here in SAS.We also have the
option to performautotuning where it figures
out the best settingsfor the models automatically.And I'm just going to turn on
some model interpretabilityoptions.I'll just turn on
global interpretabilityPD plots has already
been turned on.I'll just look at some model
interpretability results.I'll right click on the
forest, select results.And I would have to
run this to rerun it.But I've already run it.So I won't wait
while it's running.This green check mark indicates
that it's successfullybeen run.And if it hasn't
been run, you can'tcheck the results anyways.So I'll check the results here.We see the error as a
function of the numberof trees in the model.We can make it full
screen to see the legend.Training data, validation
data, out of bag sample.This is the same plot that
we saw in visual statisticsand using the SAS Studio task.We also have the same
variable importance table.It tells us the
debt to income ratiois the most important variable.We also get score code.So we get DATA step score
code as well as DS2 score codefrom the random forest model.These are useful.We can copy these
out for deployment.Tells us the required
inputs for scoringand the outputs
generated by the model.Here's a list of
all the properties.We could have checked
that on the previous page.These are what we specified.Here's the SAS Studio
code that wouldbe used to train the model.I mentioned that SAS
studio tasks was a good wayto learn the code.This isn't really
as good of a wayto learn the code because
you can tell that's clearlycomputer generated code.But it does use the SAS
procedure, PROC FOREST.We can go to the
assessment outputto look at LIFT
plots and ROC plots.This is the plot we're
primarily interested.ROC and training data.ROC for validation data.Evidently, we're building
a pretty good model here.This is basically the same plot
we saw in visual statistics.We also get fit statistics.And the most
interesting one to meis always the
misclassification rate.About 9% rate on
the validation data,which is exactly what we
saw in Visual Statisticsbecause we really are
building the same model.We've got some more options
here in Model Studio than we didin Visual Statistics though.We can go to this model
interpretability reportand see these partial
dependence plots.What this is
showing is basicallythe average target prediction
for different valuesof this input.So we see as debt to
income ratio gets above 40,our probability of
churning is increasing.If we look at credit
line age, as our number--as our age of our
credit lines increases,our probability of churn--
and this is age in days--is decreasing.So people who've
had credit for--excuse me, I said
probability of churn.This is probability
of default. Iwas thinking of a
different data set.So as our debt to
income ratio goes up,our probability of defaulting
on the loan goes up.As the age of our
credit line goes up,our probability of
defaulting goes down.So people who've had
credit lines for a longerperiod of time are
less likely to default.Our new customers are
more likely to default.These partial dependents
plots are really nice wayto get sort of a broad
perspective of whatyour model is thinking
about different data sets.We have some references
and some papersas well as a YouTube video that
goes into more detail on modelinterpretability tools.Those are included as a
link in the course notes.We can close the results
to go back to the pipeline.The model comparison
note allows us to comparemultiple different models.Here, we're just trying
to show the same forestmodel in a bunch of
different interfaces.So we won't build a
complex comparison pipelineto compare a bunch of
different predictive models.But that is the goal of
the Model Studio interface.In our next
demonstration, we'll showusing Python to call SAS
Viya models on the backendand build the same random
forest model, of course.We'll open up the start menu.Open up Anaconda.And open up Jupiter Notebook.This opens up the
Jupiter Notebook browser.And we'll go ahead
and from the homepage open up the
Python Forest Notebook.I'll go through this
Python Forest Notebookand explain what's
happening here.We start by importing
necessary Python packages.So popular Python packages like
PANDAS and Matplotlib as wellas this SWAT
package, which is howSAS is going to be connecting
from the Python interpreterto the SAS Viya CAS server.So we have this SWAT
method here calledCAS to make a connection-- this
connection object to the CASserver.So we're connecting to our
server with alias serveron this port using
username and password.We're not using any
security settings,but we could absolutely
use certificatesand token authentication.Once we've made our
connection to the CAS server,we'll load up some data.We'll use this read CSV
method through this connectionto essentially read this CSV
file into a PANDAS data frame,and then load up that PANDAS
data frame as a in-memory tableon the CAS server.By default, it's going to
load it into caslib CAS user.Since I specified
replace equals true,it gives me this warning
that I'm actuallyreplacing this table.Because I'd already loaded it--I already had the HMEQ data
table in the CAS user caslib.We'll load the
sampling action set.I think of this as like
loading Python modules,but it's for CAS actions
on the CAS server.And we'll see some examples of
using CAS actions in CASL codein the next example.But here, we'll be using
these CAS actions from Python.So we've loaded up our
data as an in-memory table.And now, the first
thing we're going to dois submit the sampling action
or the stratified actionin the sampling action set to
do a training and validationsample.So we have 70% of our
data as training data.And we want to
group by the target.So we have stratified
random sample.And we want to
output the data tableas the existing data table.Just overriding it and appending
the new partition variable.So this basically goes to the
HMEQ data table on the server.So even though this is Python
code, all of the processingis happening on the
CAS server and itadds a partition variable.We can see that here is
basically the partition.Here's our validation partition,
here's our training partition.We're now going to use
the cardinality action setto find our class and
interval variables.Basically, we're preparing our
data for predictive modeling.So we're going to use this
cardinality action setto get basically a
little summary table.And here's our summary table
that we're going to get.We're going to bring that
summary table locally to Pythonso we can extract
which variableswe want to use as
our target and whichvariables we want to
use as our inputs,and then, of course,
what our partition is.Basically, this is
setting metadatalike we did in Model
Studio on the Data tab.But we're just writing all
these Python variables.So we have this variable
named Target thatcontains just the string BAD.We have a variable inputs
that contains all the inputs.Here they are.And a variable nominals that
contain all the nominals,inputs and targets.Basically, most of this
is just Python codeto extract information from
this data card data framethat we brought back
from the server.So we created our
data on the server,and then we're bringing it
locally as a Python data frame.We're only doing this
to this summary data setnot to the original data set.Because we want to take
advantage of the distributedcomputing on the CAS server.I'll load the
decision tree actionset so I can build a
random forest model.And here, I'll use the
forest train actionin the decision tree
action set to builda model on the training data.So parts and equals one
is on the training data.So we have this WHERE clause.Our target is BAD.Our target our inputs
are all of our inputs.We specify the nominals.We'll build 1,000 trees.Each tree, we'll use about
60% of the training datato build each tree in
the random forest model.We'll split based on variance.For predicting our target and
our maximum number of levelswill be 20.So these are all basically
the same settingsthat we specified for all
of our other forest models,the one we built in Model
Studio, the one we builtin Visual Analytics
or Visual Statistics,and the one we built
using the SAS Studio task.We'll output our model
table as this RF_model.And of course, we'll
replace it because Imay have named a table
like that earlierwhen I used the SAS Studio task.When we build this
forest model, itpasses some result in
this forest object.And we can extract
some of these results.One of them is decision tree
variable importance info.So here is our variable
importance tablethat we looked at earlier.And we'll bring this table
locally to Python for plotting.So here is just a bunch of
Python code to plot the forest.Excuse me, to plot the variable
importance for the forestmodel, excuse me.And here, we see our
variable importancetable plotted in Matplotlib.In addition to
building our model,we want to score it on
the validation data.So that's what we're doing here.We're using the
forest score actionto score our random
forest model.Specifying that our model is
stored in this table, RR_model,which was just the output
of the forest modelthat we built earlier.In addition to that, we want
the error from the trees.So we want to output some
information about the areafrom the tree as well as
copy our target variable overto compare our predictions to
the true value of the targeton the validation data.We'll just look at
some of these results.In particular, we'll
bring back the erroras a function of the
number of the trees locallyto Python to generate
the same plot,average squared error
by number of trees.This is only the error
on the validation data.We would have had to do a
little bit of extra programmingto gather the error
on the training dataas well as the error on
the out of bag sample.But we're most interested in
error on the validation data,so we'll just go ahead
and look at this plot.We also want to generate
assessments like lift and ROC.So we'll load the
percentile action setand use the assess action,
similar to the assessed taskin this studio graphical
interface we used.We'll use the score data tables.So if we go up to
score model, we'lloutput the score data as
RF_score, which of course, isjust the predicted variable--
the predicted valuefor each observation
versus the target value--the true value of the target.And we'll apply the
assess action to iton it specifying that
our inputs, whichis the predictions, are PBAD 1.P_binaryapplicantdefault1.And then, the true value
is the target variable,binary applicant default.
And our event level is 1.And we'll output these
our RF assess tables.We create RF_assess, which
is lift information, like wesaw in the SAS studio task.And RF_ROC, which
is ROC information.We'll bring the ROC information
and the lift informationlocally just to look
at these tables.So this is the exact same table
we saw in the SAS Studio task.We have depth.And we can look
at cumulative liftto see how much
better our model isdoing than a random
guessing modelif we select 5% or
10% or 15% of the dataaccording to the
predicted probabilities.Or we have this ROC information
for different valuesof the cutoff.So if our probability cut off
is 0.01, how many true positivesdo we get?How many false positives?How many false negatives?How many true negatives?We'll bring this ROC
data locally to plot it,and we'll plot the ROC curve.Note that this is the ROC
curve on the validation databecause we only did the
assessments on the validationdata.This is all just local Python
code to plot this ROC curve.So we created this ROC
assessment statistic--essentially, assessment
statistic table--on the server using
the assess action.But then, we brought it
locally as a data frameto analyze locally in Python.And then, we're plotting that
data here locally in Python.We can do the same thing
for the lift curve,although we didn't do it
for any of our other models.And that's basically
the same informationthat we generated
from Model Studioor from Visual Statistics
or from the SAS Studio task,but now we've done
this using Python code.Even though we're
using Python codeto submit all of these
actions, we did notbuild this model in Python.This model was still
built on the SAS server--the CAS server, and it
was built distributed.If we have a
distributed CAS server,we're going to build this
model on distributed data.So we are still taking
advantage of the factthat we have this distributed
cluster for processingour data, but we're
able to use Python codeto make our submissions.In the next example, we'll
see doing the exact same thingusing CASL code.So we'll submit these
same actions, thingslike percentile.assess, or
decisiontree.forestscoreor decisiontree.foresttrain,
but we'll actuallyuse SAS code which is the
CASL programming language.In this last demo, we'll
go back to SAS studioand build our same predictive
models using these CAS actionsfrom SAS studio.So go back to Model Studio tab.I'll go to the show
applications menu.And I'll go back to
develop SAS code.This takes me right
back to SAS Studio.We're looking at the
results from the forest taskthat we built a
little bit earlier.In this case, we'll go back to
the Explorer and under SGF 20VIY, we'll open up
forests with CASL.SAS.So even though this
is a SAS program,it does not use SAS procedures.Instead, it's going to use
the CASL programming language.I'm just going to change my
tab layout here real quick.This is my preference
to see all the code.And then, we'll see
the results separately.So we'll start by
creating a CAS session.So we're going to end our
old-- our previous CAS session.And start a new CAS session by
specifying our CAS user caslib.I'll go ahead and run this.We create our new CAS
session and we've connected.And now, we'll talk
through the codeand then I'll just
run it one by one.We're going to import our data.So our hmeq.csv
file, we're goingto import it into work.hmeq.This is basically
taking a CSV file,loading it into a SAS library.This DATA step then loads
the data from a SAS libraryinto a CAS library,
the CAS user caslib.We in fact don't have
to run any of this.We'll go ahead and do
it for good measure.But I just wanted
to point out, we'vealready loaded the HMEQ data
table into the CAS user caslib.The rest of our code, rather
than using various SASprocedures, we'll use
one procedure, PROC CAS.We'll use a bunch of
different action setsjust like we did in Python code.We'll use the table action
set, the sampling action setfor generating our sample, and
the decision tree action setfor building our forest model.We'll use the
sampling action setto do a simple random sample.We probably should have done
a stratified random sample,but we've chose a
simple random sample.Here we could have chosen
a stratified sample.Just maybe forgot to
do so when we werebuilding our predictive model.Now, we'll go ahead and
build our predictive modelin the next action.But before we do that, we
just want to use ODS output.Excuse me, not what
I wanted to click.We want to use ODS output to
select the DTreeVarImpInfo,which is the variable importance
information table that weare interested in.It comes with our model.But by default, that
output is not captured.So we'll use this ODS
statement to capture it.We'll now go ahead and
actually build our model.So we'll use the
forest train actionin the decision tree action
set, just like we saw in Python.We'll specify our table, HMEQ,
where parts and equals 1.So our training data.We specify all of our inputs.We specify our targets.We specify basically
all the same optionsthat we did in Python.And we specify run to
build our forest model.We then want to change
our ODS graphicsso we can look at
our-- so we canplot our variable
importance information.So I'll go ahead
and run everythingup here so we can
look at some results.We see we get the information
about the forest modelthat we trained.We get our variable
importance tables.And we get the SGPLOT
results for plottingour variable importance table.We'll now move on
to scorer model.And I'll just run the rest
of the code all at once.So I'll just talk about
the rest of the codeand then run it together.So we started here.Just click Enter so I can
make sure to find this again.We use the decision tree
action set for a score action.We score our validation data,
and output it as score data--as a CAS table named score data.Actually, we'll
output it as a CAStable named validation
score, and our table namewill be scored data.So it's a little bit confusing.This table score data is
actually the model table.We'll go ahead and plot
the average squared errorby the number of trees,
which is a result weget from the scored action.So when we scored
the validation data,we did see average squared
error by number of trees.But we have to use ODS
output to capture that.Next, we'll use the
percentile.assess action--so just like we did in Python--to compare on the score data.So data that scored on-- the
validation data that's beenscored--to compare our prediction for
the target for the true valueof the target.And we'll plot ROC
and lift information.So I'll go ahead and run
all of this CASL code.And we call this CASL
code because it's allusing PROC CAS.But you can see, it's really
submitting the same CASactions--Cloud Analytic Services
actions as we did in Python.So I'll submit this
while it's running.I'll just talk a
little bit about the--really it's the same actions.We get an error on
work.errormetricinfo,but we're still able
to build our model.And here's our prediction
error for forest as a functionof the number of trees.So rather than looking
at this long table,we'll just look at
the plot that we had.We missed the plot.I think we forgot to actually
call SGPLOT to plot it.So we'll skip that.We get our plot
for the ROC curveas well as our plot
for the lift curve.The exact same plot--
and I'm actuallygoing to just go
back to point out,the lift curve has a
slightly unique shape.It's actually the exact same
lift plot as we saw in Python.It's just plotted using
a different interface.So if we go back to the CASL
code, what I like to emphasizeis that these actions--these CAS actions--
they can be submittedusing a variety of
different interfaces.So we've showed that you can
submit the exact same CASactions using Python code
or using the CASL code.As it turns out, when we built
our model in Model Studioand in Visual Statistics, it
submits the same CAS actionson the back end.So it's organizing
these CAS actionsin a graphical interface for us.But at the end of
the day, these arewhat gets submitted to the
CAS server for processing.In fact, even in
this forest task,where we have this
SAS Studio code--so if I look at this code,
this code is PROC FOREST,so it's a SAS procedure.But in fact, when it
executes on the CAS server,the CAS server ends
up translating itinto these CAS actions.So this SAS code ends up getting
translated on the back endinto something that the
CAS server can understand,and we build the same
models in all cases.That's the end of
our demonstrationsof using a bunch of
different interfaces.I only had five demonstrations
even though I mentionedsix interfaces because
the forest task,in addition to using the
graphical task-based SAS Studiointerface, we've also
rendered SAS procedural codeon the back end.So those are our six interfaces.Thanks so much.So in the previous
demonstrations,we saw these six different ways
to build the same random forestmodel on the CAS server.We used some graphical
interfaces as well assome programming
interfaces, but we ended upbuilding the same random
forest model in all cases.And in fact, we used the
same in-memory CAS library,the CAS user caslib,
throughout all of them.At the beginning of each of
these methods that we used,we loaded up the data
using essentiallywhatever local method
there was to load the data.But in fact, the data stayed
on the server the whole time.So when we load it up
the very first datatable for SAS
studio, the HMEQ datatable that we analyzed
in SAS Studio,we could have just
use that same tableusing all of the
different interfaces.This concludes our what
is SAS Viya course.Thanks so much for attending.And if you have any questions,
feel free to reach out.Thanks."
26,"Hi, I'm Troy Hughes,
developer and SAS author.And today, I'll be sharing
insights and methodsfrom my latest book, SAS
Data-Driven Development,and how these can be applied
to help build LEGO gear trains.I'll show you how to leverage
control tables and parametersto build software that is
more flexible, reusable,and configurable, which
benefits not only us softwaredevelopers, but also those end
users who rely on the softwarethat we write.I'll demonstrate how
data independence,a pillar of
data-driven design, wasessential in allowing me
to create simultaneous SASand Python versions of the
software, both of whichrely on the same underlying
control tables and parameters.And because this topic
can be somewhat abstract,I'll showcase all examples
using LEGO gears and geartrains in action.Have you ever had difficulty
selecting LEGO gears,pairing gears with each other,
or creating a gear train?Maybe you only wanted to
transfer motion from one pointto another within a LEGO model.Or in other cases, you may have
had a specific rotational speedor torque that you were trying
to achieve with a gear train.In this video, I'll be
introducing many LEGO gearsand demonstrating some best
practices for gear selectionto ensure you build the perfect
simple gear train every time.I'll first define and
demonstrate simple, compound,and epicyclic gear trains, of
which only simple gear trainsare the focus of this video.I'll next introduce
several LEGO gearsthat can be used to
construct simple gear trains,including spur gears, crown
gears, double bevel gears,clutch gears, turntables,
and differentials.I'll briefly discuss
gear ratios, includinghow to gear up to increase
speed or gear downto increase torque.I'll show how gear
trains are manuallydesigned, including tedious
calculations for gear ratio,as well as the gap or
overlap between two gears.But at the heart of
this video is softwareI've created that allows
builders to calculatehow any two LEGO gears
in a simple gear trainwill interact with each other,
even at non-orthogonal angles.The output, which lists all
possible simple gear trains,saves builders a tremendous
amount of time and effort,because they can evaluate
gear trains virtuallyrather than having to
waste time testing one gearcombination after another
to find the perfect match.Finally, I'll define and discuss
data-driven software designand demonstrate why it was
essential for this project,as well as how it
promoted softwareflexibility, reusability,
configurability,and interoperability.In a nutshell,
data-driven designfosters higher quality software.And for anyone looking for
additional information,including this SAS
and Python code,a 69-page paper
accompanies this videoand is published on both the
SAS.com and lexjansen.comwebsites.Importantly, this paper includes
the comprehensive listingof LEGO gear trains that is
referenced but not includedin this video.Gear trains transfer motion and
power from one gear to another.In some cases, this transfer
of power is the only objective.You have a gear turning
in one location,and you need that gear to
supply power elsewhere,or possibly to apply that power
in a new direction or evena new plane.Gear trains can
additionally be usedto increase gear speed or
torque, which are inverselyproportional to each other.But first, some basic
gear nomenclatureis required to distinguish
driver, driven, and idle gears.The driver gear transfers
power and is closestto the motor or power source.The driven gear, sometimes
called the follower,receives power from
the driver gear.Here, the large gray
driver gear provides powerto the smaller blue driven gear.Note that the gears rotate
in opposite directions.If one or more additional
gears sits between the driverand driven gears,
these middle gearsare known as idle
gears or idler gears.Here, the idle
gear in the middlemay have been placed
to enable the driverand driven gears to rotate
in the same direction.Among other
classifications, gear trainsare often distinguished as being
simple, complex, or epicyclic.A simple gear train
comprises two or more gearsinteracting with each other in
which each axle has only onegear.The previous examples have
all demonstrated simple geartrains.However, simple gear trains
are not always in lineand can be constructed
at right angleswith bevel gears, double
bevel gears, worm gears,or knob wheels.Here, a 36-tooth double
bevel gear, the driver,turns a 28-tooth double
bevel, the idle gear,which turns a second 28-tooth
double bevel, the driven gear,at a 90 degree angle.This video focuses only on gear
trains in which all gears arein line, that is occurring
in the same planeand having parallel axles.Compound gears have two
or more gears rotatingas a single unit on one axle.Compound gear trains contain
one or more compound gearsand are typically employed
to increase either the speedor torque of the driven
gear, in which casethe size of the gears of the
compound gear will differ.Here, a large LEGO Servo Motor
turns two 12-tooth double bevelgears, which in turn power
to separate compound gears,and finally, to
40-tooth spur gears thatare attached to the lift arm.This arrangement
of compound gearsincreases lift arm torque.However, each of
the outer axles,despite having only
a single gear size,is also a compound gear, because
two gears appear on each axle,thus both driver
and driven gearscan also be compound gears.For example, these
two compound gearseach comprise six 24-tooth
spur gears turning in unison.The intent of the
compound gear is notto modify speed or
torque in this case,but rather to more evenly
distribute torque across moregear teeth simultaneously.This provides greater
strength and stability,but can increase
friction substantially.Epicyclic gears, also
known as planetary gears,are a third
classification of gearsin which the center
of one gear revolvesaround the center
of another gear.Epicyclic gears are not further
discussed in this video.Gear ratio is the
ratio of the numberof teeth on the driven
gear to the number of teethon the driver gear.It can also be
calculated as the ratioof the diameter
of the driven gearto the diameter of
the driver gear.In this simple gear
train, a 40-tooth drivergear powers an
eight-tooth driven gear.So the gear ratio is 8 to 40,
which reduces to one to fiveIn other words, the eight-tooth
gear will make five revolutionswhile the 40-tooth
gear makes only one,so the small gear will be
turning five times fasterthan the larger gear.Anytime a larger gear
powers a smaller gear,the gear ratio will
be less than one,which represents
gearing up, in whichthe speed of the driven
gear is faster thanthe speed of the driver gear.As this speed increases,
the available torqueinversely decreases.If a simple gear train cannot
produce the required speed,a compound gear train
can be utilized.For example, the largest
and smallest LEGO spur gearsare 40 and eight
teeth, as shown here.So the simple gear train can
only increase the driven gearspeed by 500%.The LEGO EV3 Large Servo
Motor has a maximum speedof approximately 210 RPMs.So a simple gear train
with a 1 to 5 gear ratiocan reach a maximum speed
of approximately 1,050 RPMs.So if you're LEGO model
requires 5,000 RPMs,a compound gear train
can achieve this.Here, the first axle turns
at 200 RPMs, the second axle,a compound gear, at 1000
RPMs, and the third axle,a second compound gear, at
the required 5,000 RPMs.This clip shows the progression
from 1,250 to 2,500 to 3,750,to 5,000 RPMs.Now in slow motion, you
can see the driver gearturning at 50 RPMs, the center
compound gear at 250 RPMs.And the driven
gear at 1,250 RPMs.Gearing down occurs when
a smaller gear drivesa larger gear, in which
case the gear ratiowill be greater than one.Gearing down reduce the
speed while increasing torquethrough mechanical advantage.In this simple gear train, the
two gears have been reversed,and the eight-tooth driver gear
now powers the 40-tooth drivengear.This produces a gear
ratio of five to oneand a mechanical
advantage of five.In other words, the driver gear
will turn five times fasterthan the driven gear,
but the driven gearwill have approximately
five times more torqueand the ability to rotate,
lift, or otherwise move greatermass than the driver gear.If sufficient torque cannot be
achieved through a simple geartrain, a compound gear
train can be utilized.This compound gear
train has a gear ratioof 28 to 12, 40 to
five, and 40 to five.So the cumulative gear ratio
is 58 and one third to one.Note that a speed and torque
are inversely proportional.The 40-tooth compound gear
connected to the lift armmoves exceedingly slow
as compared to the drivergear on the servo motor.This is the cost of
torque, and why LEGOmodels that move
heavy loads are oftenshown as time lapse videos,
because they take foreverto complete their tasks.A touch sensor in this
model is activatedwhen the lift arm reaches the
horizontal plane, at whichpoint the servo motor pauses
for a second, after which itreverses direction to
raise the lift arm.However, because
excessive torquecan damage gears
or dislodge models,your design always
should accommodatefor the anticipated
increased torque.For example, I've now
disconnected the touch sensorso the EV3 brick won't know
when the lift arm is horizontal.Rather than pausing
and reversing,the servo motor now
continues, forcing the cranefrom its perch and stripping a
couple eight-tooth spur gearsin the process.Runaway torques
such as this shouldbe avoided, because it can
damage LEGO gears and worse,LEGO servo motors.No gear train, whether simple
or compound, is ever 100%efficient.So some torque and power
will always be lost.But where gears
mesh well, in whichthe gap or overlap between
two gears is minimized,gear train efficiency
will be maximized.For this reason, goodness
of fit between gearsshould always be
evaluated and canbe calculated for all gears
in a simple gear train.Only simple gears are discussed
in the remainder of this video.Designing a simple
gear train beginswith a selection of gears that
will mesh well with each other.Of course, simple gear
trains can containmore than just two gears.However, each
additional gear stillrepresents only a
one-to-one pairingbetween two neighboring gears.So for this reason,
the gear trainsexamined will include
only two gears.This video highlights only LEGO
spur and double bevel gears.However, a comprehensive list of
all LEGO gears past and presentis provided in the
referenced white paper--data-driven robotics.It includes photographs and
metadata for 41 LEGO gears,including gear versions
where LEGO geardesign has evolved over the
years to strengthen components.It also includes links to
each gear on the bricklink.comwebsite, without
whose exhaustive LEGOdatabase these gear metadata
could not have been compiled.LEGO spur gears are
the O.G. LEGO gears,dating back to the late 1970s.Both the eight-tooth
and 24-tooth spur gearshave been improved
over the yearswith the current version of
each pictured to the right.Note that each 24-tooth gear
covers three holes or studs,and each eight-tooth gear
covers one hole or stud.Each LEGO stud represents a
distance of eight millimeters.So a LEGO gear's
diameter in millimeterswill always equal
the number of teeth.For all LEGOs spur gears,
the number of teethis divisible by eight.So any spur gear can mesh with
itself at orthogonal angles.Here, two 24-tooth spur
gears mesh horizontallyand two eight-tooth spur
gears mesh vertically.Only the 60-tooth
LEGO turntable hasboth spur teeth and a number of
teeth not divisible by eight,although turntables are not
discussed in this video.Double bevel gears are
so named because theycan mesh in line at 90 degrees
or at intervening angles,so long as they can be reliably
attached to underlying LEGObeams.Double bevel gears are relative
newcomers to the LEGO world,having been introduced in
1999, more than two decadesafter LEGO spur gears.In 2019, and with
much fanfare, LEGOdebuted the 28-tooth
double bevel gear.Unlike LEGO spur gears, LEGO
double bevel gears cannot bepaired with gears of
the same diameter.This is because
their number of teethis not divisible by eight.This social distancing
is observed on the left,with two 12-tooth gears unable
to touch and two 28-tooth gearsunable to touch.To the right, the entire
family of double bevel gearsis demonstrated, including the
12, 20, 28, and 36-tooth gears.Again, for reference, spur
gears can touch themselves,whereas double
bevel gears cannot.This can make the design
of double bevel gear trainsless straightforward,
and in part,spurred my interest
in designing softwareto overcome this challenge.Designing gear trains at
non-orthogonal angles,in which the angle
between gear axlesis not in line
with or 90 degreesto the underlying
LEGO beam, can oftenyield uncommon yet
useful gear pairings.To facilitate this quest for
non-orthogonal gear trains,a nine by nine stud matrix can
be constructed from nine LEGObeams in which the upper left
position will be designatedas the driver gear.For example, this
16-tooth spur gear powersa 20-tooth double bevel gear,
in which the 20-tooth gearis positioned two studs
to the right and onestud down from the driver gear.Or if you wanted to create
a gear train in whichat 24-tooth spur gear powered
a 20-tooth double bevel gear,the 20-tooth gear
could be positionedtwo studs to the right and
two studs down from the drivergear.Finally, if you wanted
to create a geartrain in which a 20-tooth
double bevel gear powereda 36-tooth double bevel
gear, the 36-tooth gearcould be positioned
three studs to the rightand two studs down
from the driver gear.But wait, the
gears sort of mesh,but there seems to
be a substantial gapbetween the teeth--more than usual, at least.And upon closer
inspection, it is apparentthat the gears can be
pried apart rather easily.So this gear train might
not be a good candidatefor high torque applications,
because the gears might slip.Thus, orthogonal gear trains
can be especially useful,but they are uncommon in
part because they are oftenencountered through
trial and error,and because the gap or
overlap between the gearsmight need to be
calculated to determinetheir goodness of fit--the degree to which they
mesh well with each other.And this calculation
requires comparingthe sum of the
radius of both gearsto the hypotenuse
created by the riseand run of the number of
studs between the axle holes.In other words, this
needlessly tedious processneeded to be automated, and
data-driven software designprovided the most effective
and efficient solution.Data-driven design
can be definedas software design in which the
control logic, program flow,business rules, data
models, data mappings,and other dynamic and
configurable elementsare abstracted to control data
that are interpreted by ratherthan contained within code.Data-driven design is contrasted
with less favorable hard-codedor concrete software design,
in which changes to softwarefunctionality require
modification of the codeitself.In the next few minutes,
I'll demonstratehow data-driven design
underpins LEGO gear trainanalysis in the software.For example, the table
of gear attributesis the primary source of
control data in this software.All calculations for gear
pairing goodness of fitare derived from this table.This design facilitate
software flexibilityand configurability,
because end userscan maintain their own
individualized tables of gears.One LEGO builder
lacking some gearsmight choose to remove
specific records from the tableso that only
available gears wouldbe evaluated by the software.Yet another user, a savvy
engineer with a 3D printer,could design and cast
non-standard gearsthat were compatible
with LEGO gears,and could include these
customized gears in gear trainevaluation by adding them
to the control table.Neither modification requires
any update to the code,only to the control data.And this is the beauty
of data-driven design.A second control table comprises
pairs of common delimited XYcoordinates that
reference LEGO studs--the holes into which LEGO
axles could be positionedfor use in gear trains.XY coordinates are
expressed in millimetersfrom the source of
some starting position,such as the upper left stud.This table shows
the nine-by-9 matrixof LEGO beams
previously demonstrated.The gear train software
also defines and utilizesseveral parameters, another
type of control dataand hallmark of
data-driven design.For example, the fuzz
parameter allows the userto specify the
acceptable gap or overlapin millimeters that will be
tolerated in theoretical geartrains, thus eliminating results
that lack goodness of fit.A final facet of data-driven
design worth mentioning hereis data independence,
which promotes softwareinteroperability.Because the gear attributes
and XY coordinatesare maintained in
separate control tablesand not as hard coded
logic within program files,these control tables can be
leveraged by other softwareapplications and
programming languages,such as the equivalent
Python demonstrated here.The parameters
are also identicalin both the SAS and Python
instances of the software,allowing one set of
program documentationto suffice for both
software applications.So now that you've had
a brief introductionto data-driven design,
I'm excited to show youhow those concepts and
methods can be utilizedwithin gear train analysis.So the first thing I've done
here is open up the PDF.Again, this is located on both
the SAS.com and lexjansen.comwebsites.You can see a number of
gears that are pictured here,but more important
than the pictures arethe metadata behind them.And those metadata are
contained within the gears.csv--it's a comma
separated values file.And this can be opened in Excel
or your favorite application.CSV files and other
canonical file formatsare highly preferred
in data-driven design,because of their ease
of data transferability.For example, this
control table wasdesigned to be accessed not
only by SAS, but also by Python.For this reason, it would be
impractical to save these dataas a SAS data set
as we would commonlydo in the SAS ecosystem.Continuing to
scroll down the PDF,you'll see a reference
to appendix A,which is the LEGO gear
combination SAS program file.We can open up this file
within the SAS Display Managerand find that it
contains four macros.The first gear
report we just ran,which generates the HTML
formatted report of gearsand gear attributes.That's very useful.The second macro,
Create Rectangle,creates a matrix
of beam positions,so those XY coordinates that
demonstrate the axle holesinto which gears can be placed
to create those virtual geartrains.The third and fourth macros,
Gear Train Report and EvalCords, are the two macros that
perform the goodness of fitcalculations.A second SAS program file is
also contained within the PDF.And this file ingests and
invokes the first four macros.So after opening
it up, I'm goingto go up here and change
that default file location.I've previously been using
the SAS University Edition,so I'm going to change
this to my local D driveto ensure that the
files accurately run.After this modification has been
made, I can highlight the text,run the file, and presto, here
is the HTML formatted reportcourtesy of the SAS PROC REPORT.Opening this up, you can
see this is the report thatwas also produced in the PDF.And clicking on any
of the dynamic URLswill take you to the
brick links site.This is very
beneficial if you'relooking for additional
information about gearsor if you're trying to purchase
some of these LEGO gearsfor yourself.And while I'm on
this site, I'm goingto do a little bit of research.I recently heard a rumor that
the LEGO Group had releaseda new gear, a 28-tooth
double bevel gear--something that I don't have.And when I type in
the search engine,I find that in fact,
there are two--a gray released
in 2019 and a redreleased just recently in 2020.So of course, I need
to purchase these.But something else-- I
need to add these datato my control table.With the gear CSV
file open, I can nowmake these modifications and
add these two new LEGO gearsto my control table.This will be
extremely beneficial,because while I'm
waiting for the partsthat I've just
ordered online, I canstart designing
theoretical gear trainseven before I have
those gears in hand.As I'm making these
modifications,you can see that I'm pulling
the metadata directlyfrom the bricklink.com website.For example, every LEGO piece
has a unique part number.It's important that this number
is transcribed correctly,because it's also
required in the URL.So the dynamic links
in the SAS reportwill not work unless the
part number is correct.Something to note
about the date--although this is a new gear,
the data is listed only as 2019.This doesn't necessarily
mean that the gearwas stopped production in 2019.However, it may mean
that no new setshave been produced yet in 2020
that incorporate this gear.Once the control table has been
updated with the two new gears,the only thing left to do
is to rerun the software.Again, we're not having
to make any modificationsto the underlying code, because
the software will automaticallyrecognize that the control
table has been updatedand incorporate these new data.So looking at the original
report that was produced,you can see that there are
28-tooth gears, but not the twonew gears, just differentials
that existed prior to 2019.So when rerunning
this software, nowas we refresh this
HTML output, youcan see that the two 28-tooth
double bevel gears have nowbeen incorporated
into the report,and again, without
the need to changeany of the underlying software.Before you exit, you might want
to click on these links justto ensure that they are valid.And it looks like both 28-tooth
gear links are working,so we can close down this report
and continue running software.I had mentioned that there
were two principal controltables in this software,
the first being the gearCSV file which contains
gear attributes,and the second being the
table of beam positionswhich contains XY
coordinates for eachof the axle holes into which
gears can ultimately be placed.The next step is to run the
Create Rectangle macro, whichcreates a matrix
of XY coordinatesrepresenting the positions into
which LEGO axles can be placed.The macro has
parameters that allowthe user to specify both the
height and width of the matrixproduced.In this example, a
nine-by-nine matrixcontains 81 possible
holes into whichaxles can be placed for
gear train calculation.With the gears control
table now updatedand the nine-by-nine matrix of
axle hole positions created,the next step is to calculate
theoretical gear trains.The Eval Cords macro has
a number of parametersas listed in the
accompanying white paper.For example, setting
verbosity equal to verbosespecifies that each gear will be
listed uniquely in the output.If verbosity instead
is set to brief,then each gear will be
aggregated into groupsbased on the number of teeth.And as mentioned previously,
setting the fuzz parameterto one specifies that
any gap or overlapwill be less than 1 millimeter.With verbose on,
gears are listednot only by the number of teeth,
but also by the type of gear,the type of axle, and if
relevant, a version number.For example, 24-tooth
gears comprisespur gears, crown gears,
and a differential gear.More concise output
can be achievedby changing the verbosity
parameter to brief.Note that in this
data-driven design paradigm,only the parameter
must be changed,not any of the
underlying code thatrests within the SAS macro.Note that in the
updated results,only one 24-tooth
gear is now listed,irrespective of the type of
gear, type of axle, or version,if it exists.In general, using
the brief settingis preferred, because it creates
much more readable output.To conclude, let's
revisit the three geartrains we created
manually and demonstratehow a programmatic, data-driven
approach is much moreefficient.The first gear train was a
16-tooth spur gear poweringa 20-tooth double bevel gear.It turns out, according to
this HTML formatted outputreport, that this had a 0.111
millimeter overlap and a 1.25to one gear ratio.The second gear train was a
24-tooth spur gear poweringat 20-tooth double bevel gear.This had a 0.627 millimeter gap
and a one to 1.2 gear ratio.Finally, there was that infamous
third gear train that reallydidn't fit because of the gap.This was a 20-tooth
double bevel gearpowering a 36-tooth
bevel gear, and thishad a 0.844 millimeter gap
and a 1.8 to one gear ratio.And because we now
know that a geartrain with this substantial
of a gap is unacceptable,we can modify the fuzz parameter
from one millimeter 0.8millimeters, run the software,
and produce more optimizedoutput for further
gear train evaluation,all without modifying code,
thanks to data-driven design.It looks like my new
28-tooth double bevelgears have arrived, so
it's time to go build.Of note, a friend
recently asked,how long did it take to put
together this presentation?To which I replied,
I've been working on itsince the third
grade, when in fact, Igave a talk at a local
science fair entitled,""Gears in Action--Showcasing LEGO Gear Trains.""I hope you've enjoyed this foray
into LEGO Robotics and geartrain analysis facilitated
by data-driven design.And thank you for watching."
27,"Hi.My name is Walter Ochinko.I'm the research director at
Veterans Education Success.So you're probably
wondering, whatis Veterans Education Success?The mission of VES for short,
Veterans Education Success,VES for short, is to help
ensure the success of veteransand family members who
are using the GI Bill.They earned this benefit through
service to their country,and we work to address
obstacles they face in earninga post-secondary credential.For example, we work with the
Department of Veterans Affairsand the House and Senate
Veterans Affairs committeesto pass a bill that allows GI
Bill beneficiaries to continuewhat had been classroom
courses in an online setting.The bill was the result
of the coronavirus, whichled many schools to send
students home and switchto online instruction.The bill became law on March 21.Without this fix, VA would
have cut off GI Bill payments,including payments
to beneficiaries,who rely on the money to
cover their living expenseswhile they are in school.So today, my
presentation is goingto focus on the
paucity of metricsto evaluate student
veterans success in pursuinga post-secondary education after
they are discharged from the USmilitary.We use the publicly
available VA and Departmentof Education data sets
to tease out insights.So I want to give you some
idea about the organizationof this presentation.So it's organized
around five topics.First, we're going
to talk about--I'm going to give you some
background on the Post-9/11 GIBill, and then we're going
to talk about strengthsand challenges in using
VA and ED data sets.I'm going to outline some
key data initiatives thatwere never implemented.And finally, we're going to
conclude with some selected VESresearch findings.So in terms of background
about the GI Bill,the first thing I want to let
you know is that although a newGI Bill benefit program,
the Post-9/11 GI Bill,began enrolling
beneficiaries in 2009,about five older GI Bill benefit
programs are still enrollingeligible individuals.However, the Post-9/11 benefit
is by far the most generousand actually enrolls
the most beneficiaries.Unlike the other educational
benefit programs,it pays tuition
and fees to schoolsand provides beneficiaries
with a separate living and bookstipend.So this slide shows you a
brief summary of the basiceligibility requirements
for the Post-9/11 GI Billand for the other
benefit programs.Essentially, you have to have
served a minimum of 90 dayson active duty after
the 9/11 attacks,hence the name [AUDIO OUT]
Post-9/11 GI Bill.In addition, you have
to received a discharge.So who's actually eligible?Well, it's not just veterans
transfer their benefits.They need to do that before
they actually leave active duty.The dependents of
service memberskilled in the line of duty or
also eligible, both spousesand children.And members of the
armed services,once there they meet the
eligibility requirementof 90 days, are eligible.The National Guard
is [AUDIO OUT]once they meet that
90 day threshold.So each of the cohorts
individuals that are eligible--the veterans and service
members, the family members,et cetera--have distinct
characteristics, whichargue for using the publicly
available data to separatelyexamine subsets of those
eligible for GI Bill benefits.For example, we'll
compare the demographicsof undergraduate versus
graduate student veterans.Compared to student
[AUDIO OUT] pursuingan undergraduate
credential are morelikely to be 25 to 34 years
old, first-generation collegestudents, have a
disability, and besingle parents with dependents.The next table compares
the demographicsof student veterans
who enrolled in collegefor the first time in
2011-12 to those who leftby 2014 [AUDIO OUT] credential.Veterans who left
were more likely to bemale, minority, [AUDIO OUT],,
and working full time.Veterans enrolled in
post-secondary educationare used in the GI Bill.Why would this be?First, they may have exhausted
their 36 months of benefits.More than 6% veterans who earned
a bachelor's degree in 2008first enrolled at
least 10 years earlier.Many veterans attend
inexpensive community collegesand may be saving their benefits
to earn a bachelor's degreeat a four-year institution.And of course, some
may not be eligible.So now I'd like to turn
to our second topic, whichis the data set
strengths and weaknesses.Two federal agencies make
available data that can be usedto evaluate student veterans'
success in post-secondaryeducation--the Department of
Veterans Affairs, VA,and the Department of Education,
which I'm referring to is ED.From their perspective
[AUDIO OUT]the data sets have both
strengths and limitation.So let's talk about the
GI Bill Comparison Tool.This is a Department of
Veterans Affairs product,and it was designed
to help veteransmake an informed
choice when decidingwhich college to attend.It was not really
intended for researchers.Its strengths are that it
shows beneficiary enrollmentat each school and is
useful in tracking trendsacross institutional sectors--public, nonprofit,
and for-profit.So the challenges of
using the comparison toolare that you can't distinguish
among beneficiary cohorts[AUDIO OUT] example that it
contains no outcome data,and there are no
data on veteranswho are GI Bill eligible
but not using benefits.So another data source that
we use our annual benefitsreports.The strengths of
the benefit reportsis that they had more
detailed data on the GI Billprogram being used.They show the attendance
level of beneficiaries,for example, part
time versus full time,but only for
beneficiaries and startedusing the benefit that year.And finally, they indicate
whether the beneficiaryis a veteran slash service
member or a dependent.In terms of
limitations, they onlyprovide the degree
level being pursuedor the attendance
level for individualswho start using benefits
for the fiscal year coveredby the report.So let's move on
to ED survey data.So there are three ED
surveys that we use--the National Post-Secondary
Student Aid Study.It's the largest survey with
over 100,000 participants,and it's conducted
once every four years.The two other surveys
are longitudinal surveys,and their samples are
drawn from NIPSAS.The first is
Baccalaureate and Beyond,and the second is Beginning
Post-Secondary Students.So Baccalaureate
and Beyond trackedstudents who earned a
bachelor's degree in 2008,and BPS followed students
who enroll in collegefor the first time in 2011-12.So the strengths
of these surveysare that they allow evaluation
of GI Bill beneficiary cohorts,so for example,
graduate studentsversus undergraduate
veterans, or veteransversus service members.They have a data outcome data
such as graduation persistence,whether you transfer
to a different school,how much you borrowed, and
what your repayment status is.Then finally, they
collect demographic data.So the limitations
of these surveysis that the outcome data
is at the sector level, notindividual institutions.And another drawback is that
the students-- the surveys,I'm sorry, are designed
for the general population.And although researchers
can focus on veterans,the data is not always as
specific as would be desired.So for example, [AUDIO OUT]
couldn't tell whether or notthey were [INAUDIBLE] education
data source is called IPEDS.It's basically data provided
to the education departmentby schools themselves.IPEDS distinguishes
undergraduate veteransfrom those seeking
graduate degrees.That's its strength.Its limitation is that it only
collects data on veterans usingthe Post-9/11 GI Bill.So the last Education
Department data set that we useis called the College Scorecard.So the strengths of
the college scorecardhas outcome data title IV,
which is participating school.Initially, we'll have such data
at the program level for eachschool..Its limitations are that it
has no veteran-specific outcomedata.The metrics have
changed several timesand some replacements are
[INAUDIBLE] school graduate 10years after a roll,
and this was replacedwith graduates' average earnings
one year after completing.The third topic I'd like
to address is that sincethe enactment of the Post-9/11
GI Bill and its implementationin 2009, there have been several
attempts to require schoolsto report better
outcome metrics.And in general, these attempts
have been unsuccessful.In 2013, VA, ED, and
DOD proposed metrics,but they were never
implemented because of concernover institutional burden.This year we provided
feedback to Senateand to a House committee
bill to help strengthena bill that would require ED
and VA to share data in orderto publicly report veteran
outcome data on the GI BillCollege Comparison Tool.So as I mentioned, we use
these data sets tease outinsights about veteran success.As demonstrated the
next several slides,we use multiple data sets
to inform our researchon specific topics.For example, IPEDS and NIPSAS
for one report, or the GI BillComparison tool and the
College Scorecard for another.To provide context, we often
compare student veteranoutcomes to those
of other studentsor older and no longer
financially dependenton their parents.The next several slides
highlight our research findingsfrom several recent reports.So we recently completed
a report on veteranswith disabilities, and it
looked at their attainmentafter they enrolled.In general, compared to
veterans without a disability,veterans with a disability are
more likely to leave schoolwithout a credential.However, compared to
non-veterans who are disabled,they earned bachelor's
degrees at a similar rate.So the next report I
want to discuss comparedborrowing trends for
veterans using and notusing GI Bill benefits.We found that almost
half of veteranswere not using benefits.[AUDIO OUT] to find out
that both groups borrowedsimilar amounts in academic year
2016, and the proportion thatwas not using that
borrowed was notthat much larger than GI
Bill beneficiaries borrow.The [AUDIO OUT] report
I'd like to talk aboutis on first-time students
and whether not theycompleted a degree.So you can compare the veteran
and independent students,undergraduate student
persists and earn a degree.The demographics of those
who left without a degreehighlight some of the
risk factors underlyingnon-completion.Many were the first in their
family to go to college.They didn't have a traditional
high school diploma,they worked full time, and
they were single or marriedwith dependents.So in conclusion,
I'd like to saythat no data set is perfect.We work with the data
sets that we have,and we try to be very
cognizant of the strengthsand limitations of the
data sets that we use.I worked for 40
years at GAO, and youknow I can tell you
that my experience isthat there is no such thing
as a perfect data set.So in concluding, I
would like to thank youfor listening to
this presentation,and I'd like to acknowledge my
research colleague Kathy Pei,who still does who does all
the heavy lifting with the datasets that we use.That concludes my remarks.Thank you."
28,"TOM ANDERSON: For the
electric utilities,grid reliability may require
the monitoring of millionsof devices across an
ever changing landscapeof electrical circuits,
servicing hundredsof thousands, or
millions of customers.The utility's goal is
to first and foremostdeliver electricity safely,
reliably, and efficiently.I'm Tom Anderson,
principal systems engineerfor the SAS energy division.Today we'll be discussing
driving energy efficiencythrough improved
asset management.We'll walk through the
generation, transmission,and distribution systems
of electric utility,and describe its
operation, and demonstratehow SAS Analytics can be
used to improve energyefficiency through analytics.Shown here is the process
of delivering electricity,as described by the US Energy
Information Administration.Power plants
generate electricitythat is delivered to
customers through transmissionand distribution power lines.High voltage transmission
lines, such as thosethat hang between
tall metal towers,carry electricity
over long distancesto meet customer needs.Higher voltage electricity
is more efficientand less expensive
for long distanceelectrical transmission.Lower voltage electricity
is safer for the usein homes and businesses.Transformers at substations
increase or reducevoltages to adjust to
different stages of the journeyfrom the power plant along long
distance transmission linesto distribution lines
that carry electricityto homes and businesses.It will be this
distribution networkthat will be focusing on today.The distribution
grid or network isresponsible for
delivering electricityfrom the substation to
homes and businesses.As shown on the
right, it consistsof many nodes and
connections that assemblethe network of circuits.In our demonstration today,
we have 11 substationsthat support over 90 circuits.Each point on this map
represents a node or connectionwhere the transformer
or other device resides.Distribution transformers
step down the voltagedelivered to customers.We have over 1,700 transformers
within these circuits.Other devices include
fuses, switches,and reclosers that are in place
to maintain a level of safety,security, and low reliability.Understanding the ebb and flow
of energy across the networkis key to enhancing energy
efficiency, reliability,and safety.The distribution of energy is
based on the circuit designsand have physical limitations
of the circuit devices.Increasing energy assets,
like electric vehicles,solar arrays, and
battery stationsare impacting those
limitations and configurations.Using analytics and
devices in the field,SAS can enhance the energy
efficiencies of those circuits.When considering
energy efficiencies,utilities must have
accurate informationabout system performance,
to ensure that maintenancedollars are spent wisely, and
that customer expectations aremet.To measure system performance,
the electric industryhas developed several
performance measuresof reliability.These reliability
indexes includemeasures of outage
duration, frequencyof outages, system
availability, and response time.The most often used
performance measurementfor a sustained
interruption in the systemis the system average
interruption duration index.This index measures
the total durationof an interruption for
an average customerduring a given period.It is normally calculated on
either a monthly or yearlybasis.However, it can't
be calculated daily.The system average
interruption frequency indexis the average number of
times that a system customerexperiences an outage
during the year.Once an outage occurs, the
average time to restore serviceis found from the customer
average interruption durationindex.And the last index we
are highlighting hereis the momentary average
interruption frequency index--the average number of
momentary interruptionsthat a customer experiences
during a given period of time.And momentary interruptions are
generally less than one minuteof interruption.There are other
indexes used as well.But these are the ones that
we are highlighting today.And utilities track these
indexes to determinehow well they are performing.Utilities continue facing
new operational challenges,as there are expansions and
new devices added to the systemdaily.Expectations from
customers and regulatorsare always increasing as well.To meet those
expectations, utilitiesare continually looking for
new tools and opportunities.And now we'll
demonstrate how SAScan drive energy efficiencies
through better assetmanagement, with
the ability to readin potentially thousands of
devices on a complex smart gridmetering infrastructure,
to determine healthand worthiness of circuits.The focus of our
demonstration isgoing to be lined
distribution transformers.As I mentioned earlier, these
distribution transformersare generally mounted
on poles or padsout in front of
homes or businesses.And there's no information
being collected from these.Are just devices
out in the field.But we're going to be
using the AMI meters thatare being serviced by
these transformers,to use their data to assess
the transformer itself.Each transfer
transformer generallyhave anywhere from
one to 10 metersit's going to be servicing.And we roll up that information
to determine the total loadcapacity being used
on that transformer.The reason why we've
chose these transformersis because there
are costs associatedwith these when they go out.The just recent transformer
can range from $1,000to several thousand
dollars, dependingon the size the increase
per activity of predictingthese transformer failures can
help reduce the cost of OEMand increase the system-
reliability-- basically, makethose indexes that we're
assessing much better.Streamlining and
planning the coordinationrequired for these
transformers can also be done.Again, keep in mind
that these transformers,there's generally
hundreds of thousandsof these allocated across any
particular grid or networkthat a utility
would be servicing.Here's an example of a
reliability dashboard.We have the indexes
across the top,along with reports on the
types of causes of outages.and look at five year trends.And these are common reports
that would be used by utility.Here we're looking
at causes over trendover five year periods.And here, we're looking
at the comparisonsof interruption caused by
comparison over an hour.So sort of the average
hourly interruptions thatare happening based on
the circuits in here,as well as looking
at the core history.What are the types of
causes we're mainly having?The prearranged?Are the underground malfunctions
overhead malfunctions?Driving deeper into the
reliability reports,we can select a substation,
and look at the substation'sperformance, based on the
transactions happeningon a particular circuit
these are the circus thatare listed here.And we have customer interrupted
values, and customer minuteinterruption values, as well
as duration values in here.All of these are used to
calculate the indexes that wehave shown here on the right.And comparing these indexes,
like CAIDI and CAIFI,we can look for outliers.And when we have outliers, those
are indicators that we probablyhave something within that
system that we need to address.And in some cases in here,
areas like overhead malfunctionsare identified that are
probably happening overin a recurrence time that we
want to address or look at.Other reports, such as outage
incidents-- as we see here,we're looking at to 2018.We're looking at a
substation called Eric,and the different causes
and components thatare happening over time,
and comparing notes alsowith the weather
elements that we have--So.What types of
outages that we have,and what are they, what are the
occurrences of the weather thatare happening within those?Down below here, we have the
substation name the devices,and the devices that had
the most duration of outagesthat are happening here.Highlighting these,
we run, probably wantto look at these particular
devices, such as transformersand locations, to
determine what's going onwith those particular assets.And using the data
that we have collected,we can create assets
scorecards, where we sexuallyscore the transformers
or scores of devicesfor propensity for failure.And in this list here, I have
a list of top 100 transformersand their score.A score of 0.8 or higher
indicates that we probablywant to have an inspection
done on that particular unitor the transformer
that's out there.On the right hand
side here we havesubstations that we
can look at basedon outages that are occurring.By selecting those, we
can look at the typesof outages that are
happening, as wellas the durations
of those allergiesand the types of outages that
are occurring on those systems.Not being able to create
these types of scorecardsis done by applying our
machine learning algorithms,such as decision trees, random
forest, or gradient boost.In this gradient boost example,
we're looking at transformers.And we're taking into
variables such as the loadover a particular
period of time.We look at the load into six
hours, 12 hours, 24 hours,36 hours, and 72 hours.We take in values
like weather as well.And these are the
variables of importancethat are used to
determine the propensityfor that particular failure
or challenge that they'rehaving with a transformer.And now I'm going to demonstrate
how we can operationalizethat gradient despite
smile that we createdto score those
transformers in real time,and drive the energy efficiency
analysis to improve the assetmanagement that we're doing.Here, I have the
gradient boost model.My response variable
is the event flagthat I have assigned for this.Now an event identifies that
transformer has gone overloadedor has cause for a failure.My predictors are going to
be the data they've collectedfrom those AMI meters.Those are running values
that bucket it back 72 hours.And that bucketed information
represents time fieldsof values based on load,
variance values, median values,maximum values, and
also weather datawith the same bucketed fields.I also have indications
in here whenthe transformer has gone
over 100% over 120%, 140%,or over 150% of time.Now selecting those variables
and those predictors,I have my gradient boost model,
as defined by my selections.And here, you can see the
variable's importance,those variables those
values that made the mostimpact on my event flag--items like the variance
over six hours.So how much fluctuation did I
have in load over six hours,and 12 hour periods?As well as what was my
value for total loadat the six hour period?Other items in here
include that dewpoint values, as well as
the type of transformerthat I'm using.All these are the factors that
are laid into my ingredientboost model to
determine whether or notI'd have a failure or
a possible challenge.If I like this model, I
can then create this modeland move this into
model management,and then promote that
into other sources,like instrument
processing, whichis what I'm going to do
now by creating a pipeline.By right clicking and
selecting Create a Pipeline,I can create a new
or existing project.Going into an
existing project, youcan see my process flow
as it appears here,that was just created in visual
data mining machine learning,where I'm taking in my data.It does some imputations, and
performs a gradient boost modelthat I designed in there.i have also created other
pipelines or the processesto do the analysis on
here for comparison.Some of these processes included
other regression type analysis,as well as more
advanced processeslike neural nets, decision
trees, and random forestas well.Creating a pipeline
comparison of those,SAS will help me determine
which was pipelines, whichof those models performed dust.And here, you can see from the
models that we were producing,I have a couple of gradient
boost models of neural netand decision trees.And you can, see at this
particular gradient,boost model was the one
that was championed.From this process
here, I can go inand view the code
that was created,as well as the output that
was developed from a modelingpractice.By publishing this
model, I can makeit available to other third
party applications, as wellas SAS even stream processing.Going into SAS Model
Manager, I couldselect a project I've created.This is the asset management
model we've worked with.And you can see my
models that created.And from here, I can view
the variables and properties,see the performance and
the history of the values,and also have different
versioning as required.My gradient boost model
is the champion model.And by making that
available in Model Manager,I can now access that into
event stream processing.Even stream
processing and allowsme to take streaming data
at real time occurrences,and apply my models
to that streaming dataat tens of thousands
of cycles per second.In here, I have a project that
I've created, good reliability.And when I open this up,
we'll see the different inputsthat I have from my transaction
data from the utility grid.These are six different
circuits that I'm displaying.And from these circuits,
I'm bringing in transformerinformation and AMI meters.And now combining that
transformer and AMImeter with the weather data
and bucketing that information,joining them together, and then
applying the calculating nodethat applies the gradient
boost model that we've created.Here, you can see
the calculating.Select the SAS Model Manager,
where we published the model.In my repository, I have
several models I've created--and particularly, my gradient
boost from the grid reliabilitymodel I've created.Here, you can see
the different modelsthat we've published, including
the neural nets the decisiontrees.And we select the
gradient boost model,which was the champion
model, as selected.From that output, I do some
filtering for flags and alerts,combine information together, so
that I can report geospatialy,the locations of
those transformersthat may be in distress, and
aggregate for further alerts.And let's demonstrate
that process.Going to ESP dashboard,
we open the dashboard.And you can see, from
that network diagramthat you probably recall, we
have all the different nodesfrom those circuits
that we're reporting on.These are all of the reporting
transformers in real time.Over here, on the center
left, upper centerleft, is the location
of transformersthat have scored
where they probablyhave some sort of challenge on
them that need to be addressed.This is the streaming
data that we'reshowing, bringing
in the transformerinformation and the values
that are being scored upon,including the gradient
boost output values.Using SAS reporting
and analytics,we're able to score
devices in real timeon the utilities grid
distribution network,improving the
efficiency analysison asset management
in real time.Thank you for spending
this time with me today.And if you have any questions,
please reach out to meat TomAnderson@SAS.com.Thank you."
29,"Hello, everyone.My name is Nabaruna Karmakar.And I will be talking
to you about a projectwe did with Boston Public
Schools as part of SAS's Datafor Good initiative.The problem deals with
optimally assigning monitorsor supervisors to accompany
students with disabilitieson school buses.There are several
rules which needto be maintained while
assigning monitors to studentswith a goal of maximizing
the number of routeswithin each monitor's package.For a given academic year,
BPS manages 625 buseswhich go on 3,500 routes.On these routes, are
around 1,350 students whoneed monitors assigned to them.Each bus typically has
three to four routes,each in the morning and in
the afternoon, some of whichrequire monitors.In the morning, the bus
starts out from a yard,then picks up students
from their homes,and drops them off
at their schools,then return back
to the bus yard.In the afternoon, they
start out from a yardagain, pick up the students
from their schools,and drop them off
at their homes,returning back to the yard.As roughly 40% of
trips have monitors,a particular bus may have some
trips with monitors and somewithout any.The goal is to build
packages, shown on the right,for monitors to bid on at the
start of each school year.Each back is typically have two
to seven routes or runs in it,comprising a mix
of AM and PM runs.Only monitors who
have six or more runsreceive health insurance.Hence, it is beneficial to
create these monitor packagesin such a manner that maximizes
the number of studentssupervised by one monitor,
as long as all the studentneeds are met.After exhausting options
in which the monitor stayson the same bus,
they can be madeto switch buses at
schools, or they can stayon the bus to make connections.Building the packages manually
takes anywhere between oneto two weeks.Also, each individual
package needsto be printed on
separate sheets of paper,which for them
necessitates automation.SAS optimization was used to
develop a Mixed Integer LinearProgramming model,
or a MILP model,to maximize the number of
routes within the packagewhile ensuring full coverage.Let's talk about
some of the rulesrelated to the type of monitors.Some students need either
their own monitor, a one to onemonitor, or can share a
monitor with other students,a generally monitor.For most students, there
is no specific requirementthat a specific monitor
be assigned to them.Hence, a regular monitor can
supervise all students whodo not need special monitors.And they need to
arrive and leavefrom the same bus yard in the
morning and in the afternoon.It is sometimes
required for studentsto be assigned to
a specific monitor.That is, a student
A, has to be assigneda monitor, X. These
are special monitors.And the bus yards do not need
to be the same in the morningand in the afternoon for them.Specific monitor
individuals are sometimesdesigned for a
particular student,for example, a
paraprofessional at the school.These paraprofessional
monitors arerequired to accompany the
student during school hours.For them too, the
bus yards do notneed to be the same in the
morning and in the afternoon.Let's talk about
our first approach.We started out with
a simple MILP model.We were given a set of monitors.We tried to assign them to
the routes requiring monitors.This was a very
straightforward approachto solving the problem.However, it uses millions
of variables and constraintswhich makes the
model not scalable.It did not take advantage
of the underlying networkstructure of the problem.The second approach was
a simple network flowmodel, with the schools
and the bus yards as nodes,and the bus routes
connecting them as arcs.Since this model also had a
large number of constraintsand variables, a
binary search methodwas used for faster convergence
to the optimal solution.This model was able to solve
a smaller instance, thatis for the summer
schedule, but could notbe scaled to the
whole year's schedule.Finally, we came up with an
integer multi-commodity networkflow model, with the
bus routes as nodes,and the set feasible
connections as arcs.The margins were considered
as the commodity flowingon the arcs.The special monitors were
treated as binary commodities,while the regular monitors were
treated as integer commodities.The objective was to minimize
the number of monitors neededor the total number
of packages, whichin turn maximizes the number
of routes within the package.This approach did not
require to have the feasibleset of monitors,
but instead foundthe minimum number of monitors.To find the individual
packages, the optimal solutionfrom the MILP was decomposed
into directed cyclesusing SAS network solver.So the current method
of manually creatingthese packages takes
about one week,while the SAS
Optimization model,you can solve in 20 minutes.We also used the SAS
Output Delivery Systemto automate the
printing of the packageson separate sheets of paper.We are currently in the
process of validatingthe output of our optimization
model on anonymized dataand adding more capabilities
like maintaining the studentto monitor relationship
over weekdays and minimizingbus transfers.That was my presentation.Please feel free to
contact me with questions.Thank you."
30,"SCOTT LESLIE: Welcome to
What's New in SAS Drive.I'm Scott Leslie, development
manager for SAS Drive.I'll be presenting the paper
that Cheryl Colye and I wrotefor SAS Global Forum, 2020.There are many new
features in SAS Drive2.2, which is available now as
part of SAS Visual Analytics.I'll be going over
the main new featuresand discussing how to
use them effectively.For those who've never
used SAS Drive before,it is the content management
solution for the SAS platform.SAS Drive allows a user
to see, manage, and shareall the content that they
have access to in SAS system.Solutions from SAS
allow users to createa variety of different types of
content from reports to models.SAS Drive allows a user
to manage and collaborateon all of that content.One of the features that
we are most excited aboutis Make this a Tab.Now, you can take any photo
that you have access toand make it a tab of over
the main content area.Simply select the folder
in the tray on the left,right-click, and
select Make this a Tab.This allows you to have
quick access to any folderno matter how deep it may be
nested in the folder hierarchy.Once the tab has
been created, youcan rename it to have a more
meaningful name if you want.These tabs are
persistent across logins.So they will be there the
next time you use SAS Drive.Our next feature to discuss is
the resizable information pane.Previously, the information
pane could be opened and closed.But it remained a
fixed size when open.Having information
pane be resizableallows information,
like comments,to have more room to display.Another useful new feature is
the edition of quick accessrecommendations.While you can still add any item
you want to the Quick Accessarea, the Quick Access
Recommendations buttonallow SAS Drive to
automatically add contentthat you use most frequently
to the Quick Access Area.And often asked for a
feature of SAS Driveis the addition of a star
icon to content tilesthat are in the user's
favorite folder.This allows you to quickly
identify favorite contentat a glance.Many usability
improvements have beenmade to the Information Pane.One change is the ability
to expand and collapsesections of information to show
you just what you want to see.If you leave any section
of the Information Paneexpanded or collapsed,
it will stay that wayas you select other
content in Drive.This allows you to browse
through informationon a variety of pieces
of content at a glance.There are many types of
information about content itemsin the Information Pane.One really useful
piece of informationis showing who you've
shared content withand who shared content with you.If you expand the Shared with
Section of the InformationPane, you'll see sections for I
Shared This With and/or SharedWith Me By.This allows you to keep track
of who you have shared contentwith and who shared
content with you.Searching for content is a
common use case in SAS Drive.Each time you search for
something, a new search tabis created.This allows for
persistent searchesand makes search results
prominent in the userinterface.However, it can also
lead to clutter.If a user does not close
search result tabs,they no longer need.The Close All
Search Tabs featuresolves this by
adding a context menuon the main tab bar, which will
close all search tabs at once.If you prefer, you
can always just closethe ones you want by clicking
the x on the tab header.A completely new feature not
really related to contentis the product tour.The product tour will come up
for a new user of SAS Drive.It will point out the most
important areas of the userinterface to new users.If you ever want
to retake the tour,just select Take a Tour
from the Application menuin the top right.Sharing content is one of the
primary use cases in SAS Drive.In previous versions, you
were able to share reportswith other users.Now, you can also copy a
link to report and send itvia email or your favorite
messaging application.You can also do this from a
Visual Analytics application.But sometimes it's more
convenient to copy a linkwithout actually
opening each reportthat you want to send a link to.One of the most useful
features of SAS Driveas the Preview window.It allows the user
to get a betteridea of what an item
contains without opening itin another application.Previously, you could
preview images, text files,and reports.Now, you can also preview PDF
files directly in SAS Drive.And last but certainly not
least, a miniature versionof SAS Drive has been
incorporated into SAS VisualAnalytics so that you
can use the familiar lookand feel of SAS Drive to
select Reports to open.We call this the
QuickStart Window.With it, you can browse
through the reportsthat you have access
to, search for reports,or create a new report.The report list can
either be displayedas tiles or as a table.We think this increases the
piece of look between SASDrive and SAS Visual Analytics.Thanks for taking time to
learn more about SAS Drive.If you have questions,
please contact me.And I'll be happy
to discuss it more."
31,"Good morning, and welcome to
this session on SAS SQL methodsand more.If you're new, or even
if you're experienced,it's an opportunity to become
more proficient in usingSAS SQL to do queries.So let's get started.The first chapter-- and there
are four chapters-- justa brief introduction, give
you an overview of what we'regoing to do and the logistics.Chapter 2, we will focus on
performance considerations.We'll talk about
computer resources usedwhen you submit a SSAS program,
and how can you benchmark.Chapter 3, we will look at doing
explicit queries against datasources, like a
relational database,such as Oracle or Teradata Db2.We will use a LIBNAME
statement to do whatwe call implicit pass-through.That means it will
look at your requests,and if it can pass or
created a database, it will.If not, SAS will handle it.There are some data
set options that wecan use with implicit
pass-through,meaning direct pass-through,
and also explicit pass-throughin SQL, in PROC SQL
and PROC FedSQL.So if you understand a SQL
associated with that database,you can write it and send
queries directly and havethe database handle it.Chapter 4, the concluding
chapters on queries,perplexing performance problems.And we'll talk about some
queries, correlated subqueries,when they're useful and
when they're not useful.And the last topic,
cross-library options.And then, in chapter 5, there's
learning more, just some tipson what you can do
to go from here.What SAS offers in terms of
support in different usergroups, for example--user groups, professional
services, technical support,education, publications.And you can get
plenty of informationfrom support.sas.com on
training and these other topics.So I'm going to stop sharing.And then we are now going
to kick it into chapter 1,and we'll stop at the
end of each chapterand oftentimes have a workshop.Chapter 1 will focus on an
overview of the course, whatwe will do, and the logistics
to help us gain insight on SQL.So as an overview, here
are the objectives.We will talk about the
topics for this course.We will talk about
SAS/ACCESS, which is reallythe software behind the scenes
that allow you to access datain different sources, like
relational databases and PCproducts.We will talk about the methods
for accessing relational data.And to get you set up
for the virtual lab,there are a few steps
I need you to followto prepare for use of SAS
and SAS/ACCESS within class.About this class.We focus on a number
of things, includingcorrelated subqueries.We'll talk about the
performance of an SQL query.We'll do SQL pass-through
directly to the databaseand write it in such a way that
the database can understandit and enter your request.We will focus on
cross-library options,understanding there are
implications performance-wise.SAS options to help evaluate
the submission of your codeand what happens
behind the scenes.So there are some
options to helpyou do what's called a
benchmark, where we can lookat things like how
much memory was used,input-output operations,
how much CPU timeit took for your task
to run, and so on.This will help us
identify and eliminatesome things that are not
performing well if possible.So what do you get with
the SAS/ACCESS software?I want to talk about that.If you're retrieving data
from a SAS data set--many of you have done
this in the past.The data set PROC step,
like PROC REG, PROC FREQ,to go against SAS data
sets and return resultsas a report or a table.Now, here's a little note.With appropriate
credentials, youwill be able to write
to vendor databases.In other words,
the administratorfor that data source,
if they give youpermission to edit
or create tables,you will have be able to do so.If not, you will have
read access, thenthat's all you need to do.Just use those tables
and not change them.So what do you get
with SAS/ACCESS?You get the interface engine,
like Teradata or Oracle.You have the ability to
do SQL pass-through codenative to that
database, plus youcan use the LIBNAME statement to
access tables in the database.The interface changes.Well, there's one for each
major data source or database.Like Teradata, there's
the Teradata engine.For Oracle, what you
get is an Oracle engine.And that will permit you to use
SQL procedure, either PROC SQLor PROC FedSQL,
against the database.You can select columns
and rows from a table,or you can do non-query
statements, as longas you've been
permitted to do so,like granting access to
certain tables to a population.The LIBNAME statement
uses the engineto treat the database tables
as if they were SAS data sets,and you can use them
in procs and data sets.Behind the scenes,
the engine willgenerate database-specific
SQL for the data you'reretrieving from
using the engine,or on behalf of the engine.There are many options we
could take advantage of,and we'll see those
as we go along.So what do you get
with the Oracle engine?You have the ability to go
against the Oracle clientto make requests, go
to the Oracle server,and pull tables
from that database.In the Windows
machine, in this class,we will have
everything on one box.We'll have the SAS products,
SAS/ACCESS to Oracle,and we'll have the Oracle
server in the Oracle databasealso on that same machine.We're running on a single
machine in this class.But typically, you would
have more than one machine.So that's it for the overview.The main key things is that
we'll use PROC SQL, PROC FedQLto do retrievals against
relational databases,like an Oracle database.So what about the
course logistics?What you have to be aware
of to get the most outof this course?There are three editors, which
many of you are familiar with,or you're probably more
familiar with the onethat you typically use.We'll talk about
the files you needto bring in and also prepare
you for the virtual labenvironment.That will be the final--one of the key steps.So in this course, you
will write SQL programs to,number one, go
against SAS data sets.We call the data
Orion Star data.Plus, you will retrieve
some tables from Oracle.The three interfaces
at the bottom--you can see we have the
Enterprise Guide, Studio,or the Windowing environment.The Windowing environment
has been around the longestand is accessible under
Windows, Linux, Unix, and IBMmainframes.The Enterprise Guide interface
is only available in Windowsenvironment.And notice the first
bullet, point-and-click.So what does that mean?That means there are tasks
from this task pull-downfor processing your data.Data management, by
doing SQL joins--yeah, I could write SQL--sort.There are tasks for data
management and reporting,and analytics.But in this course, we
will write our own codewith the full
programming interface.Studio is very similar
to Enterprise Guide,but it's more of a
web-ased HTML client,and it also offers
point-and-click tasks, as wellas a programming
interface, whichyou can use in this class.So you have to pick
which one that youare more comfortable with.Actually not.If there's one you want to
try, you have the opportunityto do so in this course.As we go along
throughout today, Iwill ask you to bring
in different programs.One is called sqm01d01x.SQM is SQL Methods.That's what the
acronym is implying.And we'll give you a
chapter, plus a letter.One of these letters, like
activity, demo, exercise,solution, we'll put that
in there and then some itemnumber.Next hour, you should be fine.No problems at all.There's SAS help and
online documentation.There's information on
relational databasesyou can see online.The SQL procedure
in more detail.The FedSQL language, as well.And Database Products
User's Guide and SAS/ACCESSto a particular
database, like Oracle.Let's take a look at chapter 2.Right, chapter 2, the topic
is performance considerations.And so what we will
do here is lookat computer resources that's
required behind the scenesand what you can do
efficiency-wise to decipherhow certain resources
are being used.We can do some benchmarking, and
I will show you how to do that.So let's focus on computer
resources and efficiency.So what are the resources
that are really used?Let's take a look
behind the scenes.I want to talk about
those resources relatedto SAS program efficiency.I will show you how to do
a benchmark on resources,resource usage, and
look specificallyat those options we have to
activate to get reportingstatistics on those resources.So when you submit
a SAS program,there are resources behind
the scenes that are reallydisplayed on the screen that are
being utilized in many cases.The CPU time, the I/O,
memory, data storage,network bandwidth.Those are some of the things
that are considered resourcesfor the computer network.Programmer time is really
dependent on your skill level,so that's not included in
the SAS program resources.The CPU time.That stands for
the amount of timeit takes your server processing
unit to perform a program,process a program, and
handle operating systemtasks behind the scenes.So that includes things
like calculations,reading and writing data from
a data set, to a data set,conditional logic, like if,
then, else, SELECT-WHENs,iterative logic, your loops--functions, character,
numeric, statistical.And then all of these things
are measured in seconds.I/O. That stands
for Input/Output.Input, reading of data.O for Output, writing
or outputting data.So generally, you pull data
in from a storage device,do some processing, and
typically create new dataand store it.That's your output.Input from storage to memory,
and output from memoryto storage, or display.Memory is the size
of the work area.And we call it in
volatile memorybecause this doesn't
always use the same amount.For executable program modules,
the data that's loaded,and the buffers that are
use behind the scenes,the memory is required
for those different items.Data storage is
the physical spacethat's needed to store your
data on storage devices,such as disk drives, tape
drives, flash drives,and memory cards.The next resource,
network bandwidth.The available throughput.It can pass things to
the computer networkto move data from one point
across the computer networkto another point.A greater throughput allows
more large amounts of datato be moved.So these resources, CPU
time, I/O, memory, theseare resources that the
programmer can manage with SASprograms more efficiently.There are some
things you could do.The programmer time,
I would say no.Because again, the
programming time systemis not considered a
computer resource.It's dependent on
your skill level.If you have a novice SAS
programmer versus someonewith extensive experience, if
you give them a complex programto create, of course
that novice userwill generally take more time.All right, so what I wanted is
to point out, ask you to do--I will give you a minute.These definitions I want you
to match with the resourcesto the right.So indicate the letter that
applies to the descriptionof the resource.The physical space
needed to store data.That's data storage.The amount of time
used for calculations,iterative logic, conditional
logic, like SELECT-WHENs,if, then, else, and reading
and writing of data,that's CPU time.Size of the work
area for holdingexecutable program modules,
the data, and the buffers,that will be considered memory.Measurement are read and write
operations-- input/output.That's I/O. And
lastly, throughputfor data communications, right
across, network bandwidth.Here we go.So once again, what happens
when you submit a SAS program?The computer's
executing a SAS program.Well, the software needs
to be loaded into memory.You do a PROC and a [INAUDIBLE].The software has to
be loaded into memory.The program needs
to be compiled.Make sure you didn't
have any syntax issues.Then it will execute
that compiled program.And if you're creating a
report, it returns a report.If you're creating
one or more tables,it will store those
output table or tables.Generally speaking, when
you minimize one resource,try to reduce it, you typically
increase another resource.I want to give
you some examples.Now, one of the factors in the
performance of the utilizationof these resources would be your
table sizes, your data sourcesizes, and the type of data
sources you have, like SAS datasets versus Excel spreadsheets
or relational database tables.They all make a difference.Let's say that we're trying
to reduce the capacityto store a data set, the
amount of storage it requires.We have this large table
with millions of records.You can compress it.Compressing might reduce the
size, let's say by 40% to 50%.However, when you use
that compressed data set,SAS has to uncompress it,
and that takes extra time,so that means you are boosting
up the CPU time requiredto process that data.Again, oftentimes, reducing
one resource increases another.So you have to decide what's
most important for you.Let's look at another example.Let's say that you're
trying to reducethe I/O, the number of
operations it takes to bringin data and write out data.You're trying to reduce
those operations.Well, one of the
things you can dois increase your buffer size.The buffer is your
unit of transferbetween storage and memory.So if you increase the ability
to bring in the buffer size,you have the ability to bring
in bigger chunks of data.That will reduce
the number of timesit takes to load all your data
in and write all your data out.But as a consequence, that
means you're taking upmy memory on your computer.Reduction in I/O typically
increases the amount of memoryyou're using because it
requires more capacityto house your data coming in.So you have to decide what's
the most important for you.Is it more important for
you to reduce data storage,reduce your I/O,
reduce your CPU time?You have to decide that factor.So the things that are
part of this equationwould be, again, your
tables, their sizes,the type of tables, the
data sources, you're using,your computer environment.How often you run your programs.Is it every day, every other
day, or just a one deal?You're going to run this
one program just for todayand not continue
it going forward.Are you processing a
small amount of dataor a large amount of data?Here's a multiple-choice
question for you.""If you decrease the
disk space usage""--for example, compressing--
""which resourcemight increase in usage?""CPU time, I/O, both, or neither?Our answer, I/O. A compressed
data set, for example,requires less storage space, but
it will require more CPU timeto uncompress that data
set when you're using it.You have to understand, you
need to understand your site'stechnical environment, such
as the hardware you're using,your SAS environment, the
operating system you're using,system load.Those things can come into play.They all affect the processing.You need to understand
your site's hardware.Are you running on
a solo computer,or are you running on a
network being share by people?Memory, how many CPUs you
have, communication hardware,the network bandwidth, storage.All of these things factor in.The number of users or jobs.You're all sharing a
server, for example.Maybe more jobs run at night,
or maybe just before lunch.Understanding your
site's operating system.The resource allocation, job
scheduling, I/O capability.The installed SAS
products you have.The number of CPUs for SAS
programs that are being used.The memory for the SAS
programs, the methodsfor running SAS programs.Some of you are
familiar with, perhaps,batch mode submission,
non-interactive submission,versus interactive submissions.If you're using an
interface, you'reused to getting your
responses right away,versus running a job at night.Above all, you need
to know your data.If you don't, there are
ways to investigate.Some of you have probably
used PROC CONTENTS.You can also use some
of these interfacesto look at the metadata
or properties of someof these data sources.But not only data sets--external files,
relational databases,like Teradata or Oracle,
Db2, SQL Server, PC products,like Microsoft Access, Excel.OLAP cubes, if you've
used those before.I currently use those.Information maps.And so on.The data can make a difference.Small, medium, large.The length of your program can
factor in, your complexity,the size of your data sources.I mentioned that.And how often you
run that program.Is this a one-shot deal, and
you've got a medium data set?Or are you planning to
process a huge table?Do you have a production
job that runs, maybe,three times a week,
or every night?Those are some of the things
you need to think aboutif it's worthwhile
investigating tryingto improve the efficiency.""To improve efficiency, on which
of the following should youfocus?""A, programs that read large
data sets; B, small programs;C, programs that read
small amounts of data;D, programs that run often.I would say definitely D
because that's like a productionjob, one that runs often.And then, A, if you're dealing
with many large data sets.And it may take a
while to process them,so you're trying to think
strategically the bestoption to maybe
combine the data,for example, MERGE versus SQL.So here are the two answers.Programs that read large
data sets, and D, programsthat run often [INAUDIBLE].Rules of thumb for
relational databases.If you're trying to do a query
against relational databases,it's better to let the
database do that task.Joins, ORDER BY, sorting.If that's the case,
I would chooseSQL over the DATA step MERGE.Because the database itself can
handle that more efficiently.I/O latencies.By limiting data
retrieval, you cancontrol of certain aspects,
such as reading only the columnsrequired for operations.Some of you may be familiar
with KEEP equal, DROP equal.Use the WHERE instead
of a subsetting IF.The WHERE is faster than a
subsetting IF, typically.And keep WHERE expressions
as ANSI standardso that if you apply it
against a database table,it will be translated to SQL
that the database table canprocess.And last bullet, avoid
correlated subqueries,and we'll talk about
those later on.So this is just some
guidance, rules of thumb.Going to transition
to benchmarking.So let's say I'm
thinking about three waysto process these large tables.I'm thinking about
maybe using PROC SQL.I'm thinking about using
DATA step with MERGEor maybe using the data set
with an index to do a lookup.So I've got some options, but
I'm not sure which one is best.We can benchmark, all right?Let's talk about when
benchmarking is necessary.We actually have a %Benchmark
macro that we will useto compare the
efficiency of programs.Maybe we have a couple
we're considering, wherewe want to compare it, right?When your performance
is criticaland you want to benchmark,
because this is some taskyou're going to do a lot--and let's go back to
the comparison of PROCSQL versus maybe a DATA step.Some of the things you
might measure or lookat in your log
with these things,like real time, user CPU
time, system CPU time, memory,operating system memory.Those are factors
that you can look at.And there is information
online in the help facility,the online SAS documentation,
that describes these things.Real time is like wall
clock time in instanceswhere there's a waiting period.So we both submit
jobs at the same time.It might process part
of mine, part of yours,and there's a waiting period.User CPU time would
be the amount of timeit took SAS to
process that step.System CPU time
would be the amountof time included for buffers.So if you're running
a data set, if you'resetting up some buffers
behind the scenesto hold data coming in, data
going out, those things.And then memory for the data
in the step you're using.Operating system memory would
include a memory for the thingsbehind the scenes, like
some of the buffer queues.So benchmarking is
trying to measureyour resources used for,
maybe, your program, oneor more programs.So we're going to use
these factors to lookat different techniques.When you benchmark, these
are some guidelines.So I'm trying to
contrast, perhaps,DATA step MERGE
versus using PROC SQLor using PROC SQL
ORDER BY versus PROCSORT, just as examples.So here are the steps
you need to follow,definitely recommended.Turn on the options
before benchmarkingthat you need to track more
performance [INAUDIBLE]..Test each technique at a time.So maybe I'll run
SQL all by itselfand then start a new session and
run DATA step MERGE by itself.Run each program 3 to 5 times
and come up with averageresource statistics, and
test each technique--again I'll use the example
of SQL versus the DATA stepMERGE--in separate SAS sessions.And when you finished,
turn off the optionsthat you used to track
more information.Also exclude outliers
in your analysis.There are some
system options whichare operating system-dependent.So for example, if you're
running in an IBM mainframe,there are options associated
with that environment,and still quite a few
mainframes are there.STATS, MEMRPT,
FULLSTIMER, STIMER.STIMER is the default.
It's already turned on,and it tracks real time,
CPU time, in the SAS log.FULLSTIMER is available for
you to turn on if you want.Especially for Windows and Unix,
you can get more information.By default, as I said,
STIMER is turned on.You can get the
real time, CPU timeit took for your task to run.FULLSTIMER breaks
it down further.You get real time, user CPU
time, and system CPU time.The user CPU time is
maybe the amount of timeit took that PROC
SQL step to run.System CPU time includes
the amount of timeto set up some buffers
behind the scenes.So you can turn more details
on in the OPTIONS FULLSTIMER.The default is no FULLSTIMER.""Which of the following""--
here's a multiple-choice--""should you not do when
you are benchmarking?""Let's go through the list.A, execute the code for each
technique in a separate SASsession.I don't know, that
sounds good to me.B, use an average of
resource usage valuesrather than the minimums.So don't run each technique
once and make a decision.I like that one.C, when you benchmark, make
sure the system is idle.That doesn't sound very good.D, include only
the SAS code thatis essential for performing the
task that you are measuring.A, B, and D sounds good to me.C is the only one that
you should not do.So key principles, again, in
reference to benchmarking.You do it when it's
necessary, whenyou want to compare resources.Is it worthwhile for
you to do analysisand compare utilization
statistics on different codingtechniques?You've got this code example
versus this code example.You're trying to see which
one's more efficient.Run multiple iterations
of each technique.And remember, we
already said this.Look at the average results--average CPU time used,
average amount of memory used.Use realistically sized data.So if you're comparing
two techniques,don't use small tables.Use what you would
typically use.If you typically use table
sizes with 80 million records,test it out that way.And run each iteration in
a separate SAS session.Don't run this
technique and benchmarkand then run the next
technique right behind it.It's better to isolate that.Here's a macro
program that we'vewritten for you to
compare programsthat you are considering.So there's several
macros, actually.The %Benchmark is the
driver macro that will callthe %Passinfo macro.It's a utility macro to make
the runs more machine-readable.And the %Benchmarkparse, that
parses performance statisticsfrom your logs to
prepare a report.Then the %Benchmarkreport,
which is a utility macro thatproduces the final report.So these work sort of
in conjunction together.But the %Benchmark is the
one you start out with,which calls the other
[INAUDIBLE] macro programs.The macros are all in
the AUTOCALL library.If you've heard of
the AUTOCALL librarybefore, the way it
works, we have a programwith the same name as the macro.For example, for
%Benchmark macro,there should be a program called
benchmark.sas that createsthe macro and pretty
much executes it.So this AUTOCALL library folder
is D Master Custom AutocallMacros folder.If you want documentation,
the %Benchmark macro,you can put as a parameter value
!HELP to get some informationin the long.Feel free to try this.When you're comparing two
programs with the same macro,you put the first program you're
comparing, second program.Now, &path is a macro variable
that we assigned the valueof sworkshop to earlier.So you could type in
or substitute sworkshopas the macro variable.The bottom line is,
I'm comparing sqm02d02to sqm02d03.sas programs.You'll get output.You get summaries of each run.Remember, we're trying to
come up with average numbers,so you run it more than one.And here is clock
time, CPU time,memory used by each technique.And you'll get a chance to
try this in the workshop.So I'm going to
start a new shareand go to my
virtual environment.sqm02d01 in the Workshop folder.And try to make this
just a little bit biggerso that you can
clearly see this.All right, so here's the macro
program that I'm calling,I'm putting the value as !HELP.And I'll run this demo.And you can see in the log
notes about this macro program.Then I'll run this macro program
against the two SAS programs,d02 and d03, to compare them.So I'm not sure we need
the content of the code,but let's say it's
legitimate code.I'm just comparing
two techniquesto see which one runs
more efficiently.Remember that %Benchmark macro
program calls other macroprograms.Right.And it's giving us, it
looks like, an HTML report.We're getting some
univariate statisticson things like CPU time.And if you eyeball down
towards the bottom,there's notation on outliers
excluded for the comparison.If you croll down, so scale
all the way down to the bottom,there's a benchmark score
for the two programsI want you to look at.And it shows you the clock
time used by both programs,and then it tracks the time
used by both for comparisonand how much memory
is used by both.Now, these two are
pretty close in results,but that oftentimes
will not be the case.So then you can see the
5 runs for each program,and each clock time,
CPU time, and memoryversus the average numbers.These are some of the
tables created that areused to perform the reports.All right, so you will
have an opportunityto try this in workshop.So I'm going to stop here
to conclude chapter 2.Thank you.All right, ready
to start chapter 3.So let's get started.I'm going to share first
screen of this chapter.We will talk about implicit
versus explicit pass-throughSQL queries.Basically, what we will do
is use the LIBNAME statement.I'm referencing an Oracle
database, and what we will dois try to specify options with
just standard DATA step, PROCstep code to
encourage the databaseto handle certain
parts of the query,or try to get the
database to handlecertain parts of the query.So we will also use
data set options,and then we'll talk about
explicit path-through.How can we write native SQL
code that the database canunderstand and do some
processing on its sideto improve performance?We will use focus PROC
SQL, and we will alsouse PROC FedSQL, talk about the
differences between the two.Now let's get going on setting
up my LIBNAME statementand talk about
implicit pass-through.All right, so we're going to
set up a LIBNAME statementto set up a library
reference, likewe would for a regular
SAS data library.But this time, we will
point to an Oracle database.We will be able to
retrieve Oracle tablesand use them as if
they're regular SAS datasets and procedures, like
PROC FREQ, PROC MEANS.We'll use the DATA step
against Oracle tables.Now, if you'd like to verify if
the database is handling partof the query, like a
WHERE, or maybe performinga function, a
summary function, wecan use the SASTRACE option to
help us determine that fact.You can look in the log
and look in the notesto see if the database
was able to handle it.Here's a LIBNAME statement.Again, many of you have
probably seen this.The libref is just a word,
8 characters or less.It begins with a
letter or underscore,and then you put
numbers in the mix.The physical location of
your data, if it exists,and options.There are a number of options
you can use, like the abilityto read only, give
access to the abilityto read so you can't modify
the tables, things like that.The SAS/ACCESS LIBNAME
statement is setting upa library reference to the
Oracle database or a Teradatadatabase.And you reference the
table's two-level namejust like you would with
a regular SAS data set.And whatever
authority is in place,the access will
honor that authority.So if you have access
to certain tables,read access, then you will not
be able to modify those tables.Here's an example.We have orion_db.And then, instead of
the default engine,we specify Oracle, because I
want to go through an Oracledatabase.And then the connection
information options,like the path, the
username, password,schema which is
associated with Oracle.And if the syntax is
correct, then in the log,we should see a message
similar to this.Libref ORION_DB was successfully
assigned as follows.The engine, ORACLE.Physical name, localhost,
where that resides.Here are the options.USER=, the Oracle username,
the password, the path.That's an Oracle server
name that definesthe HOSTS file or the DNS.And then the schema which
is associated with Oracle.So the SAS/ACCESS engine
allows us to use those tablesin an Oracle
database, just like itwould in the case of
regular SAS data sets.Now, as we do queries
in the procedure or evenin the DATA step, it
may attempt to tryto parse through the query
to the database to handle.And that's a good
thing, most of the time,because a database can
better do retrieval, WHEREs,and also use mathematical
functions, summary functions,if they are able to be
converted to the Oracle version,an equivalent version
on the Oracle side.All right, so here
are some notesat the bottom just
emphasizing what I said.If operations cannot be
converted to the DBMS,like Oracle SQL, then SAS
will have to handle them.So we'll bring the
data down to SASto do, maybe, some filtering
or to create some columnsor to use functions.That can have an
impact on performancebecause you're bringing
down a huge Oracletable, because your query
did not allow the databaseto handle it, passing
data down to the SAS sideto complete the query.All right, let's look at that
first example, syntax-wise.I have a PROC MEANS, and
I have a WHERE statement.In the PROC MEANS, I'm asking
for certain statistics,MIN MEAN MAX, on total retail,
broken down by customer ID.Then we have a filter, where
year of the order date is 2010.Now, the year function will
extract the 4-digit year.Now, keep in mind order_fact
is in the Oracle database.Let's see if the Oracle
database can handlethis query against that table.If we have the
SASTRACE option on,we will get notes in the log
indicating what happened.And as you can see highlighted
in this sort of blue color,it was able to transition
this syntax in such a waythat the database can handle it.So the Oracle database
has an equivalent functionthat it can use to extract
a year from the order dateand compare it to 2010.So it was successful.The year function has an
equivalent Oracle function.Let's try to weekday function.The reason I have datepart,
because that extracts the SASdate from a datetime column.The datepart is the number of
days since January 1, 1960.Most of us know that story.And then we'll use
weekday to returna number between 1 and 7.1 is Sunday,
beginning of the week.7 is Saturday.And if the weekday is
7, I want those recordsto be processed to analyze
total retail by customer ID.So this time in the log,
because we have SASTRACE on--I haven't shown
you the syntax yet.We will get to that,
but there's a warning.The WHERE clause contains
SAS-specific syntax that cannotbe passed to the database.That's good.I would want to know that.Because I may be able to
change the syntax suchthat the database can handle
it, can make it more efficient.This is good to know.These are different functions,
or capabilities I should say,that the database, the
Oracle engine, supports.Functions like COUNT and MIN,
the LOG function, FOR function,CEIL function, average, standard
deviation, UPCASE function.So the Oracle database can
handle these functions,or it has equivalents
on its side.So oftentimes, the same result
can be produced different ways.But our goal is trying
to get the databaseto do the work for us instead
of passing it to the SAS side.In most instances, it will
be better for the databaseto handle it efficiency-wise.All right, especially when
you're writing a WHERE,because a WHERE is
filtering records coming in.So if you go against
the database table,the WHERE is applied,
it will control how muchdata is passed on to SAS.If it cannot handle
the WHERE clause,then all of that data
comes out in SAS,and SAS will filter
it on its side.So let's think about this.Selecting Supplier_Name, Count.Then the star is just
going to count the records.AS Products FROM the
product_dim Oracle table,and I'm filtering.From this table, product_dim,
I don't want all of the data.I want to filter
only for the recordswhere the supplier name
contains the word ""Sports.""Now, if you haven't
used scan, scanscan scan the value of a
column from left or right.If you use a non-negative,
an absolute value number,it will search
from left to right.If you put a negative number, it
will search from right to left.Well, all I want to check is
the last word in the string,is the last word ""Sports.""So I do a scan of
supplier_name, negative 1.Well, how do you know it's
returning the last word?Because it's looking
for a delimiter.There are default delimiters
if you do not explicitlyspecify one, like a
space, a period a comma.Those are default delimiters.But it works.We're checking the
last word to seeif it's equal to ""Sports,"" and
we do a GROUP BY, because we'redoing a count, and order the
Products in descending order.Well, let's verify if SCAN was
translated to an equivalentfunction in the database.Well, we get the
result. Check the notes.Unable, OK, unable to convert
the query to a DBMS-specificSQL statement due to an error.OK, well, it's basically
saying it wasn't able to do it.SAS will do the processing.So we get the supplier
name from here,but the WHERE was
not applied, whichmeans all that data from the
table has to be passed to SAS.All right, just
an idea exchange.How could this
WHERE be rewrittento improve the
performance of the query?Remember, we have
scan supplier_namegetting the first word
from right to left,looking for ""Sports.""Hm, what about
the LIKE operator?Some of you have used
the LIKE operator.So we will go with
LIKE because itcan be translated to
an equivalent operatorfor the database
to handle, wherethe supplier_name is like--if you have not
used LIKE before,you can type the text you
want it to search for.It's looking for a pattern
you can use a percent symbolor underscore to
indicate characters.The percent symbol means
any number characters.Underscore means any
single character.So the way this works, I'm
asking for any characters,a space, and the word ""Sports.""So ""Sports"" is at the
end of the string.All these characters precede it.All right, we get the same
result. Check the log.All right, compare, take a look.Here it is.That was submitted to
the Oracle databaseto process that table,
where SUPPLIER_NAMELIKE percent Sports.So it was able to
use that functionto have the database
performed that task.So here's the setup poll.Where would this where be
handled by the database tofilter against the
Order_Fact Oracle table.If you think about
it, I would imaginethat most databases would
support an equal comparison.So the answer is true.Let's move on to 3.2
and talk about dataset options with
implicit pass-through.We will use some of
the data set optionsand use SAS/ACCESS
data set optionsto a regular SAS data set and
options with an Oracle table.Some of you have seen the
KEEP equal or DROP equal.You don't use both.You keep what you
want or drop what youdon't want in terms of columns.It can improve
performance at times.If you have 397 columns in a
table, and you only need five,why read all of
those other columns?You can keep the five you want.That can improve efficiency.Let's take a look at
the DROP equal option.You either drop what you don't
want or keep what you want.The asterisk denotes in SQL
that I want all of my columnsfrom the product
dimension Oracle table.But next to it, I'm using
a SAS instant option,DROP equals supplier colon.The colon indicates
every column thatbegins with the word supplier.Supplier ID, supplier country.Where the product_name LIKE.And here's the LIKE operator.And we already confirmed
that the LIKE operatorcan be converted to an
equivalent on the databaseside, right--or used on the database.And what about DROP?Let's see if there
are any columns thatbegin with the word supplier.Down below, the answer is
no, so apparently it worked.Instead of the star
and then a drop,you can be explicit and tell
it which columns you want.You don't put a comma
after the last column,but that list
requires more typing.This compared to this, you
get the same exact result,and there's no guessing
the database will certainlyhandle this.All right, I'm going to
bring in sqm, this program,03a02 and submit it, check
the log, and see what effect,if any, the DROP equal data set
option produced in the DBMS SQLquery.I'm going to transition to
my virtual section, a02.And I'll make this just
a tad bit bigger so youcan see it a little bit better.And notice I'm using the
sastrace after you finallysee it.sastrace location, put
the notes in the log--this option will make
the notes more readable.And here's my proc sql SELECT
all the columns with the LIKEoperator, and then in the second
SELECT, I use the DROP equals.And then I turn trace off so
it doesn't continue to checkand give notes in a log.All right, we get two reports.Here's the first with the star.Prepared.You can see the SELECT expanded
with all the columns, evena supplier, we
supply a country--those that begin with
the word supplier.And check it out.Even the WHERE
product name LIKE,we already verified LIKE
would work by the database.And then in the next one,
I'll choose all of the columnsbut DROP supplier.So as you look at
this Prepare SELECT,it does not include
anything supplier.So both work.Both were handled by the
database, which is good.That's what we want.So I'm going to switch
back to my slides.What portion of the
SQL query submittedto the database was modified
by the DROP equal option?And I think you saw that.It wasn't WHERE.It wasn't FROM.It was the SELECT where you
select which columns you want.That's the answer.All right, so there are data
set options like KEEP equal,DROP equal.Well, SAS/ACCESS
data set options alsoexist for the Oracle
database, like DB, DatabaseCondition, READBUFF, ORHINTS.The DBCONDITION would allow you
to specify an SQL clause that'sprocessed by the database.READBUFF will control how many
rows to read into the buffer.That can potentially
decrease network activityif you are reading in bigger
chunks of data at a time.And remember, when you
improve one resource,it typically goes
against a different one.So you think about it, if
you're pulling in more data,you might be taking
up more memory.And then there's ORHINTS.That influences the decision
made by Oracle, the Oracleoptimizer, rather to
have the database processpart of the query
or all of the query.And then these
improve efficiency.Let's go back to the WEEKDAY
story, against datepart.And here are the notes.Can note be passed
through the database.We saw this earlier.Even though it cannot
translate WEEKDAY,there are some specific
DBMS, SQL, if you know it,if you understand it, that
you can specify to improvethe processing.All right, so
we'll use some dataset options to improve the
performance of a PROC MEANSon the SAS platform and then
replicate the PROC MEANSfor the database
to do that task,explicitly passing it
through the database.Let's try these options.The DBCONDITION
allows you to passdatabase specific,
like Oracle specific,WHERE clause for filter.All right, so give it a shot.Oh, also notice you put
it in quotation marks.So here it is.Next to the Order_Fact
Oracle table,I've put DBCONDITION equals, and
in quotation marks is basicallywhat I want--where to_char
order_date D equals 7.Basically, I'm looking
for the day of the week 7,which is Saturday.Well, this is more specific to
the Oracle database syntax, notthe SAS syntax.So if you understand this,
you can type it yourself.If you know the ANSI
standard for Oracleand its extra features, if
you're familiar with it,go for it.That's what DBCONDITION
will allow you to do.So it says Oracle prepared
SELECT these columns WHEREto_char order_date D equals 1.That minimizes the movement
of the data from the databasetable down to SAS.DBCONDITION.As I mentioned
earlier, this optionyou can use to control the
amount of data coming in.How many rows in the buffer?You can increase
it or decrease it.It might result in
more memory usage.Here it is.Perhaps you could
speed things upby increasing that
readbuff number?This particular change
made almost no difference,and I've put in
readbuff equals 1,000.If I made it higher, it
might make a difference.With it.Without it.Look at the statistics.The clock time which
includes any waiting period.The CPU time.The time need to actually
perform the task.They're very close at
how much memory was used.There's not a big
distinction in termsof performance between them.But sometimes minimizing the
I/O can improve performance.Let's look at this
option, this dataset option ORHINTS and
the oracle hint text.So this might provide hints
to the Oracle optimizerto make a better decision
and speed the processing.So let's try this option.We put ORHINTS, and
there's the DBCONDITION.Highly parallel tables,
full table scanners,we call this retrieve data.All right.So let's look at
the speed boost.We went from without it, and
look at your real time, 0.48.User CPU time.System CPU time.And look at the
drop with the hints.All right?So it pays, that's
denoted about them,to know the specifics
of that database.If you are
comfortable, and you'vebeen writing SQL for
Oracle or Terdata,and you know that
syntax, go for it.You can use it.And this option can
improve the likelihoodof it performing your task.So what are the advantages
and disadvantagesof an implicit SQL pass-through?Well, here's a decision
made between SAS and Oracleas to who will handle.Well, first of all, it
proves transparent accessto the database tables.The DATA and PROC step syntax
looks as you've always used it.If you have knowledge of
the database-- in this case,Oracle SQL--it's not necessary,
because it will tryto translate things for you.Some SAS functions like
MIN, MAX will automaticallytranslate to the database.Remember that you had
YEAR, and we had WEEKDAY.YEAR worked.The YEAR function, it was
translated, but not WEEKDAY.And remember SCAN--
the issue with SCAN.All right?What are some of
the disadvantages?If you reference the Oracle
tables and the DATA stepwhen you're using
MERGE, then it will notbe passed to the Oracle
database to do a JOIN.The subsetting IF will
always be done by SAS.There are certain options,
if you're familiar with them,that you can try
to use to improvethe likelihood of the database
processing, especiallywhen you're pulling
down a lot of dataif you have large
database tables.So the idea-- if you
can, get the databaseto do some of that
processing on this side.That will speed things up.All right, 3.3.We go from implicit to explicit
with PROC SQL and PROC FedSQL.So here are the objectives.We'll take a look at
PROC SQL and PROC FedSQL.We'll look at some
of the advantagesand challenges of both.We'll do explicit pass-through
through both to the databaseand then do a review
at the end of both.So what is PROC SQL?Structured Query Language.It's been around a long
time, back in the '70s.That's when it was
developed to goagainst relational
databases, so there's historyassociated with it here.The latest version is based on
ANSI SQL2, the 1992 standard.And here are some
standards most vendorsfollow like using a SELECT,
using a FROM, using a WHERE.Can use it against
any SAS data set.Includes many SAS
extensions like dataset options, SAS functions
you can use with PROC SQL.You can use case logic
like TRUE or FALSE.It can process DOUBLE
and CHAR data types.And you can do
explicit pass-through.Check out the syntax.You can connect to the database.That's required.And then you issue your
typical ANSI standardfor version of SQL
for that database.If it has enhancements,
you can use them.If it has beyond
the ANSI standard,you can use whatever
the capability is.In this case, we SELECT
columns 1, 2, through n.So you put the syntax as
FROM CONNECTION TO DBMSthrough this one here.And then in parentheses, you can
indicate your specific databaseSQL query.You can also execute other
DBMS-specific statementsby that database, and when
you're done, DISCONNECT.So that's a pass-through.You're passing your query
directly to the database,so the syntax has to be correct.It has to be acceptable, but
you still will get an error.Let's go back to the
story once again.WEEKDAY.When we received a message,
it cannot be passed.Right?Remember that?So how could we improve this?We could use data set options,
to improve the retrieval,and PROC MEANS,
or we can rewritePROC MEANS in the
form of PROC SQLand do an explicit pass-through.Remember we did this earlier.We went against the Oracle
table Order_Fact readbuff1,000 records buffer time.The database condition.And we already passed
in the database.And this is how you type it
when looking for Saturday,but you have to know the syntax.And ORHINTS to
improve the likelihoodof it performing a task.That's one option.Here's the other option.We connect to the
Oracle database,put in our credentials,
then we select star.Now before we finish
that, go below.You see in blue, this color,
this aqua-green type of color?From this open parentheses
to that close parentheses,we select to do a query
against the database.For the customer ID,
we're getting statisticslike the count
min, average, max.These arguments assign to
each respective variablefrom this table, filtering
for Saturday, the groupby customer ID, and we
order by customer ID.So this results set, when it
comes back, pass to this selectand select all the columns
from that results set.And by default, we get a
report, and then we disconnect.All right.More on WEEKDAY in
the WHERE revisited.Now if we take a closer
look at this SELECT.This pass-through has both a SAS
portion and database portion.It is possible to leverage both.So if you look at
the SELECT, we'reusing a format z
for leading zeros.We're specifying Nobs, the
MIN, the Mean, the Max.We have labels and formats.So colon outside of
parentheses execution SAS.We are enhancing
that results set.You can do that.These are labels.This report is nicely formatted
because of the enhancements.Check out the SELECT
customer_ID COUNT, MIN and AVG,and those column names.From ORDER_FACT.Here's the result. Oh.So sometimes to try to
reduce data retrieval,SAS computes the
results faster maybe.The best thing to do,
as we've said all along,is the benchmark.So I'm comparing the
means.sas program versus SQLto get the same summaries.And we look at our results for
the benchmark map or program,a nice utility program, against
the means.sas and sql.sas.You look at the clock time.You look at the CPU time and
the maximum amount of memory.So that's why it's important
to still benchmark,as opposed to going
by your gut feeling.If this is the program you
plan to run routinely--like, as a production
job, it's somethingyou'd run almost
every other day,then this will be long term,
not just for a short period.It's worthwhile to put the time
in to evaluate the performance.So with PROC SQL, you
can query SAS data sets.You can query
relational database justto run the pass-through code
native to that database.But what about FedSQL?Well, it's new and
modern, highly scalable.It's part of base
SAS like PROC SQL.It is base SAS implementation
of structured query language.Based on the 1999
standard, ANSI standard.You can process data sets
to the library reference.Very ANSI compliant.So the guidelines you
use for ANSI standard--SELECT, WHERE, FROM.There are a few SAS extensions.Most queries from FedSQL
will be executed entirelyin the database.All keywords are reserved words.You can do ANSI type
logic that resultsin TRUE, FALSE, or NULL.Once again, we turn on
the sastrace option.We begin with the word options.Notes in the log to
make it more readable.And we plug in the
fullstimer option.Remember we talked about
this earlier in the course?It gives you more
performance statisticsin the log about your step.And in this case, that SQL,
let's take a closer look.I want to select the
Customer_ID, the Count.And I'm creating a column
called number of obs.Here's the comma, the
minimum, the average,the maximum retail price as
these respective columns.From Order_Fact, which
is an Oracle table,here's WEEKDAY again looking
7, and group and order by,then return trace off.And nofullstimer to turn off.Fullstimer for
tracking performance.Despite engaged in
the sastrace, wedon't have an indication that
the database did any better.Both procedures, SQL and FedSQL,
both support undocumentedprocedure invocation
option _METHOD.You can look up more
information in this way.It's underscore METHOD option.So if you invoke that option,
you will get more information.Underscore method.You do the same task
against this Oracle table.And we get information about
the aggregation and sortingperformed by SAS.Now if you want to guarantee
the query against your tablecan be handled by
the database, youcan do an explicit
pass-through to guarantee that.All right.So let's take a look.Similar syntax to PROC
SQL to some extent.Now you don't see a connection--the connect that we
saw with the PROC SQL.If you look at the
slide here, youcan see it points out that
it makes a connection,and DISCONNECT is not required
because it's using metadatafrom the library to
make the connection.You can do an
explicit pass-through.You can also execute
DBMS-specific thingslike granting only
permission to the table.So here's PROC SQL.We put the underscore
method here,so we get more information
about the methodused to do the retrieval.I want to look at what's in
the inner parentheses first,because once again,
this is what'sgoing to the database
against that table.I want the Customer_ID,
the Count, these statisticsfrom ORDER_FACT.Look on Saturday.Group and order by.Whatever you get from
this will be a result set.That results set,
from that, we'reselecting all the columns from
that connection to orion db.So this libref
says it's pointingto an Oracle database that
determines the platform wherethis would be executed.And again, you have to type
native syntax for that databasein these parentheses.So you use the METHOD
output, and youcan see in the notes
and the log methods,the task was performed
by the databaseand process
[INAUDIBLE] So what aresome advantages of SQL versus
FedSQL for explicit SQLpass-through?The database can optimize
the summarizationin filtering,
ordering, and joining.It can optimize the
use of functions.SAS and DBMS
features can be used.SAS features that
can be translated.We can run DBMS stored
procedures and macros.If you have authority, you
can execute a DROP, a GRANT.You can DROP a table.You can GRANT authority
to read a table.What are some disadvantages
of explicit SQL pass-through?The pass-through
facility requiresDBMS-specific knowledge.You have to know
what you're typing.Only SQL code within
the parenthesesis passed to the database.Well, that gives
your results set.So as long as you
understand how to write it,it can help you efficiency wise.Welcome back.We are in chapter 4.The objectives of
chapter 4, first thingis correlated subqueries.Secondly, we will talk
about cross-library options.The idea of
cross-library optionsis in reference to
combining data sourceson different platforms
or different typeslike relational database
versus a PC file.And correlated sub queries
are queries that cannot workby themselves, so let's
take a look at both.So I want to explain how a
correlated subquery works,and talk about the implications
of using a correlated subquery,and then talk about
some alternativesto using correlated subqueries.They cannot be
evaluated independently,and what that
means is that the--when you try to do a SELECT
using that type of query,it doesn't return a
result on its own.The way it works, you
have an outer query,and you have this, the
correlated subquery.The outer query
must work in orderfor the correlated
subquery to work.Let's talk about that.Here's a case study.Let's say that I want a
report listing the customerand product identification
numbers for products orderedby German suppliers,
where Supplier_Countryequals DE as an example.Now we were actually
looking at France,but I'm just giving you an idea.So here's some
things to consider.You have three tables.You have a product
dimension tablewith product information
such as the product name,and a product ID, who
the supplier is-- well,the supplier for that particular
product ID, the suppliercountry.And the Order_Fact, the
Big_Order_Fact table, wehave order information
along with the productID of the product
that was ordered.In addition, we
have the customer IDthat identifies the customer
that placed the order,and more demographic
information about the customeris in Customer_dim.More information about the
product is in Product_dim.So there are some things we
want from some of these tables.We want the Customer_ID,
and we want the Product_IDfrom the Big_Order_Fact
table for the orderswhere the supplier is in
France instead of Germany.So we write it as follows.We put where the literal
FR is equal to whateverwe get from this query.Now what defines us as
a correlated subquery,because it's sort of related
to the outer query in such thatthis query cannot
work by itself.Let's take a closer look.I want the Supplier_Country from
the product dimension table.That's fine.It's in there.But I only want the
Supplier_Country WHEREthe ID for each record in
the Big_Order_Fact tableis found in a product
dimension table,and the Supplier_Country
return is equal to France.Now this query within
the parenthesescan not work by itself, and
the reason is the product_IDfor the Big_Order_Fact table
is not on the FROM clause.It's not in product dimension,
so it cannot work by itself.However, what happens
is that each recordfrom the Big_Order_Fact
table, this IDis searched for in the
product dimension table,and when it's found, the
Supplier_Country returns,if it's equal to
France, they willselect the Customer_ID
and product_IDfrom their results set.Wow.It looks like this took
quite a bit of timebecause it's an inefficient
way to do this look-up.This is how it works.Because this query cannot
work by itself, this subquery,this correlated subquery, what
happens is each record fromBig_Order_Fact is selected, and
this inner query looks for itsID in the product dim.So for example,
let's look for 95.It's in here, but it has US.Finds a match.Returns the US.The US is not France,
so that's a false.No output.Let's look at the next one.516.We look for it.There's a match, but
once again, whateverit returns for
Supplier_Country is not France.False.No output.Finally, once again because this
query cannot work by itself,it reads the next sequence
of record from Big_Order_Factas 010.And as you can see, it's here.First one.It has FR.Finds a match.FR's value for the
Supplier_Countryis equal to this
value, and now wecan output the Customer_ID
and the product_ID.That's how it works.So you can imagine the
bigger the order_fact,the more time this will consume.Not necessarily efficient.It processes every row
in the outer query,meaning for a Big_Order_Fact.So here's a quiz.Take a close look.What is the maximum
of rows that will beselected in this outer query?How many records in the
Order_Fact, Big_Order_Fact,will be selected from this inner
query when we look for its IDcompared to the
ID in Product_Dim?Two.10 and 62, those are the
only two, that match,that have France.Very good.So this is the results set.Those are the matches, that
were found, that have FR,but we only selected a
Customer_ID and Product_IDfrom the results set.There's got to be a better way.Because if you have
almost a million records--let's say 951,669.Each one of these records
in the Big_Order_Factwill be read sequentially
and looked upin this correlated
subquery that'srelated to the outer query.That's why we call
it correlated,because this cannot
run independent.It will work once for every
row in orion.Big_Order_Fact.Not too efficient.I can do this with
an inner join.I can do this with also a
non-correlated subquery.So if you have a better
option that's more efficient,go for it.In this example, the
correlated subqueries,query is very inefficient.The results set, either than
951,000, there's only 24,28 that match large to small.Two better options in this
particular case, the inner joinand the non-correlated subquery.Let's take a look at each.Here's the inner join.It's real simple.Remember, I want the
Customer_ID, the Product_IDfrom the Big_Order_Fact and the
orion product dimension tablewhere the product IDs are equal.And it's a simple
filter for France,for the Supplier_Country.Piece of cake.Look at the
difference in CPU timeand real time compared
to these numbers.Nice savings.Another alternative is to do
a non-correlated subquery,because this one
can run independent.Let's think about
what we're doingin this non-correlated subquery.I want a list of the product
IDs from product dimension wherethe Supplier_Country is France.So build a list of all the
product IDs for France,for the Supplier_Country
of France.Then in the outer query,
I will read each recordfrom the Big_Order_Fact
and checkto see if this ID
is in that list.So once again, we
see the savings.So where is a correlated
subquery used?Well, in this
particular example,if you're trying
to update a table,and a join is no good here.A non-correlated
subquery is no good.Well, we're running a
correlated subquery this timeto update a table.There's an update
clause that you can use,statement against a table.So we have price_list,
and we have Price_Updates.Price_Updates contains
the transactional data.They have updated
information, new information.So I want to get the new
prices from Price_Updates.So for each record
from price_list,I'm going to set the
price equal to what Iget from the results set here.I'm selecting the new
price from Price_Updateswhere the Product_ID
in Price_Updatesis equal to the Product_ID
from price_list.So this is the correlated
query, subquery,because it appends some values
from here to do the look-upto make a change.Where the Product_ID is in--look at the subquery--SELECT the ID, Product_IDs
from my_sas.Price_Updates.So this constructs, with
the correlated subquery,the ability to
update this table.There are other techniques.I mean, this works.Time is pretty good.There is an UPDATE statement.You've seen perhaps SET, MERGE.There's also UPDATE, where
we can take the original dataand update it with a
transactional table,either adding values
or replacing them.So how does it know
what to replace?It's based on your
by state, Product_ID.So these tables
have to be sorted.Now what if the variable in
updates is called New_Price?Well, in order to
apply the updatethe price within
price_list, we haveto rename New_Price to Price.Then we keep the other variable
[INAUDIBLE] 0.01 seconds.It's faster than this.So some of the key points.A correlated subquery
cannot work by itself.Now it's good for some
things like updatesand maybe a few other things.But often times, it requires
additional CPU and memory,because you may be
reading the outer tablein that outer query.For each record in
the outer query,you look for a match
from the subquery, whichis a correlated query,
so that's time-consuming.Think about some of
options we consideredlike doing an
inner join or usinga non-correlated subquery.So that's pretty much
the summary of this.Correlated subqueries are
good in some instances--for example, updates.But if you're using for
other lookup techniques,the inner join or the
non-correlated subquerymight be more proficient.The last section of this chapter
is cross-library operations.The idea here is
using data sourcesfrom different platforms, or
different types, I should say.We will look at three techniques
for dealing with this situationto mitigate
cross-library options.So we'll define what it means.To give an example, a business
analyst has an Excel workbook,and it has a list of
customer ID numbersfor which we would like
demographic information.Plus there is big_customer
dimension table in Oraclethat has information we need.So these two need
to be combined.Where you're pulling
data from SAS librefsacross multiple platforms
or from the same platformwith different credentials,
this is considera cross-library option.If you have one or
more tables thatare large, that's can be time
consuming across the network.If you're moving data down to
SAS, the bigger data you have,the more inefficient
this can be.Let's take a look.Look at the blue section
I'm highlighting.I have the big_customer
dimension table in Oracle,or it could be a SAS data set.And I have an Excel
spreadsheet, but in this case,remember we were talking
about trying to havethe database perform well.So this is actually
a database table.And we can see the
time it uses, but whathappens, this cross-library
reference with a databasemanagement system table
plus a PC product, that'sa cross-library reference.It works.The time looks pretty decent.If you check the log the
data from both tableswhen moved to the SAS
platform, the processto join, basically,
what Oracle didwas just control which columns
were retrieved in rows.An alternative for this
cross-library operationis to take that XML
spreadsheet data.Put it in the Oracle database.That's the table.Then join those
two Oracle tables,so the database can better
handle the results set.But you need read
and write access.The administrator, she
or he, has provided youwith those credentials,
or those capabilities,that you can create a table in
Oracle from the spreadsheet.In the virtual lab,
if you use that,the my_db is assigned to
a student schema in Oracledatabase, and that student
has read and write access,which means she or he can create
an Oracle table from an Excelspreadsheet like this.We're going to drop the table.In the case we created it
before, we added more customersto the list.I want to just recreate it.Create table my_db database
SELECT customers asfollows columns from
the spreadsheet.So now I have two Oracle tables.SELECT customers in
Big_Data Dimension, right.So I've created the
table I really want.Customer info as follows, the
name, customer name, country.Customer type ID, birthday.From orion
big_customers Dimensionand that newly created Oracle
tables, select_customers.Now the database
can do the join--can do that inner join--and do it on the
database side insteadof passing all that data
across the network down to SAS.It's much more efficient.You can see the
SELECT looking at log.And look at the engine node.SQL statement was passed to the
database requesting the data.That's a good sign
in most instances.If we benchmark, we can see
there is a drop in CPU time.The clock time is a
little bit more here.A little bit more
memory used, but thereis a drop in the CPU time.Always good to benchmark.And follow some
of the guidelineswe talked about in the
early part of this course.You can use this
benchmark macro.If you don't have write
permissions to the database,well, should you do?Well, we can do what we're
calling the pseudo-join.And what we essentially do
is take that smaller table,or spreadsheet.Build a format with it and
allow SAS to do a format lookup.So for the code, we can look up
the descriptions we really wantor the label feature.You need the following.You need a format name.You need a start
value to be looked up.You need a label--will be the value to be
returned for the start value.And the HLO indicates
that the start valueis a key word
high, low, or otherinstead of an actual value.So here's the
creation of this tablewe need to build a format.Remember what I said.You need format
name, start label.And we're going to make
this one for the high, low,and other values.There we go.Create a table.Call it control.Call it whatever you want.And we need a format name.I'm going to call it fakemerge.It's like a psuedo merge.I'm going to put the ID number
in 16 positions as a stringand then store it in start.That's the value
to be looked up,and then I'm going to
assign a label, Take me!for all of these customer_IDs.That's the label value.That's what will be returned.But what if it's missing out?Quote blank quote, that
indicates missing, as HLO.So what I'm going
to get with an HLO?Well, that's the conclusion.I build that table,
but I insert one more.Insert into control, the
insert clause, or statement,will insert records.And there are different ways
you can supply the records.I indicate positionally which
columns will get the values.These four.Format name, start label, HLO.And here's the values.The clause to indicate
fakemerge goes to format name--the same thing we did.Let me go back--we did here.And then the start
value would be other--anything not specified, meaning
those Customer_IDs and then Notme!HLO would be O for other.That's not the end of it.You create the table.And now that you have the format
name, the start, the label,you inserted an
additional record,you can use Control N
to point to that tableand get the format created.And I'm using what's called
FMTLIB to give me a report.So I can see, for
these Customer_IDs,what label gets returned.Other keyword is Not me!Just to give an
idea of somethingyou can do as a possible option.So it's kind of like
a pseudo lookup join.We're using a format
instead of a table.You select the columns from the
Oracle database, customer_dim.WHERE to put the Customer_ID
based on the formatequals Take me!So any of these IDs will be
able to Take me exclamation.The generated SQL code was
pretty strange, but it works.If I compared the two
options, the pseudo approach,I save a little bit
more time CPU-wise.So I like the option
so far that loadsthat smaller table into
Oracle, that XML spreadsheet,and do a join by the database.The pseudo join is good if you
don't have a lot of values.Another option is to use
DB queue, database key.And what you do is
basically specifywhich key column you
want to use to performa lookup in the Oracle
database or a database table,and that might
improve performance.So for each record
in a small table,look up a match
in a bigger table.Database key.Either a singular or
plural multiple ones.And it takes the value
from the smaller data setone record at a time, builds
an internal WHERE clause,and executes a series of
SELECTs to go find the match.This can be pretty good.Let's take a look.Create a table
work customer info.And we have the customer name.We have Customer_Country,
Customer_ID, birthdate.And here's our FROM clause.From orion_db.big_customer
dimension.And we tell it to
use the database keyto perform a lookup.And here's that smaller
table, that XML spreadsheet.So for each record,
do a lookup basedon the key where the
Customer_IDs are equal.This is the data set option
that we used for this table.And let's look at the results.Check it out.One repetition for every key
value in the smaller table.In other words, to
perform a lookupin that larger table for
every key value in a smaller.So if you have a
few values, not bad.When we have a table
that's almost the same sizeas the one you're
looking up the match in,the database table, then it's
going to not be as efficient.The smaller to the
larger, the betterif you have that scenario.Then equal size, and
you can benchmark.So in this case, this
technique, we benchmark.And we get some good
savings on CPU time.Look at the drop.Also clock time.Using the DB queue,
the database key.OK.We are ready to take
a look at the demo.Let me share my screen and go
back to my Enterprise guidedsession, and we are going
to take a look at SQM 04 D09program.This demonstration.In the comments, we have an
example of the options sastracewhich we've talked about.With these options,
we will have noteswritten to the log
that will indicateif the database was used
to do a query against oneor more tables.And a reduction of
some of the notes.Here's a percent let
path equals s workshop.And we have a percent
include, whichis used to reference
a SAS program.This particular SAS program is
libname.sas in the s workshopfolder.And it has a libname
statements for setting uplibrary references.You've seen this before.Then I have a libname
statement setting upa library reference, Excel,
using the PC files engineto point to the xlsx workbook.And here comes the PROC SQL.We are creating a table called
work customer info as follows.I want the customer
name, country--which I'm defining as
Customer_Country-- customertype, ID, the birth date.The data is coming
from two data sources,this orion_db.big_customer
dimension table.And notice that
we are making useof the dbkey we talked about
to reference the columnCustomer_ID such that, for
the records in this Excelspreadsheet, we would
only need to look upthe matching records for these
smaller number of recordsfrom the spreadsheet within
the bigger Oracle table.So don't read all of the
data in the Oracle table.Only look up those records
that match based on this key.Those records in a
small Excel spreadsheet.Again, here's the
WHERE conditionindicating what's matching.The Customer_ID in
the Excel spreadsheetequal to the Customer_ID in
the big_customer dimensiontable in Oracle.And then we cleared a library
reference for the Excel.And this is just a reminder to
turn off the sastrace option.Let's run this.Here's the table.You see the four columns
and the eight rows.Let's check out our log notes.As I scroll down, you can
see the path assignmentof s workshop.Here's the percent
include libname.sas.And as a result, they set up
all of these library references.Following that, we
have the PROC SQL stepwhich select these
columns to create custinfofrom the Big_Customer dimension
table in Excel spreadsheetwhere the Customer_ID
in the Excel spreadsheetis equal to the Customer_ID
in the database table.And here's the notes
pertaining to the eight rowsthat you observed earlier.The last option is the
multi data source optionthat can help performance
for a cross-library joinbetween a large database
table and another table that'snot located in the database.So this can improve performance
when joining a large databasetable with a small table.On the libname statements where
we put the option multi datasource to inform
option IN_CLAUSE.So again, if you
have a large tableyou're combining
with a small table,it could be an Excel
spreadsheet with a SAS data set.This option could
help performance.Without this option,
this is what happens.With this option,
notice again it'sfiltering, for the bigger table,
just those records that matchwhat's in the smaller table.Only these rows
are passed to SAS.Without the multi
data source option,we'll pull the data from
the Big_Customer_Dim.Extract the SAS.And then compare it to
the Big_Spenders table.And with this option,
the only recordsyou get for that
big table are thosethat matched the records
in that smaller tablebefore extracting the SAS.It makes it more efficient.We benchmarked.Here, they look pretty close.As I said, table sizes
make a big difference.That's why it's always
good to benchmark.Just some key points.Joining a data source from
librefs on different platformsis a cross-library
reference typeof cross-library operation.We have a small table
and large database table.You might consider--I think it's a good option
to copy that smaller table,Excel or SAS data
set, into the Oracleor whatever the database
is you're using.I like the DBKEY option.The pseudo join, if you
understand how to dealwith formats, can be useful.And we just took a look at
the multi-data source optionwhich can improve performance.That's the end of chapter 4.Just a couple of
points about chapter 5beyond what you
observed in this class.There are a number of things
that SAS offers support wise.You might consider
joining a user group.There's information online
for technical support,professional
services, education.Of course, more
courses we offer.There are over 200
different courses--programming, statistical
courses, intelligence courses.You can go to our link here.For certification,
worldwide recognition of SAScertification in different
areas and different subjects.Networking with social media.Interacting with
SAS users and staff.Ordering books online.E-books, publications, go
to the publishing site.Beyond this course, there's
your free version of SAS.It doesn't have all
of the capabilities,but enough if you want
to get some experience.And then SAS communities
for sharing ideas and askingquestions.Global forum, which
you are aware of.And there's online documentation
for SQL, FedSQL, SAS/ACCESS,and database products.Beyond this course, we
recommend the SAS/ACCESS to PICfiles course processing
databases and PC file data.Programming 3, if
you're looking to tryto improve the efficiency
of your programs.Those are some of the
stuff we recommend.And that is the
conclusion of this course.I want to thank you very
much for taking this course,and we hope to see
you in the future."
32,"JACOB NOBLE: This presentation
is covering research involvingunsupervised clustering of
National Science Foundationfunding proposal abstracts.My name is Jacob Noble .I have an undergrad
degree in physics.I spent some years
doing material science.And currently, I am
a software engineeron an AI operations team within
a data science department.HIMANSHU GAMIT: And
I am Himanshu Gamit.I studied computer
science in my undergrad.I have about five-plus years
of software engineeringexperience.And currently, I am a
data science intern.And we are both data
science graduate studentsat the University of St.
Thomas in St. Paul, Minnesota.Now to understand the importance
of this research better,we must go back to 1950.In 1950, to utilize taxpayers
dollars in an efficient way,the National Science Foundation
was created by Congress.This meant to promote
the progress of scienceand advance the nation's
health, prosperity, welfare,and to improve national defense.NSF is vital because they
support basic researchand people to create
knowledge that transformsthe future of the nation.They fund advanced
instrumentation and facilities,including posting in Antarctica
and other national researchlaboratories.Besides, as
researchers ourselves,we are familiar with the
time dedication and hard workrequired not just
to study our topic,but also to find alone
studies to build upon,which led us to investigate
the National Science Foundationdata to find out how
research gets funded.They evaluate about
50,000 proposalseach year, with the
year 2020 fundingbudget being $7.1 billion.This funding will go on
to support about 8,000to 12,000 research grants
for many grad studentsand postdocs.The data collected from the NSF
included about 330,000 fundingproposals going back to 1985,
which is about 34 years now.This data was acquired
through the NSF awards API.We brought a Python
wrapper around a REST APIto establish a data pipeline to
collect newly-funded proposals.Each proposal from the API
included information regardingthe awardee, agency, and funding
information such as the fundingdebt and funding amount.What was interesting to
us was that the API alsoprovided proposal abstracts.And these are the abstracts
used in this study.Now, how much
effort do you thinkit would take for NSF
to review and chooseas many as 12,000 proposals?Well, it does take a large
amount of manual effortto evaluate about 50,000
research proposals each year.And to assess the invaluable
quality of the proposal,NSF follows strict
guidelines all the waythrough the process.This data means the intellectual
merit and the broader impactof a research proposal.And each proposal requires
multiple reviewersthat are experts in
their specific field.And in addition,
the proposals mustbe compared to previous
and current research.And the reviewers must
determine which proposalswill have the greatest
potential and bethe most fruitful enrichment
for taxpayers' dollars,which creates the challenges
selected by scalability,consistency of the project
weighting across awardees,and the identification
of projects whichrequire special assistance.Clearly, this effort
does not scale well.So how can we use these
proposal abstracts?And here, we set out
to develop a methodfor grouping similar proposals
together based on its context.And that will help reviewers
identify similar proposalsto the ones they're
reviewing basedon textual contexts from
deep past 330,000 proposalabstracts.JACOB NOBLE: Before we
can do any real analysison the proposal abstracts,
we have to first transformthat abstract into numerical
data that could be processedby a machine learning model.To do that, we used
a document-to-vector,or a Doc2Vec model.Before we can dive into
what the Doc2Vec model is,we first must talk about
the Word2Vec model.This is a neural
network that triesto predict a word by using the
surrounding words as input.The surrounding words are
what provides the contextfor that prediction.In this example, ""I packed
my"" blank, ""it is raining,""it is obvious to us that
the missing word heremight be ""umbrella.""But a Word2Vec model
has to see many examplesand learn from a large
collection of textsbefore it is confident
in a prediction.With enough training and
textual context to learn from,it could predict that
the missing words heremight be umbrella based on
the context ""it is raining.""To prepare the collection
of abstracts, or corpus,for training this
model, we firstbuild the corpus vocabulary
by assigning a number or indexto each unique word.The word in the
corpus text is thenreplaced with its index number.This rudimentary numerical
representation of the textis then used to train
the Word2Vec model.Once the model is
trained, it canbe used to convert words into a
high-dimensional vector space,or word vector.This is often referred
to as a ""word embedding.""There are many other types of
word embeddings such as bagof words or term
frequency inverse documentfrequency, TFIDF, but
these fail to capturecontextual information
about the words,as they mainly just count
how many times a word appearsin a document.To properly group similar
abstracts together,we need to take
this a step furtherand embed the entire
abstract or document.How do we go from word
vectors to document vectors?The data prep and training
process is the same.The model is still
learning to predict wordsbased on the surrounding words.The difference is
that the embedding nowhas an additional
vector attached to itthat represents the document.And that helps with
the prediction.In our case, the document is
the funding proposal abstracts.It is important to
note that in boththe Doc2Vec and Word2Vec model,
it is not the final predictionthat we are interested in.It is actually the layers
before the predictionwhere the words and document are
transformed into vector space.These vectors are what provide
the weights for the predictionand are the actual word
and document embeddings.In this study, we embedded
the proposal abstractsinto vector space
with 300 dimensions.HIMANSHU GAMIT:
K-Means clusteringis an unsupervised
clustering algorithmthat we use to cluster
together our documentvectors from the NSF
proposal abstracts.K-Means will group together
n items into k groupsbased on the Euclidean
distance between the points.Tuning the K-Means
clustering algorithminvolves assigning data points
to its nearest cluster center.The initial cluster centers
are assigned at random.Once all the data points
are assigned to a cluster,a new center for each
cluster is calculated.The process then starts over
and reassigns the data pointsto its nearest cluster center.And this repeats until there
is no significant changein the cluster center.And for this study, we
clustered the document vectorsinto 200 clusters.Now that we know about the
Doc2Vec model and K-Meansclustering, we can
take a larger lookat the architecture
used in this study.At first, all the
abstracts are fedinto the Doc2Vec neural
network to extractdocumented embeddings.And then these embeddings
or document vectorsare clustered using K-Means.This provides
cluster of abstractsthat have been grouped together
based on their actual context.And finally, this embedding
and clustering techniquecan be used with the
new funding proposalsto identify similar properties.JACOB NOBLE: After vectorizing
and clustering the proposalabstracts, we look at two
methods for visualizingthe document embeddings.It is difficult to visualize
data in 300 dimensions,so it is important to look
into various dimensionalityreduction techniques.Principal component
analysis, or PCA,is a very common technique
in machine learning.PCA projects the
data in the directionof the largest variance.I like to think of
this as rotatingthe dimension axes so that
the largest spread of the datafalls on an axis.This is popular because it
can decrease machine learningtraining time by maintaining
variance within the datawhile using less dimensions.However, we will see that
it is not particularlyuseful for visualizing data.Another method for
dimensionality reductionis the t-Distributed Stochastic
Neighbor Embedding, or t-SNE.This is a more
modern technique thatconverts high-dimensional
Euclidean distancebetween points into
conditional probabilities.This is generally not used
as a data pre-processing stepfor machine learning,
as computation times 10to be much longer than PCA.However, it does work well
for capturing and revealingboth local and global
structures within the data.This makes it very appealing
and effective for visualizinga high-dimensional space.Here are the two-dimensional
visualizationsproduced by both the PCA
and t-SNE techniques.Each color here represents
a different cluster.As you can see,
the visualizationof the document
vectors produced by PCAdoes not do a great
job of showingany meaningful relationship
between documentsand certainly does not have any
sort of cluster organization.This is because
we are just seeingthe two dimensions of 300 that
have the largest variance.The t-SNE visualization
reveals both local and globalrelationships between documents.This type of visualization
is useful for investigatingrelationships between
documents within a clusterand relationships between
different clusters.HIMANSHU GAMIT: It
takes many experimentsto get a machine
learning model to work.And it feels even better when
you get the expected outcome.Likewise, we were surprised
with the outcome of our model.We took a deep dive
into several clusters.It did not take a
long time to findclusters of similar context.Here are some
titles of proposalsfound within a single cluster.Most of the proposals
in this clusterwere focused on early childhood
and infant development,many of which focus
on developmentof motor and linguistic skills.And one interesting
insight about this clusteris the presence of several
robotics proposals regardinginfant development,
which is connectingdifferent studies together.This cluster grouped
together research proposalsdedicated to studying
social behavior of womenin STEM programs.One proposal here
focuses on broadeningwomen's participation
in STEM, while anotherfocuses on the pathways
for introducingyoung women to STEM.This is a clear example
of a coherent cluster thatcould be useful
for NSF reviewersas it provides better curated
search regions based on contextwhen looking for funded
proposals regardingwomen in STEM.This research was to be
able to group new proposalswith previously funded proposals
based on similar context.So it is very important
that this modelcan process new texts so
that reviewers can quicklyretrieve similar proposals to
help scale their manual effortsand to improve the
quality of their review.And to test this, we vectorized
and clustered the abstractsassociated with this study.The searching of this
proposal is in the left circleof this model.And it was interesting
to see what we found.A cluster, it was assigned to
contain NSF funding proposalsof workshops to provide news
researchers and investigatorsinformation about the NSF
proposal submission process.Isn't that wonderful?And this was a
successful demonstrationof our model performance.JACOB NOBLE: There are
many other applicationsthat this embedding
and clusteringtechnique can be applied to.This would be
useful when workingwith large collections
of similar documents,such as legal documents, health
care claims, or even productreviews.Embedding and clustering can
be applied to many other datastructures aside from text.The only real limitation
is a researcher's abilityto embed a data object
into vector space.The last few years
saw great improvementsin the field of natural
language processing.The very popular
BERT and GPT2 modelswere developed and
have a large amountof potential for computational
language understanding.Despite the large
strides made in the fieldover the last few years,
there is still a lot of workto be done.Some additional work that
could be done regarding the NSFfunding proposal embedding and
clustering study performed herewould be to try and include
the rest of the proposaldata along with the document
vectors to improve clustering.Some work is also
required to help determinethe quality of a cluster.As of now, we manually inspected
clusters for coherence.However, a less
objective method wouldhelp to determine if
changes to the modelwere actual improvements.BERT and GPT2 could
also be used to bettercontextual understanding
of the abstracts,potentially providing
better clusters.Some times series
analysis could alsobe applied to this data
set and Doc2Vec methodfor predicting funding
ranges for new proposals.A lot was learned over
the course of this study.This study gave us
a great opportunityto explore natural
language processingtechniques in combination with
unsupervised machine learningtechniques for clustering
and pattern recognition.This embedding and clustering
technique has many advantages.It is capable of
learning textual context.It demonstrated an ability
to achieve coherent clusters.And it is capable of
processing new texts.However, not all the
clusters were coherentor even particularly useful.Unseen words could
potentially decreasethe quality of the contextual
understanding of new proposals.New discoveries and
scientific progressare likely to produce words that
the model has not seen before.And it is also a
challenge to determinethe quality of the clusters
as the coherence of a clusteris determined subjectively.We set out here to
develop a techniquefor improving the NSF proposal
reviewer's ability to processand review new proposals.This embedding and
clustering techniquewas successful as it does show
to produce coherent proposalclusters based on
textual context.Sir Isaac Newton said, ""if I
have seen farther than others,it is by standing upon
the shoulders of giants.""This technique
can help reviewerssee how new proposals
can help buildupon the research of others.Thank you for listening
to our presentation.Feel free to reach out
to us at the following."
33,"This tutorial is
for users wishingto take advantage of the
boost in processing speedwith base SAS programs that
will execute in SAS Viya.Hi, I'm Charu, an instructor
with SAS institute.Here's a little
background on me.You can also find
me on blogs.sas.com.On the agenda today
we'll cover three topics.I'll first show you
how to connect to CAS.And then, we'll load data into
Caslib and process it in CAS.And finally, we'll modify
SAS code to run in SAS Viya.So what is SAS Viya?SAS Viya is the
latest enhancementof the SAS platform.Open, cloud-enabled,
analytic run-time environmentwith a number of
supporting services.One of which is CAS, or SAS
cloud analytic services.Question that is probably
top of mind, is SAS Viyaa replacement for SAS 9?I'd like to assure you
that, no, it's not.You can still leverage your
SAS programming knowledgeand make tweaks to
existing SAS codeto allow it to run in SAS Viya.And we'll see an example
of this in section 3.In the tutorial, I'll log
in as Lynn, a SAS Viya user.We'll see the world from
her eyes as a Data Manager.I'll use SAS studio
as my coding clientto modify existing Base
SAS code to run in CAS.Now, let's get started
with a demonstration.In section one,
we'll connect to CASto enable data transfer
between SAS and CAS.Let's start with what we
already know about libraries.As a SAS programmer,
you're likelyfamiliar with SAS libraries
and the LIBNAME statement.Here, for example,
you have a folder,which contains various data
types, including SAS data sets,CSV files, Excel, et cetera.In order to access
the SAS data sets,we'll associate it
with the folder.So here is a LIBNAME statement
pointing to this folder,and the library is my SAS.In CAS, we also use
libraries, but theyare called CAS libraries.The CAS library, or
Caslib, is the mannerby which data is accessed
in the SAS Viya environment.At its simplest, a Caslib
is really a containerthat has two main areas.And in memory space
where the processingtakes place on a data source.In the data source,
you might includeSAS data sets, CSV files,
databases, or sashdat files.Here is a session called my
session that we've invoked,and we'll see an example of
this in the demonstration thatwill follow this.Let's get started with
our first demonstration.I'll launch Google
Chrome from the desktop.Then, I'll click the SAS Viya
tab below the address barand choose SAS Studio.Once Studio launches,
I'll go navigateto my folder structure.And within there, I've
already created a shortcutto the folder that I have my
data and all the exercises.I'll double click to launch the
exercise that I'm interestedin, the very first one,
where I'll be doingthe connection to a CAS server.So even before we connect
to the CAS server,there's some terminology that
we would like to be aware of.And the terminology alludes
to SPREE versus CAS.So CAS is Cloud
Analytic Services.SPREE is our
foundation SAS, whichas we all are familiar with,
it is the long time workhorse.It is where we have our
foundation SAS software that weuse as our SAS axis engines.So let's begin with executing
something in the SPREE.So we'll understand
and emphasizethat we do have an
independence SPREE sessionwith its own capability in CAS.So here is my
lightning statement.I'm assigning a library
called my SAS, pointing itto where my data is sitting.Submitting this code,
I use the F3 button.And on the right
side in the log,I'll confirm that the
library was successfullyassigned using the V9 engine.To validate further, I'll
click on the librariesicon on the left side.And I should see my
SAS assigned there.However, we don't see
any of our Caslibs.And by default,
Caslibs don't show upin the tree of SAS libraries.We'll be learning to do
the assignment of Caslibsin the next exercise.Next step is to start
up a CAS session.We'll call it my session.And here are some defaults
using the CAS statement.The library is CAS user,
and I'll submit this code.Once again, I'll validate that
the CAS session, my session,connected successfully to
Cloud Analytic Services.And all of this hasn't
assigned any CAS libraries yet.We'll be doing all
of the assignmentin the next exercise.In this exercise, we will be
assigning or accessing Caslibs.First and foremost,
we'd like to seewhat are the libraries
in my CAS session.This statement, Caslib_all_list
will do the trick.So I'll go ahead and
submit this code.In the log, as I
scrolled up, I'llnotice that the
session is my session.And within that, I have
different libraries.Here is CAS user.as I scroll down,
I see a librarycalled Format,
another one calledModels, yet another
one called Public,and there is one called Samples.There is only one
active Caslib in CAS.And by default, that is
going to be CAS user.I validate that with the
statement, active equals yes.This option indicates that
this is my active library.If I wish to change it from
CAS user to another library,and we'll learn how to do
that in a later section,we can do one of three things.We can change the active
Caslib in the statement,or we can create a brand new
Caslib with the same statement.Or we can drop a
session scope Caslib,and that is something we learn
about in the next session.Furthermore, now that we have
got a listing of the libraries,we can go ahead
and see the filesthat are within the default
CAS library, which is CAS user.So I'll submit my
PROC CAS user stepwith the list file statement.The results, as I
go further down,will indicate that there is
one Excel file, sales.xlxs,in this Caslib, and the log
will also validate the same.CAS has processed the
combined requests,and the name of the
file is sales.xlxs.I'll return to the
Code tab, and I'll nowgo ahead and use a Caslib
statement to createa new CAS called my CAS.I'll check the log and
verify that my CAS is nowthe active Caslib.And Cloud Analytic
Services added this Caslib.I'll go to the
libraries icon and tryto see if I can
find my CAS there.And then my CAS
library is not visible.Again, by default,
Caslibs do notshow up in the library's tree.We need to submit a Caslib
statement such as this onehere, Caslib_all
assign, in orderto see those libraries show
up in the library tree.So I'll submit that.There are a bunch
of notes in the log.And in the Navigation
pane, now Ishould definitely see CAS user.I should see my CAS, the cars
library that I just assigned.Now, I'll see in the listing, if
I'd resubmit my Caslib_all_liststatement, one more time.In the list of
libraries, in additionto what I saw earlier, all of
the libraries such as CAS userand formats, it's all
stuff we saw earlier.We should also see my CAS.I should say that
this is active.And the one difference
from the previous librariesis this option,
local equals yes.Indicating that this is
just a session level Caslib.And what does that mean?That means this
definition goes away assoon as my session ends, and
we will have to redefine itin a new session.We'll return back
to the Code tab,and we'll submit one
last piece of code.This is a PROC CASUTIL step.And within that, I have
a list tables statement,and I have a list
files statement.So it might be useful to
just step back a little bitand understand the
difference between these two.What files means
is file is the wordthat's used to refer to the
source data in a Caslib datasource.Table, on the
other hand, is usedto refer to in-memory data.So we like to see both the
data sources with the list filestatement as well as
the in-memory datawith the list table statement
within the my CAS library.So I'll go ahead and
submit this piece of code.And in my CAS library, I'll
see a couple of files in here.In the file
statement, it revealedthat I have an employee data
set as well as a product Excelfile.We don't see in any
information about tables.And we can further validate
this with the statement,no tables are available
in Caslib, my CAS,of Cloud Analytic Services.However, the two
files are visible.Next, we'll see how to
change the active Caslib.I'll head over to
my folder structure.And I'll open up
the program whereI have the code for
changing the active Caslib.So once again, there's only
one active Caslib in CAS.And I've pointed out earlier how
you could change that Caslib.In the last exercise,
if you recall,we had made my CAS
the active Caslib.We'll make CAS user, once
again, my active libraryby going ahead and submitting
this CAS statement.And I'll verify that my
CAS is no longer active,and that CAS user is
my active CAS library.How do I change it from this
one to yet another library?I'll go ahead and submit the CAS
statement providing the sessiondetails.And in the Caslib option, I'll
provide public as my library.I'll go ahead and submit
this code, and I'll say,can you also please list
for me the public library?So I don't want to see
every single library listed.And here, the note
in the log saysthis is the active library.The name of the
library is Public.That was the first way of
changing an active Caslib.For a second way, we'll
turn back to the Code tab,and we'll use a Caslib statement
to assign the library calledMy Path and provide the path.The path itself is a Linux path.And also, we'll supply an
assigned statement rightafter creating the library.How can this be useful to Lynn?This creates a session
scope Caslib called My Path.And the session scope
means a library thatis active with this session.It's a great solution
for Lynn if sheneeds files for ad hoc
processing and reporting,and she really doesn't need
to hold onto these filesafter the fact.So submit this code.And check out the note
in the log that says,My Path is now
the active Caslib.At any point in my
session, in my program,if I wish to
validate which one isthe active library
without havingto put these statements
down, without havingto put this assigned statement
or the list statement down,I can certainly use a
percent put statement,active Caslib
equals, and use theget SAS OPT function
that writes outthe name of the active
Caslib in the log.This way, you prevent all
the additional informationthat you're bound to get with
the Caslib and the list option.So there in the
log, it comes backand reports that the
active Caslib is My Path.The third technique for
modifying an active Caslibwas the drop statement
in your Caslib statement.So submit this.And as a result, it
reverts back to CAS userbeing the active Caslib.In section 1, we learn
to connect to CAS,we learn to access
Caslibs, and we learn alsohow to modify Caslib.In section 2, we learn how
to load data to a Casliband process it there.So firstly, we
learn the differencebetween session scope
and global scope,then we load client-side
data into Caslibs.And thirdly, we load
server-side files into CASand promote tables.In-memory tables can have
either session or global scope.So what's the difference?In memory tables you have
session scope by default.A session scope table is only
accessible in the CAS sessionwhere it was created.It's only visible to
the user who created it.So where is the use?Session scope tables are
useful for ad hoc data accessand analysis.It only exists for the
duration of the session.When the CAS session ends,
the table is dropped.But what if you need to share
data across your sessionsor with other users, you
must create a global scopetable in order to share data.It's also called
a promoter table,and we'll see examples of
this in the demonstration.Second, we'll learn how to
load data into memory in CAS.We'll be logged in as Lynn.Files of any kind that are
not mapped to the Caslibare called client-site files.We load files into the
in-memory space in CASusing our client software.In our case, we'll use
SAS and PROC CASUTIL.And lastly, we learn to
load data into memory in CASwith service-side files.So we've already
started a CAS session,we have access to
CAS library, now wewish to update or analyze data.CAS can only process data
and it's in-memory space,so the next step is to
load data into memory.Source data files
mapped to a Caslibare called as server-side files.These can be rapidly loaded
into their Caslibs in-memoryspace for processing.So if you need to load
a server-side file thatis stored in the
Caslib's data source,we'll use a CASUTIL
load statementwhere the CAS data
equals option.In this demonstration, we'll see
the difference between sessionscope and global
scope, somethingI shared with you in
the slide presentation,and we'll see that in
the demonstration itself.So we'll see how but
adding the promote optionto this statement.What it does to that
table versus not havingthe promote option.First, the scenario
here is, we'dlike to load mySAS.employees
into Lynn's personal Caslib,which is CAS user.And Lynn is going to create an
in-memory session scope table.It's a table that
others will want access.And when Lynn is
happy with that table,she will use the
promote statement, whichis what we're going to use
here, to create this tableas global scope so
that other users canuse this in-memory table.To begin, I want to first
use a drop table statement.And in case the data already
exists, I want to delete it.The quiet option is needed
to prevent the error.If the table doesn't
exist, you'renot going to get an error.Once it's done, I want to
submit my PROC CASUTIL statementto load two tables, sashelp.cars
and also mysas.employeesAnd the difference being
the promote option.Once I submit this code, I'd
like to answer two questions.The first question is,
what is the table scopefor mycars in-memory table.And we can verify that by
running this list tablesstatements.In the log, I'm going to see
the two tables displayed.And I'm going to see
some information therethat is quite interesting
I'd like to highlight to you.The mycars data set is a
little bit of my metadatahere in terms of number of
rows and number of columns.In addition, I'm going to see
that the mycars table is notpromoted and the myemployees
table is promoted.And we'll see what that means
to us as we go further down.I'll terminate my session,
and then I'll take a lookand see which tables
are available to mein my CAS user library.I'll read on the program that's
going to invoke and connectto CAS.And then, head back
to mylibraries.Oh, and you also need to
remember to assign libraries.Remember, it's not just enough
to give a connection statement.So for that, I have
a program alreadyhere called startup.SAS.And in there, I'm
going to designmy CAS session, my libraries.Once I've done that, I'll
head over to my libraries,expand CAS user, and there, as
expected, I'll see myemployees.So to recap, go back to my code.What did we do here?We loaded two data sets,
one of which we promoted.And then, we saw that this
table showed up as global scope,the other table was local scope.And we ended the CAS
session and restarted it.We noticed that the
table that we promotedwas still available to
us session after session.Let's build on what we just
learned, and let's go aheadand load client-side
data into CAS.In addition, we'll also see
how to explore metadata.The first thing I want to do is
load the client-side SAS dataset, mysas.employees, and my
SAS is a local SAS library.We want to load this into
a CAS table in Lynn'spersonal Caslib,
and then we wantto investigate the contents
of this in-memory table.This PROC CASUTIL step
will do the trick.So verifying the
log, mysas.employeeswere successfully added
to Lynn's CAS user.I'll verify by going,
again, to my libraries icon.And I'll see there the
table called myemployees.I'd like to list the
tables in CAS userwith a list tables statement.And in the log, I have
some metadata informationin terms of number
of rows, columns,index columns, when
the data was created,when it was last modified,
is it promoted or not.And you'll see that the
table that we promoted,myemployees, is still showing up
as promoted various employees.We didn't add the
promote option.Doesn't show up as promoted.I'd like to add another table,
sashelp.cars, into my Caslib,which is CAS user.So CAS user should
now have three tablesin there, one of
which is promotedand the other two are session
scope and not global scope.I can also, once again, I
don't have to just checkthe libraries icon.Through code, I can also verify
what tables exist in CAS user.Again, the log has a t table
show up, and two of themare not promoted, which
means they are availablefor the session only
in session scope.And one table is promoted,
so that's global scope.And if you look at the
list table statement,it's similar to our metadata
procedure in base SASwith it's PROC CONTENTS.So I'll execute PROC
CONTENTS with the _all_nods,or no descriptor option.And PROC CONTENTS gives
us similar informationabout the tables.I'd like to see some more.I'd like to look
at the descriptiveportion of the tables
in the CAS user library.So I could use
PROC CASUTIL and Icould use PROC CONTENTS
with the VARNUM option.I'm going to execute this code,
and then expand out my Resultstab to see and compare
the differencesbetween the descriptive portion
for the two procedures, PROCCASUTIL and PROC CONTENTS.So here is my CASUTIL procedure.In there, there's some
interesting data here.The type of the variable
shows up as double or char.For the contents procedure,
the type of the variables,as we know from SAS, shows
up as numeric or character.So what was the difference?SAS Viya has
additional data types.The data type double maps
onto a SAS numeric data type.SAS Viya also
supports char, whichis a SAS character
fixed with data type,and the var char data type,
which is a variable gthcharacter field.Moving further down, I'd like
to drop the tables, myemployeesand mycars from the
casuser library.And when I see my casuser
library this time,I should just see
myemployees listed there.In the previous demo, we saw
how to load a client-side tableinto CAS user.Now, we'll see how to load
server-side files into CASand promote tables.So there is this
Excel file sales@xlxs,it's in the data source
of Lynn's personal Caslib.What Lynn is going to do is
create an in-memory sessionscope table and investigate
the table descriptor portion.It's a table that
others will need.So when Lynn is happy
with the contents,she'll use the promote statement
to create a global scopetable, and the public
Caslib, a different Caslib,so that it's
accessible to everyonewho can use in-memory table
even after this session ends.We'll first go ahead
and drop this tableif it already exists using
the quiet option that wesaw earlier.Then we'll use the
PROC CASUTIL stepto load sales@xlxs into CAS
user using the replace option.And we'll call it salesxlxs.Then, you also want to check
the contents out of this table.And in the Results
tab, again, verifythe contents of this table.Since sales@xlxs
is the table name,here's the number of rows
and number of columns.In addition, I'm also going
to see that this is nota promoted table,
and we know what thatmeans from the previous demo.We also see something
interesting here.We'll see that the character
data types in salesxlxswill convert to var char.Lynn is now happy
with this table,and she can now run
the next PROC CASUTILstep to create salesxlxs
as a global tablein the public library.We'll also list the
table after that.So what kind of things
are we looking for now?In the contents, we're looking
for the table called salesxlxs.We are also looking for the fact
that this is a promoted table.Is it in the public library?Just a matter of heading
over to the librariesicon, expanding
my public Caslib,and I should see
salesxlxs listed there.In the previous section,
we learned to upload datato a Caslib and promote tables.In this final section, we'll
learn to modify SAS codeto run SAS Viya.The first part is SAS DATA step.Does the data step syntax work
for in-memory data in CAS?And yes it does.We just need to consider
whether the data isprocessed in a single
thread or multiple threads.So if you consider base
SAS DATA step, whenit's executed in base SAS,
it runs in the single threadon the SAS workspace server.Processing data
in a single threadreads data sequentially,
one row at a time.SAS Viya enables data to
be divided and processedsimultaneously on
multiple threads.When a DATA step
executes in CAS,each thread executes a
program statement on its data,returns the results
to the controller.In the second part
of this section,we learn in the DATA step
to modify new variables.So as you already
know, in the DATA step,we use it for manipulating data
in preparation for analysis.We can modify existing
values, compute new variables,conditionally process the
data, and combine tablesin the DATA step.To modify the DATA
step to run in CAS,sometimes it's as simple as
modifying the library referenceand the data statement.Here, for example,
I have a DATA stepwhere I'm building a table
called departments and CASuser using my customers, and I
have a select statement doingsome conditional processing.If the continent
is Africa or Asia,then department is going
to be general shoes.If it's something
else, for example,North America or
Europe, departmentis going to be women's shoes.We alter the bottom
half a statement thatsays, if it reaches end
of file, then threadidis the value I'd like to see.And also, _and_, which is the
iteration number of the DATAstep.And lastly, we'll modify a
DATA step code to run in Viyaby using the BY statement.You can certainly use a
DATA step to process datawithin groups.And we know that the data
must be first sorted.And then, you can
use a BY statement.And First or in last processing
to identify the firstand the last in the group.Sorting can be very resource
intensive, especiallywith large DATA sets.Default behavior when
data is loaded into CASdistribute the input data
based on the original orderamong the different threads
on multiple machines.When we add a BY statement
to the DATA step,the rows are group based
on the first BY variable,and then distributed across
multiple threads and machines.PROC SORT is no longer required.Results are returned as each
thread finishes processing.Here at thread 3,
process the DATA step,return the results first.And the order might be different
each time the program executes.So let's head over
to the demonstrationto see each of
these three topics.Let's get started with our
demonstration for section 3.First, we'll run a
DATA step and base SASand compare it with SAS Viya.A few pieces of syntax
that would be helpful,which may be new
to you, we'll usethe SAS RAF equals data set
option to force the DATAstep to execute in SAS Viya.And then, we'll also be
using two automatic variables_threadid and _nthreads_.And this is going to
give us, respectively,thread usage and thread count.So here is a base SAS DATA step.We'll execute this,
and examine the log.Threadid value is
one which indicatesthe DATA step processed
in SAS is single-threaded._nthreads_ has a value of one,
which indicates that there isone thread available in Base
SAS for processing the code.We'll next highlight and
submit the CAS statementto start the CAS session, if
we haven't already started it,and assign the SAS library
reference to Caslibswith the Caslib statement.Next, I'll highlight and
submit the second DATA step.This step creates the
CAS user.threadstablein the CAS user Caslib
and runs the DATA stepin the CAS session.The first statement in the log
confirms that the DATA stepwas executed in CAS.One of the big differences that
you notice is the fact that_threadid has different values
for each row in the log.Threads operate independently.Therefore, the log messages
were generated by each threadat slightly different times.Values represent the thread
that the DATA step wasexecuted in the CAS session.There are 16 threads available.
_nthreads_ has a value of 16.In this execution
of code, thread fivecompleted execution first,
then thread 10, and so on.If you run the program
multiple times,you might get a
different order each timethat the program runs.This is exactly what we want
to happen when a program isexecuted in multiple threads.Otherwise, the performance
gains by threading are lost.If the DATA step, which is
somehow synchronize the output.In this next demo, we'll
modify the DATA stepcode to run in SAS Viya
by building new variables.So here is a base
SAS DATA step programthat creates a new
variable conditionally.The variable called department
using select an assignmentstatements.We'll modify the
base SAS DATA stepto run in multi-threaded
environment in CAS.Before the program
can run in CAS,we'll ensure that the table,
mysas.customers, is loadedinto CAS as global scope
table, mycustomers,in Lynn's CAS user Caslib.So here is the code
that bus is goingto drop the table if it
exists, and then willload it and promote
it to global scope.Once it's done, we'll head
over to the DATA step.And this is really
familiar DATA step code.We have the END= option
on the DATA step,which allows us to track the
last observation at the endof file.And when it hits the end
of file, and by the way,this is a binary variable.It can only have a value 0, 1.The switch of 0 says,
doesn't hit the end of file.The switch of 1 says, yes,
I've hit the end of file.So only when I've hit
the end of the filethat I want to see the values
of these two variables.I'll go ahead and
submit this code.What we're doing here
is selecting continentas the variable that we wish
to conditionally process.If the value of continent
is Africa or Asia,we want to set department
to general shoes.If it's something else like
North America or Europe,we want to set department
to women's shoes.And if it's none of
the above, then wewant to set
department to unknown.Once we execute
this code, we'd liketo see the value in
the output data tab.What's the first value for
city in work.departments?You're going to see
Leinster in the DATA step.In the log, we are going to see
_threadid has a value of one._and_ has a value of 951669.That's the number
of observations thatwere read from the DATA step.So what we know of
the DATA step ismysas.customers has a little
under 1 million observations.When executed, the
original programruns on the SAS workspace
server in a single thread.I'll return back to my Code tab,
and I'll submit the second DATAstep here.This is as easy as it
gets in terms of modifyingyour code running CAS.You just replace the DATA step,
the library, with CAS useron both the set statement
and the data statement,if that is how you wish
to bring up your data.I'll submit this code.No other change except on the
DATA statement and the SETstatement for the libraries.So now, we are going to see
a little difference here.The data was distributed
across 16 threads.But even before we
look at the log,let's look at the output data.In the output data, this first
record, the first observation,has a value of Paris for city.Remember, the DATA step,
base set DATA step,you had Leinster.Let's head back to the log.In the log, we'll see that the
data was distributed across 16threads in the CAS session.The results were written
as each thread completedits processing.Thread 5 completed after
processing 60,000 rows,and then thread 16,
and then 12, and so on.If you were to add up
the values of _and_,the sum would equal 951,669,
which is the total numberof rows that were read from the
CAS user .mycustomers data set.In this last demo, we'll modify
a Base SAS DATA step to runin CAS with the BY statement.So the business
scenario runs like this.We would like a table that
includes the total costsof each continent.The original Base SAS
program uses a BY statement.We'll modify the Base
SAS code to run in CAS.So down below, I have
my PROC SORT step.I'm sorting mysas.customers
by continent,and I'll proceed to
submit this code.Once I've sorted my data, I
know that I have the abilityin the DATA step to
add a BY statementand turn on
first.last.processing,something we really
appreciate the DATA step for.We're able to get very granular
and look at low level data.So if I'm the
first in the group,I can use first.continent and
do some conditional processing.If I am the last in the
group, I have the abilityto do some other work.So in this code, I'll
be reading customers.I'll be setting on
the BY statementto turn on
first.last.processing.And then, if I'm the
first in the group,I'll set my total cost to
0, my accumulated variable.Next, I'll have a sum
statement adding up the cost.And if I'm the
last in the group,then I wish to write it out.I'll go ahead and
execute this code.And in SAS studio, when I
open up the output data tab,I'll see that I have
five rows of data.And the values are
ordered by continent.I'll proceed to go to CAS now.I'll first drop the
table and load it again.Once it's done, there isn't a
lot of difference in CAS codeother than modifying
the library names hereon the data statement
and the set statement.I'll highlight this code.And once the code
executes, I'll head overto the Output Data tab.And this time, I'll
see in the griddown below the results with
total cost are the same,but the rows from CAS are
not return and sorted orderby continent.So what happened there?When SAS programs
are processed in CAS,the default behavior is
that the data is distributedacross multiple threads.And the program is executed
on each thread simultaneouslybased on the order of
rows in the source table.You don't have to presort
the data to use the BYstatement in a
multi-threaded CAS DATA step.CAS performs implicit
ordering on BY variables.For any questions
in the tutorial,feel free to reach out.Thanks for watching."
34,"Hi, everybody.My name is Danny Zimmerman.I am a senior software
developer on SAS Studio,and today I'm going
to demonstrate the Gitinterfaces in SAS Studio 5.2
and Enterprise Guide 8.2.And with this demo, I'm going
to mimic two different users--one using SAS Studio, and
one using EG, working outof the same remote repository.A remote repository's
just a GitHub repository.Very simple, couple SAS
files and a readme file.So let's get started.In SAS studio, down
here on the left,we have a Git Repositories pane.This is a zero state that
says you can add or clonea repository, and
a little messagethat says that we need to set
up a Git profile before wecan do either of them.So to set up a Git
profile, click on Options,Manage Git Connections.In this dialog, we have
the Profiles pane, whichhas a list of your profiles.You need one for each
service you're using--GitHub, GitLab, any
internal repositories.Your Repositories pane, which
is a list of repositoriesthat have been registered
with SAS Studio and an Optionspane to limit that.Only one option-- just to
limit the maximum rows loadedin the history.So let's go ahead
and add a profile.Click on the Add button.SAS studio defaults
to SSH authentication.The user name in SSH is
just purely informational.And then an email as well--informational.And then a public and
private SSH key paths.So let's go ahead and add those.I'm currently on the
directory that has my keys,so add the public key.Add the private key,
and then save that.Now you can see that
we have a GitHubprofile with a little
checkmark-- that meansthat is the default profile.And then hit Close.Now our zero state will
update a little bitby getting rid of
the profile message,and now we want to
clone a repository.So that brings up
the clone dialog,and it's asking for the
remote repository URL.So go to GitHub, click on
the Clone or Download button,and we want to use SSH.Currently on HTTPS--
we want to use SSH.so let's copy that
URL, paste it in there,and now we want to pick a
location on the SAS Studioserver to clone
our repository too.And then SGF 2020.Oops.And then Clone.Now we have a Git
Repository tab that opens,and then our Git
Repository pane on the lefthas changed with-- it has
our current repositoryand the list of branches.Currently, we only
have the master branch.And if we had other
repositories registered,we'd have a list of
other repositorieswe could switch between.On the Git Tab we have--we can switch from
repositories from here.We can switch branches.We have the Pull button, which
allows you to pull or fetch.Pull is a fetch plus a merge--
a fetch just gets stuffinto the pulls down--fetches stuff from
the real repository,but doesn't merge it into your
working directory, and thenthe Push button.You also have a Stash button.Speaking of working
directory, wecan click on that file
explorer, and we can navigateto our local repository.You can see that we have SGF
2020 with a little Git icon,and it's just a directory
with some hidden Gitinformation in it.And we want to open
up cowboy dot SAS.And now, we want to change that.It's just, we're going to start
collaborating with another userthat's using EG.Let's make a change to cowboy
dot SAS, and then save it.SGF 2020.Now that we have
changed somethingin the local repository,
Git knows that that filehas been modified.We have a little Modified
indicator, and thenthe path of the
file, this cowboydot SAS is sitting in the root
of the remote repository--I mean, the local repository--so it's just the file name, but
if we had any subdirectoriesit would be subdirectory.And now we can click on that--
we can see our changes overhere on the left--or right.And we change it
from a purple hat--I mean, a blue hat
to a purple hat.And now we want to
stage and commit that.The staging means
that you just--whatever is in the stage area
will be included in command.So you could have
changes that sitin the onstage area that would
not be included in this Commit.So purple cowboy hat.Now we hit Commit.Now you can see that it's gone.We can go over, click
on the History tab,and this brings up the
history of the repositorywith all the Commits
from different usersand different branches.And you can see the diffs.So you can click on purple--I can see cowboy hat--the cowboy dot SAS change.It'll change from
blue to purple.Message, the Author name,
Timestamp, and a Commit ID.And so now we're
ready to push it.We have this little
indicator sayingthat we have one Commit that
we can push, so let's go aheadand push that.And once this push is done,
we will switch over to EGand pull these changes down,
and so we can show collaborationbetween the two users.So now we have--the remote repository now
has purple cowboy hat.We can go verify that over
here-- just do a reload.Purple cowboy hat-- right here.So now we can go over to EG.So the EG interface is very
similar to the SAS Studiointerface.We have a Git Repositories pane
with the current repository.I can double click
on that, open it up.Same type of deal-- we have
our staging area and our filedifferences, and we
have our history.And so now let's go ahead
and open cowboy dot SAS.For the sake of time, I've
already created the profileand cloned it
down, and now we'rejust going to get to work.So let's see-- we
need to open a file.Let's open up cowboy dot SAS.So now it's still blue
because we have notupdated our repository,
and we do not like blueas the EG user, and we
want a red cowboy hat,and so we hit Save.Now we can go over
here-- we can see,got our differences, blue
to red, and stage it.Red cowboy hat.And then Commit.Go to History-- we can
see now, red cowboy hat.And now we want to update the
repository with our changes,but before that, we need
to make sure we do a Pull.So we do a Pull, and we
get a message that says,Merge conflicts were found.So let's go back to
it, and then a little--and then Git changed cowboy
dot SAS under the covers,and now it's--EG is saying that
we want to reloadit to see the new changes.So we click Reload, and what
happened is Git did a pulland attempted to merge
the changes from the pullinto our working
directory, and so now wehave the changes we made with
red with a conflict indicator,a separation indicator,
and then the changesthat came from the remote
repository, purple.And then our
conflicting indicator.The cool thing about EG
being a native applicationis that we can use third
party merge and diff toolsto fix this merge
conflict, which is nice.SAS Studio doesn't
have that luxurybecause all the files are on
a remote server somewhere.And so now I can go back
to the Commit history.I can right click on here,
and I can open Merge Tool.I have a K diff 3
configured with EG,so that's what was updated.So I hit OK, and I want--since we changed it to
red, we want to have red.And actually, we can
make it even a compromiseif we wanted to, so let's
go ahead and do a merge.Merge, choose B everywhere.And then Save.So now, Merge Tool,
Reload cowboy dot SAS,and we can see that--oops.Wrong application.And we can see
that it's now red.Well, let's compromise
and just makethe bottom of the hat purple.And Save.So now if we go over here--we do not want that one.That came from the Merge Tool.We just want cowboy dot SAS.So now we have the
red and the purple.Red with red, top with purple.I can type.Now we can commit.And now we can go
ahead and do our push.We can see how we got the
merge and we're pushing.Now if I go back to--hit Remote Repository, reload--I can see now we
have the cowboy dotSAS with the red and
purple, and then Ican go over to SAS
Studio and do a pull.And we can see red with purple.So that's just a
quick demonstrationof the core features of SAS
Studio and Enterprise Guide Gitinterfaces.And I just wanted to show that.Thank you."
35,"RAJESH SELUKAR: Hello, and
welcome to the SAS Global Forum2020 pre-conference tutorial
on state space modeling.I'm Rajesh Selukar, a
statistician developer at SAS.In over two decades
of work at SAS,I have developed software
for many different areasof time series analysis
and forecasting.As a part of this work, I have
also authored two procedures--PROC SSM and PROC UCM, which
are used for state space modelbased data analysis.I have divided this tutorial
into three sections.The first section introduces the
state space model and the twoSAS procedures--PROC UCM and
PROC SSM--that are speciallydesigned for state
space modeling.The second section
shows you whereto look for additional
information.Basically, we quickly
go over the web linksfor the documentation of
the UCM and SSM procedures,and the books and articles
I have found useful.Finally, in the last
section, we willcover some interesting examples
of state space modeling.Strictly speaking, the models
discussed in this tutorialare called linear Gaussian
state space models.In fact, there are other
more general state spacemodels, such as nonlinear
and non-Gaussian state spacemodels.However, we will not discuss
them in this tutorial.From now on, a state space
model, or SSM for short,will always mean a linear
Gaussian state space model.Historically, SSMs
were developedto deal with
engineering applicationssuch as digital signal
processing and system control.In the last few
decades, they havefound uses in other fields,
such as econometrics,social and environmental
sciences, and many more.In these other fields,
SSM is primarilyseen as an adaptation
of linear regressionfor sequential data, where
the regression vector evolveswith time as an auto regression.In this tutorial,
we will emphasizethis regression-centric
view of SSMs.As a model class,
SSM is very rich.For example, it encompasses
most popular time seriesand longitudinal data models.SSM-based analysis is applicable
whenever the data consistsof one or more continuous
response variables,and possibly some predictor
variables recorded over time.When we do have such a
sequence of observations,the data analysis
can have many goals.A few of these are listed below.For example, you may want to
forecast the future valuesof the response
sequence, or you maywant to discover the underlying
patterns, such as trendsand seasons.For all these and many
more, an SSM-based analysiscan be very useful.An SSM is defined by a
set of three equations.The first equation, called
the observation equation,writes the response
vector at time t, Y_t,as a sum of three terms.The first term, X_t beta, is
the usual regression effectwith X_t as the
design matrix at timet and beta the regression
coefficient vector.So X_t is the design
matrix and betais the regression coefficient.The second term, Z_t
alpha_t, is also similar.But in this case, the
regression coefficient, alpha_t,is time varying, so this
regression coefficientis not static.It can vary with time.This time varying regression
coefficient vectoris called the state vector.The third term, epsilon_t,
is the usual noise term.It is modeled as a zero
mean Gaussian vector.The second equation, the
state transition equation,describes the evolution
of the state vector.According to this equation,
the state at time (t + 1)is a sum of two terms.In the first term, the state
vector at time t, alpha_t,is multiplied by a matrix T_t.This matrix T_t is called
the state transition matrix.The second term, Xi_(t+1), is
the disturbance or noise term.This is also modeled as a
zero mean Gaussian vector.The last equation defines the
initial condition for the statetransition equation.It specifies that alpha_1,
the state vector at time 1,is a Gaussian vector
with mean A_1 delta.Finally, let me summarize
the overall setup.Here, Y_t is a
sequence of responses,possibly vector valued.X_t is a sequence of
predictor variable vectors.Epsilon_t and Xi_t
are independent,zero mean, Gaussian
noise sequences.They are also called disturbance
sequences or disturbancevariables.Alpha_t, also called the
state, beta and delta,are regression like vectors.The matrices Z_t, T_t, the
covariances of disturbances,like the covariance
of epsilon_t,the covariance of
state disturbance Xi_t,and A_1, which appears
in the initial condition,are called system matrices.The system matrices
are mostly known,but can depend on some
tuning parameters.A_1 is always fully known.A_1 is usually an
identity matrixor is a matrix made up of the
columns of an identity matrix.If you are new to
this type of model,it can appear rather complex.However, I assure you that after
seeing some concrete examples,it will start making sense.In the next few slides, we
will see some concrete examplesof SSMs.These SSMs are good
starting examplesand they also form the building
blocks of more complex modelswe will see later.So let's see the first
example of an SSM.Random walk plus noise
model is very handyto describe a random quantity
with slowly changing mean.For example, suppose Y_t
denotes a two-dimensional vectorthat contains my systolic
and diastolic bloodpressure measured at time t.So suppose this
bivariate Y_t containsmy systolic and diastolic
blood pressure measurements.We can write Y_t as sum
of two parts-- alpha_t,my true blood
pressure at time t;and epsilon_t, the measurement
error associated with my BPmeasuring instrument.Assuming that my health
remains constant,or I remain healthy during
the observation process,we can model the time evolution
of alpha t as a random walk,that is, state alpha
at time (t+1), that is,my true BP at time (t+1), is
a perturbation of my previousblood pressure by a random
quantity, a random amount.So such a model is
called a random walk.If my blood pressure
doesn't havean upward or a
downward trend, thisis a reasonable model
for blood pressure.The model for
alpha_t is completedby assuming that
alpha_1, my true bloodpressure at the start
of the observations,as some unknown vector delta.This is the initial condition.Now let us see what are
the different systemmatrices in this simple SSM.Since Z_t alpha_t term
in the first equationis simply alpha_t, the system
matrix Z_t, in this case,is an identity.Similarly, the T_t alpha_t
term in the state equationis also just alpha_t, so the
transition matrix T_t is alsoan identity.Clearly, the A_1 matrix
in the initial conditionis also an identity.The only non-trivial
system matrices in this SSMare the observation noise
covariance, sigma_epsilon,and the state disturbance
covariance sigma_Xi.We are assuming that epsilon_t
and Xi_t are IID disturbances.Such sequences are also
called white noise sequences.Note that all the system
matrices in this modelare time invariant.In more general SSMs,
these can be time varying.Before I finish
with this example,let me point out two important
special cases of this model.If the state disturbance
sigma_Xi is a zero matrix,that is, basically these state
disturbances are themselveszero, then the state equation
becomes alpha_(t+1) equalsalpha_t.That means the state
essentially is time invariant,or all states are equal to
the initial state delta.This means the
entire model reducesto Y_t equals delta
plus epsilon, that is,Y_t equals constant plus error.So the constant plus
error is a special caseof random walk plus noise model.There are many other
interesting special cases,but I want to point
out only one more.Since my systolic and
diastolic blood pressuresare affected by the same
process inside my body,it is conceivable that the state
disturbance Xi_t is actuallya scaled version of some
one-dimensional disturbance.That is, instead of Xi_(t+1)
being truly two-dimensional,it is actually a deterministic
vector times a one-dimensionalrandom quantity.In other words, the
state covariance,the disturbance covariance
of the state disturbances,is of rank 1,
rather than rank 2.Econometricians call such random
walk a cointegrated process.Such a model is conceivable
when the two processesin this two-dimensional
random walkactually have a common trend
and they are a manifestationof a common trend.There are several
other interesting submodels of this random
walk plus noise model,but we don't have time to
discuss all of them right now.Let's move on to another
useful state space model.In this slide, we
consider a process whosemean pattern is sinusoidal.In the earlier case,
the mean patternwas a slowly varying pattern
without any particular shape.Here, we are considering a mean
pattern that is sinusoidal.The plot shown in the slide
illustrates such a process.For example, in this plot,
we have round circles,which are observations,
Y_t, and a connected curve,sinusoidal curve, which is
the mean of this process.Note that the curve in the plot
is not a perfect sinusoidal.Its shape gradually
changes over time.For example, the
first peak, here,is taller than the third peak.Such a time varying
sinusoidal patternis called stochastic cycle.In real life, we rarely
see perfect sinusoids.They change over time because
of many other disturbances.They look like
sinusoids, but the shapedoesn't remain exactly
the same as time goes on.It turns out that a
stochastic cycle like thiscan be written as Z alpha_t
for a suitable two-dimensionalstate vector alpha_t.The state alpha_t represents a
point on the circle at time t.According to the state
transition equation,the state at time (t+1)
is obtained by rotatingthe alpha_t point on the
circle, by a fixed angle.This is because this transition
matrix, which is given here,is the so-called
rotation matrix.It rotates a point on the
circle by a fixed angle.Because of the disturbance term
in the transition equation,the rotation is
slightly perturbed.This gives rise to the time
varying sinusoidal pattern.So if this disturbance
term had not been here,then this sinusoidal
pattern in the plotwould have been a perfect
sinusoidal pattern.But because of this
disturbance term,the shape of the sinusoid
changes over timeas either a larger
disturbance is addedor a smaller
disturbance is added.For added modeling flexibility,
a damping factor rhois included in the model.A damped cycle represents
a sinusoidal patternthat eventually dies out.That is, it dies down to zero.When rho is equal to 1 and there
is no disturbance in the stateequation, we get the
perfect sinusoidal pattern,that is, the cosine curve.Just to summarize, let
me point out the fourparameters in this model.The defining parameters
of this modelare lambda, the
frequency of the cycle,or equivalently, the
period, 2 pi over lambda;the damping factor
rho; the covarianceor the variance of the state
disturbance sigmaSquared_Xi;and the variance of the noise,
observation noise epsilon_t.These are the four parameters
of this state space model.This is a state space model of
dimension 2, state dimension 2.And in the observation equation,
z alpha_t term, actuallybasically picks out the
first element of alpha_t,like the x-coordinate of
the point on the circle.So as the point is rotated
around this circle,the x-coordinate
goes up and down.It could be x or y-coordinate.It doesn't matter.And that up and down is what
generates the cyclical pattern.So stochastic cycle is an
important building block,a building block model
in the SSM literature.As a last example of
an SSM, let's considerstochastic seasonal patterns.A seasonal pattern is a periodic
pattern of general shape.That is, instead of
having a periodic patternwith a very well-defined
shape, like sinusoidal shape,this is a periodic pattern
of more general shape.That is, it repeats itself
every k period, say,but the pattern of
this k time pointsis not very well-defined
like a sinusoidal.In order to define
or represent sucha general periodic pattern,
it's useful to remembera fact from Fourier analysis.A periodic sequence
with period kcan always be created as a
sum of k/2 undamped cyclesof appropriate periods.So for example, if gamma_t is
a periodic pattern of period k,then we can always
write it as a sum,psi_1t + psi_2t
+ up to psi_k/2t.Each of these
psi's are sinusoidsof appropriate periods.These constituent cycles
are called harmonics.For example, a seasonal
pattern of length 12,which corresponds to
monthly seasonality,can be created by adding six
cycles of periods 12, 6, 4, 3,2.4, and 2.If the season period, k, is
an odd integer then k over 2is rounded down.For example, if
season length k is 7,then it can be presented
as a sum of three cycles.You can create a
stochastic seasonal patternthat changes over the
years by combiningundamped stochastic cycles.That is, if you want
to get a perfectlyrepeating periodic pattern,
you add perfect sinusoids.If you want a slowly
changing periodic pattern,then you add stochastic
cycles insteadof deterministic cycles.You can control the variation
of the seasonal patternby controlling the
disturbance variancesof the constituent cycles.If you recall, in a stochastic
cycle, its variation over timeis controlled by the variance
of it's disturbance term.So similar thing happens with
a stochastic seasonal pattern.You can control its
variation-- time over time--by controlling the
disturbance variancesof the constituent cycles.You can control the smoothness
of the seasonal patternby dropping the high
frequency harmonics.For example, in a
monthly seasonal pattern,if we drop the high frequency
harmonics correspondingto periods 2.4 and 2, we get
a smoother monthly periodicpattern.We get it even
smoother if we dropa cycle with period 3 from
the harmonic correspondingto period 3 from
the representation.So this way, we can
create a slowly changingseasonal pattern of desired
smoothness and desiredvariability.So this is a very flexible model
for generating time varyingseasonal patterns.Having seen some simple
state space models,it's now time to move on to a
little bit more complex statespace models.In practice, we often
need to model phenomenathat are more complex
than the examplemodels we have seen so far.For such situations,
suitable modelscan be formulated by combining
appropriate submodels.In effect, one starts by
postulating that the responsesequence Y_t is the sum of some
well chosen components, suchas, say, trend, mu_t; season,
psi_t; some cycles; regressioneffects, X_t beta;
and noise epsilon_t.Although not obvious at
first, this model equationis in fact an observation
equation of an SSM.In order to see
this, we just haveto realize that these
components that wehave included in
the model equationthemselves have
state space form.For example, if this mu_t were
to be a random walk model,we already know its
state space form.We know how its state looks
like, how its state evolves.That is, we know its
transition equation,we know its initial condition.Similarly, if psi_t is a cycle,
then we know its state spaceform, we know how its
state vector looks like,how its transition
matrix looks like.So these component
terms actuallydefine a larger
state space model.So if we consider a state space
model by augmenting these statesections--the first section corresponding
to the first component,second section corresponding
to the second component,third section corresponding
to a third component,if it's present.Similarly, we can form
the system matrix Z_tby creating a block
matrix Z_1t and Z_2t.Similarly, the
transition matrix canbe created by a block diagonal
matrix with blocks T_1, T_2,and so on.So the components define an
augmented state space form.And the entire model can be put
in our usual state space form.The representation of SSM
in such a component formis called an unobserved
components model.So unobserved components
model is just a different wayof specifying an SSM.This way of specifying SSM
is easier because we can,by plotting the
response sequence,or because we have the domain
knowledge about the responsesequence, we can postulate
an appropriate model thatcontains proper explanatory
variables, or the predictorvariables, proper trend
terms, proper periodic terms,and proper type of noise.So this is how an SSM is
formulated in practice.This is a very much
regression-like approachof formulating a model.Finally, I will conclude
the introduction partof this tutorial, or the
introduction to the SSMpart of this tutorial,
by describingthe output of a typical
analysis, SSM-based analysis.So for example, after you have
postulated an SSM or a UCM,you can use procedures like
UCM and SSM for model fitting.The following list
shows the main sectionsof the output produced by
these UCM and SSM procedures.These procedures will give you
estimates of model parameters,such as the estimates of
cycle frequency, disturbancevariances, and the
regression vector beta.You will get forecasts
and interpolated valuesof the response variables.You'll also get forecasts
and in-sample estimatesof the latent
components, such as mu_t,psi_t, or whichever components
are present in the model.It is very useful
to plot and lookat these model components
(estimated components),because just like
in regression, wecan decide whether a component
conforms to our expectations,and it can be dropped
if it is insignificant.In general, this UCM is
a very expressive formof time series modeling.Similarly, estimates of
the linear combinationsof these components
can also be estimated.For example, you can have
estimate of trend plus cycle.You might think, what's
so difficult once weknow mu_t and psi_t, what's the
problem with getting estimateof mu_t plus psi_t?Actually, of course the
estimate of (mu_t + psi_t)is just the sum of the
estimates of mu_t and psi_t.But the standard error
of these estimatesare more difficult to compute
because the estimated mu_tand psi_t are
correlated estimates.So in addition to the
estimate of (mu_t + psi_t),you also get standard
errors of these estimates.In the output, you also
get model residualsand the residual diagnostics.You get model likelihood and
likelihood-based informationcriteria, such as
AIC and BIC, whichare useful for model
comparison; and alsostructural break diagnostics
and many other things.After that brief
introduction to SSMs,let us look at the
capabilities of PROC UCM, whichis designed for univariate
time series modeling.By univariate time
series, I meanyou are studying a
single response variabley, which is recorded
at a regular interval,such as daily,
weekly, or monthly.As an example, suppose y
denotes daily electricity demandat a utility between
10:00 to 11:00 AM.Of course, the utility is
producing the electricityat all hours of the day, but we
are focusing on this hour only.Depending on the location
and the customer baseof this utility,
this daily demandwill exhibit a variety
of time series patterns.For example, the
demand over timecould be trending
upward, downward,or somewhere in between.It may show day of
the week seasonality.There could be holiday effects.The weather, particularly
the temperature,could also be a factor.Therefore, a model
for such time seriesmust be able to account for
all these different features.With the UCM procedure,
it is quite easyto build a comprehensive
model for such situations.It provides a rich collection
of ready to use componentsthat can capture a wide variety
of time series patterns.For example, you can use
different types of trends,such as random walk trend, local
linear trend, and many others.You can use many types
of periodic patterns,such as stochastic
cycles and seasons,and you can finely control
their characteristics.It also offers many ways to
incorporate regression effects.For example, spline regression
for nonlinear behaviorand transfer function regression
to capture delayed effects.That means the
effects associatedwith lagged regression values.Finally you can incorporate
more general components,such as ARMA noise.Usually, a simple white
noise noise term is used,but you can also incorporate
more complex noise patterns.It of course provides
very rich outputthat I mentioned earlier.For an illustration of PROC
UCM, I have chosen a case studyfrom a book by Pelagatti.This case study deals with the
modeling of hourly electricitydemand in Italy.Nine years of hourly
electricity demand historyis available for
model development.From the modeling
perspective, these dataare quite interesting.They exhibit several different
types of seasonal behaviorat different scales, and
the demand on holidaysis usually different
than other days.The plot on this
slide illustratessome of this behavior.In order to keep
things simple, Ihave chosen to display only
a small portion of the data.The demand during the hours
of 10:00 AM in the morningand 10:00 PM in
the night is shownfor the years of 2007 and 2008.From the plot, you can
see that the 10:00 AMdemand and the 10:00 PM
demand have similar patterns,except that the morning demand
is higher than the night.Although not easy to see,
the dominant periodic patternin the plot is the result of
the day of the week seasonality.You can also see a month
of the year pattern.For example, the electricity
demand during Augustis quite low compared
to other monthsfor both year 2007 and 2008.This is because it is a vacation
month for many Italians.Now let us see how
to model these data.Modeling hourly
electricity demandis a well-studied problem,
and many different time seriesmodels have been proposed.You can use the UCM procedure
to fit most of these models.For now, however, we will
follow the modeling strategysuggested in Pelagatti's book.As per this strategy, we
treat this hourly dataas a collection
of 24 daily timesseries, one for each
hour of the day.For each series,
the electricity loadis decomposed into five terms.mu_t denotes a
random walk trend.X_t beta is a correction
for the holidays,such as Christmas and New Year.Gamma_7 is a day
of the week effect,modeled as a trigonometric
season of length 7.For added modeling flexibility,
the three constituent harmonicsin this season can have their
own disturbance varianceparameter.Gamma_365 is a day
of the year effect,modeled as a trigonometric
season of length 365.For reasons of parsimony,
only the first 16constituent harmonics are
included, and all of themshare the same disturbance
variance parameter.Remember that, for a
season of length 365the constituent
harmonics are nearly 180.So we are only taking
the first 16 of them.So this will be a fairly
smooth seasonal component.Finally, epsilon
denotes the noise term,which is modeled as
a simple white noise.In order to use PROC
UCM to model these data,an input data set,
say, load, is created.Since the demand for
each hour of the dayis treated as a separate
series, the hour of the dayis used as a BY group variable.Within each by group, the
data are sorted accordingto the date of the observation.So this is the ID variable,
date, and its interval is day.Additionally, dummy
variables correspondingto the desired holidays are
included in the data set.These are the day before
Christmas, Christmas,and so on.Suppose the name of the
electricity demand variablein this data set is eload.So eload is the demand variable.Now we can specify the
model that we want to fit.The response variable
and the simple regressorsare specified in
the model statement.So this model
statement specifieswhat is the response
variable and whatare the simple regression
variables in your model.So the X_t beta
term is specifiedby specifying these regressors.And load_t is this
variable eload.The gamma_7 effect, the
day of the week effect,is included by specifying
three constituent harmonicsin this season.These three cycle statements
are these three harmonics.The first harmonic of period 7--these are undamped cycles,
so rho is equal to 1.And these are held fixed.The period is held fixed at
7 and the damping factor rhois held fixed at rho equals 1.And NOEST means don't
estimate these parameters.Keep them at their
stated values.The third parameter of
this cycle, the disturbancevariance, is left
unspecified and willbe estimated using the data.So this finishes
the specificationof the gamma_7 term.These three cycles together
make up this gamma_7 term.The gamma_365 term, which
is a season of length 365,is specified by the
season statement.In UCM, if the
constituent harmonicsof a stochastic season share
the same disturbance varianceparameter, it can be
specified more easilyby the SEASON statement.And the fact that we are only
using the first 16 harmonicsis specified by this
keep harmonics option.If this keep harmonics
option is absent,then all harmonics of this
trigonometric season are used.The noise component is specified
by the IRREGULAR statement,and the random walk
trend is specifiedby the LEVEL statement.So irregular, level, three
cycle, and the season statementand the model statement, this
is the model specification part.Now, the estimate
statement controlsthe parameter estimation
aspects of the problem.So back equals 14 option,
in this statement,says that the last 14
observations in the databe not used during
parameter estimation.And the plot equals
panel option saysthat produce a panel of
residual diagnostic plots.Similarly, the
forecast statementcontrols different aspects
of the forecasting output.In this case, the two
options, back equals 14and lead equals 14, cause
the last 14 observationsto be used for
forecast validation.The OUTFOR equals option
specifies an outputdata to store the forecasts of
the response series and modelcomponents.This entire model fitting
for this 24 seriestakes about 15
minutes to finish.This is a fairly large data
set and many parametersto estimate, so it
takes a little time.But in 15 minutes, all this
more than 70,000 observationsare processed.This slide shows the
time series plotsof the components estimated
for the 10:00 AM series.For easy viewing, only a small
section of each plot is shown.Note that the horizontal axis
for the gamma_7 componentspans only a few weeks in
2006 through 2007 year,whereas all other plots show
the span of the entire 2007to 2008 year.So for these three
other plots, the x-axisspans 2007 to 2008
year, daily points.The vertical axis
in these plots,however, differ in scale.For example, this vertical axis
goes from 41,000 to 45,000,whereas this y-axis goes from
minus 15,000 to plus 5,000.The irregular component
where the vertical axisvaries between minus
2,000 to plus 3,000.So the variation on the
vertical side, pay attention.They are not on the same scale.Such plots are really
useful to understandthe contribution of each
component to the model,as well as whether
their patternsconform to our expectations.For example, the trend plot
shows that the mean demandvaries around 43,000 units.The plot of the day
of the week seasonshows pronounced differences
between different daysof the week.For example, this is
the lowest demand day,this is the highest
demand day, and so on.And on lower demand
days, the demandgoes down by as much
as 13,000 units.So it can go down as
much as even 14,000.The day of the month plot
shows that the month of August,for the month of August,
the demand can go downby about 10,000 units.Comparing with the trend plot,
they're one below the other,it appears that the correction
for the August monthcould be improved slightly--maybe by adding a
few more harmonicsin the definition of the season.The look at the plot
of the error componentsuggests that the model
for the month of Augustneeds some improvement.This is the month of August.This slide shows a panel of
residual diagnostic plotsand the time series plot
of the series forecastin the validation period.So the residual diagnostic
plots are in the left paneland the forecast plot
is in the right panel.This series has more
than 3,000 data points,so the visual inspection of
residual diagnostic plotscan be tricky.For example, the
Q-Q plot is easilydistorted by a few
outlying points,and the bounds around
the autocorrelationand partial autocorrelation
plots can be overly tight.The histogram of the
residuals is kind of OK.So on the whole, the
residual diagnosticsdoesn't show very
alarming deviationsfrom our model assumptions,
although this might be debated.The forecasts in the
validation periodindicate that the model may
be useful for forecastingin the short horizon.For example, most of
the observed demandseems to follow the
forecast pattern.So of course in the real
modeling situations,one would do a lot
more model checking.For now, we will stop here.As mentioned
earlier, temperaturecan be an important driver
for the electricity demand.The relationship between
temperature and demandis usually nonlinear.The demand is higher
during hot summer,as well as in cold winter days.In the case study data
we modeled earlier,temperature information
was not available.If such temperature
data were available,the earlier model
can be augmentedby adding a term for
nonlinear temperature effect.So the earlier model could have
been augmented with a nonlineartemperature effect.This code segment shows
you how to do that.Suppose the tempload data
set is the load data setwith additional variable
temp, which indicatesthe ambient temperature.You can include the nonlinear
temperature effect in the modelby using the
splinereg statement.So in the splinereg
statement, youwill specify the
variable that youthink has a nonlinear effect
on the response variable.The splinereg statement shown
here creates a cubic spline.So degree equals 3 says that
we want a cubic spline with 10evenly spread knots in
the observed temperaturerange seen in the data set.So this statement creates
an appropriate spline termwhich will add a nonlinear
temperature effect to our loadmodel.Even though we don't have the
temperature data for the casestudy, an example in
the UCM documentationillustrates a similar analysis
on a different electricitydemand data set that does
have temperature information.For these data, the left
panel shows the scatterplotof temperature versus
electricity demand.So this is the average
daily temperature observedin that data set, and this
is the electricity demandon those days.So the relationship between
temperature and demandseems to be this
nonlinear relation.For during the
pleasant temperaturesof 50 to 60 degrees Fahrenheit,
or between 50 to 70,the demand is lower,
whereas it increasesas the temperature goes up or
as the temperature goes down.The right hand side plot shows
the estimated temperatureeffect obtained by including
a spline term in the model.The spline term, this
is included the waythat we saw in the last slide.And as you can see, the
estimated temperature effectis very similar to what
the scatterplot shows.So that means we clearly
got this term right.With this, we complete
the discussionof the UCM procedure.The rest of the tutorial
illustrates the capabilitiesof the SSM procedure.PROC SSM is designed for
general state space modeling.Unlike PROC UCM, which is
designed for univariate timeseries modeling, PROC SSM
can handle very general typesof sequential data.For example, with
PROC SSM, you cananalyze univariate and
multivariate time series,panels of univariate and
multivariate time series,and univariate and
multivariate longitudinal data.Longitudinal data
are sequential,but not necessarily
equally spaced in time.PROC SSM syntax
enables you to easilyspecify really general, possibly
non-standard SSMs, as wellas many important standard
SSMs, such as ARIMA, univariate,and multivariate
UCMs, and many more.Like PROC UCM, PROC SSM
also provides rich tabularand graphical output.In this slide, we will go
over the PROC SSM statementsthat are used for
model specification.Suppose that we want
to specify a model,Y equals X beta, plus
mu_t, plus psi_t,plus some other
components, plus error.And suppose that each
of these componentshas some underlying state space
model with its observationequation, transition equation,
and its own system matrices.Each one of them
has its own state,and the overall state is
formed by combining these statesubvectors, as we saw earlier.So the question is, how
can we specify such a modelby using the SSM syntax?First, note that the model
specification statementsare designed such that
many commonly needed modelparts, such as different types
of univariate and multivariatetrends, cycles, seasons,
and noise modelscan be easily specified by
using keyword-based syntax.Also, simpler
subspecifications canbe combined to form more
complex specifications.And most importantly, you can
use the data step languagefor defining system matrices--you can define regressors on
the fly using the data step.With this feature, you
can specify arbitrarilycomplex SSMs.Now let us look at the
individual statements.So let's begin with
the ID statement.The ID statement is used
to specify the time indexvariable.When the data are not
simple time seriesand when the system
matrices are time dependent,the ID variable can
be an important partof the model specification.For example, in many
longitudinal data casesthe transition
matrix elements arefunctions of the distance
between successive time points.The PARMS statement
is used to specifyparameters needed for the
user-defined components.So we'll see several
examples of that,so I will explain it later more.The next statement,
the STATE statementis used to specify the
transition equation of a statesubvector such as alpha_2,
so for example, alpha_2or alpha_1.Using the options in
the STATE statement,you can specify highly complex
state transition equations.So we'll see the examples
of these also later.The COMPONENT statement
is used to specifya component associated with
the previously defined statesubvector.For example, if you
have defined alpha_2using the STATE
statement before,you can use this
definition of alpha_2to define a component
based on that.So for example, the
definition of psias a dot product
of Z_2 and alpha_2,will be defined in the
COMPONENT statement.That is, with every
component term in the model,there is an associated
state and component pair.The next statement,
the TREND statement,simplifies component
specificationfor some commonly
needed component types.For example, components like
random walk and ARMA noisecan be specified
by using keywords.In effect, the TREND statement
does the appropriate statecomponent pairing
behind the scenes.So for example, if
mu or psi happento be components that have
keyword support in the SSMprocedure, then you don't
need the STATE and COMPONENTpair for them.You can just use
one TREND statement,to specify that
component with a keyword.The next statement,
the DEPLAG statement,is used to add lagged response
variable terms in the model.The IRREGULAR statement is
used to specify the noise termin the observation equation.And finally, the
model statement isused to specify the observation
equation of a responsevariable.In multivariate case, a
separate model statementis needed for each
response variable.So in a model statement, you
specify the response variable,one for each response
variable, if this y happensto be a multivariate vector.And you specify the
regression variables.You specify the components
that you have defined before.And that completes the
model specification.As mentioned earlier
the data step statementscan be used to specify the
elements of system matriceslike Z, the transition
matrix, and the disturbancecovariances, and so forth.For defining these
elements, you canuse the data step programming
statements, such as ARRAY, DO,IF THEN, and ELSE, and
the mathematical functionsto create system matrices.We will see the
examples of these soon.In fact, let us see
a simple exampleof specifying an SSM
using the PROC SSM syntax.So suppose we have a
univariate model, y,equal to a regression
variable effect, a random walktrend, a cycle, and noise.In SSM procedure, there
is keyword supportfor all the components
in this model.However, for
illustration purposes,let's specify the
cycle componentwithout using the keywords.Recall the dynamics of
the two-dimensional stateassociated with
an undamped cycle.We can specify it as follows.So remember our stochastic
cycle definition.So a cycle can be defined as a
dot product between a vector Z,(1, 0); and a two-dimensional
state, alpha_t.And this two-dimensional state
has this transition equation,with transition matrix T as a
rotation matrix, this rotationmatrix.And the state disturbances
have a diagonal covariance.In fact, the covariance is
like sigmaSquared_c timesan identity matrix.Its initial condition is quite
simple, alpha_1 equals delta.Basically the A_1 matrix
in the initial conditionis simply an identity.So we will specify a model
with a regression variable,a random walk trend, a
cycle, and a noise component.So let's begin the specification
of the cycle component.The first two parms
statements definelambda and sigmaSquared_c.This cVar corresponds to the
sigmaSquared_c parameter.By the definition
of lambda, it rangesbetween lower bound
of a number close to 0and an upper bound near pi.So the lambda ranges between
0 to pi, as we wanted.The variance parameter,
cVar, has a lower bound of 0.And then we define the
transition matrix cycleT, a 2by 2 matrix, as an array of
four elements, a1 through a4.In the SSM procedure, when
an array defines a matrix,it's read row-wise.So a1 and a2 define
the first row,and a3 and a4 define
the second row.So a1 is cos(lambda), a2 is
sine(lambda), a3 is minus a2,and a4 is a1.So the transition matrix
of the cycle componentis defined using these
programming statements.Now, the state
statement actuallydefines the transition equation
of this cycle component.So the state statement says
consider a state sectioncalled cycleState.And this state section
is of dimension 2.Its transition matrix is of
general shape, signified by g.And its elements are
given by the array cycleT.Similarly, the
disturbance covarianceis of identity type multiplied
by a variable, cVar,in this case.The initial condition
is specified--the matrix A_1 in
the initial conditionis specified as A1(2).Essentially, in
this case, it meansit's an identity
matrix of size 2.Finally, the cycle
specification iscompleted by specifying
the corresponding componentstatement, which says that
the CYCLE component isa dot product of a vector z
and the state section alpha.So this component statement says
that let's define a componentcalled CYCLE, which is
the dot product (1, 0)times the cycleState.Thirdly, we are going to
use keyword specificationfor the random walk trend.So that is specified using
the trend statement thatsays randomWalk is random walk.So randomWalk is just a name
and the keyword specification rwsays that it's
actually a random walk.The irregular component
is specified like this.It names whiteNoise
as white noise, again,internally it's
modeled as white noise.So finally, the
model is specifiedby the response variable, the
x variable as a regressor.RandomWalk, CYCLE,
and whiteNoiseare the other components to
be included in the model.So that finishes the
model specification part.This kind of finishes
the introductory partof PROC SSM, the introductory
discussion about PROC SSM.Shortly, we will see several
concrete data analysisexamples using PROC SSM.However, prior to
that, let us seewhere you can get additional
information about state spacemodeling, PROC
UCM, and PROC SSM.SAS documentation, which
is freely available,is a good starting point.The UCM and SSM procedures
are part of SAS/ETS.Let us quickly check the
SSM documentation link.The documentation is
divided into several parts.So the documentation
link usuallyopens in the Syntax section.We already have seen how the
SSM procedure syntax looks like,so let's go to other parts.So the Getting Started example--the Getting Started section
contains an SSM-based analysisof a well-known panel data set.So this section is a good
section to start with.Then you'll see that
the example sectionhas 18 more illustrations
of SSM-based data analyses.So there are plenty
of worked examplesto learn how to
specify the model,the variety of situations you
can use the SSM procedure,and many other things.We will go over some
of these examplesin this tutorial itself.The web-based documentation
is very convenient.There is one more thing.It is better to use
web-based documentationbecause you can get the
latest documentation.And secondly, and in
fact more importantly,the web-based documentation
includes linksfor the example data and code.For example, the first example,
which we will see laterin the tutorial, you can just
click on the code and datafor this example.So as you can see,
the data and the codeassociated with this
example is shown there.You can just cut and paste
this and run this examplein your SAS Editor.So this is a very useful
and convenient featureof the web-based documentation,
which if you download the PDFdocumentation, you will have
to specially go to this sectionlater, to this code thing.There will be no link,
necessarily, for the dataand code for that example.Another important section
is the Details section.It has several sections
explaining the theorybehind the SSM procedure.Let me mention a few
of these sections.The State Space Model
and Notation sectioncontains the precise form
of the state space modelthat can be handled by PROC SSM.You will notice
that this form ismuch more general
than the form I usedat the start of this tutorial.I used the simpler form to
keep the notation simple.The filtering,
smoothing, likelihood,and structural break detection
section provides the detailsabout the famous, Kalman
filtering and smoothingalgorithm, which is the main
computational tool for statespace modeling.Other sections provide
many other details,such as the precise forms of the
keyword-based models supportedby PROC SSM.So the documentation
is a great resource.The Reference section
contains the important papersthat are relevant for
state space modeling.So let's go next.So in this references
section, I havelisted the books
that have greatlyinfluenced the development
of this procedure.For example, the first
book by Durbin and Koopmancontains rigorous treatment of
linear SSMs in the first part,and its second part deals
with more general SSMslike nonlinear or
non-Gaussian SSMs,which we are not
going to discuss.The second book by
Harve is considereda classic in this field.It continues to be
the go-to referencefor univariate and
multivariate UCMs.The book by Pelagatti
is the most recentand probably the best book to
get started with this subject.So these three books
are quite useful.Another thing is that the
syntax of the SSM procedureand even the topics that are
dealt by the SSM procedureare influenced by these books.So virtually every
example in these bookscan be easily specified and
modeled using the SSM syntax.The SAS Global Forum papers,
the two SAS Global Forum papersI have mentioned here,
also act like an additionto the UCM documentation.They deal with two
important topicswhich are not usually covered
in the SSM literature.So the topics are functional
modeling of longitudinal data,and detection and adjustment
of structural breaks,all using the
state space models.I think you will find these
references, these booksand references, quite useful.Additionally, the SAS
education departmentregularly offers courses
on state space modeling.For example, you can
take the half day courseon PROC UCM and the two
day course on PROC SSM.So these are offered regularly.You can check the
schedule in the linksthat I have provided here.Now we will go over four
illustrative examplesof using PROC SSM.First three of
these examples arepart of the SSM documentation,
and the last oneis discussed in one of
the Global Forum papersI've mentioned.Therefore, I'm going
to keep the discussionof these examples fairly brief.So in case I miss
something, you can alwaysgo back to either the
documentation or that paper,and you'll see all the details.Of course you can write
to me if you still don'tfollow some of these things.These examples are meant to
highlight the versatilityof the SSM procedure.Sometimes you are
going to get lostbecause these examples,
at least one of them,are kind of tedious,
but bear with me.So let's start with
the first example.In this example, let's look
at the time series plotof a bivariate quarterly
time series whichis observed from the
first quarter of 1969up to the last quarter of 1985.These are the two time series.The response variables
are f_KSI, the blue line,and r_KSI, the red line.And these represent the
log of quarterly totalsof the front-seat and rear-seat
passenger injuries in UK.So the top line, the blue line,
represents front seat passengerinjuries, and the lower one
represents the rear seatpassenger injuries.And this vertical line
which is shown on the plotsignifies the introduction
of a law, a seat beltlaw that was introduced in
the first quarter of 1983.By this law, the
front seat passengerswere compelled to
wear seat belts.So the goal of this
study is to seeif this law has been effective
in reducing the front seatpassenger injuries.In order to answer
this question,one must understand
the historical patternsof this bivariate series.So when we look
at this series, wenotice that both
f_KSI and r_KSI haverelatively flat trends and an
obvious quarterly seasonality.It also appears that the
two series move together,which is not surprising
since an accident islikely to affect both the
front and rear passengers also.It also appears
that there is a dropin the f_KSI level after the
introduction of the seat beltlaw.But the question
is, how significantit is statistically
and so on and so forth.We will consider a
model that is consistentwith these observations
and see whether the drop isstatistically significant.Now, here is the model that
we consider for these series.So this is a bivariate
series with the first elementof the vector, response vector,
f_KSI, the second elementof the response vector, r_KSI.And here is the regression term.So in the regression
term, we haveX_t, which is basically an
indicator of start of that seatbelt law.So it's 0 before 1983
Q1 and 1 thereafter.And this variable is supposed
to affect only the f_KSI,the front passenger.The rear passengers,
the regression is 0.Then there is a bivariate
random walk trend.Because we saw the trend
didn't have an obvious upwardor a downward trend.So a random walk is a
reasonable trend model.And also, since I mentioned
that these two series appearto move together, we are
going to put a restrictionon the disturbance covariance
of this random walk.We'll see that in the
model specification later.The third term is
the seasonal term,the trigonometric seasonal
term for quarterly seasonality.And the last one is a
bivariate white noise.So let's see how to specify
this in the SSM procedure.So the input data set is
specified as seatBelt,and it has the
necessary information.The data are quarterly and
the date variable in the dataset indexes the time.Here is a data step statement
that defines the variable X_tthat we need.It's defined on the fly using
the date variable in the inputdata set.It says that basically
if this condition fails,that means if the date is
before 1983 first quarter,then it's 0, and
it is 1 thereafter.The first state statement
defines a two-dimensional statevector.It's of type white noise and
it has general covariance.So its covariance is a
two by two covariance.And wn1 and wn2 are the
first and second elementof this error vector, and they
define white noise componentsthat will be used later in
the definitions of modelsfor f_KSI and r_KSI.Then we go on to define
the trend component,which is called
level in this case--which is going to
be random walk.It's defined by
a state statementand the state subsection
is called LEVEL.It's also of dimension 2 and
it's of type random walk.And we say that the
disturbance covariance of thishas rank 1, signifying that
these two series kind of movetogether.And the first element
of this random walkis called rw1; second, rw2.And they, again, will be used
in the observation equationsfor the response variable.Similarly, a state corresponding
to the seasonal component,and here the type is season
and the season length is 4.So that defines
quarterly seasonality.Actually, here, the disturbance
covariance of this season termis set to be 0.That is, when it's left
unspecified in this case,it means it's 0.It defines essentially a
deterministic seasonalityof length, 4,
because it turns outthat the seasonal
component hardlychanges in the history of this
series that we have in our dataset.Finally, let's see how
the model is specified.So we need two model
statements because wehave bivariate series.And the first series, the f_KSI,
has this regression variable,and then the components
that we have created.And the output of this
analysis will be putin an output data set Belt1.Actually, this procedure can
produce a lot of graphicaland printed output also,
but to keep the syntaxwithin this page, I have kept
this very crisp and small.This output data
set will containestimates of all the
components that are defined.For example, estimate
of beta, estimatesof mu_t, Xi_t, the noise.And in addition, we can
define linear combinationsof these components.We want to define
what is basicallythe seasonally adjusted series.Seasonally adjusted
series, in this case,for f_KSI would mean the
effect of X_t beta plus mu_twould mean a seasonally adjusted
front-seat passenger series.So we have defined a
linear combination.The sa stands for
seasonal adjustment.It's just a name, which is I
want the procedure to estimatethe sum.And this will be output
in the output data set.And here is the plot of this
sum with its standard erroror the confidence band, and
the series plotted around it.So you can see that the
seasonally adjusted componentgoes through the data
points quite well.And you'll notice now the drop
in the trend after 1983 quiteclearly.It turns out that
the estimated betaparameter is negative 0.408
and standard error is 0.025.So the drop is
statistically significant.So this is how you can
conduct an analysisof a bivariate series.We could have had much
more general model.So now let's move
to the next example.The model specification
in our first examplewas relatively simple.All the model components
could be specifiedusing keyword support.The example we consider now
is considerably more complex.So this example is
going to be a little bittedious to go through.I hope you will bear with me.So anyway, let's begin.In this example, we'll
model the time evolutionof the yields of government
bonds of different maturities,that is, we will model the
so-called yield surface.We have monthly data from
January 1970 to December 2000on bond yields of 17 bonds
that range in maturitiesfrom three months to 10 years.As an illustration,
the graph in this slideshows the yields of bonds
of maturities 3, 60,and 120 months.So that is three months,
five years, and 10 years.So you can see how
the bonds behave.Particularly in the
early '90s, these bondsgive very different
yields, but in later years,the yields are not so different.That is, the short-term
bonds and the long-term bondsdon't differ in the yields
much in some later years,but earlier, there is a
good bit of difference.The goals of these,
to be specific,let's suppose we
have the followinggoals for our analysis.Goal 1, model the
interrelationshipsbetween these 17 bonds
of different maturitiesand produce short-term
forecasts for the bond yields.And goal 2-- which is somewhat--it's an important activity
for hedge fund managers.Actually, the people who
hedge, like the companies,they have to hedge
their future risks.So the goal 2 is,
using the fitted model,predict the yield of
a hypothetical bondwith a maturity of
42 months that is nottraded on the general exchange.So a bank might issue
such a thing to a company.They may get into an agreement
over a bond of 42 months.Now what the question is, what
should be the reasonable yieldsof such a bond.Yield surface modeling
is an important problemfor econometricians and many
models have been suggested.A model called dynamic
Nelson-Siegel model, whichhas a few different variants,
has shown good promisein this area.We will use PROC SSM to fit
a version of this model.In order to specify this, model
we will need some notation.Let Y_(t tau) denote the yield
of a bond of maturity tauobserved at time t.So t denotes the time index
and tau denotes the maturity.We will consider an SSM for
a Y_(t tau) which is basedon a three-dimensional state
alpha that follows a vectorauto regression of
order one with mean mu.So the state vector
that underlies this SSMis three-dimensional.Its elements are alpha
_1, alpha _2, and alpha_3.And we'll assume that this
alpha vector is a vectorautoregression with mean mu.The observation equation of
this model is bit involved.So the observation
equation is given here.So the observation equation
has just two terms--one which is a vector product
of z and alpha, and the otheris the error.This error process,
or the noise,its variance changes
with the bond maturity.So for a bond for
three months, thereis a error variance associated
with it; bond for six months,there is a different error
variance associated with it.So for each different
maturity, we'llassign a different
error variancefor the observation error.Secondly, the system matrix
Z depends on two parameters--the maturity tau and a positive
parameter called lambda.So let me explain this Z vector.So the first element of
the Z vector is just 1.The second element
of the Z vectoris a somewhat complicated
function of lambda and tau,and Z3 also is another
function of lambda and tau.So in effect, by writing
down the dot product fully,we get the observation
equation for the yieldat time t for a
bond of maturity taucan be decomposed into this
term, which represents--I'll explain this term.So these terms-- four
terms, basically.Three terms resulting from this
dot product and the noise term.So the first term, alpha_1t,
is the overall yieldat a given time point.And this doesn't
depend on the maturity.It only depends on the time.It changes over time,
but for a given time,it's just the bond yield
for that time period.So if I don't tell you
the maturity of a bond,you will say probably
its yield is alpha_1t.The second term,
Z2(tau) times alpha_2t,is a correction for
short-term bondsbecause this multiplier
Z2(tau) becomes smallwhen tau is large.So for large bond maturities,
this term becomes smaller.Similarly, the third term,
Z3(tau) times alpha_3t,it's a correction term
for medium-term bonds.That means it's non-zero
only when the bondmaturity is of medium term.Other times, this
term is negligible.Finally, this is the noise term.So this model has
several parameters.The different
parameters for the errorvariances for these 17 bonds,
this lambda parameter, whichis involved in
these elements of Z.And of course the
transition equationof this alpha that
I didn't mention,but it's a VAR process.So it's VAR(1)
process, so it hasthe AR1 matrix associated with
it and the error covariancematrix.We will later see
that we will restrictthe AR1 matrix to be diagonal.But still, these are not
univariate AR1 processeswhich are uncorrelated,
but they will be correlatedunivariate AR1 processes.So these elements of alpha
are correlated AR1 processes.Another thing to note, or
a general comment to makeis that this model provides
a probabilistic descriptionof the behavior of
bonds of all maturities,not just the ones
available in our data set.That is, it provides a model
for the entire yield surface.So the tau here can be one
of the bonds in the data set,but it also postulates
a model for bondswith other maturities.That is, it provides a model
for the entire yield surface.This will help us achieve
our two analysis goals.Extrapolate the observed
bond yields into the futureand interpolate the bond
yields for the unobserved bondwith maturity of 42 months.Note that we have 17 observed
bonds and an unobservedbond we are interested in with
the maturity of 42 months.The yields for the unobserved
bond are set to missing.So the input data set
will have cells for the--at every time, there will be a
cell for this unobserved bondyield, but it will be missing.After the model is fitted,
these missing valueswill be filled with the
model based predicted values.So that's our plan.Now, before we specify
this model in PROC SSM,let's go over the organization
of the input data set.The input data set
has four variables--the date variable that
indexes the model.On this slide, I
am only giving youa portion of the syntax
needed to specify this model.So basically, the name of the
input data set and other thingsare not shown here, just so that
the syntax fits on this slide.So the date variable, the ID
statement is not shown here.So the ID statement will
have a monthly date variable.The response variable is Yield.So this will
eventually be specifiedin the model statement.So Yield is the bond yield,
which is our response variable.And maturity, which indicates
the bond maturity, sobond maturity is displayed
or measured in months.So that column
will have maturityassociated with the bond.And a fourth variable,
which is primarilyincluded to help
with the programmingstatements in the
syntax, it sort ofhelps define some
arrays that we need,that variable is called
MTYPE for maturity type.It's just a label variable.It indexes these 18 bonds--the first 17 bonds
that we have observedand the 18th bond we
are interested in,an unobserved bond
we are interested.So these bonds are just simply
labeled as 1, 2, 3, 4, 5,up to 18, so that we can
create some arrays later.So we have these four
variables in the data set.Now I can begin to explain
the statements in the syntaxand show you how that defines
the model we are interested in.So the first few statements,
these statements,actually define the
observation error.So eventually, we are going
to have an irregular statementwhich will define a
white noise sequence thathas variance sigma.But sigma is a variable that
can change its value from rowto row.So when a row represents a
bond with certain maturity,this sigma would be a parameter
associated with that maturity.So in order to do that, we have
defined 17 variance parameters.Sorry.It's here in the
parms statement.And they are positive numbers.They should be positive.And this array simply
has these 17 variablesas elements of this
array so that we cansay that sigma equals mtype.When the bond type
is i, then the sigmais that corresponding sigma i.For the 18th bond, which is the
unobserved bond of maturity 42,we want its variance
to be somewhatsimilar to the variances
of its neighboring bonds.So arbitrarily, we have
assigned its varianceto be the average of the
bond with maturity 36and bond with maturity 48.These are the neighboring
bonds around maturity 42.And so these statements
create a white noise sequencewith variances that change with
maturity, not with the time.They are invariant as far
as the time is concerned.Now we'll specify the Z vector.So the Z vector depends on
a positive parameter lambda,and then it also depends on
the variable maturity, whichis an input data set variable.So these statements
define the Z vector.So now this Z vector, along
with the three-dimensionalautoregression
makes the first termof the observation equation.But rather than specifying
the autoregressionas a general generic
state space model,we'll use the key word support
available for VARMA modelsin SSM.But the VARMA model
defined in the SSM syntaxis always zero mean--the keyword supported
VARMA model.So what we'll do is that we
want an AR model, AR1 model,with mean.So we'll divide the
specification taskinto two steps.First one will define a
VAR1 model with mean 0,and its mean will be defined as
a three-dimensional random walkwith zero for the
disturbance term.So it will be constant.So with these two,
these statements,define the needed
autoregressive term.So finally, the yield is
defined as the two componentsfrom the zero mean and mean
part of the autoregressionand the white noise.That completes the
model specification.So this may appear a little
bit complicated specification,but it shows you you can
do a number of thingswith these tools of data step
statements, the data stepprogramming statements, and
the parms statement, and so on.Now let's see some
of the output.I'm not going to go over
too much of this output.So this plot shows you the
estimate of the overall yield.And it of course
changes over time.So for example, in the mid
'80s, the bonds, overall bonds,yielded pretty high.The yield was more than 14%.In those days, actually, if
you kept money in the bank,your savings account
paid you 8%, 9% interest.And of course as years
go by, both the savingsstarted paying less and
the overall bond yieldsalso went down.This shows you the estimated
yield surface for the 3 months,60 months, and 120 months.You can think of them
as denoised yields.This plot is somewhat
not very well done.I'm sorry.But there is actually a line.The crosses and
pluses and circlesare the actual observed
yields, and thereis a line going through them,
which is the estimated yield.And then as you can see,
after 2000, the yield surfaceis behaving somewhat
interestingly.Maybe the inversion
of yield curve.I'm not an economist, so
I'm not going to say much,but the behavior
appears interesting.These are short-term
bond being comparedwith relatively long-term
bonds of 5 years and 10 years.Finally, we'll see what
are the predicted yieldsof a hypothetical
bond of 42 months.And I have purposely plotted
the neighboring observedbond yields, 36 and 48 months.And we would expect if the
model is doing a reasonable job,the yield of this
hypothetical bondshould be sandwiched
between these years.And that's what
exactly is shown here.So the model, of course
I have not shown youmuch about this model.It's residual analysis,
or many other things,but this is just
to let you know itseems to behave as one expects,
as one would like it to behave.So that finishes
the second example.Let's go on to
the third example.In this example, we
consider the problemof creating a series
of monthly GDP numbersthat are consistent with
the available information,such as the official GDP
numbers which are published onlyquarterly, and the industrial
production numbers thatare published monthly.So what we want to do is
that since the GDP numbers,official GDP numbers, are
only published quarterly,but for a variety
of reasons, peopleneed GDP numbers
at finer intervals,for example, monthly.So this is called a
temporal disaggregationor temporal
distribution problem.And state space modeling
is a popular choiceto solve this problem because
if a variable is modeledby an SSM, at a particular
time interval, for example,the GDP is modeled
at, say, monthlyinterval, its aggregated
from, for example,the quarterly form,
also follows an SSM.So of course we don't
observe monthly GDP.But if we propose a
model for monthly GDP,then a model for quarterly
GDP can be deduced.Anyway, so this is a very
useful property of SSM.So we are going to use this
property of the state spacemodels in this temporal
distribution problem.Let's denote the quarterly
GDP, which is observed as GDP,and the unobserved monthly
GDP, as GDP dagger.The industrial
production is denotedby a variable called INDPRO.So clearly, by our
definition, the observed GDPis the sum of the
unobserved GDP numbers.So for example, if this
GDP is the March GDP,then this is March GDP--sorry, quarterly GDP
published in March,then this should be the
sum of March's monthly GDPplus February GDP
and January GDP.So January, February,
March added togethermake the quarter
ending in March.So our goal is to estimate
these monthly GDP numbers usingavailable information.So in this case, we
also have informationavailable on the monthly
industrial production.So the input data set that we
will consider has this form.So we have this date
variable which indexesthe monthly observations.The GDP variable, which
is published quarterlyhas missing values in the
months where it's not publishedand the value where
it is publishedis not missing like this.And monthly industrial
production numbers,which are available every month.Also, we also have one
additional variable,let's call it, startQ.This variable is needed later
in the SSM specification,model specification.It's a variable that indicates
the start of the quarter.So in the case of quarter,
it's very definite.Every quarter has
only three months.But imagine if you
had monthly dataand you wanted to
distribute it to daily data.In that case, each month
has different numbersof days and things like
that, so this variableis needed to basically indicate
the start of the aggregationinterval.So here, this startQ variable
is 1 when the quarter begins.The proposed model, we are
going to propose a modelfor the bivariate series
industrial production and GDPdagger.So this is only a partially
observed bivariate seriesbecause we don't really
observe GDP dagger.But a bivariate model for
this partially observed serieswill actually imply
another state space modelfor the fully observed bivariate
series industrial productionand quarterly GDP.This implied model is
not easy to describe,but you don't have
to worry about it.This implied model will be
fitted by the SSM procedurebehind the scene.So the SSM procedure will
realize that you are actuallyspecifying a model
for an aggregated formof the variable.So it will formulate
the implied SSM,do the necessary computations,
and output the results for you.So let's first see
what is the modelwe are specifying for the
partially observed series.So this INDPRO and
GDP dagger are assumedto be sum of a bivariate trend.In this case, this trend is
an integrated random walk.I have not shown
you the plots of GDPand industrial production, so
you don't know the reasoningbehind suggesting an integrated
random walk as a trend.If you look at the
plots, you'll seethat the GDP is trending upward
and integrated random walkcreates a smooth trend.So also there is
a bivariate cycle,which is another term
in the model, whichcaptures like business
cycles and things like that,and a bivariate white noise.Further information
about this problemcan be seen in example
16 of the documentation.And here is we how we
specify this model.So the model specification,
in this case,is similar to other--we have already seen at least
two examples of bivariate modelspecification, or maybe
at least one full exampleof a bivariate
model specification.So you'll start with a bivariate
trend, bivariate cycle,bivariate noise.These are model
component specificationsfor this bivariate series.But remember that this
is an unobserved series.And we want actually model for--the implied model for this.So the only difference is that.So in the model equations,
this industrial productionis an observed variable and it
is not an aggregated observedvariable.But GDP, or I mean aggregated--GDP, on the other hand,
is the observed variable,which is an aggregated version
of this unobserved processthat we defined.So this is indicated by
this keyword, DISTRIBUTE.The distribute means that GDP
is an aggregated variable.And these terms actually refer
to the disaggregated processunderneath it.And the start of the
aggregation intervalis signified by
this variable, whichis specified in this thing.So this specification,
with this specification,SSM procedure will do the
proper disaggregation,will use the proper
implied model,and produce the desired
calculations, and the output.So here is the desired
monthly plot of monthly GDP.You can of course have it
tabulated output in a data set,but this just shows you the
output of the calculations.So these monthly numbers will
be consistent with the observeddata.So they will have
used the informationin the industrial
production seriesand will have distributed
the quarterly GDP numbers.So these numbers will sum to
the quarterly GDP numbers,but for January, February, March
months will add up to quarter.But they will have
a pattern imposedby the available information.So this concludes
the third example.Let's go to the final example.So in this example, we
look at the data generatedby a study that studied the
drug theophylline, whichis used to treat lung diseases.12 subjects were given different
initial doses of this drug,and its serum concentration
was measured at different timesduring the next 25 hours.The measurement times
for different subjectswere different and
unequally spaced, that is,the data are longitudinal.So this study, we are
modeling longitudinal data.So this plot shows the evolution
of the serum concentrationof this drug in these
four subjects over time.So when the initial
dose is given,it's not absorbed in
the blood right away.It takes some time
to absorb, the drugto be absorbed in the
body, in the blood.And after it has
reached its maximum,the body starts excreting it
and it gets eventually processedthrough our body.So a model called
one-compartment model describesthis process quite well.And the one-compartment
model is commonlyused in pharmacokinetic
analysis.It turns out that the
one-compartment modelcan be formulated as
a state space model.So before we go there, let me
explain the particular modelwe will study in this example.So this model postulates
that for any subject,this curve, this observed
curve, can be thoughtof as sum of these three terms.The first term is a
product of the initial doseand a common mean curve.This mean curve is
the global curve,which is implied by this
one compartment model.The next term, Xi, is the
subject specific variationfrom this implied form.So this is what theory suggests
the actual curve should be,But in a particular subject,
the observed curve will deviate.And this is subject
specific deviation.And then this is because
the subjects may differin their properties, on that
particular day, that person mayhave eaten something
particular way, or whatever,whatever, whatever.And the epsilon
represents another sourceof error, which might be
the measurement error.So this is what the model is.And this model has
a state space form.I'm not going to describe
the state space form.I'm not going to go over the
model specification this time.This example is discussed
in this Global Forum paperI mentioned earlier.You can see much more discussion
about this model and the statespace form of this.I'm just going to quickly show
you the estimated components.And here, again, I'm
giving you the denoised, soa sum of the components.This process, the
fitting process,you can get the estimate
of this common term, mu_t,which is of interest
to pharmacokineticists,but they also are interested
in the subject specificdenoised curve.So these are the plots
for this denoised curvefor two subjects.So the subject 1 was initially
given a dose of 4.02 units;subject 2 was given
initial dose of 4.4 units;and here is what the
denoised curve lookslike for these two subjects.So you see that the SSM
procedure is very flexible.It can be applied to a large
variety of sequential data.And in fact, I have hardly
covered the possibilities.But I think we need
to stop somewhere.I want to thank you
for staying with me upto the end of this tutorial.Because of the lack
of time, I haveleft out many important
topics, such as modelselection, details about
parameter estimation,and so on.I still hope that
I have given yousome idea about the
usefulness of state spacemodels in general and
the capabilities of PROCUCM and PROC SSM in particular.Both these procedures are
continually being worked on.For the near future, I'm working
on many initiatives at SAS,as well as my
personal initiatives.So we are working on
readiness for cloudand distributed computing,
readiness for streaming data,improved scalability,
and many morefeatures, like keyword
support for many more models.So there is a lot of
new things happeningto these two procedures
in particular and manyother things at SAS.I welcome your feedback.You can write to
me by email or useone of these communities,
SAS communities,to express your feedback.Thank you."
36,"Hi.And welcome to this video.I'm going to be presenting
Paper 4079, what'snew in SAS data management.My name is Nancy Rausch.I'm a senior software manager
in research and developmentat SAS.There are a series of key
themes available in the latestversions of SAS Data Management.These are connectivity,
automation,something that we're
investing heavily in,and improvements in data
quality and in ETL capabilities.So let's talk first
about connectivity.Connectivity runs the range of
where you can save and retrievedata from in your system.And that comes from
places like in memory,in Hadoop, in database, and
even streaming sources like ESP,and of course in the cloud.SAS also offers connectivity
to all of these locations.You can see some of them
listed on this slide.And they run all
kinds of types toofrom things like message cues
to Java to social media sources.If you have data set,
it's a good chancethat SAS has a data connector
that can connect to it.One of the most
interesting recent changeshas been in support of
columnar storage, whichcan be a very efficient
way of storing datain certain kinds of systems.And you can see from
the performance numberson our system that
we ran for our teststhat there were some significant
performance advantagesof storing data in this format.We support both Parquet
and ORC formats.When you're working
with your data,you can see how
to store it here.So if you're in the
user interfaces,there's a place where you'll be
able to, when you're saving outdata, specify the
format that you wantto save that data in to get
some of these performanceadvantages.Another nice feature is
support for image files.You may want to bring image
files in if, for example, youwant to do something
around deep learning.There is a way to
bring image filesthrough in through the UI.You could see this
in SAS Data Explorer.You specify the directory
and your image files,and you can import them.When they come in, they
come in as each imageon its own row of data.The image itself is stored
in binary in a column.There is a preview
feature available too,so that you can view it
if you want to see it.And then there's a
little bit of metadataabout the images that you can
see, such as what type of imageit is and where it came
from and even the sizeof that particular image.Another nice feature is
support for ESP data windows.And you can see that here.So you can bring data
through your streamingconnectors right in to
your user interfacesor into your VA reports or
wherever you want to use it.We also support integration
with SAS Drive, which is a wayto share content between users.You can upload data
to the SAS folder.And when you do that, you
can bring it into the systemthrough the Explorer's
Manage Data panel.Here, you can see you Select
the Folder's Location.And that will let you
search for data that youuploaded through SAS Drive.This is a nice
collaboration tool.There's also the ability to
upload and download files.So you can download tables that
you have stored in the system.And you can specify a couple
of formats that you want.You can download a sample
or the entire file.Moving on to automation,
this is a placewhere we're investing heavily,
bringing AI into the datamanagement space to
improve your productivity,using automation to
help speed up some tasksthat normally you would
have to do by yourself.So one area we're investing
is in the area of discovery.Suppose you're faced with a
huge pile of data content,and you don't exactly
know what's in it.We can peer inside that content
and identify a series of thingsand tag them, so
that you can getan idea of what's in
your data when you don'thave any kind of idea before.And we call that discovery.So what we do is we run
our profile capabilities,and we turn on
identification analysis.And what that does is it
peers inside your data,and it identifies things like
contacts or other content.And when it finds it, it
puts a tag on that data,so you can identify it.There is a series of things
you can identify and discover,which includes this list.Plus, there are more.And you can write your
own identifications.This identification
analysis is a key componentof our personal data
protection offerings.Here's a quick example.Suppose I have all this
data in this table.Some of the titles might mean
something, but some don't.And I would have to look
inside this data myselfto notice that there are
names and addresses in here.If I run the automated
identification,I can detect the same
information but automatically.So it can identify organizations
and social security numbers.And then I can run a
report on that datato see what was in it.Another nice capability is to
use our suggestions feature.What suggestions does is
it looks inside your data,identifies certain kinds
of content patterns,and then makes recommendations
about how to improve it.So you can see here,
for example, it'sidentified duplicate
data, that thereis a missing unique
identifier, that thereare numeric values that you
might want to center and scale.Suggestions can be run
at the table level,as shown over here on the left
or on a particular column.So how does suggestions work?What we do is we run a
series of decision treesthat we have pre-trained
on data that wehave worked out
through here at SASand then delivered models for.And so each decision
tree analyzes that data.If the data crosses
a certain threshold,then it will make
a recommendationwhich we call a suggestion.There are a variety of
pre-trained available.You can see some of them here.And there's some that are very
helpful, such as imputation.If we recognize that data is
above a certain threshold,more than 50% of the
data has content in it,but it is not 100%,
and we can impute,we'll recommend imputation.Another nice handy suggestion
is converting columns.If we detect that a
column is really a numericbut it's been coded
as a string, we'llmake a recommendation
to convert it.And we'll fill in all the
details for you to do that.Suggestions also
improve as you use them.So the models generate a report.And as you select them,
that causes a feedback loop,which then can go
back into the modelsto make better and
better suggestions.So finally, let's move
into data quality and ETL.There are a number
improvements in this area.In the area of
data quality, we'veadded the ability
to call data qualityalgorithms from DS2 code.As you can see in this example,
we're calling DQ standardizeddirectly in our DS2 code.We have a new Proc
called Datametrics,which will run our
profiling capabilityand generate details
around your data.So for example, in
here, I'm runningthat same
identification analysisthat I used to identify
content in my data lake.And you can see the results.It's found a name, an address,
a phone number, and evena government ID in my data.Proc Datametrics produces
a number of tablesthat you can use to
build your own reports.Another nice data
quality featureis cross field clustering.This uses the entity resolution
action set to cluster datatogether.And in this example,
I'm matchingon either home phone and
mobile phone or home phone,date of birth, and name.And so what that will
do is it will identifypeople or items, whatever
based on the rulesyou supply and
cluster them togetherto help you deduplify your data.This is a feature that's
in the works right now.We're working on it.And its data flow
support in Viya.This is an example of
what it will look like.There are nodes on the diagram.And you can build lines
to relate them together.That shows how data flows
through your system from sourceto target.We're pretty excited
about this feature.It's in the works now.It should be coming
out later this year.Another nice capability
is the abilityto support jobs flows
in Environment Manager.So here, you can
create jobs and thenstitch them together
similar to data flows.But these are for processes.So I can put process one
followed by process two.There is also support for
logic and for running processesin parallel.So if some process
generates some resultsthat then I'm looking for,
and I can check that value,I have the ability
to provide and andor logic between the
different processes.All of those processes
can be scheduledusing the jobs and flows
feature in Environment Manager.This is a licensed feature.So if you don't have
it in your system,you'll need to request that
from your account executive.Thank you.That's a summary of some
of the key capabilitiesavailable in our staff
data management offerings.I appreciate your time and for
listening to this presentation.And certainly, if you
have questions or wouldlike more information, you
can reach out to me hereis my contact information.Thank you for listening."
37,"CARL PALOMBARO: Hello, everyone,
and welcome to our projectfor SAS Global Forum 2020.This is our project on
determining college studentsuccess.My name is Carl Palombaro.KATHERINE FLOYD: Hi, everybody.My name is Katherine Floyd.KAYLA BARRIER: Hello, everyone.This is Kayla Barrier.CARL PALOMBARO: So why are we
looking at student loan debt?Well, according to
debt.org, US student debtis over $1.4 trillion, with over
$2,800 accruing every second,with the average student
owing over $37,000.Since 1980, there has
been a 241% increasein public school tuition
and a 344% increasein private school tuition.Therefore, our goal
for this projectis to determine how much
of an individual salaryit will take to repay their
student loans based on whatthey majored in and
which college theyattended, and ultimately, how
they can become debt-free.We also want to have
a look at the bestuniversity for each major
based on the mean salaryafter graduation.The approach we took
for this projectwas that of a student
about to go off to college.The student either
knows what departmentthey want to study under what
school they want to attendor that they just
want a college degree.They then must choose a school,
department, or both, dependingon what they knew before.Once they've graduated, they
will earn a salary basedon the department and school.That salary is then used for
their cost of living expensesand school loans.The money leftover will be
their disposable income.As you may have caught on
from the previous slide,we have made some
assumptions for this projectto keep the scope reasonable.First, the student
will be attendingone of the public universities
offered in North Carolina.Second, the student will
have in-state status,and therefore, we
will not be handlingout-of-state scenarios.Third, the student will be
completing the full educationat one school in four years.Therefore, we will
not be lookingat any transfer situations.Finally, once
graduated, the studentwill be employed
in North Carolinaand will be using the
standard repayment planwhen we repaying their loans.For those of you not familiar
with the UNC school system,we have provided a graphic
of the schools which havebeen considered in our project.KAYLA BARRIER: For
our project, wewere able to gather
our data from NC Tower.NC Tower is a web-based
delivery systemproviding aggregate
information on studentswho attended public
universities and communitycolleges in North Carolina.This data includes
programs of studies,degrees obtained, further
enrollment, and wageand employment information.For our project, we
used two data setsthat provided the primary
information regardingthe employment, salary,
and degrees attained.We cleaned them to only
show bachelor's degrees,as that was the degree we
were most concerned with.Rows with missing
salary informationwere not considered in
the analysis of salaries.A summary statistic of
all public universitiesand all subject areas
were not considered.We also included two new
columns in the data sets.We added a campus abbreviation
column and department column.The department column bins
the 192 unique degreesinto nine departments.Here we show how we broke
down the departments.In this project,
the focus was solelyon federal student loans
in the 2017 school year.These include subsidized
and unsubsidized loanswith a 4.53% rate
and Parent PLUS loanswith a 7.08% interest rate.Subsidized loans will not
accrue interest while in school.However, unsubsidized and
Parent PLUS loans will.After graduation,
subsidized loanswill accrue interest
continuously.The calculated repayment amounts
are based on the standard loanrepayment plan,
which gives studentsa consistent monthly
repayment amount thatwill repay the loan
in 10 years, assumingall payments are made on time.When looking at loan data sets
on the federal student aidwebsite, this data set seemed
most relevant to our project.It offers information
regarding loans availableand how many were
actually dispersed,as well as how many students
at North Carolina collegesreceived these loans.This information is
offered for every campus,as well as for each loan
type or focusing on--subsidized, unsubsidized,
and Parent PLUS loans.From this information,
we were able to calculatethe total loan
amounts per universitywith interest and an
estimated monthly repaymentamount using the
standard repaymentplan for federal loans.CARL PALOMBARO:
Next, we're goingto have a look at what some
of the state it looks like.There are two major data
sets we're focusing on--the employment data set
and the salary data set.Both sets consist of
data from 2006 to 2017.We will mainly be focusing
on the reported mean wageand employed graduates
from the employment dataset, and the reported mean,
median, 25th percentile,and 75th percentile wage
from the salary data set.Here we take a look at the
comparison of tuition and feesfor each of the schools.We can see that Elizabeth
City has the lowest tuition,while the UNC School of the Arts
has the highest tuition cost.KATHERINE FLOYD: As
mentioned earlier,we have nine departments which
our degrees are binned into.On the right, we can see
the sum of the graduatesfrom each department from the
UNC school system in 2017.You can see that health
sciences had the highest numberof graduates from that year.And on the left,
we've calculatedthe overall percentage
of graduatesfrom each department to give you
a feel of how many students arewithin each department.CARL PALOMBARO: This scatter
plot shows the average salariesafter one year for each
department at each school.Now, let us introduce the
cost of living to this graph.In North Carolina,
the cost of living,on average, is about $25,000,
so if we add a line at 25,000,the graph looks much
more interesting.We will discuss this later on.KAYLA BARRIER: Using
the loans data set alongwith data containing in-state
tuition for each schoolof interest, this report
shows each school'sfour-year tuition,
along with the breakdownof the average amounts of
the three types of loanstaken at each school.It also shows the average amount
each student will take outover the course of four years.For a closer look at how
much of a student's salarywill go towards loan
repayment, this reportwas created as well.This table displays each
school's four-year tuition,yearly loan repayment,
the average salaryfor each department
after graduation,and the percentage of
their monthly salarythat should be towards their
monthly loan repayment.KATHERINE FLOYD:
Based on an analysisof what would be left
over is disposable income,we can see that about 70%
applied science and engineeringgraduates would be able
to afford their loanpayments comfortably,
whereas about 56%of fine arts, humanities,
and social science graduateswould not be able to
afford their loan repaymentplan within the
traditional timeline.We are now going to explore the
three cases mentioned earlierin the presentation.For each specific
case, we will bemaking a couple of
assumptions justfor the sake of
this presentation.For case 1, we are going to
assume that the student knowshe wants to study biology, which
is within the Natural SciencesDepartment.We will also assume that
this student has the gradesto be accepted into
any of the UNC schoolsthat they might choose.So the first thing
he decides to dois look for which school has
the highest average salaryfor their students graduating
from the Natural SciencesDepartment.After his inspection, he
determines that the top choicesare East Carolina University,
NC State, and UNC Chapel Hill,all being in the
low $40,000 range.He ultimately determines that
he wants to attend UNC ChapelHill, even though East
Carolina technicallyhas the higher mean salary.This student also
looks at the trendof salaries from the top schools
after one year and up to 10years after graduation.In this case, we see
that UNC Chapel Hillhas the highest salary after
10 years in the workforce.Once the student has
graduated and receiveda job with a salary
in North Carolina,they must then consider the cost
of living in North Carolina.The chart above shows
the cost of livingfor a single individual
with no childrenwhile living a very
minimalistic lifestyle.We can see that this
individual shouldplan to spend approximately
$25,000 a yearto live in North Carolina.The data for this graph comes
from a living wage calculatorcreated by MIT, which can be
accessed from the source seenon the page.This graph uses the mean
salary data from 2007to 2018 student loan
information and the costof living in North
Carolina to calculatethe amount of disposable
income the studentwill generate in
the coming years.As seen in the
graph, the red barsindicate what the
individual owes each yearfrom loans and cost of living,
while the blue bars indicatehow much disposable
income there willbe at the end of each year.Assuming the student
graduated at the age of 22,they will begin to have
a disposable incomeby the age of 27.KAYLA BARRIER:
The second case isthe student who knows
and is determinedto go to one particular school.In this case, we can assume
the student wanted to goto East Carolina University.The first place
the student walksis to see where
other graduates havebeen successful in
graduating and receiving jobsafter college.By looking at the chart, the
student notices that business,and humanities and
social scienceshave the highest
employment after college.Comparing the two
department salaries,the student decides to pursue
a degree in business at ECU.Similar to the previous case,
this chart shows the studentthe cost of living
in North Carolinato calculate the amount
of disposable incomethis student will generate
in the coming years.This chart shows
that this studentcan expect a disposable income
two years after graduating,and be debt-free
within 11 years.CARL PALOMBARO:
The final case isa student who only knows
that they want a degree thatis affordable, but will not
put them into debt for yearsto come.First, the student compares
the tuitions and the loanamounts for each school to find
that Western Carolina has oneof the lowest tuition
costs of the UNCschools and a
reasonable loan amount.Therefore, they decide
on Western Carolinaas their school of choice.Once the school has
been chosen, theylook to see how well the
students from the schooldo in each department.They noticed that the applied
science graduates havedone very well for themselves,
and decide the applied sciencewill be their
department of choice.Much like the
other two cases, wecan see the student's
debt and disposable incomefrom the graph here.After only two
years, this studentwould have a disposable
income beginning to appear,and being debt-free before
their 31st birthday.Our second goal for
this project wasto determine the best
choice of universityfor each major based on mean
salary after graduation.Here we can see which
department's graduatesdo the best from each school
based on their mean salaryafter one year.As mentioned, for
our second goal,we wanted to see the best
department by mean salary.However, the goal is
get hired after college,so we added a
percent of graduateswho actually get employed.This graph here shows the
school with the best employmentrate for each department.This graph here shows
the highest mean salaryafter one year for
each department.In summary, we have looked
at three cases, whichwere able to predict possible
salaries, loan repayment,and disposable income.We've also found the top
schools for each departmentaccording to 2017 data,
along with the top employmentopportunities from
each department.What we have presented does not
cover every possible scenario,but is a tool to
show students someof the concepts they may want
to consider while looking aheadto college.KAYLA BARRIER: This
project has the abilityto help students make
the best choices whenchoosing a life path.I believe college is an
exceptional option for studentsgraduating high school, and
with guidance from this project,we can help students
make the best choicesto determine how life
after college would look.KATHERINE FLOYD: A
college educationis getting more
and more expensive,with less financial
gain than in the past.I feel that our project
represents a powerful toolin being able to aid students,
at least in North Carolina,in determining the best
financial decision theycan about college.College should not
be taken lightly,but with a little research
and determination,it can be possible for anybody.Thank you for your time."
38,"KIRK PAUL LAFLER:
Hello, everyone.I'd like to welcome you
to my hands-on workshop--A Hands-On Introduction to
SAS DATA Step Hash ProgrammingTechniques.I'm Kirk Paul Lafler,
and I'll be conductingthis hands-on workshop today.As a SAS software
user since 1979,I've worked in a
variety of capacities--as a consultant, application
developer, programmer, dataanalyst, educator, and author.The presentation objectives will
be to understand hash objects.We're going to explore
the world of hash objectsfrom an introductory
point of view.Then we'll turn our attention
to the basic hash object syntax.We'll explore the
syntax related to usingthe hash objects themselves.Then we'll turn our attention
to hash object examples.I'll cover examples
from search or look up,to match merging
techniques, and sorting--both ascending and
descending sorts.The tables or data
sets that we'lluse in all the examples
will be the SASHELP.CARStable consisting of 428
observations or rows and 15variables, and a
table or data setthat we'll create during
the hands-on workshop itselfcalled WORK.COLORS.It will consist of six
observations and threevariables.Let's begin.We'll start looking at and
understanding hash objectsthemselves.What is a hash object?Essentially, it serves or
acts like a data structure.It provides an efficient
mechanism for fast data storageand retrieval.It's implemented
in the SAS systemas a DATA step construct.It's not available in
PROCs or procedures.It essentially contains
an array of itemsthat maps keys to their
associated or respectivevalues.At the end of the
DATA step process,the hash object is
automatically removed.How does a hash object work?The contents of a table
are read into memory once.SAS can then repeatedly
access the memory as oftenor as necessary as it needs.Assuming that memory-based
operations are nanosecondspeeds, they're typically
faster than disk-based,which are millisecond
speed operations.Consequently, users
experience faster operations.Using the two tables--the SASHELP.CARS and
the COLORS table--we have a key called
TYPE on both tables.And we're going to
see, later on, howwe can match/merge these
two tables togetherbased upon the key.Now let us turn our attention
to basic hash object syntax.Like any language, hash
objects has its own unique setof syntax that we'll
explore and learn.The hash object is used
by calling methods.It looks and behaves,
oftentimes, like a function.There's 26 known methods.The basic syntax consists of the
name of the hash table, whichis essentially user-assigned,
followed by a dot, the desiredmethod by its name,
and any specificationsor arguments that need to
be passed to the method.Some examples-- just a couple,
and we'll explore these furtherlater--HashKey.DefineKey,
where we're goingto define a key, and
HashKey.Find, wherewe're going to perform
a look-up, or search.Now, what I've done
here is I've listed,alphabetically, the 26
known hash object methods.The methods that are
listed here in whitewe will not cover in
this hands-on workshop.But the methods that are listed
in blue, such as the ADD,DEFINEDATA, DEFINEDONE,
DEFINEKEY, FIND, and FIND_NEXTwill be illustrated in one
or more of the examplesthat we're going to cover
in this hands-on workshop.Continuing
alphabetically, we seethat we have HAS_NEXT
and OUTPUT methods.The ones listed in
blue, again, willbe covered in examples later
on in this hands-on workshop,where you actually will
be able to try them outand experiment using your own
version of the SAS software.Whether you use SAS 9.4, the
licensed version, or the SASUniversity Edition
doesn't really matter,because the hash objects exist
in both software versions.And then finally, in
alphabetical lists,here are the remainder of
the hash object methods.Now, the methods
that I'm not goingto cover in this workshop,
as well as all the ones I do,can be found with further
details in the referencesthat I have listed at the
end of this presentation.So I list many
references that you'llbe able to find more content
should your curiosity be there,and how to use, how
to apply, and thenwork with the hash
objects themselves.Now let's turn our attention
to some actual hash objectexamples.In this first
example, our purposeis to create a car's
color data set, or table.We're going to use the
SASHELP.CARS table,or data set, and we're going
to create this new data set--a temporary data set--called WORK.COLORS.It has three variables--Type, which is a character
variable, Exterior_Color,and Interior_Color, which
are all character variables.And as you notice
here, the data linesare going to actually type in
the physical data lines, whichconsist of six observations--or rows-- into this code,
followed by the semicolon,and then the run.Essentially, what
this code's goingto do when you execute this code
is it will define and createthe Cars Color data set,
consisting of the sixrows and three variables.And it's going to look something
like this when you print them--either using proc print, or SQL
SELECT, or some other procedureto display the data.Notice, if you've entered
the information incorrectly,all the data lines--you'll see that the
data all lines upcorrectly under each one of
the variables themselves.And just to validate
and verify, wehave the ability to use
proc print or proc sqlto actually display the
contents of the WORK.COLORS dataset or table.Here I'm showing proc print
using the var statement,or proc sql using the select
statement and the from clause.And you choose whichever
one you're familiar withor prefer to use.And then when you're
done, go aheadand submit that and
run that code in orderto verify and view the contents
of the WORK.COLORS data set.And if you've entered all the
data in correctly in the datalines, and your code worked--checking your log, make sure
that everything is correctand there are no data errors or
no syntax errors of any type,you should see the
WORK.COLORS data--the six observations, or
rows, and three variables.Now we're going to look at
how to conduct or performa hash search, or lookup.A search or table
lookup operationis a common task
performed by hash objects.The result of a hash
search or find operationproduces a result of
a single observationthat matches a user-specified
search criteria.The following methods
will be used--DefineKey, DefineData,
DefineDone, and Find.In exercise 3, I'd like for you
to start entering this code.So in this first statement,
the data statement,we're going to initiate
and create a name,a temporary SAS data
set to store the resultsfrom our hash object processing.And just to keep
the resources down,we're going to use the keep=
data set option as well--just to list out Origin
Type, Make, Model, and MSRP.Now, this statement-- if
0 then set SASHELP.CARS--this technique allows you to
capture metadata from whateverdata set we're processing--
in our case, SASHELP.CARS--so we can use and parlay that
information into the hashobject itself.Next, we're going to
enter Origin = USA.And you can use--as long as they're bounded--single quotes or double quotes.In this case, I'm
using single quotesto assign a variable
and character stringvalue that will be used for
search or lookup purposes.And then what we're going to
do is just the first iterationthrough the data step.Now, keep in mind--a data step will
iterate as many timesas there are records
or observations.In this case, we're using
the automatic variable--the automatic system
variable called _n_--to tell SAS that just
the first iterationthrough the data step, we
want to declare the nameof our hash object as HCars.That's a user-assigned name.And the data set that we want
to point to is SASHELP.CARS.And we also want to
define the key as ORIGIN,and we also want
to define the datathat we want to keep as
Type, Make, Model, and MSRP.And then we're going
to tell it we're done.The next thing
we're going to do iswe're going to use
the find methodto determine whether the
specified key is foundin the hash object,
and then issue the stopto stop our step.Putting this all
together, this datastep itself, when
executed, is goingto create a hash
object in memory,collecting or
capturing the metadatainformation from
the SASHELP.CARS,parlaying that to the
hash object itself.So it's going to carry on
and pick up the metadata--the metadata being defined
as data about data,or information about data.And just like we did in
exercise 2, in exercise 4,we're going to look at the
results set from that data stephash object execution.We can use proc
print or proc sql.We'll repeat this over
and over for each oneof the major examples.Of course, you could use
proc report or proc tabulateand other things, but
just to keep it simpleand to concentrate just on
the hash object, I thought,let's just use proc
print or proc sql--whichever you're familiar with--just to explore the results set.In this case, it found USA, SUV,
Buick, Rainier, and the MSRP.With the default action
of the hash object,once it finds a value
in the hash object,it automatically stops.Now let's turn our attention
to how we can actuallyprocess a data set where
multiple observations areassociated with the key.In other words, you have
duplicate observationsof the key.We're going to use a lookup with
Find, Has_next, and Find_next.We're also going to specify,
when we declare the hash,assigning the name--the MULTIDATA-- ""YES"" argument.Now, the different methods
we're going to use for thisare DefineKey, DefineData,
DefineDone, Find, Has_next,and Find_next.Now, the code itself--the font I used is
a little bit small,but I wanted to try to
get it all on one slide,rather than bounce it
across two or more slides.So I apologize for the
size of the font that'sbeing used here, but I just want
to go over what the code is.First thing we want to do--we want to create a data
set to store our results to,and then we want to capture the
metadata from the SASHELP.CARS.And just like we
did in exercise 3,when we did a search or a
lookup, the first iterationthrough the data step--if _n_ = 1 in only the first
iteration of the data step,we're going to declare
a hash, assign a name,and point to the SASHELP.CARS
as our data set that we wantto read, and with
multidata-- ""yes,""define our key as
ORIGIN, define our data--Type, Make, Model, MSRP--and then DefineDone.Then let's say we
just want to lookat USA manufactured cars from
the SASHELP.CARS data set,or table.We can say do Origin = USA.If HCars.FIND = 0, then do.Essentially, what
we're doing is we'reassigning the variable
and character string valueto be used as our key.Perform a lookup
what the find methodto determine whether the key
exists in the hash object,and if it does, output the
contents to the SAS data set.Then traverse the data
set, looking for other datavalues that match the key.And it continues
looping through this,collecting all the observations
that match Origin = USA.That multidata-- ""yes""
and the looping techniquethat's used here, along with
the HAS_NEXT and the FIND_NEXTgives us the ability to
find all the origin USAmanufactured cars, or anything
else that we want to use.And just like we did in
the previous exercises,we want to actually use
proc print or proc sqlto observe or view the
results of our hash objectto see what was
collected and whatwas stored in the data set.So in this case, our data set
that we created in exercise 5was WORK.Hash_search_all, and
we want to display the Origin,Type, Make, Model, and MSRP.Now, as you see, the
results are much morethan the exercise 4,
because in this example,in this exercise,
what we're doing isis we're collecting all
the USA manufactured cars,and not stopping at
the first result,or the first successful match.It's going to collect them all,
and store them or save them outto the SAS data set.In our next example, we're
going to turn our attentionto performing what's called as
a hash match-merge, or join.Now, you can perform
all different typesof joins or match-merges.In this particular
case, we want to createan intersect, represented in
the Venn diagram by the teal,or the cyan-colored AB circle.The hash data set construct
uses the following methodsto perform a match-merge--DefineKey, DefineData,
DefineDone, and Find.So the intent here is just
to find the intersect.In exercise 7, we're
going to, once again,use the hash object in a data
step to create a data setcalled, in our case,
WORK.Hash_match_merge.We want to then use if
0 then set WORK.COLORS.We're going to capture,
or collect the metadatafrom the WORK.COLORS data set
that we created in exercise 1.And like what we've done
in the past exercises,the first iteration
through the data step--and we only need
to do this once--is we need to build, or define,
or declare our hash object.In this case, it's going
to be called HColors,and it's going to point to
the data set WORK.COLORS.We're going to define
our key as TYPE,and define the data that we
want to keep and maintainin our data set Exterior_Color
and Interior_Color.Then we're going to
tell it we're done.Then we're going to
perform a disk-based readfrom the SASHELP.CARS data set.We're going to do this
with a set statement.Now, this is a
disk-based read, so yes,this is a little slower than if
we were to bring it into memoryand then process in memory, but
the one thing we've done hereis we've gone ahead and read
into memory the WORK.COLORS.The SASHELP.CARS is
being read and processedfrom the disk, or the server.And then finally, we're
going to use the FINDmethod to determine whether
the specified key is foundin the hash object, and
then output the resultsto the data set that we
created on the data statement.So if HColors.FIND, the key,
is the TYPE variable = 0,then output.Now, that output is
the output statement.We're going to see
a little later,we're going to use
the output method.So this program is going to
create a match-merge data set.We're going to store the results
to WORK.Hash_match_merge,capture our metadata
from the WORK.COLORS,and basically parlay that to
the hash object itself thatwe're declaring and defining
in memory called HColors.We're defining our key as
type, our data Exterior_Colorand Interior_Color
of the hash object,and then we're going to
read the SASHELP.CARSas a disk-based
operation, and thenprocess the match-merge
just as if wewere doing a data step merge or
an sql join with the intercept.Exercise 8-- we're going to,
just like we've done before,use either proc print or proc
sql to view the results--just to make sure that
we've done them correctlyand created a intersect
from the two tables,SASHELP.CARS and WORK.COLORS.And we can see that we have
all the variables that we'vespecified here--Origin, Type, Make,
Model, and MSRP--from SASHELP.CARS, and
Type and Exterior_Colorand Interior_Color from
the WORK.COLORS table.And these are matches.So when you run this
exercise, exercise 8--exercise 7 and exercise 8--make sure that you
delve into the detailsso you can learn
the whole process.Now we're going to turn to
the next type of applicationthat we can perform
with hash objects.It's called a hash sort.Sorting is a common task
performed by all SAS users.Using hash programming
techniques,SAS users have an alternative
to using the sort procedure.Now, I'm not saying to
use this over proc sort,or whatever sorting
mechanism that you're using.But this is another
thing you mightwant to put in your skill set.The methods to perform a SORT
is DefineKey, DefineData,DefineDone, the Add method,
and then the Output method.So we're going to
explore two new methodsthat we haven't seen.In this exercise 9, just
like we've done before,we're going use the data step.In this case, we're not going
to actually name our data seton the data statement.We're going to use the _null_,
because we're going to usethe output method to
name our data set later.If 0, then set SASHELP.CARS.Capture and collect the metadata
from the SASHELP.CARS tableor data set.The first iteration
through the datastep we're going to go
ahead and declare or defineour hash object.In this case, we're
going to name it HSort.And notice we have an ordered--'a.' As you can imagine, we want
to perform an ascending hashsort.So the ""a"" stands for ascending.And then we're going to
define our key as MSRP.And you can change
that around if youprefer to use a different key.We're also going to
define what data we want,and the result set, with
our Define Data method--Origin, Type, Make,
Model, and MSRP--and then tell SAS
that we're done.We're then going to perform
a disk-based operation usingthe set statement
against our SASHELP.CARS.Notice we went to
set up an indicator.end=eof-- eof standing
for end of file.And then what we're
going to do iswe're going to use
the ADD method.We're going to use this
ADD method, basically,to add data to the hash object.And when we're processing the
last record, the end of file,if eof--or if end of file--then, using the
output method, wewant to write the results to
a data set that we assign.In our case, we're going to
call it WORK.Hash_Sorted.So this program is
a very useful wayto perform sorts using
hash, and it basicallyfollows previous examples--
for the most part.Now, it does include the ADD
method and the output method,so I wanted you to
see how those work.And then when you
output the results,they're automatically going to
be ordered in ascending orderbased upon the key.In our case, we define
the key as MSRP.And just like we've
done before, wewant to inspect,
or view, and verifythat our hash object worked
as we expected and hoped.So we can use proc print to
display the WORK.Hash_Sorteddata set, or proc sql to display
the information to our output,or to our results window.And we can see,
from the results,if you look at the MSRP column--
the last column in this dataset--the MSRP is in ascending
order by the dollarvalue for each one of the cars.So it looks like it worked well.Now, I only printed out
the first 20 records,or displayed the
first 20 records,but there's over 400 records
that are contained in here.And of course, you can subset,
et cetera, in the hash object,or in your proc
print or proc sql,to reduce the size
of the result set.In exercise 11, we're going
to reverse the sort order,and we're going to perform
a descending hash sort.Just like we did in
exercise number 9,we're going to say
data_null_, and then if 0,then set SASHELP.CARS, capturing
or collecting the metadata fromthe SASHELP.CARS
data set or table.The first iteration
through the data step--if _n_ = 1, then we're going
to declare our hash and name itcalled HSort.And this time we're
going to say ordered--'d,' for descending.And the d can be uppercase
or lowercase, doesn't matter.Just has to be
enclosed in eithersingle or double quotes
that are balanced.Then we're going to define
our key, define our data,and tell it when it's done.And just like we did before,
I'm showing here a setstatement to perform
a disk-based readfrom the SASHELP.CARS
data set or table,and we're also setting up
an end of file indicatorwith the END =
parameter that we'regoing to use at the very
end of this program.Just like we saw with
the ascending hash sort,now that we're doing a
descending hash sort,we still need to be able to
add data to the hash object.And we're going to do
this with the ADD method.And then we're going
to execute if endof file when we're
processing the last rowor last observation,
what we want to dois we want to output,
using the OUTPUT method,to a data set that
we want to write to.In our case, I'm showing,
here, WORK.Hash_Sorted.It's a user-assigned name.So all the results, in
descending order by MSRP,are going to be written to
WORK.Hash_Sorted data set.So this descending hash
sought routine essentiallycaptured the metadata
from the data set that wewant to read data from--SASHELP.CARS.The first iteration
through this stepwe want to declare or define our
hash object, calling it HSort,telling SAS that we
want the data setto be ordered in descending
order by the key MSRP.And just like we did
before, exercise 12,we're going to issue a
proc print or a proc sqlso that we can view
or display the resultsfrom our hash object that we
wrote to perform or emulatea descending hash
sort in exercise 11.So once this code is entered--either proc print or proc sql--you run, it you'll be able to
see the results on your resultswindow.And if you look at
the MSRP column--the very last column
in the results--you can see that the largest
values for ManufacturerSuggested Retail
Price appear first,and gradually, as you go
down the number of rows,you see that it's getting
smaller and smaller.So it looks like our descending
hash sort worked correctly.In conclusion, I
wanted to give youan understanding of
what hash objects are,what their purpose is.Essentially, they
give us the abilityto efficiently
read, manage data,because it takes
advantage of real memory,and memory speeds
are faster than justdisk-based processing.We then turned our attention
to basic hash object syntax.I showed you that there's
alphabetical listsof 26 different methods.We looked at about
eight of those methodsin various examples.And then we looked at
hash object examples.We did a search, or
lookup type of process,where it found the
first occurrenceand it automatically stopped.We then turned our
attention to howwe can process and search,
or look up duplicates.And then we looked at how to
perform a match-merge, and thensorts.Further reading and research--there are a number
of wonderful books,papers that are available to
SAS users throughout the world.Paul Dorfman and Don
Henderson, in 2018,came out with their book on
Data Management Solutions--Using SAS Hash
Table Operations--A Business Intelligence
Case Study.It's published by SAS Press.You also notice that Paul
Dorfman and others have writtenpapers, and these papers can be
all be found on lexjansen.com.Peter Eberhardt,
2011, and then in 2018I also wrote and presented
a paper on hash objects.So there's a number
of hash objectpapers and wonderful resources.A little self-publicity--I'm a SAS Press author.I have written a book for
SAS Press called PROC SQL--Beyond the Basics Using SAS.It's currently in
its third edition.The third edition
came out in 2019.It's available at SAS Press,
as well as Amazon.com,and many different
online booksellers.I want to thank you for
viewing this hands-on workshop,and I also want to thank Lisa
Mendez, the SAS Global Forum2020 Conference Chair,
the SAS Global Forumteam, and SAS Institute
for all their hard workin making SAS Global Forum
2020, the virtual event, happen.And I also want to
thank all of youfor taking the time to
view my hands-on workshop.Should you have any
questions, pleasefeel free to reach out to me.My email is right here--KirkLafler@cs.com, or LinkedIn.If you'd like to connect
there, I'd be happy to do that.Again, thank you so much.Have a great virtual event
SAS Global Forum 2020,and I definitely look forward
to meeting you and seeing youat a future conference.Bye, now."
39,"Hi, my name is Elizabeth Bales.I am the Senior Manager of the
Globalization R&D team at SAS.Today, I would
like to talk to youabout how you can see all of
your characters in SAS studios,otherwise known as
exploring the great unknown.Many of you are familiar
with this characteron the screen that's
staring at you right now.I'd like to share with
you some strategiesyou can use to manage unexpected
characters like this one.When you read data
files into SAS,open SAS program
into SAS Studio,or simply want to
convert charactersto ASCII when that is possible.When you read a
data file into SAS,it's important to
know the encodingof the data in the file.Here, I'm displaying a file
I created in an editor calledNotepad++.That tells me that the encoding
of the data inside the fileis Windows-1252.That encoding is actually
the same encodingthat SAS calls WLATIN1.I've highlighted
a few charactersthat we'll look at
in just a moment.The Windows LATIN1 encoding
is a single biting coding.That means every character
in the characters that onlyrequires one byte in memory.However, when you convert
Windows LATIN1 charactersto UTF-8, some of
those characterswill require more than one byte.There's an excellent document
page in the National LanguageReport Reference
Guide that will helpyou understand this better.I've highlighted
those same charactersthat were highlighted
on the previous slide.Let's look at how
SAS manages thosewhen it reads in the data.I have a simple data--I have a simple SAS program.I'm going to run that
in a UTF-8 SAS session.And you'll see that some of
those characters, the onesthat I happen to
have highlighted,were displayed in the SAS log
as unknown characters usingthat symbol we looked
at the beginning.And when I try and
display the data setting,even though the data set was
created correctly, my SAS--my Proc Print step failed.Well, I want to do
some debugging on this.I'll run Proc contents
and check things out.The data set encoding is UTF-8.That matches my SAS
session encoding.And by the way, I'm using UTF-8
for all of my SAS sessionsthat I'm demoing today.So UTF-8 is my
data set encoding.But unfortunately, the
data inside my data setis not UTF-8.It's Windows LATIN1.why?Well, the reason is because
when SAS reads in a data file.It simply reads the data
unless you-- and assumingthat the encoding of that data
is the same as your SAS sessionencoding.In this case, we
know that was not--that those encodings
were not the same.But SAS was not told that.So it didn't know to do anything
about transcoding the data.It created the data set with
WLATIN1 characters inside.And there are several
possible solutions.I will mention
two of those here.And one solution is to run Proc
Datasets with the correctingencoding option and change the
data encoding in the data setheader.Note, this only changes
the data set encoding.It does not change
the actual encodingof the data inside the data set.I'm going to choose today
to recreate my data set.I'll specify the file
name encoding optionand pull that, that the
data in the file is WLATIN1.I made that change and
reran my data sets.My characters display
successfully in the SAS log.I also find that my Proc Print
runs successfully and displaysall of my characters.When you open SAS programs,
you may also find at that timethat you have
unaccepted characters.Here's an example.I'm using a copy of the same
program, except that I'm not--this time, I've modified as
a title statement for my SASoutput.I'm including a dash
character that's not actuallyas ASCII character.Again, you'll see that
I've created this SASprogram using the
Windows-1252 WLATIN1 encoding.Some of the characters
that you see on the screenare actually auto formatted
characters-- are actuallyimpacted by an
auto format featurethat some browsers use to
convert simple punctuationcharacters to special types
of graphic characters.They make text files
much easier to read.However, those of typographic
characters are not ASCII,and that is going to
cause the problem here.You'll notice that when
SAS Studios opens the file,it was unable to render
that special dash character.And also, when the Proc Print
step ran, there was no error.But again, that
character renderedas that unknown
character symbol.And the reason is that SAS
Studio was expecting the SASprogram file to be UTF-8.By default, the texting encoding
that SAS Studio is expectingis UTF-8.I could change that
default text encoding.However, that would
actually applyto every SAS and text file
that they've been doingfor that SAS Studio session.Instead, I would like to be
able to specify the encodingfor this one particular file.I'm going to do that--show you how I did that
using the Open Dialogue.I opened the open dialogue
and selected it with encoding.When I select the Open
button in the dialog,SAS Studio presents
me another dialogthat allows me to specify the
encoding I wanted to use--in this case,
again, Windows-125.SAS studio has no
problem opening the fileand converting the data from
WINDOWSLATIN1 into UTF-8.We can see that the
title and the outputalso is displayed correctly
where those dash characters isdisplayed correctly.Finally, you may want
to simply convertsome of those unexpected
characters to ASCII.I'd like to introduce you to
two SAS features, both functionsthat can help you.The KPROPDATA
function can't manageits unexpected characters.You can select the
conversion-- the wayyou would like to convert
those unexpected charactersin your resulting data stream.The BASECHAR function converts
actual accented charactersto an ASCII form if
one is available.I have a very
simple SAS program.It has three observations.The first two observations
have some of those punctuationcharacters that were impacted
by that auto format featureI mentioned earlier.The last observation has an
accented character in the word,café.I apply BASECHAR to the
character variable task.And I'm also applying
KPROPDATA directlyto that same character
variable task,using a new option called PUNC.Let's see how fast of those
two functions impact the data.The original first--
original textis displayed in
that first column.You'll notice those special
quote marks and that long dashcharacter, as well as the
word café with the accent.After BASECHAR is applied
the punctuation characterslook the same as they
did in the original.However, the word café
has been modified.That lower case é, accented
character has been convertedto a simple ASCII.The k after in the final
column, the KPROPDATA results,showed that those
punctuation charactershave been converted from those
special typographic charactersto simple ASCII punctuation.There's more information
available in the NationalLanguage Support
Reference Guide,as well as several papers
that are available online.You can look for
those referencesin these slides
which will be madeavailable on the SAS
communities GitHubfor SAS global forum 2020.There are two papers that were
written for SAS Global Forum2020 that you might
want to check out,and some pages in the National
Language Support ReferenceGuide that you may find helpful.I would like to thank
you for joining me today.I would like to hear from you if
you have questions or comments.Thank you very much."
40,"Hi, everyone.My name is Andy Ravenna.And I'm a technical
trainer at SAS Institute.Today, I would
like to talk to youabout how to train machine
learning models both quicklyand interactively.This is basically a subset
or a sample of a coursethat we can teach for
you at SAS Institute.There's going to be
some links down belowthat you can click on to
get more information if youneed them.But in the meantime, I'm going
to go ahead and get started.Let me switch over and
grab my PowerPoint slides.There we go.And so what we're going to
talk about in the short timethat we're going to
spend together right nowis I want to give you a very
broad introduction into SASVisual Data Mining
and Machine Learning.Then I'm going to
very quickly jumpinto some machine
learning modelsand show you how to
create some models.And you'll notice
that we actuallyhave several different
machine learning modelsavailable to you in SAS
Visual Data Mining and MachineLearning.But in the time that you and
I are going to spend together,I'm going to cover
just two of those.I'm going to focus in on
neural networks, whichare one of my favorites
and are really cool,and also talk to
you about forests.So let's go ahead
and get started.I'd like to start by
giving you a broad overviewor an introduction into SAS
Visual Data Mining and MachineLearning.SAS Visual Data Mining and
Machine Learning is a web-basedproduct that's actually an
add-on onto two other SASproducts--SAS Visual Analytics and
SAS Visual Statistics.There is actually sort of
a hierarchical relationshipamong these three products.You're going to start
with SAS Visual Analytics.And that's where you're going
to start to explore your dataand clean up your data.Then on top of
that, you're goingto add SAS Visual Statistics.And that's where
we begin to performboth supervised and
unsupervised analysis.Finally, on top of
that, we're goingto add SAS Visual Data
Mining and Machine Learning.That's what you and I are going
to be talking about today.And this is where
we begin to startto build machine
learning models, thingslike neural network,
support vector machines.So it turns out that
all of these productsare basically taking advantage
of in-memory capabilitieson these SAS servers.And we call this SAS
Cloud Analytics Services.We call that CAS.And basically, you're going to
have a whole grid of machines,a whole bunch of
machines that aregoing to be linked together.And they're going to allow
you to load up big data.And that big data is
going to stay in memory.And not only does the
data stay in memory,but also the analytics
are performed in memory.One of the really cool
things about puttingyour data in memory is that
basically, it persists.And multiple people
can be working offof that same one table that's
been loaded into memory.And they can all
be building models.And that's one of the reasons
that SAS Viya is so efficient.We're going to focus in on SAS
Visual Data Mining and MachineLearning.And as it turns out,
there's actuallytwo different
approaches we could use.We could use this programming
approach or the visual dragand drop approach.So we've got these two
different approaches.Now, if you and I
wanted to write code,then what we could do is
we could take advantageof writing either these things
called CAS Actions, whichare sort of a more
fundamental unit of code,or a little bit
friendlier, you and Icould write SAS procedures.Some of you may
already be SAS coders.So you're already
familiar with the conceptof a SAS proc or procedure.Well, you and I are not
going to be doing that.We don't want to write code
today, even though we could.We're going to take advantage
of this visual drag and dropapproach.Now, actually, it turns out
that SAS Viya is so flexiblethat we actually have multiple
ways of being able to do that.We could use, for
example, SAS Studio.SAS Studio allows
us to take advantageof these things called tasks.And while you're
pointing and clicking,then SAS is going to be
writing code in the background.And you can actually look at
that code if you'd like to.Another option would
be to use Model Studio.Now, Model Studio
is very powerfulbecause it allows you
to build pipelines.And so you can create many
different models very quickly.But you and I are not
going to be doing that.We're going to be focusing
here on SAS Visual Analytics.Remember, SAS Visual Data
Mining and Machine Learningis an add-on to SAS
Visual Analytics.Let's dig just a
little deeper in twoSAS Visual Data Mining and
Machine Learning on SAS Viya.Now, I've shared
with you that youcan take this programmatic
approach or point and clickapproach.If you're using
Visual Analytics,and you want to use
the visual interface,you have the ability
to build these sixmodels that are listed here--Bayesian networks, factorization
machines and forestsand gradient boosting,
neural networks,and support vector machines.However, if you'd
like to write code,there are additional options
available to you in the SASStudio Programmatic Interface.So for example, I might have a
student that would come to meand say, hey, Andy.I'd like to be able to perform
robust principal componentanalysis.That's at the very
bottom of the list here.And I'll tell them, sure.You can do that.But you're going to
have to write codeif you want to do that.Well, you and I are
going to be focusing inon this visual interface.And what I want to start
with is by sharing with youhow to build models
using neural networks.So let's go ahead
and get started.Before we actually build
a model in the software,I know some of you
are going to be veryfamiliar with neural networks.And some of you are going to be
brand new to neural networks.So let's just talk a little
bit about neural networksbefore we actually build one.Neural network models
were originallyinspired by neurophysiology
and the interconnectionsbetween neurons.If you think about
it, our brain isvery similar to a
machine learning model.It takes data in
in order to learn.And the more data and the
better data that we get,the better the machine
learning model isgoing to perform just
like the better our brainis going to perform.Let's think about
me as a child, whichwas a very, very long ago.And I'm not going to
tell you when that was.I didn't know, for example,
the differences between applesand oranges until I
actually learned that.I learned that apples were red.Oranges were typically
more orange in color.I also learned that apples
were very hard on the outside.And you could eat the skin.But with oranges,
you peeled them open.And they tended to
be a little juicier.So just like I learned that
information as my experiencegrew, these neural
networks, whichare a great example of
machine learning models,they are also going to continue
to learn and grow and performbetter with more data
being fed into them.You can see here that the
basic unit of computationin a neural network
is the neuron.Let me point that out
for you right here.So here's my neuron.Some people call this a node.And what it's going
to do is that neuronis going to receive input
from some other nodesor from an external source.And it is going to
compute an output.So here is my output over here.Here is my input.And in this slide, you can
see that my neural networkis depicted with
just a single neuron.And in this example,
what we're trying to dois take a linear regression and
view it as a neural network.So we can kind of think of
that neural network as a CPU--a central processing unit.And over here on
the left hand side,if you think about a linear
regression, what I have isI have my parameter estimates--
all of my betas times my input.And so what the neural
network is going to do--it's the central
processing unit.Its responsibility
is to add togetherthe weighted inputs of these
parameters that we have here.It's also going to add
together the y-intercept,and then it's going to place
that sum into some sortof transformation function.Now, for linear regression,
that, of course,is going to be the
identity function.And then what we'll have
over here on the outside,we'll have our output coming
from that linear regression.Here we have an example
of a relatively simpleneural network.Now, just like a
regression, we'regoing to be making a prediction.Over here is my prediction.It's my y hat.And we're going to
get that predictionfrom this particular formula.So that prediction
is going to usethe values of our
input variablesin this mathematical equation.We can think of our
neural network formulaas a regression of
the response variableson a set of derived inputs
called hidden units.You can see in this
particular example,I've got one, two,
three hidden units.Let me show you what those
hidden units consist of.So now, here are
my hidden units.And you can also see down
here that for each oneof these hidden units
that what it looks likeis that these are
sort of a regressionon the original inputs.So I've got my x1
input and my x2 input.You could kind of
think of maybe we'relooking at the age and the
income of either a patientor a customer.So these hidden units can be
thought of as a regressionon those original inputs.And in a way, what
we're kind of doing iswe're sort of doing a
regression on a regression.And that's not too far
away from what's going on.Now, you also might have
noticed that down here,each one of these
hidden units has what'scalled an activation function.And in this case, it's
the hyperbolic tangent.Now, what that's
doing for us is it'sgoing to make sure that
anything that comes outof each of these hidden units
is bounded between positive 1and negative 1.Because when neural networks
are trying to find the solution,very large values
cause difficulty.So we're kind of
bounding that output.I also want you to notice
that the componentsof a neural network have a very
different naming convention.And that's because of our neural
network's biological roots.So instead of
parameter estimates,betas, what we're seeing
here are these Ws.So I've got a lot of W0s
and we call those weights.And then instead
of a y-intercept,in the neural
network, we actuallycall that a bias estimate.And one final thing that
I want you to noticeis that if this was
a linear regression,I would have much, much
fewer parameter estimates.And so you can see even
with the relatively simpleneural network that I have
a lot of weight estimates.So that neural network has
to find a lot more valueswhen it's working off of
this neural network model.What are advantages
of neural networks?Well, it turns out
that neural networksare very popular because
they're universal approximators.So if we have a
sufficient numberof hidden units and enough
time, a neural networkcan model any
input-output relationshipto just about any
degree of precision.They can handle both linear
and non-linear relationships.Now, unlike a
regression, it's notnecessary to specify the
functional form of the model.In other words,
unlike a regression,we don't need to know the
relationship between the inputsand the outputs.So with a linear
regression, you knowif we want to test
an x squared term,we have to add that x
squared term into the modelto see if it's significant.With a neural
network, we don't haveto specify any sort
of relationshipbetween those
inputs and outputs.We just basically
build the modelto define what our
model is going to be.And it's going to be able
to work off of the data.A final advantage of neural
networks is that once trained,a neural network is among
the fastest executingpredictive models.And that means that a trained
neural network can efficientlystore large volumes of data.And that's something
to think aboutif you have several
options when you're tryingto pick a champion model.But there are some disadvantages
to neural networks.So what are the disadvantages
of neural network?The big one is this lack
of interpretability.Some people call neural
networks black boxes.And that's because
we can't reallyexplain why a neural network
gives you a certain result.Let's say, for example, that
I built a neural networkto determine whether a customer
should get a loan or not.And let's say that the neural
network denies Andy a loan.Well, one of the things that's
kind of interesting about thatis that we can't really tell you
why the neural network decidednot to give me a loan.Now, let's contrast that
with a decision tree.Let's suppose I had
a decision tree.And we denied Andy a loan.Well, then I've got
a full set of rulesthat I can use to help explain
why Andy didn't get a loan.Andy, we didn't give
you a loan because youdon't make enough money.And you defaulted on
your last three loans.OK, now, I understand.A second disadvantage
of neural networksis that unique optimal
values for weightsare really not
guaranteed because youmight run into a local optima
versus the global optima.So it's possible to run a
neural network multiple timeswith the same data and get
slightly different results.Let's dive in and build
a neural network modelin SAS Visual Data Mining
and Machine Learning.So I've logged in to SAS Viya,
which by default is goingto bring me into SAS Drive.And what we want to do is go
to this very upper left handcorner menu.And you and I are going to
go into Visual Analyticsby selecting Explore
and Visualize.That brings me directly
into a brand new reportwhere I can start to
build my neural network.So I'm going to come
over here to Data.And I went ahead and
preloaded in a tablethat I'm familiar
with working on.It happens to be data that
is collected informationover a campaign series
where we kept trackof whether a customer from
a bank made a purchaseor not for different
kinds of items.For example, this bank offered
loans and credit cards and bankaccounts.And so now, we want to be
able to use that past dataand determine whether a
customer is going to makea purchase in the future.The primary target
variable is down here.It's called Target
Binary New Product.It's actually coded
as zeros and oneswhere a one indicates that
a customer made a purchaseduring the campaign season.And zero indicates that
they made no purchase.Now, because it's coded
in zeros and ones,Visual Analytics is actually
storing it as a measure.So I'm going to do a right
mouse button click on that.And I'm just going to
very quickly convert itto a category.So now, we've got that
categorical primary targetvariable, which
happens to be binary.And we're going to be able
to use it as our targetto make some predictions.I'm going to come over
to the Objects paneand scroll down and find
Visual Data Mining and MachineLearning.And here's my neural
network object.And I'm going to double click
on it, even though I could justdrag and drop it to a canvas.And now, Visual
Analytics is saying,hey, Andy, you need
to assign some data.Now, before I do that, I
want to go up here and switchan option, which we're going
to go to our Interface optionsand we're going to
disable the Auto Refresh.And basically,
what that means isI'd like to make lots of changes
before I refresh the screen.I don't want the screen
to refresh every time Imake a small change.Now, I can click on Assign
Data here in the middle.Or I could go over
to the Roles pane.I'm going to try here.I'm going to add in
my response variable.That's going to be this
Binary New Product.And now, we want to
add in our predictorsor our explanatory variables.Well, we have some data here
that's already been cleaned.And that's actually
the 12 variablesthat I'm going to select here.They are continuous
variables that include thingslike the average sales
that a customer madeover their lifetime and the
customer tenure in months.So this is a whole bunch
of continuous variables.Then I'm going to
scroll up to the top.And I'm also going to add in
two categorical variables thatshow us an account activity
level and our customer valuelevel.And you can see the
cardinality of those twocategorical variables.So the account activity
level has x, ys, and zs.And the customer value
levels are a through ewhere a is the most profitable
and interesting customerthat we have.I'm going to click on OK.And now, I've added
in informationto build this neural network.So now, it's time
to click on Refresh.While that's refreshing,
I want to showyou some options
over here that wecould have taken advantage of.First of all, there's
an Autotune button,which is really
awesome when you'rebuilding a lot of these
machine learning modelsbecause many of
them have what welike to call hyperparameters.A parameter is
something that youcan calculate from the data.But a hyperparameter is
something that you cannotcalculate from the data.So if we use the
Autotune feature,we can actually get
suggested valuesfor those hyperparameters, which
is very powerful and very, veryuseful.I'm not going to cover
all of these options.But I do want to show you here
we have the L1 and the L2.And for those of you that are
familiar with neural networks,you know those are our
regularization parameters.And they can help us find a
model that generalizes well,in other words, a model
that's going to perform wellnot only on our training data,
but also on new data thatcomes down the pipeline.Then I'm going to scroll
down just a little bit moreto show you that by default,
the neural network gotbuilt with one hidden layer.And in that one
hidden layer, therewere 10 neurons using the
hyperbolic tangent activationfunction.So now, it's time to
take a look at our model.Now, before I look
at our model, Iam going to take advantage
of one other option, whichis under Model Display.I'm going to change that plot
layout from Fit to Stack.And what that does
is it just gives mea little bit more real
estate here to work with.So here is the neural
network diagram.We sometimes like to call
these multi-layer perceptronsbecause what we have here
is we have our input layer.Then we have our hidden layer.And we have our output layer.And you can see that as I
mouse over each of these itemsthat we get a pop up.And we can actually
use the mouseto help us drill in
just a little bitso that we can get a much
closer look at what's going on.Now, one of the
things that I likeis to take advantage
of a couple of options.I'm going to come over here to
the Options pane one more time.And under the Network
Diagram options,I'm going to turn on
the neuron labels.And I'm also going
to take advantageof taking this percentage
of links to display.And I typically like to bring
that down to about a third,somewhere close to that.And when we refresh, what
that's going to allow me to dois focus in on those
particular nodes or neuronsthat are more important.In other words, they
have bigger weights.So it cleans up the
diagram just a little bit.So you can see in my
input layer, down here,it looks like the count
purchased over the past threeyears-- so that's how many
items a customer purchasedin the past three years--seems to be an
important variableor input to this
particular neural network.Now, as we take a
look at this, the sizeof each of these
circles or neuronsare actually the absolute
value of their weights.And then you can actually
take a look down here.And we have a color-coded
chart to show uswhether each of these neurons
is positive or negative.So here are the 10
neurons that we built.And for example, if we look
at neuron number three,you can see that the
neuron average is negative.And that's why
it's blue in color.But it is rather large.So that's why that circle
is one of the bigger ones.And then if we came over
to the orange neuronhere, which happens to be number
five, and we bounce over that,you can see that that neuron
average is a positive value.And that's because--
and so that means ithas a little bit larger effect.Now, what is exactly in
each of these neurons?Well, we would have to come
over to the input layerto see what helps
create that neuron.And that's why these neural
networks are a little difficultto interpret because
we really can't saywhat makes up this
neural network.Let's take a look at the
second tab, the Iteration Plot.And this is just the
Objective/Loss function.So we can see it is
smoothing out towards the endas we found a solution.I think it's more interesting
to do a right mouse button clickand switch over to the
relative importance plot, whichshows how each of
the different inputsare important to this
neural network in relationto each other.And you can see here, just as
we saw on the neural networkdiagram, the count purchased
over the past three yearsis the most important input to
this particular neural network.Also, if we do a right
mouse button click,we can see the
partial dependence.And partial dependence
plots are very usefulbecause they help
us sort of beginto get an understanding of what
each of the models are doingand how these inputs affect
the output of the model.So for example, you can see
this is the partial dependenceplot for the count purchased
over the past three years.And what we're seeing here is
that as the count increases,our predicted probability
is also increasing.So that's very interesting
and useful information.Let me finally come over
here to the Assessment panel.By default, we're going to
see the confusion matrix.And these are those
four counts thatmake up a whole
bunch of statisticswhen we're trying to evaluate a
model that predicts categoriesfor us.So you can see here these
are my true positives.So these are the
purchasers that my modelidentified as purchasers.And over here, we have
our true negatives.And of course, these
are the nonpurchasersthat were identified
as nonpurchasers.And then we also have
our false negativesand our false positives there.If we do a right
mouse button click,we can look at the lift chart.Now, the lift chart
gives us an ideaof what kind of
lift or advantagethat the neural
network gives us.The blue line
represents the model.And you can see at
the 5th percentile,my model has about a 3.5 lift.So what exactly does that mean?It means that using
this model, if Iwere to grab the top 5th
percentile of my customers,I'm 3 and 1/2 times more likely
to reach a purchaser over justpicking customers at random.And hey, that sounds
pretty good to me.I'm also going to do a
right mouse button clickand switch over to
our receiver operatorcharacteristic chart
or our ROC chart,which is another assessment
plot that we liketo use to help us see how
well our model is avoidingmisclassification.We don't want to have
misclassifications.I don't want to identify a
purchaser as a nonpurchaser.And I don't want to identify
a nonpurchaser as a purchaser.Now, if my model was a
perfect classifier, whatwe would see-- let me
use my control hereso I can-- have I
got a little bit?Nope, I don't have it.I don't have that red
flashlight for you.Sorry.What we would see is
this outline of the box.So if this blue
curve was so largethat it went to the outside
reaches of this box,that would be the
perfect classifier,which means we want models
that have very full curves.In general, this is not
a bad-looking curve.One of the other ways that
we can use this-- some of youprobably take a look at
the area under the curve.I like to use the KS statistic.The KS statistic is a
measure of the separationbetween the maximum
separation between this curve,which is our model, and then
our baseline model, whichis picking customers at random.And so you can use
that KS statistic.And of course, a larger value
means a better performer.I also just want
you to notice herethat this ROC
chart is indicatingto us the cutoff at about 20%.And so wherever that
maximal distance lands,it is a suggested cutoff that
would work well for the model.And the reason
that's important isbecause when I do a
right mouse button click,and I take a look at the
final assessment plotthat I want to show you,
which is misclassification,you can see that right
now, this model lookslike a much better
nonclassifier than a classifier.But that's because if you
take a look at the options,by default, the prediction
cutoff is set to 50%.But we saw a suggestion on
a ROC chart that said 20%is probably going
to be a better fit.So when we refresh
that, I think we'regoing to see a little
better balanced of a model.And here we go.This is much better
balance here.So now, I can see that my
true positive rate doubledto over 160,000 true positives.The final thing that
I'd like to show youbefore I move into talking
to you about forestsis if I click on
this maximize button,you're going to see that
there's a detailed table hereavailable to you with all
kinds of great information.A lot of times,
I'll have studentsthat use this SAS Visual Data
Mining and Machine Learning.They go, hey, Andy.Where are all the numbers?I want to see the
tables of information.And here's where they are.They're hidden here
in this details table.I can see all the
assessment statisticsthat are available to me.I can also see the
values that wentinto the misclassification
and the ROCand the lift charts and
the confusion matrix.So all those numbers are
here in this detailed table.And of course, if we want
to go back to the model,I can restore that and get a
nice view of my neural networkonce more.OK, here I am back again.I promised I wasn't
going to go anywhere.I just have a
couple more slides Iwant to go through before
we get into a demonstrationof showing you how to build
a forest in SAS Visual DataMining and Machine Learning.Like I did with the
neural networks,I just want to give
you a high leveloverview for just a minute
talking to you about exactlywhat a forest is
in case you're notfamiliar with that
machine learning model.Really, forest comes
from the concept of weall know that a forest consists
of a whole bunch of trees.Well, that's exactly
what a forest isin the machine learning world.It's a whole bunch
of decision trees.We actually call that
an ensemble model.When you aggregate
more than one model,and you create the
final predictionby combining the predictions
from the individual models,that's what we call
an ensemble model.Now, there are different
kinds of ensemble models too.Sometimes, we create
an ensemble of modelsfrom several different
types of models.So for example, you could
combine the predicted outputfrom a neural network or
support vector machineand let's say a decision tree.And that's going
to be our ensemble.But in this case,
when we're talkingabout a forest of
trees, our ensembleis going to consist of
a whole bunch of modelsof the same type.In other words, if we
have a categorical target,then we have a whole
bunch of decision trees.If we have a continuous
target, in that case,we have a whole bunch
of regression trees.So we call this a forest.And the reason
that it was createdis if you work with
individual decision trees,you probably already know
there's an inherent instabilityin a single tree.And that comes
from the fact whenyou have minor
perturbations in the datathat you can end up with a
completely different tree.So this is seen as
sort of like oneof the weaknesses of working
with an individual decisiontree is that sometimes we
see a different set of rulescome down.And if you have changes
at the top of the decisiontree, the parents in
the decision tree,they affect the children nodes.And that's just
like in real life.Whatever happens or affects
the parents also happensto affect the children as well.So that's why
forests were created.As it turns out, when we
are creating a forest,the forest algorithm
that's goingto be used in SAS Visual Data
Mining and Machine Learning--I like to think of
it as sort of takinga horizontal subset of the
data and a vertical subsetof the data.So that means that
forest algorithm is goingto do a sampling of the row.So it's going to grab a
certain number of rows,as well as sampling the input
columns, that vertical subset,for every individual
tree that it makes.And as it turns
out, there's a termthat we use called bagging.And that's that term
for averaging many treesthat are grown or what we
call bootstrapped samples.Those are going to be when we
grab samples with replacement.Now, the whole idea
here is the hopethat if we have this much
variability in the waythat we sample when
we create the tree,we're perturbing
that training data.And that's going to
work better than justdoing a horizontal subset.Gives us a lot more variation
in our trees in our ensemble.And the whole idea here is that
if we have more diverse trees,that's going to give us a
better predictive accuracy.That's what we're looking
for at the end of the day.So once again, don't
forget the conceptis let's say my forest
consist of 100 trees.That means when I'm ready
to build the next tree,I'm going to grab a certain
percentage of the rows.I'm going to do that
horizontal subset.And I'm going to grab a certain
percentage of the columns.Maybe I've got 10 columns.I'm only going to
grab three randomly.And then I'm going to
build that next tree.And that's one of my 100 trees.Now, when I'm ready to
build the next tree,I'm going to grab another
random horizontal subsetand another random set--a vertical subset
of those inputs.So what exactly are
advantages of our forest?Well, forests have some
of the same advantagesthat decision
trees do because weknow that trees automatically
handle missing values.As they start to
grow in the tree,they can be handled in
many different ways.And there's several different
options of handling them.But what that means is we don't
have to put as much preparationinto our data.In other analyses,
sometimes we havewhat's called a complete
case analysis whereif we have a missing value,
we need to basically finda way of handling that.We typically do it
through imputation.But with decision
trees, you don'thave to preclean your
data if you don't want to.Another thing that forests
have just like decision treesis this concept of
variable reduction.And that is automatically
the most important inputsare going to be chosen
to make the splits on.And so very naturally and
inherently, the decisiontree and thus the
forest is goingto be able to handle and create
variable reduction for us.We know if you perform
a regular regression,regression doesn't do
variable reduction for you,unless you actually employ
a variable reduction method.Well, forest is going to be
able to handle it automatically.Another concept that is
an advantage of a forestis that the trees in our forest
are all built independentlyfrom each other.Because remember,
for each tree, we'regrabbing both a vertical
subset and a horizontal subset.And that tree can be
built independentlyof the other trees.And especially if you think
about something like gradientboosting where in order
to build the next tree,you actually need information
from the previous tree,that means those trees
are built in sequence.Well, Viya has
really been createdto handle this concept
of more memory.So if you've got
additional memory and moretrees so those independent trees
can be built at the same time,they do get built. And that
happens more quickly for us.Forests also tend to
generalize better.Remember, what that means
is over a decision tree,decision tree can get very,
very what we call overfit.In other words, it's
going to perform reallywell on the training data.But then on the validation
data or on future datathat comes down
the pipeline, it'snot going to perform so well.Well, because forests have this
whole concept of a whole bunchof different decision trees,
we add all that variabilityinto the trees that
we're building,and we're going to
be averaging overall of that, that
means that forestis going to generalize better.Finally, some people consider
forests an ideal modelto be compared against.So another way of
thinking about thisis that if you are looking
at several different models,you want the models
that you're looking atto perform at least as well
as a forest does before youwant to start to consider it.So sometimes, we kind
of think of that forestas a good baseline
model, and then wewant to be able to
do better than that.There are disadvantages
of forests as well.Just like with the
neural networks,it turns out that forests
are difficult to interpret.So once again, forest is
going to perhaps give youa very accurate decision.But it's not going to
be very easy to describewhy the forest decided to give
you that particular prediction.And that's because
we're averagingover a whole bunch of trees.If we had a single
individual decision tree,the rules are very
straightforward.The other disadvantage of
forests is that in orderfor you to get a
really good prediction,sometimes you have to have a
very large number of trees.And remember, it takes
time to build each tree.So one of the things
to keep in mindis that if you're trying to pick
a champion model among models,you want to check
and see how long it'staking your forest
to actually createthat real-time prediction.OK, I think now,
it's time for meto go ahead and jump
back into the softwareand show you how easy it is to
create a forest in SAS VisualData Mining and
Machine Learning.In order to show you the
building of a forest,I'm going to use one
of my favorite featuresin both SAS Visual Statistics
and SAS Visual DataMining and Machine Learning.So I've come back to
the neural networkthat we examined in
my last demonstration.And when I click over
here on the Roles pane,I'll remind you that remember,
we created this neural networkby having the Target Binary
New Product as the responsevariable.So that's my categorical
target variableconsisting of zeros and ones
where a one is an indicationthat a customer makes
a purchase or hasmade a purchase in the past.And that's what we
want to try to predict.Then if we take a look
at our predictors,you'll see I've
got a total of 12continuous variables along
with two categorical variables.I'm going to go to
the upper right handcorner to what we call the
overflow menu or the snowmanmenu.We call it the snowman
menu because itconsists of these three dots.And you'll see we have
some more options here.One of the options
is to Duplicate as.But I'm actually going
to hold down the Alt key.And now, you'll see that
that changes and gives methe alternative menu, which
is Duplicate on a new page as.And we want to create a forest.So basically, what
I'm doing hereis I'm copying and pasting
this neural networkand plugging it into a
forest on a brand new page.Notice when I look at
the roles for the forest,it has the exact same
response variableand the exact same
predictor variables.And that's a really
cool feature.So I can very easily
create an additional modelwith just a few clicks.Right now, you'll notice
that my forest is running.So I'm going to switch
over to the optionsand show you what this forest
is going to be created of.Let's take a look at just
a couple of the options,not all of them.Notice my forest does have
an Autotune button justlike my neural network did.And once again, that'll help
me find good starting placesfor any of the hyperparameters
that we're not sure of.For example, one of those
would be the number of trees.So you'll see that by
default, the forestis going to actually build 100
trees using a 60% bootstrapsample.So that means for every
decision tree that gets built,we're going to go out
and grab 60% of the datain order to create
that one tree.We can see that the voting
default is by majority.The splitting
criterion that's usedis the information gain ratio.Then we take a look.We specify how the missing
values are assigned.Here's something
also interesting.So our maximum number of
branches is going to be two.So we can split
in two directions.And each tree is going to be
six levels deep at maximum.The leaf size minimum is five.So in other words,
we have to haveat least five
observations or five rowsin order to create a leaf.I think that's enough
of the options.Let's take a look
at our results.I am going to scroll down and
take advantage of this plotlayout option where I change
Fit to Stack so we can examineeverything on a separate tab.I'm going to go ahead and close
up the Options panel here.And take a look.We get a variable
importance table.And hey, isn't this
the exact same variablethat was most important
to that neural networkaccount purchased over
the past three years?Well, let's take a look.We'll go back over
to my neural network.And let's click on the
Partial Dependence taband do a right click and change
over to Relative Importance.And there it is.Yeah.So that's kind of interesting.I always think that's
interesting informationand perhaps useful information.When we're creating
several different models,we can see which of our inputs
tend to be most important.And sometimes, that can give
us some important informationthat we might want to take
into building our next model.There is an air plot here.And we can see the
misclassification rateas the number of
trees increases.So we can see by the
time we get down hereclose to about 70 trees, it
seems like we hit one minimum.And my misclassification
rate does seemto be perhaps about 0.1712.We also have a KS
statistic that canhelp us compare across models.Some people would rather use
the misclassification rate.So you'll see if I click
on the Summary bar there,I could actually compare
misclassification ratesif I want to as well.And once again, there is also
some additional statistics herethat you might be interested in.I actually am fond of the
KS and the C statistic.But I'm going to
leave that as KS.Then if I come over to
my assessment panel,we'll see the same
pieces of informationthat we got from
the neural network.So I do get that
wonderful confusion matrixwhere I can see
the true negativesand the true positives
and their cohorts.If I do a right mouse button
click, and I click on Lift,we'll see here that if we do
focus in on the top five--tuck 5th percentile
of these customers,we'd get a lift from
this model of about 3.4.So once again, that means
if I use this model,I'm 3.4 times more
likely to reacha purchaser in the
top 5th percentilethan just picking
customers at random.And that does seem like
a pretty good deal to me.I'll do a right click again
so we can look at our receiveroperator characteristic chart.And we can see that this KS
statistic doesn't seem quite asrobust as the KS statistic that
we saw for my neural network.And you'll see that
the cutoff is actuallyreally low at about 1%,
which is kind of interesting.So if we look at this
misclassification,once again, it doesn't look like
this is a very good classifierbecause it's doing a better
job at nonclassifications.But I think we'd have to modify
our cutoff value in orderto get a better representation
and also get a betteridea in terms of
how we would wantto use this particular
model if we moved forward.I also want to show you just
like with the neural networks,if you click on the
maximize button,you do get a nice, detailed
table that gives youall of the numbers,
all of those statisticsthat you might be
interested in looking at.So the numbers sometimes are
hiding in this details table.But I wanted to point out
how you could get them.I think that's all I
wanted to show you today.Thank you so much
for joining me.Thank you for your time.If you are interested in taking
a course in SAS Visual DataMining and Machine
Learning where we actuallyspend a total of two full
days examining this software,and building models, and
talking about the optionsand the inputs and
the outputs, thereis some information
in the links below.And we'll be happy
to reach out for you.If you have any feedback
about the video,please feel free to leave it.We love comments and questions.And thanks.We'll talk to you soon."
41,"GLENN CLINGROTH: Hello, and
welcome to the SAS Global ForumVirtual Conference for 2020.My name is Glenn Clingroth, and
I am the development managerfor SAS Model Manager.And today I will
be discussing howto integrate open-source models
into your model lifecyclewith the help of
SAS Model Manager.The model lifecycle, as seen
in the diagram on this slide,covers data exploration
and modeling,deploying models
into production,monitoring the
model performance,and ultimately
training new modelsand restarting the process.SAS Model Manager
is a key componentin this process for
models built in SAS.But in the last few
years, open-source modelsbuilt in languages
such as Python and Rhave become very popular
with data scientists.There are several
high-quality packages,such as scikit-learn and
XGBoost, that are widelyused in many organizations.The challenge for
many organizationsis that the data scientists are
very good at creating models--sometimes tens and hundreds of
models for a single problem.The problem then becomes
keeping track of the model codeand all of the analysis that
is associated with the model.Model Manager was
designed to track the codeand associated analysis.It also supports comparing
models, testing model code,deploying models to
its production system,and monitoring the
performance of the models.Model Manager has always been
able to perform these tasksfor SAS models, but since
many data scientists nowincorporate Python and R into
their data analysis projects,Model Manager now also
handles these modelsas first-class models.This means that
open-source modelscan be registered,
tested, compared,deployed, and monitored
either on their ownor alongside SAS models.To help support the
open-source modeler,we provide a GitHub
site that providesa variety of resources.On the site, we provide a set of
sample models in Python and R,scripts to perform
common API tasks,and a Python library that I'll
show more examples from laterfor registering and using
Python models in Model Manager.Also, to better support
open-source modelers,we recently released our first
version of Open Model Manager.Open Model Manager is
a standalone deploymentof SAS Model Manager
and encapsulates datamanagement, model testing, and
model performance managementinto a single docker
container thatis easily deployed without
needing a full IT department.It provides significant
enhancementsto improve the import, testing,
and publishing of Python and Rmodels.At the same time, it also
supports most SAS models.When publishing a
model, Model Managermakes it easy to create
model containers thatallow you to execute your model
in any docker or Kubernetesenvironment, including direct
integration with the AWSand Azure.This approach works great
with many DevOps pipelinetools, such as Jenkins
and GitLab CI/CD.While Open Model Manager is
great for managing models,it is not integrated with the
rest of the SAS solutions,such as Model Studio
or Visual Analytics.But all of the
features that havebeen added to Open
Model Manager will alsobecome part of the full
version of Model Managerstarting in Viya 4.01.Model Manager has a
comprehensive REST APIthat covers all of
the functionality thatcan be performed through
the Model Manager UI.The APIs referenced in the
links on this slide coverall the functionality
described in this presentation.And as mentioned
earlier, we have alsocreated a Python
library that provideshigh-level calls that are good
for working with Python models.Within Model Manager, the first
key area of the model lifecycleis model registration.Using tools like
scikit-learn and TensorFlow,modelers are increasingly
using machinelearning to discover
models from their data.Our Python library,
named pzmm, providesmethods to read the
machine learning modelsand extract useful
analytic data.The processing
captures metadata thatis important to model analysts,
including the input and outputvariables, fit statistics,
and lift and ROC charts.At the same time,
code is generatedfor executing the packaged model
both inside of Model Managerand in production deployments.Pzmm then packages the
model and registers itwith Model Manager for the
next steps of the lifecycle.The pzmm package
provides an easy wayto take a model, such as a
scikit-learn decision tree,and generate Python code
that Model Manager thenuses to manage, test,
and deploy the model.The pzmm program on
the left takes as inputa reference to a model--commonly in a pickle file--some information about the
input and output columns,a reference to the
training data set,and some information about
what to call the modeland where to store
temporary information.The right score code function
generates the programon the right, which is
a simple program thatloads the model from
its pickle file,and also provides of calculated
values for missing data values,and adds additional code that
is important for Model Manager.The items that Model
Manager requiresto properly manage the
model are quite simple,but easy to overlook.First, import the
settings package.This is a simple package
that defines global variablesassociated with the model.Second, define the score
method with all the inputcolumns as parameters.This, as is very obvious, allows
Model Manager to map the inputdata into the model.Third, declare the output
columns as a docstring,starting with the word output.The order of the
variables in the docstringshould match the
order that the outputvariables are returned in the
return statement at the bottom.As you can see, this program
returns two outputs--a probability and
a classification.These will be mapped to the
columns EM_EVENTPROBABILITYand EM_CLASSIFICATION
in the output data set.And fourth, specify the path
of your pickle file usinga variable defined the imported
settings package, pickle_path.Since Model Manager can
deploy the model to multiplelocations, using
settings.pickle_path allowsfor dynamic deployment without
needing to rewrite the sourcecode.As mentioned earlier, in
addition to generating sourcecode, pzmm also analyzes
the model's training datato generate important training
analysis for the model,such as fit statistics
and lift and ROC charts.Once the score code and
training analysis are completed,pzmm packages the model and
imports it into Model Manager.Once in Model Manager, all of
the information about the modelis available to review.The generated code can be edited
for further customization.You can also view and edit
the variables and other modelmetadata.For any complex
task, you will likelywant to create multiple models
using different trainingparameters or algorithms so
that you can find the onethat best solves your problem.model Manager manages
models and projectsso that they can be
analyzed together.This provides input for
selecting the champion modelto put into your
production system.One of the key features
of Model Manageris its model comparison.This lets you-- a user
compare multiple modelsto view the training
analysis side by side.Here we have selected a
Python decision tree modelto compare with an R model
using logistic regression.Here we see the
comparison of the models,showing the input variables
used by the models and a varietyof fit statistics and charts.Model Manager allows comparison
of multiple models at one time,so you could have three or
four models here as well.While the model code was likely
tested when it was generated,it is prudent to also
test it against moreproduction-like data.The testing interface allows
for specifying multiple testsagainst different
models and data sets.This example shows that
the model failed the test.Model Manager provides a
full runtime log of executionto help with analyzing problems.While pre-deployment
testing occurs in a sandboxenvironment, Model
Manager also providesvalidation in the actual
deployed environment.Publishing validation works
the same as score testing,but runs where the model was
deployed, such as in SAS's CASor Micro Analytic server, or in
a standalone container in AWSor Azure.Once the best model has
been selected and validated,you will want to deploy
it to a production system.The publishing component allows
for deploying multiple modelsat one time.As mentioned, models are
deployable to a varietyof locations.For open-source models,
model containersare popular and can be
deployed to AWS, Azure,or a private docker registry.Containerized
deployment is a good wayto get your models into
your analytics pipeline.Model containers are
standalone runtime environmentsfor a model.Basically, Model Manager
produces a docker containerwith a Python or R environment
suitable for running the modelcode.Model containers provide
a simple web serviceto pass in CSV data and
retrieve the results.The container's web service
API is documented on the GitHubsite referenced above.Here is a simple Python
program showing executionfor a single row of data.The web service call can
take many rows at one time.The amount of data
is only limitedby the amount of memory
allocated to the container.The Python program
reads the input datafrom a CSV file, which is passed
to the executions endpoint.No specifics of the model are
needed for running the model.Results are retrieved
using the query endpoint,and run logs are available
by using the log endpoint.Once the model is
running in production,you will want to monitor
the model's performanceto assure that it is providing
the information that you hopedfor.Model monitoring is a
complicated, but very importantprocess.In the diagrammed process,
the production modelscores the data as it is
entered into the system,and the scored data is then
maintained until a processedoutcome occurs.The process outcome could be
on-time or late loan paymentsin a banking system
or mechanical failuresin a manufacturing system.But once the outcome
is known, the datais fed back to Model Manager
so that the monitoring processcan analyze how the deployed
model is performing.The model monitoring process
produces a set of output chartsthat give a data analyst
an overview of the model'sperformance.The charts show the performance
of the model over multiple timeframes.In a banking system,
this may be quarters,but in a manufacturing
system, thismay be days, or
even parts of days.Monitoring allows
the data analystto see when the
model has decayedand needs to be replaced--at which point, another
model lifecycle is started.Thank you for watching, and
please send any commentsto the email address
on the screen."
42,"DAVID OLALEYE: Hello.My name is David Olaleye.Today I'll be presenting my talk
on explanatory machine learningplots for epidemiological and
real-world evidence studies.For the outline, first, I'll
give you a brief explanationabout why I designed
the PMFAC macroprogram for checking the
validity of machine learningmodel predictions.Then I'll talk briefly about
machine learning modelsand the existing
interpretabilitytools that can be used to
explain model predictions.And then I'll spend some
time talking about the postpredictive model features
association check (PMFAC)plots that can be used
to augment the existinginterpretability tools for
investigating model predictionsfrom machine learning models.I'll conclude with
some takeawaysand where to get the
code that I presentduring this presentation.Machine learning algorithms
are now widely usedin real world clinical
settings, and theyare used for a variety
of clinical outcomes,such as readmissions, healthcare
resource utilization, treatmentpathways, and
medication adherence.Traditionally, for most
real world evidence studies,statisticians and
scientists have oftenrelied on classical
statistical models,such as logistic or linear
regression and decision treesto estimate model features and
to perform causal inferenceanalysis.On the other hand,
machine learningalgorithms, such as gradient
boosting, random forests,and neural networks
are underutilizedin real-world evidence studies.This is partly due to
the interpretabilityof these models, hence they are
nicknamed ""black box models.""In the pharmaceutical
and healthcare settings,model interpretability
is very crucial.Regulators, clinicians,
and decision makersneed to ascertain that
machine learning model resultsare accurate, reliable,
and they are free of bias.Now let's talk about
the machine learningmodels that are used to
generate predictions.They can be categorized
into two, namely,the black box models, which
consist of random forest,gradient boosting, and
neural network models.These models are very good
for big data analyticsand they tend to generate
highly accurate predictions.They are good for detecting
non-linear relationshipsand complicated interactions
among model features.One constraint is
that they are veryhard to interpret due to a
large number of parametersin these models, and
the association layersembedded in the algorithms.White box models,
on the other hand,such as logistic or linear
regression, survival models,and decision trees are easy
to understand, and easyto interpret the
features in these models.They often contain
fewer model parameters,and they can handle multilevel
nonlinear data structures.They are often the preferred
choice of statistical methodswhen it comes to hypothesis
testing and causal inferenceanalysis.Let us now look briefly at
the interpretability toolsthat we currently use
to explain predictionsfrom black box models.The first one is variable
importance ranking,and it's often used
to discover featuresthat are ranked
as best candidatesto include in a model.We also have partial dependence,
individual conditionalexpectation, and LIME plots.These are model
agnostic tools thatcan be used to gain deeper
insight of the inner workingsof black box models.They can be used to examine
the impact that different modelfeatures exert on
model predictions,and we can also use them to
do what-if scenario analysisimpact on one or more variables
on the target variable.The PMFAC plots, that
is the central thrustof this presentation,
are useful and canbe used to augment the
existing interpretability toolsfor machine learning models.They are good for checking
the posterior predictivedistribution of
model predictions,and it can also be used to check
for consistency and sensitivityof the model with data.Why is model
interpretability important?Because our ability to
interpret black box modelsis important for trust,
transparency, and fairness.When we examine the
two plots on the slide,we modeled the probability of
30-day readmission using foresttree and gradient boost models.The pattern of association
between age and 30 dayreadmission risk represented by
these two plots look similar.But when we look at the effect
for different age groups,we see some differences.This is important because once
a champion model is selected,we need to be able to ascertain
that the model features we willbe using to explain the
predictions are free of bias,and they cannot give
us unintended results.And that's why evidence
generation becomes important.Attributes of a good model
should include reliability.By reliability, we
mean results producedby a machine
learning model shouldbe independent of the
modeling method used.As data scientists,
we should expectto see identical results
from multiple algorithms thatperform similar analysis
on the same datawhen applied to
the same question.The same thing is true
for replicability.This is the ability to
replicate and reproduceresults from a machine learning
model in different databases.For example, for real-world
evidence studies,evidence generated from
the analyses performedagainst administrative claims
from one large insurancecompany database
might be strengthenedif it can be replicated
on claims datafrom another large
insurance claims database.The next one is
generalizability.This points to
the value of beingable to generalize model results
beyond the modeling trainingdatabase that
produced the results.One thing that makes
a data scientist happyis when identical
analyses are performedagainst different
databases and the modelsshow consistent similar
results across these databases.Therefore, there is a
need for a tool thatwill allow you to
gain some deeperunderstanding of the predictions
generated by these models.So now I'd like to talk more
on the post-prediction modelfeatures' association
check macro that I wrote.What does it do?First, it allows you
to obtain posteriorpredictive distribution
of the model predictionson the training data.Second, it can be
used to generateplots to perform sensitivity
and discrepancy testcheck of the observed
test statisticsand the scored replicate
samples of the data.The tool can also be used to
perform a post-prediction checkof the association
between model inputvariables and the
predictions, and iscapable of detecting non-linear
association and featureinteractions in the replicate
samples of the observed data.These are some
examples of the plotsthat can be generated
using this macro program.For illustration
purposes, I willexplain each plot
with the use caseon 30 day readmission
risk mentioned earlierduring the presentation.First let me talk briefly
about the use case.We are going to predict the
30-day hospital readmissionrisk using the Model
Studio Pipeline,and also using the PMFAC macro
to generate plots to explainthe model predictions.The data that I'm using comes
from the UCI Machine LearningLab, and I explain
this in the paper.The Model Studio
Pipeline can be usedto build model and also generate
interpretation of the modelfeatures.Here I built five different
machine learning models.Three of them are black
box models, and two of themare white box models.The champion model selected
after running the pipelinewas the forest tree.And from the forest tree
we are able to generateimportant features,
and these featuresare listed on the slide.Now that we see the
important features,one of the things
that we can do isto use the partial
dependence plotto look at the relationship
between featuresthat are highly ranked
and the probabilityof 30-day readmission.Here we've seen three
different models fittedand what is
noticeable about theseis that we see an increase
in the risk of readmissionas number of inpatient
visits increases.What is also descernable is
that each of these algorithmspresents a different
picture of the relationshipbetween this model feature,
number of inpatient visits,and 30-day readmission risk.How do we explain
why we are seeingcontrasting picture of
model predictions lookingat the same question?This is one example where the
importance of the PMFAC macrocomes into play.As I mentioned
before, it can be usedto generate posterior
distribution of modelpredictions in the
training database,and we can compare that to
the predictions generatedfrom the replicate samples
of the observed data.So the step for completing this
macro is listed on the slide.First, there is a need to create
the random replicate sampleof the observed data.And this can be
done using Data Stepor we can use PROC HPSAMPLE to
generate the replicate samples.The next step is to create a
partition variable, especiallyif there is a need to
validate the results generatedfrom a training sample.We can use PROC
Partition to do that.And another optional step
is using the PROC Binningto bin high
cardinality variables,or interval-level variables,
into variables with fewercategories, if we need
to use those variablesas plot-variables.The next step is to now
fit a machine learningmodel of choice.It can be one.It can be multiple.And once that is done, the
results from these algorithmscan then be passed
into the PMFAC macro.The first thing the
PMFAC macro doesis to create the replicate
samples of the trainingdata, or the validation
data, or the test data,and then generate
plots that can beused to explain the
association between the modelfeatures and the
model predictions,and to also assess the model
fit of the champion model.Now I'm going to go through
the different plots thatcan be generated using
the PMFAC macro program.The first one is the
model-agnostic posteriorpredictive distribution check
of the predictions generatedby the black box model
using the observed data.The other set of plots
that can be generatedallow you to perform
sensitivity and discrepancy testcheck of the test
statistics generatedfrom the training
sample in comparisonwith the same set of
statistics generatedfrom the scored replicate
samples of the training data.We can use this to compare
multiple machine learningmodels and to
decide which one wewant to use in the real world.The next set of plots
that can be generatedallow you to look
at the associationbetween the model input
variables and the predictionsthemselves.This can help us to
really understandthe nature of the association
between each featureand the target variable.And last but not the
least, is that theyare capable of detecting
non-linear associationor feature interactions
that can be usedto improve the model
in case these are notcaptured in the training sample
used to select the championmodel.So the first set of plots
are the summary plotsthat check the posterior
distribution of the modelpredictions.Here we have used a
gradient boosting modelto generate histogram plots
based on the training sample,and also based on the
replicate samples generatedfrom the observed data.The test statistics
and the p-valuesuggest that there is
no apparent discrepancyor deviation of the model
fit of the training datawhen compared to the model
fit for each replicate sample.The next set of plots that
the macro program generatesis to allow the user to perform
model comparison and assessmentcheck.The plot on the left displays
the model predictionsfrom a random forest model,
and the plot on the rightdisplays the model predictions
from a gradient boosting model.We can see that the distribution
looks similar across the twosets of model predictions
from the two algorithms usedto fit the 30-day
hospital readmission risk.The next set of plots that
the PMFAC macro program allowsyou to generate are the Model
Feature Assessment Check plots.In this case, we
can use the plotto examine the functional
relationship between the modelinput and the model predictions.The plot displayed represents
the check of the blood glucosetest measurement based
on the training data,and the right plot
shows model predictionsbased on the replicate
samples also generatedfrom the training data.We can notice that the
effect of this variableon 30-day readmission risk, is
dependent on the blood glucosetest measurement.The observed association
between these two variables,that is, blood glucose
and readmissionrisk, at the observed data level
is also found in more than 2/3of the replicate
samples, suggestinga good fit of the model
to the training data.We can also use
the tool to checkfor non-linear association
and features interaction.The PMFAC plots for age
group and blood glucoseare shown on this slide.The left plot presents the
features association checkof age group and readmission
risk, controllingfor the blood glucose variable.The same features
association checkis performed for the
replicate samples,and this is shown
on the right plot.The plot shows a
quantitative-type interactionbetween blood glucose
measurement and agegroup in relation to probability
of 30-day readmission.We can notice at the
replicate sample level,there is evidence that
suggests interactionof age group and blood
glucose test variable,even though it is not clear-cut.There are other type of
plots that can be generatedusing this macro program.Here, I am showing
the relationshipbetween diabetes medication and
blood glucose test variable.As you can see, the plots show
a qualitative-type interactionbetween blood glucose
measurement and diabetesmedication with regards to
probability of 30-day hospitalreadmission.What is noticeable
is a greater riskof readmission observed among
patients with no blood glucosetest measurement when
compared with thosethat had blood glucose
test measurements.And at the replicate
samples level,we also see the
evidence of interactionof diabetes medication with the
blood glucose test variable.We can also observe
this associationat each unique level of the
blood glucose test variable.There are other plots
that one can generateusing the PMFAC macro program.Here is the same one looking at
number of diagnoses and bloodglucose measurement variable.The information
that is importanthere is that we see that
the more diagnoses recordedfor a patient, the greater
the risk of readmission, whichmay suggest evidence
that sicker patients areresponsible for the
disproportionate shareof observed readmission events.The picture on the right,
based on the replicate samples,shows what is observed using
the entire sample to examinethe relationship between number
of diagnoses and probabilityof 30-day readmission.We can also look at
how length of stayaffects probability
of 30-day readmission.So again, here we are
controlling for blood glucose,and we also see a
linear positive trend.As length of stay increases,
so do the probabilityof readmission increases.And we see that difference among
the unique levels of the bloodglucose test variable.One interpretation might be
that those that are really sicktend to spend more
time in the hospitalbecause they are being
taken care of, comparedto those patients whose
sickness status are less severe.So what are the takeaways?I've gone through the different
interpretability tools thatcan be used to explain
predictions generatedfrom machine
learning algorithms,whether it's a black box model
or it's a white box model.These tools allow you
to explain predictionsand also to examine
the functionalrelationship between model
inputs and output predictions.The PMFAC macro plots that I
also showed and talked aboutduring this presentation
allow you to carry thatfurther, in that they
allow you to checkfor consistency and
reproducibility of machinelearning model results.They help to reveal
input variable dependencyand interactions,
and they can alsohelp reveal how changes
in input variablescan influence model predictions
in different populationsamples.There is a limitation, however.The PMFAC macro plots are not
meant to be used for hypothesistesting, and also they cannot
be used to reject or acceptthe validity of a model fit.The code that I used
in this presentationare published in the
conference paper,and also I will also make
them available on GitHub.Thank you for your time."
43,"STEVEN HUELS: All
right, well, we'recertainly excited to be here
at SAS Global Forum 2020.I'm Steven Huels.I am Senior Director for Red
Hat's Center of Excellence.And what that means is
that I'm responsibilityfor how we implement
our AI strategy,what that strategy
is, and how thatimpacts our overall internal
use cases, how that impactsour product, how we
work with our customersand then how we collaborate
with our partner ecosystem.And today I'm going
to talk to you, alongwith Vince Powers from our
Global Partner organization.Vince is a solution architect.He's been working with
our key strategic partnersto build joint solutions
using Red Hat products.And most notably, he's
been working with SAS,as they've been
working to implementSAS Viya on top of OpenShift.And today we're going to talk
to you about accelerating AIinnovation.So first and foremost, I want to
start with some of the key usecases that we end up
seeing in the field whenwe talk with our customers.And that is that AI and ML
powered intelligence softwareapps are helping companies
achieve their core businessgoals.And those business goals
include increasing customersatisfaction, regain
competitive advantage,increasing overall revenue,
and reducing operational costs.And in doing that, the
use case that we'reseeing across
multiple industriesall derived from basically the
same grounding; that they needaccess to data, they need a
flexible dynamic platform,and they need to be able
to implement analyticson top of that platform.And companies say
all phases of AI/MLare demanding on
the infrastructure.So it's everything from data
management, data preparation,to AI model training
and inferencingare putting demands throughout,
on their infrastructure,on their availability
of information,and how they're able
to react to changesand needs and requirements
from their customers as well.And so why does all this matter?Well, at Red Hat we're
a platform company.And we need to make sure
that the platforms we'reputting in place are meeting
the core requirementsof our customers
and our partnersso that they can deliver
value to their customers.And so from Red Hat's
perspective, our strategy,we call it our four
pillars and a foundation.And the first part
of that pillaris basically we see AI/ML as the
most important work load that'sgoing to be running
on our platformsacross the hybrid cloud.So that means that when you're
thinking of things like RHEL,OpenShift, OpenStack,
we want to make surethat all of the advanced
analytics librariesare optimized to run
on that platform.We want to make sure
you can take advantageof any of the hardware
optimizations or acceleratorsthat are put in place.So we have entire
set of R&D. And we'recommitted to making sure
those things are all optimizedso that when you put your
workload on our platformsyou get an optimal experience.Second, Red Hat's
a software company.And we're a company
like any other company,where we can basically
take advantage of AI/MLin our operations and
how we productize,go-to-market with our products.And so we are applying
this to our core businessand improving overall
operational efficiencyin all those core use
cases I had mentioned.Third is how we implement
AI/ML in products and servicesthat we offer to our customers.So these may be forward-looking
things like our Red HatInsights product, which
is making recommendationsto customers on a daily
basis based on analysisof just some data that we
analyzed from all the customerswe're collecting it from.But it also will be in
services that you may notknow you're directly
interacting with, like whenyou opened a support ticket.We may try to provide a
solution right out of the gateto, again, try to get
you back up and runningand answer your questions.The fourth pillar
is then how do weenable customers to implement
intelligent applicationsor applications with analytics
embedded in them on topof our platform to provide
value out to their customersand to their users?And this is a
combination of things.This is bringing together
Red Hat's platforms,along with a lot of the
core foundational elementsthat we offer in our
Middleware portfolioto handle things like
streaming data, queuing,decision management, those core
components in any applicationworkflow.And then how we
build around thatwith our partner ecosystem,
with partners such as SAS.So that when customers want
to build out a specializedapplication they can
have access to the toolsthat they desire and
their users are demanding,and have the confidence that
through our certificationprocess, that application
is going to run justas well as anything else on
top of Red Hat platforms.And then finally, we have
our data as a foundation.And this is existentially,
a part of itis the fact that in order
for any analytics to run,you have to have access to data.And so how are we
enabling customersto become data driven?How are they aggregating
data, giving themthe tools and the software
in order to do that.And then so we have
our SAS productwhere we have our S3 storage.And anyone who's been
working with analyticshas used object storage
for that analysis.And SAS has an S3 adapter.So conceptually how does
all this come together?Well, at the very top we've got
our typical project lifecycle.We've got our business
goals, data engineers workingon gathering, preparing
data to make ready for datascience to develop
those ML models.Next the data scientists
work with software developersto deploy those ML models into
your application developmentprocess.And finally, our AI/ML powered
intelligent applicationsare deployed.And the ML models start
inferencing, making predictionsbased on new data that it sees.And you have to continuously
monitor and manage those modelsin production to
make sure they'redoing the right predictions,
they get retrained as needed,and they're always
kept up to date.So in order to execute
on this process,you need an ML
software tool chain,something like TensorFlow,
Jupyter Notebook, Python, SAS,and data services,
whether they'reSQL, NoSQL, object storage.And you need all that to be
supported on top of a selfservice hybrid
multi-cloud platform thatempowers these data scientists,
data engineers, and softwaredevelopers to be agile
and collaborativethroughout that entire process.And we don't want them to
have to depend too much on IToperations for individual tasks.And so the hybrid
cloud platform thereshould have integrations with
all those hardware acceleratorsI was talking about.GPU is a great example.It helps speed up that model
development and inferencing.And finally, the
hybrid cloud platformshould offer
consistent experienceacross all clouds, as well as
on-premise and edge locations.So what are the
challenges then whenit comes to executing
these AI/ML suites?Well, everyone's experienced
some of these, right?ROI's unclear.Talent is in short supply.Operations are siloed.It's hard getting data.If you do get data,
it's hard makingthat set of access reproducible
or the tasks reproducible.And then getting access
to some of the specializedinfrastructure is really,
really difficult, right?Some of it's expensive.Some of it's hard to find.And pulling it together
and out of siloscan be a real challenge.This is where Red Hat
OpenShift and Kubernetes, basedon Kubernetes basically,
is an optimal fitfor your analytic
workloads, right?It offers the agility
needed to respondto demands from your data
scientists and your dataengineers.It is portable, right?You can move these things.OpenShift gives you a
single pane of glassacross all of those different
cloud footprints I mentioned.It's flexible, where
you can provisionworkloads and environments
as you need them.They can scale up dynamically.You can reallocate
resources based on whereyou have a greater need.And they are scalable.There's auto scaling by
default. You can write--you have the
ability to influencethat auto scaling based
on your own set of rules,based on your
business, which willkeep all of your applications
up and running for a long time.So why Red Hat, in
this case, right?Well, first and foremost, we
have a proven track record,especially when it comes
to open source, containers,and Kubernetes.We've been doing this for
a really, really long time.A lot of the emerging technology
when it comes to analyticsis being done in open source,
which, again, gives usjust a natural fit there.Next we've got a
comprehensive portfolio.Earlier I touched on how
we bring our productsand platforms to
bear on this and thenhow we partner with partners
like SAS out in the ecosystemto bring those tools and
make them available to users.We have those partner powerful
partnerships I've mentioned.And we're an active
open source advocate.We're a leader and a trusted
provider of open sourcein this space.And I talked a little bit
about those partnerships.And that's part of what
we're focusing on here today.So we have a number of
strategic partnershipsout there, SAS being
one of our core ones.In fact, if you watchedour Red Hat Summit, you'll
see that SAS was on stagewith us giving a presentation
during one of our keynotesas well.So with that, I'm going
to hand it over to Vince.And he's going to go a little
bit deeper into what we'redoing with SAS on OpenShift.VINCENT POWER:
Thank you, Steven.So as Steven showed, we have a
very strong vision and strategyand how we know it's
an important pieceof the technology
ecosystem going forward.So how does Kubernetes
and Red Hat OpenShiftactually fit into that model?We've built OpenShift
as a platform that'llhosts your containers and
support any of your DevOps lifecycles for both traditional
apps, newer cloud-native apps,and any AI and ML
functionalitiesyou want to build.This is across from the
edge, to the data center,into private, hybrid, and
multi-cloud scenarios.We have more partners
than just the AI system.That's part of my
role, is to makesure we have a full ecosystem
to provide that consistencyand to have access to
the latest innovationacross the environment, across
the technology landscape.So that includes multiple
languages, multiple partnerslike GitLab and GitHub to
help with your code pipelines,and Nvidia for GPU
and accelerated.And we work with these partners
so that across any cloud or anyon-premise, from Bare
Metal right through to--pick your hypervisor of choice--it'll provide a consistent
experience for administratorsand developers to be able
to deploy your AI and MLworkloads.So when you're talking
about SAS Viya,specifically, it provides
the functionalityto plan and prepare your data,
explore and visualize, buildmodels, and manage your
models in its own environment.But it needs a base
to run consistently.And SAS' specialty is the actual
models, manage the models,making them work, or
making your data valuable.And Red Hat's position
is to work with SAS Viyato enable it so it can work
on all the different cloudsand Bare Metal
solutions to providethat consistent environment so
Viya can do what it specializesin, and Red Hat can
do what it's best at,which is making
that ecosystem workso Viya can thrive and help
you bring your data to life.So exactly what is in OpenShift?OpenShift is the leading
enterprise Kubernetes platform.It has full automation
across the entire lifecycleof your application.From the bottom,
we work on, as Isaid, physical, virtual, and
multiple types of clouds.Red Hat Enterprise Linux is
a proven, trusted platform.It's the base for OpenShift.But in the middle of
the whole OpenShiftplatform is Kubernetes.It is the standard
upstream version.And we certify with
the latest versionsto make sure that we provide
that consistent experienceand you get all the
benefits from Kubernetes.Around that, we rack
automated operations,which help with seamless
deployments, autoscalingand on public clouds,
and self-updatingfor the actual platform itself.And then, in addition
to the base operations,we provide service meshes,
activation services.And even in development
environments,that could be used to assist you
in your day-to-day life cyclewith building, imaging, and
containerizing platforms.And as I said, Red
Hat Enterprise Linuxis a proven, trusted
platform wherewe have a very strong
security ecosystem.We've extending that into
our OpenShift environmentand provide API management
capabilities, multiple storagepartners, the ability
to do multi-tenancywith network isolation.And then, on top of that, this
isn't your CI/CD pipeline.There's container registries,
both the private one,and access to private registries
and public registries.There's roles-based
authenticationwrapped around it
so we can providethat full, trusted
platform that youneed to make sure your data
is secure and being guardedfrom unwanted access.That's part of the consistent
container platform.We provide, as I said,
the automated operationswith multi-tenancy.We enable security
features by default,which provides better
out-of-the-box security thanmost major Kubernetes
distributions.It has network traffic
control with policies.And we have the capabilities
to do both monitoringand chargeback as part
of the base platform.So you install your solutions,
and metrics are automaticallyavailable to be visualized
through our consoleand exposed to any
external monitoringsystem you're already using.So the lifecycle for full,
from day-1, day-0 through day-2operations, we install the
infrastructure provisioningwith our embedded core OS
that is designed specificallyto underpin OpenShift.It doesn't have
any functionalitythat it isn't required, which
minimizes the attack footprint.When you deploy OpenShift, it
gives you the ability to dothe full stack deployment
on-premises with your BareMetal, on your VMWare
environment, or even--pick your public cloud.And it is the same experience
across all environments--whether it's Amazon, or Azure,
or Google Cloud, or Bare Metal,the operations for
administration and developersare the same.We work with security partners
and certification agenciesto make sure we meet the
latest criteria for allof our security certification
that our customers demand.This is network isolation,
everything is auditable,and we support signing
and network policies.And then, when you're
actually operating,and once you're running, you
have this advanced clustermanager and other
solutions to allowyou to manage multiple clusters
across multiple data centersfrom a single pane
of glass, builtin monitoring and alerting,
and with the latest version,OpenShift 4, we provide
over-the-air updatecapabilities.So you update your
cluster in real timewithout having to go offline
and have scheduled maintenancewith those.So that's part of the
day 2 operations, whichis where Red Hat is strongest
with the OpenShift platform.In addition to that, we
support an open source projectthat provides operators
for Kubernetes.The operator framework
is our contributionto that environment, where
it provides an easy wayto build the actual operator
that runs in Kubernetes.So what an operator
does is it allowsyou to codify a lot of the
human tasks that are requiredto deploy an application--whether it's having an
enabled cluster, whether it'sprovisioning storage
automatically, all that getswrapped into an operator.And the operator
framework can give youa framework around
that, so when youstart to build your
containerized platformin-house, the operator
will allow any userto deploy your solution
on top of OpenShiftwithout having to learn all
the intricacies around datastorage, and security, and
having it work uniformlyacross all environments.This allows for a truly seamless
solution for your users.So all of the operators that
our partners work with and buildare available
through our OperatorHub inside the
OpenShift console,and there's also a community
site called operatorhub.io.These operators
are just one click,and one or two click
installs inside OpenShift,and from that point, they'll
provision, manage, and monitorthe application they're
responsible for.So from multiple
database vendors--there's over 100
available right now,including SAS Viya
down the road.It will be available this way.So when you go to Deploy, you
just install the operator,and Viya be enabled
to be deployedin your environment in a
relatively self-serve manner.So as an administrator, you'll
create an operator subscriptionwhich makes it available
in your environmentthrough the web console
interface or the command lineif you choose.And then, once it's
enabled, any developercan go and enable that
search for them to use.So for Viya, it would
deploy a Viya instancefor the developer to use
in their own namespacewith all the security and
network policies requiredto have an isolated and
secure environment for themto work in.And then, once a developer
is in and running,they have all the capabilities
of the platform availableto them, whether it's
from self-serve--installing with self-service
provisioning, the consistencyacross multiple
clouds, the abilityto use multiple languages
and able to connectto any of existing CI/CD
pipelines in your organization,and OpenShift even
has some capabilitiesto do those for those having
an external CI/CD pipeline.And then, again, everything's
audited, logged, and monitored,and available out of the
box to make your life easierand to provide value to
get up and learning quicklyas you build your AI and ML
pipelines in your organization.So thank you for your time,
and if you have any questionsor would like
further information,just reach out to us, and
we'll be available to respond.Thank you."
44,"Hi, everybody.My name is Diane Hatcher.I am with Core Compete.And I'm happy to be here
today to present to you.My topic today is called ""Reduce
Your SAS Carbon FootprintWhile Increasing SAS Value.""Thank you for tuning into this.My objective for
this presentationis really to share with
you how you can essentiallylower your total
cost of ownershipand increase the value of your
SAS environment-- essentially,reducing your carbon footprint
of your SAS environment.Very similar to reducing your
carbon footprint in generalwhen you're talking
about emissions.Basically what we want to
do is lower the emissionsand increase the
quality of life.Same type of thing within
the SAS environment,you have the opportunity to
reduce your carbon footprintof your SAS environment,
while at the same timeincreasing its value.So what is your SAS
carbon footprint?So this is an analogy that I
make around what actually makesup your SAS ecosystem.Your SAS carbon footprint
is made up of essentiallywhat you would think in
terms of your SAS assetsitself, your SAS workloads.But it also includes
infrastructureto support your SAS
environment, as well asthe data that is
used and createdwithin that environment.So as SAS usage increases in
your company, what happens is,of course, your users
are getting valueout of that SAS environment.And what they're getting
is, tangibly they'recreating SAS workloads
that run on your server.They're creating SAS data sets.They're creating
analytical models.And they're creating reports.All these things build up
within your environment,because you're saving the
code, you're saving the models,you're saving the reports,
you're saving the data, right.And as that usage
increases, what happens is,the infrastructure required
to support those assetscontinues to increase.So things around the
infrastructure itself,the amount of
storage space, right.SAS requires high speed
storage to really perform well.And that can be quite expensive.And so IT is
constantly being askedto buy more storage to
support your SAS environment.You have to buy more servers
for supporting those.As the workloads grow, you need
a place to run those workloads.And of course, because you're
processing data, the moredata that's being processed, you
have network bandwidth thingsthat you have to consider.And there's all the
tools that you'reusing within the SAS environment
itself to really optimizethe use of SAS.And all these things
grow over timeand create this
carbon footprint.And for organizations that
use SAS for many, many years,the environments can be
quite large to manage.And so essentially
what you can dois, actually, you
should be performinga tune-up on your environment
so you can essentially clean upthe emissions from
your SAS environmentso you can reduce
that carbon footprint.And at the same
time, what that does,it helps you get more out
of the SAS environmentwithout having to spend a lot
more money on infrastructure.And so the first thing
we want to do in this,and what we can do within
this, is really discoverhow SAS is really used
in the organization.I'm talking about
really digging outinto the X-ray of your assets.So if you think about
if you go to the doctorand you get an
X-ray, you're reallygetting a really
clear vision of what'shappening underneath your
skin within the body.It's more than taking your
temperature, for example.So I kind of look at if you
look at overall server usage,for example, your CPU
utilization, that'smore like taking
your temperatureand seeing if you have a fever.But what I'm talking
about is actuallygetting it at a much
more granular levelso you can see
specifically what'shappening in that environment.The good news is, is
all that informationis available within
your SAS environment.So all these elements
are already therein that environment.So what are those elements?So the first thing is
really talking to the users.I know that this seems
to be a very common--or not necessarily
common-- but itseems like a no-brainer type
of thing, a common sense.But usually when people
talk to the user about SAS,I see that they ask questions
like, what procedures are youusing?What kind of data
step are you using?And those type of things.That's not really what
I'm getting at here.When you talk to users, what
you really need to understandis really to discover
the business processesthat they are doing
to support SAS.And you need to discover
why they're usingSAS to solve their problems.You need to understand
what those problems arethat they're solving.And you need to
discover how they'resolving the problem--
so how are they usingSAS to solve those problems?And you need to discover
where they're running it,where they're using SAS.Are they using SAS on a server?Are they using SAS locally?All these are really important
to understanding what'sgoing on in the environment.And finally, how often are
they solving these problems?Is this a daily problem
that they have to solve?Is it weekly?Is it monthly?Really want to kind of get into
the day-in-the-life of your SASusers to really understand
what they're doing in their SASworld.The next element to look at
is the actual SAS code files.This is where you
can really lookat what procedures they're
using, what data steps they'reusing, how are they
writing their code--those types of things.And you discover the coding
methodology that's being used.And this is good information
to understanding,are they leveraging PROC SQL,
are they leveraging a database,are they using PROC SORT--things that may be better
optimized than the waythey're doing it now, and really
understanding those codingpractices.And you can probably find that
these coding practices areactually fairly common across
the different departmentsand different user groups.And so it's kind of
a SAS cultural thingthat could be easily addressed
if it's something easy to fix,from an efficiency perspective.The next thing to
look at is storage.All right, you want to discover
what data sets are beingstored, where
they're being stored,and how they're being stored.Are they being stored
as SAS data sets?Or they being output
the Excel files?Or are they writing
to a database?Understanding what is there,
what data sources are actuallybeing used, and then how
they're being stored.SAS log files, to me, is
the gold in the system.So if you don't have sys logging
turned on in your environment,I highly recommend
that it's turned on,because SAS logs contain such
a rich amount of informationfor you to discover
what's actually happening.So SAS code files gives you
a pattern kind of thing.But the log files tell
you what's actuallyrunning in your environment.So it gives you
that look in saying,these are the code files.This is the code that's
actually running.This is what they're doing.This is how long it's taking.And this is when this
is all happening.So the log files are
really kind of the cruxof a lot of the information
you can use in orderto do the tune-up
of your environment.Finally, there's SAS metadata.So you know there's two main
types of metadata in the SASenvironment.There's a SAS metadata server
if you use the metadata server.There's also within
SAS Foundation itself,there is metadata that SAS
Foundation itself captures.And both types of
metadata are extremelyimportant in understanding
not only the SAS assetsthat you typically find, like
SAS data sets and things,but it also helps
you identify whatelse is in the environment.Am I using Enterprise
Guide projects?Am I using Enterprise
Miner models?Am I using Web Reports?Am I using Visual
Analytics Reports or WebReports to do reports?And I using STAR processes?These are all things that
are important to understandthrough the metadata layer.As a result of gathering
of the informationabout these elements,
you can reallystart to understand now what is
happening in the environment.So the bar chart at the
top part of the screencan be used to analyze, over
time, your server utilization.When I say ""utilization,""
I don't in termsof system utilization, but in
terms of how it is being used.So you think about
what days of the weekare more busy than
other days of the week,or what these are
the month or morebusy than other days of
the month, what departmentsare using the system more
than other departments.The bar chart on the right
is actually a time of day.So what workloads are running
at different times of the day?Workloads that are
running in the morning,versus in the afternoon,
versus in the evening,and even looking at
it on the weekend,are there times of what
we call ""off-peak times""that can be leveraged in
order to provide more capacityto your users.In the bottom left
corner is a tablethat could show the
results of your storageand analyzing your storage.It's as simple as taking a look
at, are you archiving old data?Is there data that's
out there thathasn't been active or
accessed within a year?Can you reclaim storage space
on this expensive, high speedstorage drive by archiving
that data and freeing up room?The pie chart in the
middle represents reallypatterns within your workloads,
so what's actually going on.Are users doing data prep work?Are they doing ETL?Data prep versus ETL?Are they doing analysis?Are they writing reports?Ans so really understanding what
the purpose of these workloadsare.If you look specifically
at just the procs--the procedures themselves
and the methods thatare being used, what you'll find
is the vast majority of theseare data step and PROC SQL.So you're going to
get the impressionthat SAS is being used
primarily for data preparation.But what's really happening
is that these data preparationsteps are happening in
order to do a report,or in order to do an analysis.So there's a lot of
data prep work upfrontto create the data that
you need, and then there'sonly one or two
procedures after thatthat you need in order
to run the analyticsor to create a report.And so the majority of the
code that you're looking atis probably going to be
data-preparation-related.And so that can skew
the view that youhave on the type
of things that arehappening in the organization.So if you look at a workload as
a whole, what you can then dois essentially determine
what the actual intentof that workload is.It gives you a more
accurate pictureof what's actually being done.The other thing that
you can look at,as well, is look at the data
sources that are being used.If you are using SAS data
sets versus a databaseversus flat files, you can
compare the performancebetween these
different data sourcesand see if you have
I/O bottlenecksor if your I/O bottlenecks
are related to specificallya specific data source type.Finally, on the far
right, what you seeis a tile map that
essentially takesa bunch of log files'
workloads and looksat the total CPU in real time
for each of those workloads.And then you can map these in
a report like a tile chart.And these, by the
way, were createdin SAS Viya for this purpose.But taking a look at analyzing
this output and saying,oh, look, this box that I
have drawn over this chartindicates that 25%
of my CPU usagewas actually based upon these
25 to 30 jobs that are written.Of all the hundreds of jobs that
were running in my environment,only about 25 to 30 of
them took up 25% of my CPU.Right.So that's a great
opportunity for youto understand, well, what's
happening in these workloads?I should take a look at
those in more detail.So as an ecosystem
itself, right, SASis part of a broader ecosystem.SAS is hardly ever
used in and of itself.You're either pulling data
from somewhere else, somethingthat's not SAS.You're writing it out so
something not SAS can read it.Whether that's a
marketing campaign,whether it's a forecast, whether
it's inventory optimization--whatever it is,
that data is beingused to solve another problem.And so in order to
create or performa comprehensive
tune-up, you reallyneed to understand
the entire ecosystem.So you need to first break down
how does SAS fit in the broaderscheme of things?Since SAS is part of
a broader ecosystem,you can look at SAS as a
puzzle piece within the biggercorporate puzzle.How does SAS fit within
the overall objectivesof the organization?Where do the executives
see SAS adding value?And how can we improve that
value that SAS provides?Breaking down the
information technologythat's involved from an
information technology team,IT is going to have a vision
for the overall technicalarchitecture for
the organization,how SAS fits within
that architecture,and what the IT policies
are related to that.They're also going to
have their own perspectiveon the challenges of SAS--
how it is to support SASas an enterprise
solution, what challengesthey face with SAS with
integration with other systems.So it's important to understand
it from their perspective.And we've already talked about
the SAS user stakeholdersand understanding their
business processes--understanding/breaking
down their objectivesthat they're trying
to achieve and whichSAS is helping them to achieve.And of course, we also
talked about the analysisof the SAS assets themselves.So breaking down the workloads,
breaking down the code,breaking down the data that's
being used by SAS in orderto understand, to get
the full picture to doa comprehensive tune-up of your
organization's SAS footprint.So to create the
formula of successnow that you have all the
elements broken down--you have a broader ecosystem--now you can really take a
look at understanding, how Ican reduce my carbon footprint?How can I reduce my
total cost of ownership?So from a data perspective,
we can reclaim storage spaceby archiving old, inactive data.We can also look at
workload optimizations--understanding which
workloads are taking upthe most resources
within the environmentand being able to either
optimize them or movethem to execute during
an off-peak time period.For opportunities to
increase the value of SASis look at applying
coding practices.We talked about being able
to analyze code files,understanding the current
methods that are being used.Well, they may be better methods
that could add more value,make things perform
faster, or maybe helpyou to do things in a much more
efficient and effective manner.And SAS is always coming out
with new features or software,taking advantage
of these featuresto either increase performance
of the environment,or to take advantage
of new methodologiesand new algorithms that may
be better and more efficientthan ones that you've
been using in the past.So the elements that are there
that make up your ecosystemare already there.The elements are already there.And they're telling you a story.So you can take
all the informationthat is behind those
elements and essentiallyread behind the lines to know
how to reduce your carbonfootprint.So being able to do a tune-up of
the environment by essentiallycleaning up the engine and
optimizing the performanceof your SAS environment--so essentially reducing
your carbon footprint.So thank you for listening.Feel free to contact me
with any questions you have.Stay safe.Stay well."
45,"Hi, everybody.It's Chris from SAS
here, and welcometo ""Doing More with SAS
Enterprise Guide, Tipsand Advanced Techniques.""I'm really excited to
bring you this topic today.I was on the
original developmentteam for SAS Enterprise
Guide in SAS R&D,and I'm an avid user, to this
day, of SAS Enterprise Guide.And I know all the nooks, and
crannies, and productivity tipsthat I get through
the day with, and I'mexcited to be able to
share them with you.So let's get started about
what we're going to learn.These are the topics
that we're goingto cover during these sessions.Of course, I'm going to give
you a little bit of backgroundabout Enterprise Guide,
maybe some behind-the-scenesknowledge that you
didn't have before evenif you've used it for a while.We'll talk about
the basic topics,like customizing your
workspace and importing data.But we're going to go a
little deeper than whatyou might get from an
intro course or tutorial.And I want to show you some
tricks about the Data Viewer,how to get your
content out to Excel--I'll show you about five or
six different ways to do that--how to take advantage
of projects and processflows, and I'm going
to spend time talkingabout some special tasks that
are built into Enterprise Guideto help you do some cool
things that maybe youdidn't know about.We'll talk about the
Query Builder, of course.And if you're a SAS
programmer, you'regoing to want to stay
tuned for those programmingproductivity tips that
I've got, and we'lltalk about using source
management, like Git,with SAS Enterprise Guide.And , finally we'll get into
automation using scripts.And I'll make sure
that at the end,you're connected with all the
Enterprise Guide resourcesyou'll need to be successful.Ready?Let's get started.Whether you're new to Enterprise
Guide or a grizzled veteran,it's great to take just a
moment and talk about the basicsabout, what is SAS
Enterprise Guide?It's a client application
that connects youto SAS and SAS resources.So it isn't SAS itself.It isn't the thing
doing all the workto manage data
and run analytics.It's a client app that connects
to SAS, where all of that workgets done.Enterprise Guide
gives you accessto the data in
your organization,whether that's SAS data sets,
or flat files, or Excel,or databases.It is your interface into
a lot of data preparation,and that can be in
the form of code,or it might be using
the Query Builder,or other fit-for-purpose
tasks within Enterprise Guide.There are lots of
tasks and wizards--hundreds-- available
in Enterprise Guideto make everyday tasks simpler.But there's always the
option to break out the codewhen you want to do
something more advanced,or maybe that's just your
preferred mode of working.Enterprise Guide lets
you organize your workusing projects and process flows
to help you stay more organizedand repeat flows or
sequences of thingsthat you have to do again and
again in an intuitive way.So Enterprise Guide is kind
of your gateway to SAS.Now, Enterprise Guide is not
the only client applicationout there.There are other tools,
like SAS Studio, whichis a browser-based
client to SAS,and the SAS Add-In for
Microsoft Office, whichis kind of like Enterprise
Guide in that it installson your desktop,
but it runs onlyin Microsoft Office
applications like Excel or Word,or PowerPoint.But in concept,
all these tools areoperating in the same way, in
that they're a dedicated clientapp that many people
may use, but they'reconnecting to an installation
of SAS that's actuallydoing all the heavy lifting.That SAS might be on
your local desktop,or it more likely
is centralized,maintained by your IT
department or by somebody elsein a central way.And it's collecting
the work that youwant to be done in the form
of requests as you pointand click in the
client application,and it's sending those requests
over to SAS to fulfill,and then sending you
back those results.Those requests might be
just, open this data setand show me the values, or
it might be, run this query,or perform this
linear regression,and then it brings you back
the results for you to viewand do further action with
within Enterprise Guide.Now, many of us who have
used SAS for a long timemay have started using SAS
installed on our desktops.So a lot of SAS users still
call that Base SAS or SASfor Windows.That is a way of working that is
becoming less and less common,and here's why.Organizations are wanting
to more centralizetheir computing resources
and their data resources.Rather than having
everything scattered amongstthe organization on
individuals' workstations,they would rather have
their SAS installation--which could be quite large--installed on a central machine
or a series of machines.And their data-- it's
not always a great ideato have that scattered
around the organization.Organizations like to be able
to centralize access to that,as well.SAS Grid technology
is another waythat customers use SAS to
scale up the way that their SASjobs run, maybe because they
have lots and lots of SASusers, or maybe they
have SAS processing thathappens that's very
intense and theycan spread the work of that SAS
work across many, many nodesto scale up and get things
computed more quickly.So SAS Grid technology works
great with SAS EnterpriseGuide.Security is another big concern.So a lot of organizations,
like financial institutions,or insurance companies,
or hospitals--they have access to
sensitive data, datathat it would be very
bad if that data wereto escape the confines
of the organization.So these companies,
organizations--they centralize
access to their dataand require that their analysts
access them using these clientapplications.That way, they can mitigate
and limit the potentialthat that data is distributed in
ways that the company would notauthorize.Going along with that,
the centralized schemeallows companies to have
better control over whoaccesses the data,
and to have visibilityinto when that data is
accessed, and by who.And so these
auditing capabilitiesthat are built into SAS,
which are facilitatedby using client applications,
like Enterprise Guide,are really, really important
and help to minimize and managethe risk that organizations
who have to do importantwork with sensitive data--the risk that they would face.And finally, let's not
discount the simplificationof software updates.SAS is a big application, and
to install it and update itcan be quite a chore, and it
needs to be tightly controlled.Mission-critical
applications that rely on SASoften have to go through
a lot of validationevery time that the SAS
installation is updated.By keeping that in
a central place,that minimizes the work
that needs to be done.We're not having to
update SAS on allof our analysts' machines.Instead, we can update it
in just one place, where allof our analysts can access it.Likewise, SAS Enterprise
Guide, as a Windows clientapplication, is easy to update.So recent releases
of Enterprise Guidehave automatic
updates, actually,that they can go out
and check the SASwebsite for the
availability of updatesand then prompt the
user to update that justwith the click of a button.Now is a good time to check,
if you don't already know,what version of Enterprise
Guide you're running.The most recent release at
the time of this recordingis version 8.2, and
version 8.2 and 8.1 lookand feel very similar
to each other.The family of releases
just before 8.1was the 7.1 family of releases,
leading all the way up to 7.15.There were several 7.1
releases in that time frame.Or do you not know what release
you're running and haven'treally paid attention?It's a good idea to check.Most of what I'll be
demonstrating and talkingabout in this tutorial relates
to Enterprise Guide 8.2,but almost everything that I'm
talking about applies to 8.1,as well.And much of it also
applies to the version 7.1family of releases.These are the more modern
releases of Enterprise Guide.And hopefully, you're running
at least one of those.8.2 would be ideal, of course.But Enterprise Guide
goes way back in history.The first release hit
the field in 1999,and it has been
around since thenwith many major releases,
some of them coincidingwith major releases of SAS.Now, one of the great things
about Enterprise Guideis that it is backwards
compatible with older SASreleases.So even though you
might be using 8.2,you could still talk to an older
version of SAS 9.4, or SAS 9.3,or even SAS 9.2.But I kind of hope that
you're not running SAS 9.2.That release has been out
and outdated for a long time.But it gives you the flexibility
to keep your Enterprise Guideversion up-to-date
and it doesn'trequire you to update your
version of SAS, which is oftena more onerous process for
organizations to undertake,and something that's
much more controlled.If you're like me, you're
going to spend a ton of timein this tool.While the default layout and
appearance of SAS EnterpriseGuide is designed to help
any user be productive,it is worth taking the
time to explore the waysthat you can customize
the environment.There are a ton of
options available.So you can change the layout
change, the color scheme--so much about the tool that
you can customize to helpyou feel productive.Let's take a look at
some of the optionsyou have available to you.Now, the main layout
of SAS Enterprise Guidelooks something like this.Along the top, you have
access to the main menus,and the toolbar
for quick actions.Don't let the small
menu fool you.There are a ton of features
that are available whenyou start digging into this.Over on the left, by default,
we have the Navigation area.This is going to show you things
like your current project,if you're using a project,
and current files that youhave available and open.Down below, in
this Servers area,part of the Navigation area,
that gives you access to what'sgoing on with your SAS servers,
your libraries, your data sets,and variables.You will also have
access to the Tasks menu,and if you're
using SAS metadata,items that are available
there, like storedprocesses, and
registered metadata data.In this main area here,
we have the Work area.So this is where all of your
content is going to appear.So whatever you're
working on at the moment--that could be code,
that could be data,it could be results that
have come back from SAS.This is where you're going to
spend most of your focus timein the tool, and you can
arrange this pretty muchhow ever you want.By default, the Start
page in Enterprise Guideis designed to be a
nice launching-off pointfor you to get going.But you can even
customize that, as well.Let's take a look.So first things first--dark mode.It's all the rage.All of the cool
applications have dark mode.SAS Enterprise Guide
is no different.In version 8.1, the
developers added a dark modeto SAS Enterprise Guide.Here, you see an animation
just toggling back and forthbetween the two, dark mode
and regular light mode.But in a demo just after
this, I will show youhow to enable this for yourself.But quick hint-- there is just
a nice quick keyboard shortcutthat allows you to toggle
between the two, dark modeand regular light mode.That Control-F2 toggle will
toggle between the two,but it won't affect any custom
editor settings you have.So a lot of times, those of us
who work in code or look at SASlogs, maybe we've
customized the editor a bitfor, say, custom fonts.That's one thing
I always change.So the dark default dark mode
setting won't affect that.So there's an extra step
you need to do to customize.And again, I'll show you that
in a demo just coming up.Another thing to
note is that not allof the windows within Enterprise
Guide respect the dark mode,or what we call
the Ignite theme.So a lot of the Task windows,
and the Query window,some of those pop-up windows--they still use the normal
Windows default color scheme.So the dark mode is
not going to carrythrough to every single window
within Enterprise Guide,but it does cover many of them.And also, you
might find yourselfwishing that you can customize
the color schemes a little bit.And at this moment, you can't.So you can only toggle
between Ignite, dark mode,or Illuminate, the light mode,
and the default color schemesthat each of those
themes give you.You can't customize
those themes If youwanted to vary them at all.Another aspect of
Enterprise Guidethat is going to be pretty
exciting and worthwhileexploring is the wide variety
of workspace arrangementsyou have.Here, this animation
is just flippingthrough a number of the
different configurationsthat you could
possibly have, howyou can arrange your
windows so that youcan see the work that you're
doing and be more effective.In earlier releases of
Enterprise Guide priorto version 8.1, many users
found themselves a little bitconstricted because
they could onlysee a couple of views
of things at a time.Say, one data set
and one code view.And if you wanted to
see something else,you had to close one
of those other viewsin order to open
up something new.But in 8.1 and
8.2 and later, youcan have as many windows
open as you want,and you can arrange
those windowsin all kinds of different ways.You can have them arranged
in geometric panes.You can tear off tabs of content
and float them in your window,and even over to
multiple displays.So you can really spread out
as you work in Enterprise Guidethese days.Because there are so many
options for the windowlayout within
Enterprise Guide 8.1and later, I'm going to just
share some tips for how you canavoid getting
yourself in troubleor get yourself back
to a good spot, in caseyou get a little bit lost
with so many windows going on.My first step is to
use the presets thatare within the View menu.There are some really
popular default layoutsthat are available to
you in those presets,and you can always
choose one of thoseif you find yourself
at sea with a layoutthat you've constructed
for yourself.You can revert back to one
of those more comfortablepreset layouts.You can float windows
across multiple screens.And at the bottom
of my presentation,you can see an example
of where I've done that.I have two displays
usually in my workplace,and I often float
windows between the twoto give myself more room
to spread my work out.And you can also use
F11 as a shortcut keyto toggle back and forth between
a full-screen view of whateveryour active window is.This gives you the
opportunity to focuson a single piece of content--say, a piece of code
that you're working on--without being distracted
by other paneswithin the Enterprise
Guide tool.And I mentioned that Start
page at the beginning.That Start page, amongst
many of its features,also shows you the recent
content you've had open.But one of the
really cool thingsthat that Start page offers is
also a list of pinned items.You can easily pin
an item that you'vethat you've selected
or used recentlywithin Enterprise Guide,
and that pinned item willappear on basically a sticky
list at the top of that Startpage.I use this quite often when
I'm working on a project,sometimes over a
matter of weeks.It's one of the more active
things I'm working on,and I just pin it
for the time beingso that I have quick
access to it wheneverI come back into the tool.Also, Enterprise Guide
has a Favorites menuthat you can customize.So you can add your favorite
tasks to a quick list of tasks.You can add task templates
to these favorites, as well,and I'll show you how to do
that later on in this tutorial.Providing that list of Favorites
and recently-accessed itemsprovides a quick
access to the thingsthat you use most often
within Enterprise Guide.Because there are
so many features,this Favorites list
and this Recent listgives you quick
access to the thingswithout having to
hunt around too much.In this demo, I'm
going to show yousome of those workspace
customization tipsthat I promised.OK.First, let's go ahead and
do the dark mode thing.So Control-F2 is the
quick keyboard shortcut.If I just press the
Control-F2 combination,you could see it
toggles back and forth.I prefer dark mode and I'm going
to show you a couple of thingshere in dark mode.But for most of
the tutorial, I'mgoing to keep it in
the normal light modebecause it just shows
better on video.But while I'm in
dark mode, there'sa couple of other things I
want to customize, as well.I do spend a lot of
time looking at code,and what's very important to
me are the programmer fonts.The default fonts
within Enterprise Guideare not the best, in my mind.The default, I think,
here is Courier New,but I prefer some other
special programmer fonts.So I'm going to go ahead
and find those settingsunder Program, Editor Options.And then under Editor Options,
check the Appearance tab.And under the
Appearance tab, you'llsee you have the ability
to customize justabout every aspect
of code and elementswithin your program to give
them different color treatments.I'm going to change my preferred
font from Courier New to onethat I like called
Cascadia Code.This is one that I actually
downloaded from Microsoft.It's free.Another alternative you
might like is Consolas.That's another one that's
built into Windows.Consolas is a good one, but I'm
going to pick Cascadia Code.You could see this reflected
here in this preview window,and when I click OK, you can
see it right now reflectedin my Code window.I'm not done yet.I'm going to go back
into these Editor Optionsbecause I want to
affect not just code,but I want to look at
what the logs look like,So I would like to also
affect the log output,so I'm going to pick
Cascadia Code for that.And then also, listing--sometimes, I use a
text listing output.I'm going to pick
Cascadia Code for that.There, all applied.Now, I'm done with that.But sometimes, I
go back and forthbetween different settings, and
going back into these EditorOptions again just
to show you a trickyou can do to get back to
frequently-used settings.I'm going to pick that
Appearance tab again,and you'll see there's an
item here called Scheme.This allows you to group
a set of preferenceswithin your appearance
settings together under a name,and then you can easily load up
the collection of preferencesagain to get back to them.So I've already done that
with my DarkThemeCode theme.But if I hadn't, if this was
everything I wanted to do,I could use the Save As button
here and give this schemea name, such as
DarkThemeCode and save it,and then I have that for
easy access later on.I've already done
it, so I'm justgoing to go ahead and
pick DarkThemeCode here.And then I need to do
the same thing for Log.I already have a DarkThemeLog.And then I'll do the
same thing for Listing.I have a DarkThemeListing,
and then click OK.And then I have my
named schemes applied.So that got everything just the
way I want it for dark theme.But I said I'm not going
to stay in dark themebecause I understand as
you're watching these videos,it can be a little
difficult to see,through the videos,
how things are going.So I'm going to change it, for
better contrast for the video,back to the light scheme.So I'll do Control-F2
again to go back.You can see it didn't
apply it to my editor.For that, I need to go back
to these Editor Options.And under Appearance,
instead of DarkThemeCode,I'm going to pick a scheme
I saved previously calledLightThemeCode.And you see it has the
same Cascadia Code applied.So I just repeat that
step for the Log file,and again for the Listing file.And there we go.Apply it, and there.Now, I have it just
the way I want.While I'm in here, let me open
up a project I have saved.I'm just going to
open that up here.I had it pinned on my Start
page, you might have noticed.That gave me quick access to it.I'm just going to bring up
one of the process flowsthat it's already saved
in my project and run it.While it's running, you can
see that the different statusesare highlighted for
the different items.As they complete there, they
turn from yellow to green.But what's not
happening is outputisn't opening automatically.If you've used previous
versions of Enterprise Guide,you might have experienced
where as tasks or code finishesrunning, results start popping
up coming out of those tasks,and that can be disruptive
in your workflow.So by default in
version 8.1 and 8.2,those results are not
popped up automatically.Actually, I'm quite
used to that now.I like that as items complete,
the results don't pop up.Then, I can go and just open
up the results that I want.For example, this program ran.I'm going to go ahead
and open up the data.So I can see, OK, now I have
my data view for this item.Now that I have a
few things open,let's start playing with
the layout, the workspace.So I like this nice
big view of my data.If you remember a
few minutes ago,I talked about how
you can use F11to toggle the
full-screen mode, so Ican see nothing but my data.So I'll just press F11,
and you can see, OK,now the whole screen is taken
up by just my Data view.And I can spend time in here,
focused as I would like to,on just the data.And to get back out of that,
I'll just press F11 again,and that takes me back to my
normal Project Layout view.I can also do things like grab
this tab and drag it around.And you can see the
interface highlightsto indicate where I might be
able to dock this tab if I'minterested in docking it.First of all, I don't
need to dock it.I can just float it.So you can see here, I
have floated this window.I can resize it as I like
and float it just in here.And I could even drag it
over to another monitor,if I wanted to.It's now off your view, but I
have another monitor over hereon the side that I could
still see the data in.I could also dock it.And you could see
in the center here,the little indicators
that show whereI might be able to dock this.Do I want to dock it to the top
of my view here, to the bottom,to the right, to the left, or
right back here in the centeragain as its own tab?So this provides a
really easy way for meto move my windows
around and understandexactly what's going to happen
when I release my mouse buttonand allow the item to dock.So I can look here
and I could see,OK, this is a great example.I'm now looking at my data here.I have my code.That's not the code
that ran this data.Let's open up the code
that generated that data.OK.I have my code.And I could see my code, I
could see also over here my log,and I can see the data.Not a lot of space
to focus on here.I can't really see
all of my code,and I can't really
see all of my log,and I can see just
part of my data.So again, I can drag
this Code window around.And notice, it
all comes togetheras it's all kind of packaged
up in one docking window--my code and my log all together.But I don't need to
keep it that way.I can take that log and I can
move it around, and dock itoutside, too.So even though, by default,
these items that are closelyassociated, like the
code with the logand its output data
and other results,are all bundled together
in one super window,I can break that up, too,
as I need to in orderto spread out and have
more room to work.I'm just going to leave this
docked there for right now.And then I'm just
going to dock thisback to where it was In there.Now, my content windows
aren't the only thingI can move around.I can also move around
these resource panesover on the side.So I can pick one of these
up and float it, as well,and dock it to another spot.And you can see all the options
I have in this little previewindicator, showing where
those things would be docked.So for example, if I want to
keep my Project resources overto the side, I can easily
just dock that over hereon the right.And likewise, maybe I want
to move my Servers list overthere, too.I can do similar and
just move it to bedocked underneath that one.So instead of the default
view of having those resourcesover on the left, I've now
moved them over onto the right.That's a preference
that some people have.Within here, I can also
decide which thingsto show or not show.So for example, I
don't need to haveall these things available.So if I don't use
Prompts, for example, Ican just hide that
pane, and then itgoes away and doesn't
take up a spot in my list.I can always get it back by
finding it in the View menu.You can see all the resources
here that are available,and you can turn them
on or off, as needed,as your preference dictates.I can also move things
around, like if Iwant to move this
Servers item up here,I can put that over here up
in this list of tabs, instead.So I can have my
Project and my Serversright here so that I'm
not looking at them eachat the same time, but
they are together.And maybe I want to go ahead
and just close these panes,and then I have
more space in hereto see more items in
my list as it expands.So lots of different
options you have in orderto make room for yourself and
the kind of work that you do.And if you get yourself
so out of whackthat you can't remember
how you got hereand where you can find windows
you've closed or rearrangedin such a way that you
can't get to them again,you can always go
to the View menuand say Reset to Default Layout.And after a verification
prompt, you can restore it,and then you're back to
back to the way you were.One of the first
things we need to dowhen we start using
Enterprise Guideis often import data
into our project.Importing data is easy
in Enterprise Guide.It's really just a
point-and-click operation.But if you want to take
it to the next step,there are some
important things youshould know about how the
Import Data task works,and some techniques you can
learn to help bring thatto the next level so that
you can be more productiveand reuse those import
steps in other scenarios.Let's take a look at
what's going on in import.So the Import Data
task is basicallya point-and-click wizard.It gives you really easy
access to import flat files,like CSV, tab-delimited
files, or Excel files,and a few other types, as well.We're going to talk about
how this works, though,behind the scenes,
and how to usesome of the advanced
options thatare in the task to do even more
and build repeatable steps.So how does it work?Well, the Import Data task
works on a local file.So that means you
pick a file that youcan access from your local PC.That may be on your
local hard drive,or it could be on
a network drivethat you can access
from your PC.But regardless, it
is a file you usuallyselect from your local
system, whether it'sa CSV file, or an Excel
file, or something similar.The first thing that the
Import Data task doesis it scans that file to
determine the field namesand types.So from Enterprise
Guide, it looksthrough at least the first
so many records of the file.And by default, I think that's
about 4,000 records it'sgoing to look through at the
most just to make its bestguess at the field names and the
types, whether that's numeric,or character, or does it look
like a date, or a currencyvalue, that kind of thing.And then that will set you
up for the rest of the stepsthat the wizard does.It will also try to clean
that file or cleanse it,in case that you are
bringing in a file--especially a text file--which maybe doesn't have the
most standard formatting.And then it will present
you with options for input.So it will give you the
list of those field names.It will give you
all of those guessesthat it made, in terms the
field names, and the types,and the lengths, the
INFORMATs and FORMATs thatshould be applied.And then you have
the opportunityto modify and
customize any of thosebefore you actually
commit to do the import.Once you've made all
of your selectionsand you press Finish,
and the Import task runs,it actually takes
this cleansed filethat it prepared for you,
based on all of the selectionsyou made, and it copies that
file from your local filesystem over to SAS.So it has done
this work for you.It has scanned the file.It has made a copy of the
file, so it's basicallyduplicated the data
locally behind the scenes.And then it moves
that file over to SASwhere it's going to run some SAS
code to actually do the import.So the SAS code that
is running is actuallyformulated to run on
this cleansed versionof the file, not
necessarily the exact filethat you selected for input.It's on the file
that Enterprise Guideprepped for the input work
to happen in the SAS session.But is that what you want?Many of us do work with
dirty data or filesthat are in the
non-standard format,and so we can appreciate this
cleansing work that EnterpriseGuide is doing, and
the scanning it does,and the guessing it does.But what if your data's
already clean and alreadyin the shape that we need?Do we need Enterprise Guide
to do all that work for us?Well, good news, you can use
some of the advanced optionsto bypass this step and not
create this cleansed file thatgets moved over to SAS.And that will save some time.And you can also limit the scan,
so limit the number of recordsthat Enterprise Guide will read
in order to do it's guessing,and that can sometimes
save you some time.You can then generalize
this import step.That's another option you
have, so that it can be runoutside of Enterprise Guide.So if all you're doing
is pointing and clickingthrough in a project, you
just need your file in SAS,maybe you don't need to do any
of these advanced modificationson your options.But if you want to reuse
this step in other contexts,then it's a great idea
to go ahead and selectthese performance options,
bypass the cleanse,generalize that import step.It will create for you a DATA
step, or if it's an Excel file,it might even create a
PROC IMPORT step for youthat you can run in
other contexts outsideof SAS Enterprise Guide.In this demo, I'm
going to show youhow to import a file in
SAS Enterprise Guide,and then to take
that import processand modify it to make it
more flexible later on.So I'm going to start by
opening an existing file,and I'm just going to pick
one from my local computer.And I have a series
here of CSV filesthat actually contain my
online movie streaming history.I'm going to pick the top
one here, the biggest one.And you'll see that just
opens up in Enterprise Guide,and I can get a preview--just a text view-- of
what that file looks like.Looks like a pretty
straightforward CSV filewith a title and a date.That is the title of a show and
the date that it was streamed.Now, in order to import
this using the Import task,I'm going to have to create a
project in Enterprise Guide.So if I start the
Import task, itwill create a project
for me, or I can justclick the Create Project
button over here.And I can go ahead, and
just for good measure,go back to this text file view,
right-click on the header,and say Add to Project.So now that file is
included in my project.And then from there, I can
right-click on the file nameand select Import Data.And this will bring up
the Import Data task,which will do the first level
of scanning the file for meand get me started
with the whole process.So for simple files, I could
just click Finish right here,just trusting that
the Import Datatask is going to do everything
it needs to do for me,clicking Finish,
and it will createa data set with the
fields that it detects.So let's see what that does.Ah.We can see it did
the right thing.It created a title that seems
to be long enough to accommodatethese fields, and a date.An important thing to notice
is it's a real date field.Even though the file
input is clearly text,the Import task
detected that, OK,this looks like a date
format, so let's import itas a proper date.That way, we can do
sorting and comparisonsin math on that field later on.But I'm not done.I'm going to go back and
modify this task to optimize ita little bit.The first thing
I'm going to do isI'm going to check out the
Performance button here,and you'll see that I
have the opportunityto Bypass the data
cleansing process, whichwill prevent Enterprise Guide
from creating a cleansed datafile for me.Before I get into that, let's
take a look at the actual codethat was generated by the task.It's pretty simple code.It's a DATA step that's
reading in this data file.But look at the path.The path is this big,
long, convoluted pathwith a system-generated
name that actuallyisn't the file that we picked.It's a copy of the file,
which looks probablya little bit different.In fact, let's go ahead and just
open that up in another editorand just see what that
looks like, because I'm nowI'm curious.So I'm just going to open
it up here in Notepad++.And you could see it
looks similar to the filethat we opened before, but
doesn't have the headings,because the headings are
encoded in the program thatwas generated.And the separator
character is not a comma,but it's actually--I think it's a
delete character thatis generated in this
cleansed file, somethingthat is unlikely to be
encountered in the file itself.So Enterprise Guide picks
that character as a delimiterso as to not interfere with
the real content of your file.But all of that is really
unnecessary for mostclean, prepped data
files, and so I'mgoing to bypass that process.Going back into Modify
Task, and then I'mgoing to go into Performance
and select Bypass the cleansingprocess, and click
OK, and Finish.And now if we go back into
the code, we'll see, ah.That is that actual path of
the file that we selected.The delimiter is actually
the hex value for comma.And everything seems
to be all right now.Furthermore, we can
further modify the task.If this was an Excel file, on
the final path, we could say,Generalize import step to
run outside Enterprise Guide.And in this case, where
everything is happening local,it may not make such
a big difference.But if it was more
complicated than this,the code that it generates
might be slightly different,more optimized for running
outside in another environment.As it is, I could
just take this codeand I could drop it into another
SAS tool, like SAS Studioor even just base
SAS on Windows,and as long as the file
is where it says it is,the Import task would work.I can further modify this code
if I want to, to optimize.Sometimes, the Import Data task
is very explicit in the codethat it generates.It's going ahead and assigning
a FORMAT and an INFORMATto all the fields.And while those things are
important for the date field,because it tells SAS
how to read the dataand how to interpret
it, for our text fields,it's not as important.I don't need an INFORMAT to
show me to read it as a textcharacter of a certain length.SAS will do that automatically.And I might, if I want to
generalize this for more useother places, change the title
to be a different length.To modify this task code
to suit my purposes,I could just begin
to type in here,and I'll get this prompt that
tells me the code is read-only.Do I want to make a copy
of it that can be modified?If I say yes--now, you might
have noticed when Iwent to import that data that
I actually had multiple CSVfiles in that folder.I actually have five CSV
files, and they're all exactlythe same format.And so I'm going to change
this code to actuallyimport all of them at once.This is a nice trick that the
INFILE statement can do for us.Instead of importing
just one file,I can import a series
of files by justusing the standard
filename wildcard notation.So instead of _Dadasaurus,
I'm going to just do _*,and that will match all of
these files that are listed herein this directory.While I'm in here, maybe I
want to go ahead and modifythe Title to be a
little bit longerto accommodate some longer
fields that might exist.And I'm going to go ahead and
change the name to Netflix_ALL.And let's go ahead and get rid
of the FORMAT and INFORMAT,because as we said,
I don't need those.But I'm going to change
the FORMAT, actually,to be DATE9 because
I prefer that.But the INFORMAT needs to be
the same because it is MMDDYYYY.And I'm going to go ahead and
remove the format descriptoron this INFORMAT
descriptor, because I don'twant to limit the
number of charactersI'm reading on the INFORMAT.So if I click Run
here, you'll see now Iget also still a
clean nice clean file,but it's a lot longer.It's all the records here.And with further
modifications to this code,I can also encode the filename
that's going to appear,which file it came from, by
using the filename optionon the INFILE statement.I'm not going to go
into that right now.I actually have another tutorial
on the SAS Users YouTubechannel that you can
check out that goesinto this in much more detail.So I encourage you, if
you're interested in knowinghow to read multiple
text files using SAS,starting with the process
in Enterprise Guide,you can go check out my more
extensive tutorial elsewhereon the channel.Obviously, when we're
working with SAS,we are working a ton with data.Let's spend some time looking
at the kinds of cool thingswe can do within the Data Viewer
within SAS Enterprise Guide.Let me open up some data.And just to make it a little
bit easier to work with,I'm going to go ahead
and make this full screenso I can focus on it.So I'm just going to press F11.Now, let's look at some of
the features and functionsof the Data Viewer
within the tool here.Obviously, we can do
our standard navigation.So I can use the arrow keys to
move around between records.I can use the End key to get
to the end of the record.I can press the
Home key to get backto the beginning-- start of
the record, the first column.If I want to navigate all the
way to the end of the data,I can press Control-End,
and that gets me downto the last record
in the last column.Or if I want to get
back to the top,Control-Home will
do that, as well.Bam.We can also go to
a specific record.If we know exactly the
record that we want to go to,we can press
Control-G, for Go To.And if I know the
record number--let's say, 3,000--I can type that in and navigate
to the particular ColumnI want.In this case, I'll
just leave it at title,but I could pick a
different column.Click OK, and then you could see
that the selection navigates usright down to the
3,000th observation.So I can also find in here.So just as you think
you should be able to,you can press
Control-F, and you canfind a piece of text or a value
within the whole data set.So if I want to find,
say, Tiger King,and search the whole table--so Find-- and you can see,
there it is down here.It was done in record 5,793.Pretty cool.Once we're in the
view here, we canlook at the different
properties for the column.So if I just hover
over each column name,you can see it shows me
the main attributes--the Name of the column;
if it has a label,it would show there;
and the lengthof the column in
bytes in SAS terms.I can also navigate way
over to the scroll thumb.And as I move the
thumb around, youcan see it's showing me
a preview of which recordI'm navigating to and the total
number of records available.So that total number
of records availableis accurate when we're
working with a SAS data setor with any data source for
which we have a known recordcount in our metadata.If we're looking at a SAS
Data View or some databases,for example, we may not
know the exact record countas we're navigating, so that
number will be indeterminate.But in this case, this
is a SAS Data Set,so we know exactly how
many records because that'sencoded in the data set.I can also look at the
properties of a columnjust by right-clicking
and selecting Properties.And I can see the more detailed
properties for each column--the Name, the Type, whether
it's the main group--Enterprise Guide assigns
a group to a column type,so it could be a date type,
or currency, for example,or a date time.All of these are
numerics in SAS.They are treated
specially because theyare special types because they
have certain formats assigned.If the column isn't wide enough
for us to see the whole value,we can auto-size the column
just by double-clickingon the border of it, and
then Enterprise Guide Viewerwill expand to accommodate the
length of the largest valuewithin that column.So in many ways,
this Data Viewerworks like any
other tabular viewerthat you're probably used to,
so nothing really special there.I want to spend
some time talkingabout how you can filter
your view of the data,and that is by using
the Where tool.So up here on the left, you can
see there's a button for Where.And if I click to expand
it, it will show mea field where I can type
in my WHERE expression.And you can see,
actually, it showsme an example for this
data source of whata WHERE clause might look like.So this is a standard
WHERE expression in SAS.So if you are used to
using the WHERE statementwithin your DATA
step or within a PROCstatement, the syntax that's
valid there is also valid here.It's the exact same thing.So for example, if I want to
do WHERE the title contains--and I'm going to use
the question markas a shorthand for contains--office and press
Enter, you can seenow my view is filtered
to just records--and there are plenty of them--that contain the word office.But I can get even fancier.I can combine with Boolean
values, Boolean expressions.So I can say ""and""
I want the date--this would be the
date streamed--greater than, let's
say, 01jan2020.And notice I'm expressing
this date value using the dateliteral syntax that we're
all familiar with in SAS,where I have a
quoted value and thenthe d to indicate
it's a date value.So I'll press
Enter here, and youcan see now I'm
filtered down to justthe occurrences of streaming
that happened in 2020.If I want to go and change
that to, say, less than 2020,so before 2020, I'm going
to press Enter there,and you can see, OK, now, I've
just got the pre-2020 dates.So pretty simple.I can also do between.So I'd say WHERE date BETWEEN,
let's say, 2018 and 2019.So just to remove
the ambiguity here,I'm going to put some
parentheses in thereto help that, and press Enter.And now you can see I've
got only the ones thatoccur within 2018.And how many records?I can look over
here on the thumband see it's 213
episodes of The Officewere streamed in the year 2018.So big year for The Office.We can also use SAS functions
within these expressions.So another way to express what
I've done here with the BETWEENoperator is instead
of using BETWEENand has having to
specify two dates,let's change this to WHERE the
date is just in the year 2018.So I'm going to say WHERE
the year part of date--I'll just say the
YEAR function, whichwill return just the year--equals 2018.And I get the same results.If I want to easily just
change this to, let's say,look at 2017, you can see,
oh, there's very many fewer.And if I go to 2019,
I could see, OK,let's look over
here in the thumb.140 records there.And then let's see what this
year has been like in 2020so far.165.So it's shaping up to be a
big year for The Office here.So all of these expressions--it can be quite flexible to
explore in here without havingto do the work of, say, writing
code or starting the QueryBuilder, which would create a
whole other data set, which allof that takes time and storage.So this can be a faster way
to get a quick view of what'sin your data.Let's see.You can also look at all
the metadata propertiesfor all the columns by clicking
the Properties button up here,this little icon in the
toolbar, to view the properties.And you can see this is,
in general, the propertiesfor this data set.And we have a Columns
tab that shows us nowall of the column
information in one place.And if I want to save
this off-- to, say,create a data dictionary
or something like that--I can Copy to clipboard and
paste that into an Excel sheet,if I want to, and there
it is, ready for any kindof reporting I might
want to do on this data.And while we're in
here, let's look at whatit takes to copy the data out.If I want to take
some of this dataand put it into another
place, I can justselect the data I
want, right-click,and I can, of course,
copy the values.But this feature, called
Copy with headers,is actually pretty neat.When I Copy with headers,
what it puts on the clipboardis a tab-delimited series
of values for these records,including the headers.So if I create a new sheet
and paste that in, youcould see in Excel, I have
basically a little minispreadsheet that contains
part of the data.This is a pretty neat way to be
able to share data out quicklywithout a big process.One of the realities we
have to deal with when we'reusing SAS in a
large organizationis that eventually,
the people we work withare going to want
their output in Excel.Now, to some of us, that
feels like a tragedybecause obviously, we much
prefer working in SAS.But in order to keep our
colleagues and managers happy,we need to find a way
to get the great outputwe've created with
SAS into a formatthat our constituents
can be familiar with.There's no shortage of ways that
you can export your SAS contentto Microsoft Excel.I'm going to go through
a few of those here.Each method is different and
serves different purposes,and it's important to have
all of these in your toolkitso you can decide the best
method to use for whateversituation you're in.But first, what
do you use today?When you're working
with Enterprise Guide,do you just use the Export
menu in Enterprise Guide,or the Share menu to dump your
data right into an Excel sheet?That is one thing
you can do, but thereare many more options.Or do you do it the traditional
way of PROC EXPORT from a SASprogram?Certainly, that
still works, too.Or maybe you stand on
principle and you justwon't create Excel files, and
you'll leave your colleaguesto their own devices to figure
out how to get the data.If so, well, good for you.But for most of us, we
don't have that option.These are just a few of the
methods we'll talk about.I really like to talk
about Send to Excelbecause I think it's an
overlooked method thatis really simple to
use for ad-hoc purposeswhen you just need to quickly
get some SAS output to Excel.I'll talk about ODS Excel.That is something that
is built into SAS.You can create your report
output pretty much the wayyou like it in SAS, using
the SAS programming language.There are some options built
into SAS Enterprise Guideto make that even easier.And then we'll also cover
the Export or Share methodsthat are built into the menus
within Enterprise Guide thatallow you to move
your data directlyinto an Excel spreadsheet.And we've already talked a
little bit about the Copywith headers approach.That is another simple way.When you just have a
few records that youwant to jump into
an Excel spreadsheetwith or maybe paste
into an email message,don't forget that.That's a really
simple method you canuse without a lot of process.All right.Here we are back in
SAS Enterprise Guide.Let's take a look
at Send to Excel.You'll find it under the Share
menu in Enterprise Guide 8.1and later.In earlier versions, you'll
find it under the Send To menu.So we have a number
of options heredepending on what other tools
you might have installed.But I'm going to go ahead and
pick Send to Microsoft Excel.What this does is
it actually launchesan instance of Microsoft
Excel, and thenit actually sends the
content from Enterprise Guideinto that sheet.It's kind of like a
glorified copy and paste,or yeah, it's exactly like
a glorified copy and paste.Who am I kidding?So it's bringing up Excel and
actually sending the recordsto Excel.And it ran quickly.Now, while I'm in Excel,
I can modify the sheetfurther if I would like
to say, bold, the headingsor do other kinds
of things in Excel.And then I can save
this Excel fileand share it with other people.And that's a pretty typical
way that, on an ad-hoc basis,you might share
content to Excel.But there's a couple
of downsides to it.One is, first of all, if
the data is very large,then it's going to be very
slow to actually send to Excel.It's going to take a long time.And so it may not be the
most efficient thing to do.If it's something
you want to repeat--if you want to have a process
in your Enterprise Guide projectthat does this every
time you run the project,this isn't the way to do it.This is purely
driven by the menu,so you have to point and
click to make this happen.But there are
automated ways that youcan send your content to Excel,
so let's talk about that next.Let's talk about ODS Excel.Now, if you're familiar
with programming in SAS,you are undoubtedly familiar
with ODS and the many differentoptions in ODS that we have--
they're called Destinations--to create output in
different file formats.The default one in
Enterprise Guide,and in many tools
that we use, is HTML.So we can create report
output using HTML.And you'll see here, I've got an
example of a report I created.It's using HTML.And it's got a graph and
it's got some tables.All of this together might
make a nice report in Excel,but it's HTML right now.So how do we turn this
into an Excel report?In Enterprise Guide,
it can be very easy.Under Tools, Options,
let's explorethe many different kinds of
results that we can have.So HTML, as I said,
is the default.But we also have Excel.We have our RTF, basically
for Word, PDF, and PowerPoint.We have our traditional listing,
which is just text-only.And then we have the
proprietary SAS Report,which is used among
some other SAS tools.Most of us are good with
HTML in our day-to-day,but sometimes, we need
Excel for other people,or we need PDF for static
reports, that kind of thing.So you can change
any of these optionsand set them at the application
level to set your preferences.And then with each within
each of those options,you have options to say
how you want that HTML.Which appearance
style would you want?How about Excel?There's metadata you
can add into Exceland have all kinds of controls.But remember, when you
set these options herein the Options dialog, it
affects everything you'redoing in Enterprise Guide.It is an
application-wide setting.So it would be nice
if we can just controlthis on a task-by-task basis.And guess what?We can.We can get to that in the
properties for each taskor program that
we're working with.In this case, I'm working here
with a program that generatedthis graph and table output.So I'm going to go ahead
and select View and SetProperties, this little
icon in the toolbar.And you'll see a mini
version under the Resultstab here of my options.And so to get Excel
output, I'm justgoing to select to
Customize results formats,and then uncheck HTML and check
the Excel option, and click OK.And then I'll run this.And then as it completes, you'll
see I no longer have this HTMLreport.Instead, I have a
placeholder for Excel.Enterprise Guide won't open
the Excel file inside the app,but it will give me an
easy way to launch Excel sothat you can see these results.And so we can see
what this generatedwas an Excel file that has
one sheet per piece of output.Maybe that's what you want.This is the default
output from ODS Excel.But there are lots of options
we can select to customize that.Let's take a look at
what that looks like.I'll just close that.Here, I've modified
my code a little bit.Recognizing that I'm going to
be creating some Excel output,I use the ODS EXCEL
statement and this ID option,which signifies I want to
basically modify the EnterpriseGuide-generated
ODS EXCEL statementto add or modify some options.I'm going to change the
SHEET_INTERVAL to none, whichmeans that it's not
going to break outthis content among
different sheets.I want it all on one sheet.By default, the SHEET_INTERVAL
is PROC or TABLE--I think it's TABLE--and so every piece of output
gets a different sheet.And then I want to customize
the name of the sheet,called Shows.So having made that
change, I'll just run this.And it comes back
pretty quickly.So I'll just open up
this Excel output again.And we can see
what is different.You could see, instead of having
multiple tabs, multiple sheets,I've just got the
one, called Shows,and everything is
included on the one sheetnow as a single report.Great.That's a great start.Let's take this
to the next level.What if I want to not just
have my report output,but I actually want to
share out my data, as well?So let's say I want to create
an Excel report that containedthe report I just showed
you plus the raw datafrom my input, so that we
have that for referencewithin the same Excel workbook?It's a little more
coding we needto add to make this happen.So what I'm going to do is
undo this modification I made.And I'm just going to hit
Control-/ to comment thatquickly.And then I'm going to
uncomment these other itemsthat I've staged here.Control-Shift-/
does that for me.And just by way of
explanation, really quickly,I'm going to, first
of all, define whereI want this Excel file to be.So by default, when Enterprise
Guide generates ODS statementsfor you, the file
it generates isin a temporary file
in the SAS session,and Enterprise Guide
does you the favorof bringing in that file
back into your sessionso that you can easily view it.I'm going to be explicit
about where I want this to be.I'm just going to put
it in the WORK folder,calling it report.xlsx.And I'm going to use the ODS
EXCEL statement myself withmy own ID, specifying
that FILE output,and then keeping those
options I set before--the SHEET_INTERVAL ='none'
and the SHEET_NAME = 'Shows'.Then at the end, I'm going
to uncomment these items.When I'm done
creating that report,I'm going to close that
ODS EXCEL destination,but then I want to append,
in the same workbook,that detailed data that comes
from my viewing data set.So for that, I'm going
to use PROC EXPORT here,specifying the DATA = viewing.I'm going to use the same
output destination, OUTFILE.And in this case, I'm just going
to be able to append to it.So one thing to
know about ODS EXCELis when you use ODS
EXCEL, it alwaysis going to create
a new xlsx file.But PROC EXPORT can add
to an existing Excel file.So we wouldn't be able to do
this in the reverse order.I can't use a PROC
EXPORT and thenuse ODS EXCEL to
append onto a filethat I created
using PROC EXPORT.But I can use ODS EXCEL and
then use PROC EXPORT to appendonto the thing it created.So I'm going to
specify DBMS = xlsx.And this does require SAS
Access to PC File Formats, whichin practice, most people have.But if you don't,
just keep that in mindthat in order to use PROC EXPORT
to generate native Excel files,it does require that
module from SAS.I'll REPLACE the sheet
if it already exists.In this case, it won't, but
that's just specifying that.And then the SHEET_NAME--I'll just call this ALL VIEWING.Then the next thing I need
to do-- this is important--is I want to make sure that in
my properties that EnterpriseGuide is generating
the code for me,I want to customize these
results formats again,but I'm going to
deselect everything.So I'm going to clear
out all of these options,which basically means,
Enterprise Guide, Idon't want you to create any
default ODS output for me.Click OK, and then
I will run this.Now we can see a much
longer filename here.It's coming from my temp
area in my SAS session.But let's open this up
and see what we got.So what we have is we have
that report that we saw before,same as it was.And now we have a second
sheet here, ALL VIEWING,that is the raw data that fed
into that report for reference.So I've packaged up
here, in a single Excelsheet, the nice
output from my projectso it's ready for
my constituents.Finally, let's look
at how Export works.So within Enterprise Guide,
I can also share directlyto Excel, an Excel file.Again, under the Share
menu, I can use Export.So instead of using
Send to, whichwould automate Excel and bring
up an Excel session for me,if I'd rather just
write directlyto an Excel spreadsheet
file, I can use Export.So when I click on
Export, first of all,I need to generate a
name or select a name.So let's call it Viewing.But you can see
my files of type--I have a lot of
different file typeshere that I can pick from.But I'll pick xlsx.And then I can just save
that, and that's it.That file was created.Let's see if I can
find it on my drive.Here it is, viewing.xlsx.If I open this up,
you can see here,it's just a native Excel file.Nothing really special about it.When you export this way,
it's exporting just the data.So there's no formatting-- that
is, no appearance formatting--no ability to export a
graph, or anything like that.This is just the data rows.And in this case, it's
Enterprise Guide writingdirectly to the Excel file.So this style of export does not
require SAS Access to PC filesbecause its Enterprise Guide
doing the work, not SAS.There is no SAS code
generated to makethis export step happen.If I want to repeat this step
every time I run the project,then the thing to do is to
select Export as a step.So very similar to what
I just selected before,I would just do Share,
but instead of Export, Iwould select Export
as a step in project.And now I have a slightly
different interface,a little bit wizard
to step through,but I have pretty
much the same options.So I'm going to
select this VIEWING,and then I want to
export as an xlsx file.I have an option to use
labels for column namesif I'd rather, and
then click Next.I'm going to put it
on my local machine.It opens that in a temp folder.And I'm going to select to
Overwrite existing output,so every time I run
this, it will overwritethe version that's there.I have a summary here of
what's going to happen,and I click Finish.And then it runs, and
you see it exports.Bringing up and viewing
my temp folder here,I can see that, indeed,
the file is there.And it looks very similar
to what I just showed youwith the plain old Export task.But the difference is that
this is a task now in my flow,and you can see it actually has
a little bit of a log showingme what happened, that
the Export job completed.And you can see that
here, now, in my flow,there is this Export
step that now exists,and it shows me
the flow of things.So once this data set is created
and this whole flow runs,this Export step
will happen againand generate that file for me.Let's talk about
Enterprise Guide projects.One of the things that
makes Enterprise Guidedifferent from some
other SAS toolsis that you can
organize and storeyour work within these
project files, whichcan make it easier for you to
maintain your work over time.Let's talk about some of the
other advantages projectshave for you.But first, what you need
to know about projectsis that you don't
need to use them,at least within Enterprise
Guide 8.1 and later.It used to be that every action
that you did within EnterpriseGuide, first thing
that would happenwas it would create a
project file for you.And if all you wanted to do was
write code or open some data,a project file may have felt
like a little bit of overkill.Well, now, if all you're doing
is coding or viewing data,you no longer need to
have a project file going.But you do need
project files if youhave any kind of interactions
with tasks or queries.So that is going to
basically kick offthe creation of a process
flow within your project,and you'd need that
project structure in orderfor those things to happen.Project files change over time.And the way it works
with SAS EnterpriseGuide is project files are
always forward-compatible.That is, you can have a project
file from a previous releaseand open it in a future release.However, once you've saved
that project file in the mostcurrent release,
you are not goingto be able to take it
back and open it upin an earlier release.That's a frustration that
sometimes people run into,but it is just the way it works.And the way to plan
for that is justto make backup copies
of your files every timeyou do a version
migration, just in case.Project files are a great
way to organize your work,and they can be self-contained,
in that all of your code,and your data, and notes can
all be in this one project file.But they are opaque
to other processes.That means, there's nothing
else that can reallyread Enterprise Guide projects,
other than Enterprise Guide,or in some cases now, SAS
Studio can open up project filesand bring some of
that work forward.But there aren't
many other tools--well, no other tools that can
really read a project file.So a project file is not really
shareable with people whoare not using Enterprise Guide.You think of projects
like a recipe.They contain the
list of ingredientsof everything you've done
in order to accomplishthe work that the
project representsand the instructions
for how to combine them.But it doesn't always
contain those ingredients.That is, you may have
a reference to codeor a reference to data,
but the code and the datadon't need to be within
the project file itself.They can live in another
place in a file system.Let's talk about some
of the special thingsthat you can do with
projects, though.You can, of course, as I said,
organize your work and adddocumentation--
never a bad idea--using the Notes feature.And these can be like a sticky
note within your projectand process flow.You can create
relationships between itemsin your project and your
process flow with links.So you may notice, as you build
up your project and your flow,that items are linked together
and that this task createsthis data and
creates this output,and so those things are
represented in links.But you can create
your own linksthat help to not just
document but also enforcea sequence of events that
happen in your project.There's a special
process flow, calledAutoexec, that you can
add to your project.When you have a
project that containsa flow called Autoexec, much
like the traditional SASprogram that would
kick off at the startof your traditional SAS
session, the Autoexec flowis a flow that will kick off
when you open your EnterpriseGuide project.So if you have a
special task, or saya piece of code, or
a LIBNAME assignment,or something that needs
to run before anythingelse in your project,
you can include thisin an Autoexec flow,
and basically in code,enforce that sequence
of events happening.Projects can be much more
portable among team membersif you use relative
paths for things that youreference within your project.So files you might reference
within your projectinclude SAS program
files, or flat files,or Excel files that
you're importing.You can mark a project as having
a relative file of references,and so the project,
as you move it around,will retain those linkages.If you move that file
to another directoryor you give it to
a colleague whohas maybe a different
directory structure,everything will work
relative to each otherwhen you use this feature.It's possible to include
conditional brancheswithin your process flow.You can set up rules for
different parts of your projectto run, depending on conditions
as your flow executes.And if you have a
special sequenceof tasks that need to
happen in a special order,you can capture this in
what's called an Ordered List.An Ordered List let you pick
tasks from across your project,no matter what flow
they're in, and add themas a sequence of things to run.And then any time you run that
Ordered List, just those taskswill run in the order
that you specify.And you can share work
across different projectsby using copy and paste.So you can copy a
task, or one item,from a project to
another project,or you can copy an entire
sequence-- a whole flow--across different
projects, and everythingwill be maintained as you
copy from one to the other.I want to share with you a short
demonstration of a special taskcalled the Project Reviewer.Now, it's not built into
SAS Enterprise Guide.It's a custom task.And you can download it
for free from my blogat blogs.sas.com/sasdummy.Just search for
Project Reviewer,and you'll find the blog
with a link to the download.A couple of special
instructions areincluded for how you
would install thisinto your SAS Enterprise Guide.Once it's installed, you'll
find it in your list of Tasks.I already have it
installed, so I'mgoing to go ahead and
find it in my Task list.It's called Project
Reviewer, so I'm justgoing to type in
project, and you couldsee the task it shows me here.It works, of course, only
if you have a project open.So I already have
a project open.And it will show you the list
of process flows you have,as well as what's
in those flows.So in this second flow, I
have seven items that execute,and they are programs, and
queries, and other tasks.They were all modified
by me, so it's justshowing who last modified
them, how long theytook to run the last time they
were executed, when they werecreated, when they
were last modified,and whether they
have any errors.All of this is
information that youcan get from these
items in your project.You can right-click
on any item that runs,and select Properties, and
see all of this informationavailable to you.The Project Reviewer
task just sums it all upfor you in one view,
and then gives youthe total running
time for the project.In this case, it's a pretty
fast running project.It only takes four
seconds to run.I can also create a
report out of this,and encode this report
into my process flowso that any time that
the project runs,it will actually create a
report about how it ran.So I created a report and
it ran pretty quickly here,and you can see a summary
report of basically what I justsaw in that window, but
now it's here in my projectin report form.And this task is now
included in my project.So anytime my project runs,
this Project Reviewer taskwould also run for me.It's just a handy little
tool to get a little bitmore information
out of your projectand make things a
little bit more visiblefor others to see
what's going on.One of the special subtleties
of SAS Enterprise Guideis how files get moved around.Most of the time, we're
just pointing and clicking,and we're selecting files from
our local machine, generatingfiles on a remote SAS session,
and everything just works.It just connects and shows up
in our Enterprise Guide session,as if by magic.But what's happening
behind the scenesis Enterprise Guide is actually
copying files or moving filesback and forth between your
local PC and a remote SASsession.You can take control of how
Enterprise Guide does thiswhen needed.And this is important,
because sometimes, youneed to explicitly copy files
from one place to another.And there are some special
tasks within Enterprise Guidethat allow you to do that.One of them is called, fittingly
enough, the Copy Files task.And think of this
as sort of like FTPwithin your Enterprise
Guide session.You can pick any file from
your local machine, or seriesof files, and copy them to
a location on the remote SASsession.Likewise, you can copy
files from a remote SASsession, a remote directory
in your SAS environment,and copy them to your local PC.Think of doing this when
you have a local Excelfile that you want to run
PROC EXPORT on the remote SASsession, or you've generated an
Excel file in your remote SASsession, and you want to
download that to your local PCto attach it to an
email or distribute itto some colleagues.In addition, there are a
couple of other special tasksspecifically for data sets, and
it's the Download Data to PCand the Upload
Data to the Server.These tasks specialize in
selecting data set filesfrom your local PC and putting
them into SAS libraries,or selecting SAS library
members and downloading themto your local PC as
a SAS data set file.Let's talk about the Upload and
Download Data Sets tasks first.They deal with SAS7BDAT files.So we're copying
SAS7BDAT files--that is a SAS data set file--from your local PC to
a remote SAS library.The Download version
of the task copiesdata from any SAS library
to a local SAS7BDAT file.And it's important to
note that this doesn'tneed to be a SAS library.It could be a database library
that you're pointing at.It will still make
a copy for youand download it to your
local PC as a SAS7BDAT file.If you have used SAS
CONNECT and PROC UPLOADand DOWNLOAD in the past,
this works like those.But it's not using SAS CONNECT.Instead, it's using
the connectionthat Enterprise Guide
has with the SASserver to move these
files back and forth.The steps, therefore, are
not represented in SAS code.That is, even though there could
be a little bit of code that isgenerated to fixing encodings,
which I'll talk aboutin a moment, most of the work
that's happening-- that is,the file transfer itself--is not in a SAS program that
you could copy and use itin some other environment.As I just mentioned, there
are some postprocessing stepsthat can happen during
these tasks to fix encoding.So if you have a local
SAS7BDAT file thathappens to be Unix-encoded,
but you upload itto a Windows Server
or vice-versa, well,these tasks will fix
that encoding for you.They will rewrite the data
set into the native encodingof the SAS session where
you're sending it to.The Copy Files task is a
much broader workhorse.It can move any file
between your local PCand your remote SAS session.You can move one file
at a time, or youcan move a whole
batch of them usingstandard wildcard notation, such
as the asterisk or a questionmark.You can also generate
dynamic names and foldersas instructions for
which files to copyand to where using
SAS macro variables.If you're moving text files
back and forth, especiallybetween the Windows
and Unix environment,you have the option of
fixing those line endings.That's because
those environmentsuse different conventions
for the line endings.And it also supports the
task template mechanismthat's built into
Enterprise Guide, whichmakes it easy to reuse
among different projects.So if you have a set of files
or a type of copy operationthat you have to repeat over
and over again in just aboutevery project that you do,
make a task template for that,and then it's easy
to just drop thatinto any project
where you need it.In this demo, I'm going to show
you some examples of the CopyFiles task in action.Let's imagine that I have
a remote SAS session--let's call it SAS App--and I have some local
CSV files that Ineed to import using SAS App.And then on that
SAS App server, Ineed to read those files
in, do some analysis,create a report--say, an Excel-formatted report--and then download them
back to my local PCso that I can use it there
or share it with colleagues,for example.So in order to
get this done, I'mgoing to need to upload
those CSV files to SAS App,do all of the work in
SAS to create the report,and then download the
Excel file when finished.So on my screen
here, on the left,I have a list of my data files
that I'm going to import.It's these CSV files that I've
been using in other demos.There's five of them.The first thing
I need to do is Ineed to create a folder
on my SAS App serverto receive these files.Maybe you already
have one set up.But in my case, I
just want to make surethat, regardless of
the file structure,I just need a temporary
space for wherethese files are going to go.So what I've done
in my program hereis I've identified, first of
all at the local directory,where these are coming
from as a macro variable.And then, using a trick
in the LIBNAME statement,I've identified a
path for these to gointo-- just a subdirectory
of my work folder.And then I've created,
using the LIBNAME statement,that subfolder that I can
address using this macrovariable called NetflixData.This is important because
in the Copy Tasks task,I'm going to be able to
use these macro variables.Let's take a look at the
Copy Files task right now.So I'm going to find
that in my Task menu.And I can just search
in Tasks for a copy,and you'll see the Copy
Files task comes up.And I'll just
double-click to open that.Let's take a look at the
fields that the Copy Filestask lets us use.First of all, I can pick
which SAS server to use.In this case, I
want to use SASApp,because that's the
folder where I'mgoing to copy these files to.I'm going to Upload from my
local PC to the SAS session,because I need to copy
the CSV files up there.And then I would specify
the files to copy,and I could specify the full
path with a wildcard using,say, *.csv, and then specify
a Destination folder on SASAppwhere I want them to go.I've already done that, so
let's go ahead and open upthe Copy Files task I have
in my process flow here.So just modifying this,
I'll just show youwhat I've already selected.Upload, and then I've used
that macro variable I defined.This is the macro variable that
defines the local path wheremy files are coming from--&localdata backslash *.csv--
backslash because Windows likea backslash for the filenames.And then the Destination
folder is that folderI created, which I've assigned
to the Netflix Data macrovariable.It's very important that I
click Resolve macro variablesin source and destination
paths so that the task knowsto expect some macro
variables here,and it will resolve
those for me before ittries to do any of the copying.Just in case, I selected
to Overwrite existing fileswith the same name.And because these
are text files,I'm going to go ahead
and Fix the line endingsbecause if they're
Windows text files,they'll have a different
convention for the line endingthan Unix does, and my SAS App
happens to be a Unix server.So when I run my program here--first, to define
my macro variables.So that ran.And then I'm going to go ahead
and run my Copy Files task.And let's open it, and you
could see the log generate hereas it's copying those files.It resolved those
macro variables,told me what files it found
that matched my wildcard--*.csv-- and then for
each file it uploaded,it had an entry in the log.Now, this is not
a SAS program log.This is a log from the task.Remember, there's no SAS
code that's happening here.This is all just
work that the task isdoing using Enterprise Guide.OK.Let's go back to
the process flow.My next step is I'm going
to read the data in,and this is using the
same imports stylethat I used before.I'm just going to assign a
filename to this path withthe wildcard, *.csv, and then
use INFILE to read them allin, in one fell swoop.So let's go ahead and
run that on SAS App.Great.Now, back to my process flow.I'm going to run another
piece of SAS codethat generates the Excel report.And again, this SAS
code may look familiar.I've used it in other
parts of this tutorial.But in this case,
I'm going to set destto the folder on
my SAS App server--the same folder I
put the data in.And I'll call this
report report.xlsx,and then generate the
report using ODS EXCEL.And then I'm going to
tack on the detailed datafrom my report data as an
extra sheet using PROC EXPORTin the same Excel file.And then I'm going
to use the Copy Filestask to copy that down.So let's look at the settings
for this Copy Files task.So similar to the Upload,
but in the other direction.So in this case, I'm going to
Download this file from my SASsession to my local PC.The Source I'm copying from
is that folder on my SASApp/report.xlsx.That's the file I
intend to createusing that piece of
code I just showed you.And then my local Destination
folder is that &localdata macrovariable that I defined earlier.And just like I did
before, I need to make sureI select Resolve
SAS macro variables,and I'm going to Overwrite
files with the same name,even though currently,
there's no conflict.And I do not need to fix our
line endings, in this case,because it's an Excel file.It's binary.Line endings is not an issue.So let's go ahead and run.I'm going to go ahead
and run this to the end,so I'm just going to
right-click on my taskhere and say Run from
selected item, whichwill run this task and
then the one following.And then as that Copy
Files task completes,you'll see the
report.xlsx appears herein my local folder,
and I can justdouble-click to open that in
Excel and see what we got.And there's my report.Here's my Excel report
from ODS EXCEL, and thenall that detailed data
that was created usingPROC EXPORT following that.So in a production capacity, I
can just rerun this whole flowand it will copy the local
files up to the server,import them, create the
report, and then downloadthat final report
back to my local PCto complete the
roundtrip of processing.I want to show you one more
thing related to the Copy Filestask, and this is a
trick you can actuallyuse with just about any task.Let's say I've defined
the settings in my taskthat I want to be able to
reuse in other projects.I can create a task
template to do that.The way I do that
is I can right-clickon the task within my project
and select Create tasktemplate.And my next step is to
define a Name for it,so I'm just going to call this
Copy Netflix Files to Server.And I'll just Create it.Actually, if I have
a lot of templates,I can organize these in
folders and do all kindsof cool productivity things.But for now, I'm just going
to make a generic Copy NetflixFiles to Server.And now, if I search my Tasks--let's see.Under SAS Tasks, I'll go copy.And you'll see here,
under my Tasks Templates,I have a new entry called
Copy Netflix Files to Server.If I open this up, you'll see
it comes prepopulated with allthe settings that I had
saved in the version Icreated in my project.I can use this task
in a new projectto carry those
settings over so that Ican use them again and again.This is a great way to be
able to reuse your work.You can also even
share your templatewith teammates using the
Task Template ManagementSystem that's built
into Enterprise Guide.Cool way to reuse your work.No Enterprise Guide
tutorial wouldbe complete without talking
about the Query Builder.Let's face it.Some of us spend a lot of
time in the Query Builder,so it makes sense to make sure
that we know all the thingsthat it can do for us so
that we can take advantageof its many, many features.So here are a few
things you shouldknow about the Query Builder
in SAS Enterprise Guide.First of all, it
creates PROC SQL code.And in a nutshell, that's
really all it does.But PROC SQL is big.SQL, as a programming
language is big,so there's a lot going on here.The PROC SQL that
Enterprise Guide generatesis mostly standard SQL, but
it has some differences.It offers point-and-click access
to a bunch of data prep tasks,and here's just a few of those.Obviously, the basic filter
with the WHERE clause,all kinds of joins, being
able to recode variables,to sort data, to summarize
and group those summaries,calculate new columns,
connect to your databases,select and subselect--so subquery or nested queries--SELECT DISTINCT-- that
is, to dedupe data.You can filter on
join conditions.You can filter on the summary--
that's the HAVING clause.It can prompt for values.You can use Enterprise
Guide promptsto prompt an end user for
values during your query.And then you can add SASisms,
like a label and a formatto your output.So this is a great time to
think about what databasesare you using in your workplace,
because the Query Buildergenerating SQL is compatible
with many databases thatare out there.So do you use databases, or
are you just SAS data only?Or maybe you use a database
at your enterprise,like Oracle or Teradata.Or maybe you use a cloud-based
database, like Amazon Redshift.Or maybe you don't really
know because somebody else setall these LIBNAMEs up
for you, and you justpoint Enterprise Guide
at data and query away.Let's talk about a few
of the things you can do.Of course, you can do
the simple filtering.So once you throw your table
into the Query Builder,it's easy to just go
over to the Filter taband add one or more filters.And you can combine these
filters in multiple ways.You can ""and"" them.You can ""or"" them.You can group them to create
more complex logic, as well.You can summarize.So you can use the aggregation
expressions, like SUM or COUNT,to just summarize data
that's in your data source.And then Enterprise Guide
will automatically groupthose summaries by the remaining
fields within your data sourcethat you've selected to include.You can compute new columns.So for example, you want to
create a dummy variable thatis a 1 or a 0 based on the
value of one of your fields?Well, that's easy to do.You can just use the
Expression Builder to computea new column, and there's
some built-in expressionsthat you can use,
or you can createa more complex expression using
a full-on Expression Buildertool that lets you
select from a whole bunchof different functions
and operatorsto build more
complex expressions.Of course, you can join data.So you can add multiple tables.The Query Builder
supports up to, Ithink, 32 tables, which
would be a crazy largejoin if you were to do it.Most of us join maybe two
to four tables at a time.But I know some of you
out there are joininglots and lots of tables.When you join tables
in Enterprise Guide,the Query Builder
will automaticallyselect a key field
to do the join.The join field will be
based on fields thatare the same name and type.So it's not the most
sophisticated wayto auto-select a join condition.You might find yourself having
to delete joins in the QueryBuilder and reform them
using the fields that youneed to use.It's a great idea, if
you know your data,to be explicit about
how things are joined.Of course, you can have all
the different types of joins--LEFT JOINs, RIGHT JOINs, INNER,
and FULL, OUTER as well asa number of natural
JOINs, as well.And just like we saw
with the Copy Files task,where I demonstrated using
a task template to savethe work we've done so
that we can reuse it later,you can use Task Templates
in the query settings.So you can create a query
and create a query template.So a lot of us use
some complex queriesthat can take a
long time to build,and it's a really
nice feature to beable to reuse that query
later in another project.That's what the query
template allows you to do.You can create a template out of
a query you've built. Save it.And then in another project,
you can bring it up againand reconnect it with
either the same data sourcesor different data sources
that have the same attributes,and get a great leg up on
reusing the query that you'vebuilt in the past.In this tutorial, you've seen
me do a fair amount of SASprogramming.I use Enterprise
Guide for almost allof my SAS programming.It's one of my
favorite environmentsfor getting work
done with SAS code.Why?Because I think that
Enterprise Guide offersjust one of the
best SAS programmingexperiences that
there is out there.Yes, there are great
coding environmentsfor general-purpose
coding, but therearen't any other environments,
other than SAS EnterpriseGuide and perhaps
SAS Studio, whichreally understand the
context of programmingin the SAS environment.The SAS language now is
baked into the Program Editorwithin Enterprise Guide.That means the language
elements are there.The keywords are
colored appropriately.There's just so
much of programmingthat's easier when you have
an environment like this.Also, Enterprise Guide's
aware of the environmentthat you're working in.So it knows which libraries
are available, which data sets,and even the variables
within those data sets.It can give you a list of
FORMATs, and INFORMATs, and SASfunctions.So there's so much
available to youwithin this environment to just
make the job of coding so mucheasier.Next, I'll share
a few tips for howto get the most of your
programming environmentin Enterprise Guide.Number one, turn
on line numbers.If they're not on
already, make surethat you go to your
program, Editor Settings,and turn on line numbers.They should be on already if
you're using 8.1 or later,but if you're using an earlier
version of Enterprise Guide,they might not be on by default.Next, think about what
you want to do about tabs.It may not seem like a big
deal, but tabs can make or breaka team.You really need to
agree on whether you'reusing tabs for indentation
or spaces for indentation.There are options within
Enterprise Guide that you canset to control this behavior.You can set the tab size.You can insert spaces
for tabs so you can stilluse the Tab key, but it's
actual space charactersthat are added into your code.And you can even replace
tabs when you open new filesfrom other places.If there are snippets of
code that you use a lot,great idea to define
those snippets asabbreviations within the editor.Then you can type
little shortcut piecesthat will autocomplete
to the full-on codethat you need to
use all the time.I do this for
LIBNAMEs every day.I have some SAS libraries
that I access all of the time,and I have abbreviations
set up thatmake it easy for me to just
define those in any program.This is a cool thing that's
new in Enterprise Guide 8.1and later.It's that you can actually
not just see the variableswithin your data
set, but you candrag those right into in
into the Program Editorby selecting them from your
Server tree, your Library list,and drag them right in, and then
add them right to your code.Pretty cool.We all have our favorite
ways to format SAS code,in terms of our
indentation, where we breaklines, all that kind of stuff.But Enterprise Guide
can format code for you.It may not be able to conform
to all of your preferences,but it's an easy way to
take code that is otherwisemessy and difficult to read.Highlight it or just
select the whole file,right-click, and
say, Format Code.Or use the Control-Shift-B
keyboard shortcut in 8.1or later, or Control-I
in earlier versionsof Enterprise Guide, that will
format the code automatically.Don't forget that you can zoom
in and out of your Programwindow.Really handy when your
program is really long.You can zoom way out and get a
great idea of your program flowand how the structure is.This is a little-known
trick, but itcan come in handy sometimes.You can use the Alt
key, and the mouse key,and Shift to highlight columns
of code or columns of textso that you can then
select these blocks of textas a column and copy and paste
them into someplace else.Pretty neat thing to be
able to do sometimes.Of course, every editor
has find and replace.But don't forget that
Enterprise Guide,like other powerful
editors, also allowfor regular expressions to
match on things that you'relooking for and also replace
things that you're looking for.So using regular expressions
and learning a little bitabout how regular expressions
can help with pattern matchingto do more complex find
and replace operationscan really save
you a lot of timewhen you're making big changes.Another hidden feature
in Enterprise Guideis the ability to split
your programming window.You can right-click
in the windowand select Split,
and split the view,and it will break your
program into multiple viewsof the same code.And then you can scroll
those views independently.So even though you're looking
at just one program file,you're looking at
different parts of it.It can be really
handy when you've got,say, a DATA step
with some columnsdefined in one part
of the program,and then you need to, later
on in the code, referencethat data set and its columns.You can kind of pin the DATA
step definition to the topand then work in your code
down below in a different view.And if the Program Editor
isn't good enough for you,or there's some
other thing you needto do that the Program
Editor doesn't support,you can always right-click
and open the code outinto your favorite
default editor.I sometimes use Notepad++ or
VSCode to do other things.SoYou can, from within
Enterprise Guide,open up your code in one
of those other editors.Make changes there.Enterprise Guide
will detect whenyou've made changes in another
editor and you've saved them,and it will update your
view within Enterprise Guideto reflect the latest
changes you've made.And don't forget the
DATA Step Debuggerbuilt into Enterprise Guide
since version 7.1.2, I think.The DATA Step Debugger
is a great wayto find out what's going
on inside your DATA step.It's only good for
DATA steps, but it'san interactive
debugging environmentthat allows you to
really figure out what'shappening when you get stuck.It's also a great teaching
tool and learning toolto know how the DATA step works.Like any programming
environment worth its salt,SAS Enterprise Guide
also integrates with Git.Git is a version-control
system that basicallyhas taken the world by storm.You may know it from
working with GitHub,but there are other
commercial systems,like Bitbucket and
GitLab, that alsointegrate with this protocol.Git is an open-source
version-control systemthat is embedded into
many, many tools,and Enterprise Guide
is one of those.And it's just one
of several SAS toolsthat have Git
integration inside of it.It's a distributed
version-control system.It's unique from other
version-control systemsin that every developer gets
their own clone of the coderepository.It supports a variety
of different workflows.And as I mentioned,
it is open-source.Git aficionados often
work exclusivelyon the command line with
Git, but Enterprise Guideoffers a nice user interface--
a standard user interfaceflow for how you would clone
repositories, when you makechanges, commit those
changes, push backto the original repository, view
history, all of that in a waythat you might expect,
especially if you'veworked with other tools.Git has its own
vocabulary, as well.It has quite a lot to learn.I've talked about it
extensively in other forums,so while I could spend an
hour talking about it here,we don't really have that time.But if you want to
go deeper on this,I encourage you to search
on communities.sas.comfor Git integration.And you'll find links
to other webinarsthat we've done that
describe the Git integrationspecifically with SAS.Just a real quick overview--Enterprise Guide supports
Git in a couple of ways.One is without actually using
any Git infrastructure at allthat you need to set up.It can support it
just completelyinside the project file.So if you develop SAS programs
and embed them in your projectfile, you can take advantage
of Git-like functionalityby having a program history.You can commit your changes into
this local, project-specificrepository.You can view history.You can revert back
to other versions.So you can get quite a lot
of the Git-like featuresfrom within that without
any additional setup.But if you do have
Git in your workplace,and that may take the form of
Enterprise GitHub, or GitLab,or Bitbucket, or
something else, then youcan take fuller advantage
of the Git integrationwithin Enterprise Guide
and SAS tools thatallow you to clone repositories
of code to your local machine,work with them within
Enterprise Guide,commit locally, push
back those changesto the original repository,
do all the branchingand fetching that you would
expect to be able to doin a full-fledged Git system.So I encourage you, if
you're working on a team,especially, to learn
more about this topicbecause using source
management with SAS codereally brings a level
of discipline and rigorto your projects that is
basically table stakes nowfor anybody who's
developing code.By now, you've realized that SAS
Enterprise Guide is primarilyan interactive tool that
you use point and click.But that doesn't mean you
can't automate things.In fact, Enterprise Guide has
a pretty rich automation modelthat you can use to
actually script operationsand make Enterprise Guide
do things unattended.In fact, this is exactly
how Enterprise Guide workswhen you schedule a project.When you use the interface
to schedule a projector schedule a process
flow, Enterprise Guidegenerates a script file
for you and adds itto the Windows Scheduler
to automate this activity,maybe when you're not even
at your desk or logged in.So we can use that same
mechanism to script EnterpriseGuide, but writing
our own scriptsto do whatever we
want within the tool.It's not just for scheduling.Now, most often, VB Script
is the language that's used,but it doesn't have to be that.You can use PowerShell.You could use Python.Any tool that can automate
COM-compliant tools--these are Windows applications--can be used to automate
Enterprise Guide.Here's a very simple script,
just a very basic kindof Are You There? script for
automating Enterprise Guide.It declares a
couple of variables,an Application object
and a egVersion object.And then it initiates the
Application object withWScript.CreateObject()
in VB Script.And this just
generates an instanceof the Enterprise Guide
8.1, and then thatgoes out to the console--the name of the application
and its version.And the result?Unexciting, but effective.You get to see that Enterprise
Guide actually started up,and it can report
back its version.And this is a great sort
of first step scriptto use just to make sure that
everything is working in termsof your scripting mechanism.And then you can move on to
do more complicated things,and we'll take a look at
that in just a second.Let's take a look
at some examplesof automating SAS
Enterprise Guide in scripts.All my examples here are
going to be using VB Script.It's pretty easy.It's built into the
Windows operating system.But as I've said before, you
could use PowerShell or Pythonif those are the languages
you're familiar with.Let's take a look
at the first one.Basically, I'm going to show you
a demo of the one I showed youjust a minute ago in slides.This is just creating a
new Application object.It's a great script
to run when youwant to just make sure that all
of your mechanisms are working.So I've got here my script
in Visual Studio Code--VSCode-- and then I've got
a terminal open underneath.It's just a normal command
terminal for Windows.And so I'm going to go ahead
and just call cscript.exe.And this is in my Windows
directory under SysWOW64because, confusingly
enough, that'swhere the 32-bit
version of cscript is,and the version of SAS
Enterprise Guide that I'mautomating is a 32-bit version.So this is important
to remember.The version of
cscript or wscriptthat you're using
to automate hasto match in bitness to the
version of Enterprise Guidethat you're using.So if you're using 64-bit
Enterprise Guide, that's fine.Just make sure you
use the 64-bit versionof the scripting runtime that
you're using to automate.So I'm just going to
automate this NewApp.vbs.So I'm going to run that.And you see it comes
back, lickety-split,Enterprise Guide, Version 8.2.That is the version
that I am running.Great.So let's clear that.Let's move onto the next script.It's a little more complex.I'm going to use this
script to actually createa new project in
Enterprise Guide,add a program, just by
adding in a new code object,and then run that program.So we do that by adding to the
CodeCollection in the project.Setting the server--
in this case,I'm just going to
use a local server.First, I'm setting the profile.The profile is the metadata
profile that I'm connected to.And there's a special value
here for the metadata profilecalled Null Provider,
which means,don't connect to a
profile and I'm just usingmy local SAS for this work.The program I'm running is this
is just a simple PROC OPTIONS,and then after the
program runs, I'mgoing to save it to my
local current directoryin this file called
testProgram.log.And then this
little function hereis just a helper to get the
current directory for wheremy script is running.So let's run this one.And when that comes back,
you see over on the left,in my list of files here, a
testProgram.log was created.And let's just
throw that up here,and you can see
the contents of it.It looks like a SAS
log, so perfect.It did its job.Let's move on to
the next example.In this one, I'm
going to just usethe automation object to show
the list of available profiles.So we mentioned the special
Null Provider value, which says,don't use a profile.But you might have
one or more profilesthat you use in your enterprise
to connect to different SASenvironments.So let's see what
I've got set up.This automation
step will show mejust a list of
available profilesand some of their details.And that runs pretty quick.And you can see here, here's
the metadata profiles I have.And each one has a set of
details, which in the video,I'm going to blur out because
it contains some secrets so Idon't want to reveal
some of those addresses.OK.Clear this.And then finally, let's
move on to just runninga project in batch.So in this example, I'm
going to, once again,launch Enterprise
Guide automation,setting the active profile to
one of my other SAS profilesthat connects to
a SAS App server.And then I'm going to add a
new program to my new project.And then I'm going to
run this program here.Now, I've just coded
the program herein text right inside my script.In reality, you might have
the program sitting on disksomewhere, and you could use VB
Script to just read in the fileand assign it to the
sasProgram value here.In this case, I am just
creating a simple DATA stepwith a bit of a subset, and
then running PROC MEANS on that,and then I'm going to
save out the log output.And then if this project
creates output data sets, whichI think it will, then I'm going
to use this Enterprise GuideOutputDatasets object to save
that data set as an Excel file.So I'm basically automating
the export to Excelwithin my script.The program in SAS is
creating a data set,but Enterprise Guide is going
to save it as an Excel file.And then I'm going
to also save itthe ODS output,
which in this caseis going to be a listing output.So let's run that.And as you see, these
results come back.We have a log again.Just throw that up
in here and see.That looks pretty good.We have a listing
file this time,so let's throw this in here.And it looks like it's the
output of our PROC MEANSand listing format, so perfect.And we have an Excel file.So let's just double-click
and open that in Excel.And there is my data that was
created up there on the server.So all of that automated.It can be run unattended.Perfect.And that's it for this
tutorial, but there is much morethat you can learn.Here are some resources you
can use to learn more and askquestions, if needed.Of course, please visit
communities.sas.com.That is a great place to ask
questions of your fellow SASusers.And there is an active
Enterprise Guide board there,lots and lots of users who are
willing and eager to answeryour questions.Also, I'm there, too, so I am
happy to jump in and answerquestions.I have published
a lot of articlesalready about what I know about
SAS Enterprise Guide at my blogat blog.sas.com.You can just search for
SAS Enterprise Guide there,and you'll find lots of articles
to continue your learning.We have webinars.They can be found in the
Ask the Experts sectionin the communities, as well.Just look for Enterprise
Guide in there.This channel, of course--youtube.com/sasusers-- there's
lots of Enterprise Guide videossome of which I've recorded.So please check those out.You can find me on
Twitter @cjdinger,send me email, contact
me on LinkedIn,or message me on communities.Please, I want to
be able to help you.I want to hear from you.Leave me comments and continue
your learning, and happyEnterprise Guide-ing."
46,"Hello, and welcome
to this super demoon future-proofing
customer experience.A little about me before we
get into the presentation--I'm Joan McGowan.And I'm a global banking
industry principal at SAS.I've worked in the industry
for close to 80 yearsin various roles.And I've had the
privilege of beingable to study our
industry closer,both from the
perspective of the bankand also its
surrounding ecosystem.And that includes
technology providers.My focus has been on the
impact of new techniquesand technologies.I advise on the intersection
of technology and business.And I've published,
authored, over 30 reports.Plus I've written numerous
articles on this subject,again looking at the impact
of different technologieson various banking domains.I guess my overall
takeaway, the key takeaway,is that it's not generally
the fault of the technology,assuming it's built right, but
rather the lack of preparationand governance
done by the banks.We continue to stumble on the
operationalizing of enablingtechnologies such as AI.But I do see a sea change.And we now have the
impetus and lessonslearned to get there
in a straighter path.In today's session I
will show a short demoon how SAS uses
advanced analyticsand artificial intelligence to
create a future-proof customerexperience strategy
that automates customerengagement in real time,
influences, nudges,and speeds decision.This will bolster brand loyalty.The customer is the center of
your universe, my universe,or at least they should be.And you are rightfully
obsessed with them,getting them, growing the
relationship with them,keeping them, and when they
leave, maybe getting them back.And the decisions you
make around each customerat every stage of
that journey with youdictate the quality
of their experience.And ultimately
your bank's successat providing a continuing
excellent experiencewill be one of your major
sources of differentiation.Everything else you
do can be copied.Empathy influences decisions.Consistent, positive
customer experienceis achieved by being empathetic
at every stage of the customerjourney.And that means fully
understanding the customer'smotives and desires.And good decisions,
right decisions,maximize the quality of the
experience for your customers.Decisions drive experience.And experience drive
the value whichthey get from the
relationship, whichin turn dictates the value
they create for your bank.McKinsey rightfully
notes in this quotethat delivering excellence
in customer experiencecan have a significant positive
impact on your businessperformance with gains
of 5% to 10% in revenuesand reduce costs up to 25%
within two to three years.But remember, the
reverse is also true.Poor experience is a killer.It's a journey.We're not just doing this once.We need to be able
to run the cycleat every step of every
customer journey,steering the customer
engagement to meet the customergoals in the light of a
changing view of what they need.And we need to do--we need to be able to do
this for potentially millionsof customers at a time.The short demo that
you're about to seeshows how our data
analytic-based solutionmakes it easy to collect
information, link and updateindividual behaviors
to an identity, i.e.,to the individual.The information is then analyzed
to determine next best actions.And it can be done in
both batch or real time.And this provides a one-to-one
personalized customerexperience that will
nurture a good decision.And again, decisions
drive experience.Experience drives value.And note the
continuity of accents.SPEAKER: By adding a
single line of scriptto a website or a standard
SDK to a mobile app,we start to collect
rich, granular data thatprovides first party
context and identifiesbehaviors of both known
and unknown customers.By simply refreshing
the page, westart to collect that
information in this windowon the left-hand side.This window is purely there to
just visualize the clickstreamdata we're collecting.But we get information such as
the page title, the referrer,information such as the screen
size, the browser and devicethey're using, as well as all
first party cookies includingtheir Google Analytics and
their Facebook identifiers.Within our system,
we have somethingcalled Identity
Management, as we can see,if I now log in, my
identity should nowchange allowing
me as a marketeerto utilize all the individual
behaviors that I'veshown within this session
across all my other sessions,maybe yesterday, last
week, last month, notonly within the website, but
across our mobile app too,being able to truly
understand that identityacross multiple devices.As we can see here,
just to prove that,I started with identity.And as soon as I've logged
in with my PII data,my identity has changed.And the other one has actually
been retired, thereforebeing able to
provide that value.Furthermore, we can
start to utilizeanalytics for the first time.All that data we've collected
and allowed and providedas a data asset on premise
can be used by data scientistsor analysts to create next best
actions, which can subsequentlybe loaded into the system, in
this case, car insurance whereI can see that I have
personalized the home banner,and the secondary one, which is
savings, which I can see hereon the bottom right-hand side.Next best actions
is a true use casethat many customers
want to achieveacross many
different industries.But what we've shown there
is batch next best actions.And that's something that
other customers and vendorscan replicate.But how about we take
that into real time,taking that data that
we're collecting,streaming that live into
an operational system,and being able to make
true decisions on that,setting us apart from
this customer intelligencecompetition, allowing us
to open the conversationabout operationalizing
analytics and providingone-to-one personalized
customer experiences,what customers expect and want.Our competition simply
cannot stream that data.They cannot do
real-time decisioning.And they can't actually
run a real-time model.This gives us an opportunity
to not only replace solutionsbut also to actually
enhance solutions,especially with
Salesforce customers,and in some cases,
Adobe as well.To show this live, we're
going to go to the Loans page.We are now showing VA on Viya
where data from that websiteis being streamed directly
into an operational system.We can see there are 13 distinct
users and many events beingcreated.We can also see not only
website traffic, but alsomobile, as you can see
that coming in here.And if I actually
go across, we cansee that it's all being
done here by an individual.So let's bring
back up the websiteand actually start
a loan application.Again hopefully you'll
see that data comingin in terms of a page view.I'm going to go to
this Calculate Loan.Now, what we should see here
is information that I put in.So I'm just going to
put I'm a homeowner.I've got dependents of two.I've got a loan amount,
a loan purpose, a term,monthly income, and
monthly expenses.If I click Apply,
that data first of allshould come into Viya.And we have it there--2 independents, 1,000.Now what we've also
been doing, not onlyare we actually
collecting that data,but we're also doing
a real-time decision.And in this case, I've been
told that unfortunately Ido not get the loan.This just shows the
capability of beingable to do true
operational analyticsand to actually
change the customerjourney and the
experience across notonly marketing, but
all different channelsand use cases.Highly targeted touch points
are even more valuablewhen part of a customer
journey, being orchestratedacross multiple channels, such
as web, email, a mobile app.We also supplement this
with other capabilitiesthat our competitors
have as well,such as A/B testing
or multivariatetesting across web and mobile.But what sets us
apart is that abilityto call out to a decisioning
and modeling environmentto make sure that they
are not only testing,but they are optimized to be
the most relevant and consistentacross all the different
touchpoints of data we have.JOAN MCGOWAN: So SAS provides a
comprehensive, unified customerengagement and
decisioning platform,which allows you to manage
all aspects of the customerengagement process.And the solution solves for all
of the common challenges whichcurrently prevent banks from
delivering excellent customerservice.And finally, because we
have developed our platformrather than bought an integrated
set of disparate applications,you don't need to worry about
integration and everythingworking together.I want to thank
you for listening.And if you have any questions
on this presentation,the demo, or the
technology deployed,please do reach out to
myself, Joan McGowan,or my colleague, Kate Parker.Thank you."
47,"KEVIN LEE: Hello, everyone.Welcome to Machine Learning
Pre-conference seminarat SAS Global 2020.My name is Kevin Lee
and I'll be teachingtoday's Machine Learning
courses for SAS programmers.We were going to start with
intellectual and earningsAnd then, we're going to
discuss about this main concept,such as hypothesis
function, costfunctions, gradient
descent, and learning rate.And then, we will talk about
the process and workflowof the machine learning.And then, we will
also discuss the typeof machine learning, such as
supervised machine learningand also unsupervised
machine learning.And then, we're going to
discuss the algorithmsso for the commission
can use neighbordysentery random
forest, XGBoost,logisitic regressions,
linear regressions, K-meansclusterings.And then, we also explain
about how to validate or verifythe trained model.And then, we would
talk about howto make voting in better
with hyperparameter tunings.And then, we would discuss
about artificial networks, whichis a game changer
of machine learningand its main concepts such
as activation functions, lossfunctions and also optimizers.And then, we will talk about
the deep neural network, whichis actually make machine
learning much morepowerful and efficient.And then we're going
to talk about howto improve the performance
of deep neural networkswith bias and variance
and also regularizations.And then, we also talk about the
practical deep neural networkimplementation, such as
convolutional neural networksand recurrent neural networks
and transfer learnings.And then finally,
we're going to talkabout how motoring AI
is impacting our livesand how it's going to be
impacting our futures at homeand also at our careers.So let me introduce
machine learning.About two years ago,
my wife asked me honey,do you know about
machine learning?Do you remember the
case or situationthat somebody ask you,
but expect you to know?And that's the reason
that my wife ask.She knows that I'm doing
programming, statistics,and most importantly working
with data all the time.If you look at
these three things,it is kind of like
our job descriptions.We use SAS to all
those programs.And then, we use statistic to
predict and build the models.And then most importantly,
we work with the dataall the time.I'm sure that some
of you alreadygot asked that whether you
know about the machine learningor not.So let's talk about what
is a machine learning.The meaning of machine
learning is applicationsof artificial intelligence that
provide the system or machineto automatically
learn and improvefrom experiences without
being explicitly programmed.The key here is that improve
from experiences without beingexplicitly programmed.So what is a difference between
explicit programs and machinelearnings?Explicit program
pretty much providerule based programmings.For example, when you like
to catergorize age groups,we define what age groups is
going to apply to what age.As you see this example,
if age is less than 65,we put that age or patient
into age group what?If it's between 65 to 69, we
put that into age group 2.And so on.In the machine learning,
you don't apply rule.We just reading data
and then algorithmto decide what age going
into certain age groups.So there is no rule
based programmingin machine learning.So how does machine learn?Let me ask you questions.Can you tell me
about this picture?Yes, it is a cat.Then, how do we know
that it is a cat?We actually seen this before.Humans learn from
our experiences.We learned by
seeing it, taste it,here it, feel it, and so on.What about machine?Can machines see this one?Can machine fee this?Or can machine taste it?Or hear it?Machine actually learn from
the data and an algorithm.Just like we see cat
pictures for thousands timeand we figure out what is a cat.So when we actually train
machine, we look at imagesand then algorithms.Eventually, when
machines review data,they could tell when
these are cats or not.So key here thing
is that machine willlearn from data and algorithms.It is their experiences,
just like humanlearn from our experiences.In the algorithm, there are
actually three key functions.Hypothesis functions,
such as y equal x plus b.It also minimize functions.Cost function to minimize
and also gradientdescent, which is optimizers.So we actually
put the input datawith the output, such as x0, x1,
x2, xn and y, which is answer.And we actually train our
algorithms with this dataand the machine can learn
from their experiences.So what is the main concept?We're going to talk about
three concepts which isvital about machine learning.If you understand about how
this three concept of functionwork together, then
you actually understandhow a machine learns.We're going into this hypothesis
functions, cost functions,and gradient descent.Hypothesis function is
measuring arguments and modelthat we will train
with the data.There are a couple types which
we're going to discuss laterin our courses.Linear regressions,
logistic regression,support vector
machine, decision tree,and artificial neural networks.And then, it's main
function and formulais theta 0 x0 theta 1 times
x1 theta 2 times x2 and so on.This is one of the example
of hypothesis functions.And then we try to find
about the parametersof these hypothesis function.That is the ultimate goal
of the machine learningalgorithm and trainings.Cost function, otherwise
a loss function,is the measurement of how well
our hypothesis function fitinto the data.So basically, there
is a differencebetween actual data point and
hypothesized or predicting datapoint.For example, for a
linear model, square Imean error is the
cost functions.Gradient descent is an
otherwise optimizer.It is engine that
minimize cost functions.By doing this, we are able
to find the proper parametersor optimize parameters.So let's talk about how
the machine learning gettrained and also workflow.This probably going to give
you about a high level of howa machine is going to work.Let's imagine that we are
teaching young childrenabout the cat pictures which
they never seen before.First step we're going to
do is show the full setof the pictures
which may containcat and dog and other pictures.And then, tell the
kid that some arethe cat pictures and some
not cat pictures, maybe dogs.And then, you could explain
why they are pictures,they are cat pictures, and
why they're not cat pictures.And then we again, show the
next set of the pictures.We actually redo this process.Showing the pictures and also
explain why some are pictures,some are cat pictures,
some are not cat pictures.We repeat this about
couple of timesuntil children understand which
are cat pictures in general.So some kid probably
figure it outabout maybe 100 pictures and
some kid probably 120, 150.It depends on kids.But eventually, if we showing
the pictures thousandsof pictures, then kids will
eventually figure it out.Machine learning is not so much
different about teaching kidsabout the cat pictures.Our first step is
about the machinelearning training process is
we select data and hypothesisfunctions.And then, start with
the random parametersof hypothesis
functions such as data.And then, process first set of
data in hypothesis functions,such as we are showing
first set of cat pictures.And then, we find
the predictive valuefrom hypothesis functions.Which can mean that a kid
will tell us which are catand which are not cat.In machine learning
process, we aregoing to calculate
the cost functionpretty much by actual--the difference between
actual and predictive values.And then, we minimize
cost functionusing gradient descent.What that mean is that we
actually tell the kid whichcat, which are not cat.The next function,
which learningis update the parameters
of hypothesis functionbased on cost
function minimization.You can see more
detailed informationwhen we show by step by
step with some example.I will repeat step 3 to
7 with a new set of datauntil cost function becomes
0 or at the lowest point.Let's look at the example.First step of
machine learning isselect the data and
hypothesis function.Let's imagine that we have
this data point with a x and y.You can see that this x point.You got y point.On X point, you can
add the y point.And then we select the
hypothesis functionas a linear regression.Y theta times x.The next step we start with the
random parameters of hypothesisfunction.Remember we have a theta x.So now, let's imagine
that's kind of setthat we're going to set
the theta equal for.and then let's process first set
of data in hypothesis functionswith the initial parameters,
and predicted valuefrom hypothesis function.We actually set our
initial parameter as 4.So if you plot this number,
this first set of x data--1, 2, 3-- and then our
y hypothesis functionbecomes 4, 8, 12.And then next step
a machine doesis you calculate
cost function, whichmeans pretty much the
difference between actualand predicted value.3D model cost function
is square mean areas.We're going to talk
more detail later on,but that's a cost
function that we'regoing to use for linear model.So the difference between the
cost function of actual valueand predicted value
becomes 9.33 if you plotall these numbers together.2 minus 4, 8 minus
4, and 12 minus 6.If you actually
plot this number,it goes something like this.Right there.So cost function
versus parameters.If you plot this cost function
versus in different parameter,that is a shape
you're going to see.When the hyperparameter.And then parameters equal
4, then the cost functionis going to 9.33.When it's going to
be 3, it's at 2.33.And then when hyperparameter
is going to be 2,then it's going to be 0.So our goal is to find--to this 2 using the data,
and then [INAUDIBLE]..So here comes the
gradient descent.The definition of
gradient descentis engine that minimize
cost functions.So it is actually the process
of getting to the lowest errorvalue, which is the cost
functions in the machinelearning algorithms.It is kind of walking
down the mountainto find a goal
located in the valley.So gradient descent actually
helps the cost functionbecome less and less up to the
optimal cost function point.So what machine
learning does is ittries to minimize cost function
using gradient descent.It is more about
[INAUDIBLE] right now,but it takes the derivative
of that cost functionsince it will give a direction
to the moving forward.And then it will step
down so that it's goingto be minimized cost functions.And then learning rate is
another concept about the costfunction, and it
will actually showhow much it will change
based on the gradient descentand also cost function.So as you see, the point
that if the parameter ison the right side of
the optimal point,it's going to be negative.And then if it's
on the left side,then it's going to be positive.So that where your
parameter is going to be y,it's going to go into the lowest
point of the cost functions.So it will help that cost
function become lower and lowerbased on the parameters.And learning rate determines
how fast it will learn.That's why it's
called learning rate.For example, let's say
the parameter is just 4,but the learning rate is 0.1.Then it will decrease to 3.8.However, if it's 0.5
as the learning rate,then it's going to
decrease 1 point,so it's going to go down to 3.So it will determine how fast
the machine will learn basedon the parameters perspectives.So the next step is
the machine learningwill update the parameter
of the hypothesis functionsbased on the cost function
using gradient descent.So initial hypothesis function,
you studying-- so 4x equals--theta is set to 4 initially.And then cost function
become-- because there'sa difference between actual
point, which is 1, 2, 4, 6.So it calculates cost
function-- it becomes 9.33.And then using gradient
descent, our parameterwhich is theta gonna be
changed from 4 to 3.8.And then our modified
hypothesis functionusing the first set
of data becomes 3.8.And then next step is going
to be 3.6, and 3.4, and so on.So we repeat 3 to 7--training with a new set of data.Calculate all the
cost functions.At the second iteration,
the cost will become 7.56,and the gradient descent
is going to be 3.8 to 3.6.And then for after a second
iteration of the data,the train data that we get
on the hypothesis function is3.6x.So these iterations
from step 3 to 7will go on until the
cost function gets to 0,like 0 data equals 0,
or if there is no longera decrease, or no more data.So basically, the cost
function and gradientdescent to find the best model
by going through iterations.From starting this point, next
point, next point, next point,next point, next points.And then our hypothesis
function changes each time--each iteration until it hits
to the 2x just like this data.So in the grand perspective,
the model training machinelearning starting the
hypothesis functions,going through a new set
of iteration of data,and then calculate
the cost functions,and apply gradient descent,
and update the parameterof hypothesis functions.It goes on until
the cost functionsget to 0, or the lowest
point, or the specifiedset of iteration.You actually set this iteration
about a certain time--maybe 100 iterations--or your cost function becomes
0 or an optimal point.So that you can actually get
the best model that can providethe best predicted value.So in this best model,
then if your x this point,and it also provides the y
values using the best models.What's interesting is
that with the same modeland same hypothesis
functions, using more dataprovides a better model
and better predictions.However, if you
get different dataand it provides
different model, whichis different slopes
or data, and itprovides different predictions.So one good thing
about machine learningis that when your data
changes, your model alsochange automatically.Because sometimes, market
trends or some datayou're going to get is
changed, that you don'tneed to program all over again.Using the current model, just
train with different data.Then it will work just like
machine learning by itself.However, if you get bad data,
it will provide a bad model,and it will give
bad predictions.So data is most important
thing in machinelearning-- one of the most
important things in machinelearning.We call that garbage
in and garbage out.So there could be bad example.We're not sure if
it's a cat or humans.So now let's talk about
machine learning and alsothese algorithms.Let's say we do have
different problems.Construction, airlines,
electrical engineering,hospital, or farms.Different job usually
means different professionsand different problems.So for each problem that we
have, such as construction,flying airplanes, fixing
electronics, carryingthe patients, raising
farms, we needto have a different
professional, whichmeans find the right person
for some specific jobsand problems.Machine learning
is very similar.We have to find the
right algorithmsfor some specific problems.So our algorithms based
on what kind of problemthat we're going to have,
and also what kind of datawe're going to have.And you find the
right algorithms,and then we actually
change those algorithms.There are two types of
the machine learning data.One is supervised
machine learning.What we mean is that
it has input data,has a label, which means
it has a correct answer.In this case, it's
the y result. It'susually for some
specific purpose.And then there are
mainly two types.The classifications
for the distinct outputvalues, and regressions for
the continuous output value.Other type is unsupervised
machine learning,which means input
data are not labeled,which means there are
no correct answers.It is popular for
exploratory analysis.And then it's main type is
that clustering analysis ,using k-means clustering.In supervised machine learning,
there is a classification.Classification is pretty much
about the categorical target,either binary or
multiple classifications.If at that point you said is
the output either a yes or no,or 0 to 9, or sometime
mild, moderate, and severe.And it's main algorithm
is logistic regression,support vector machine, k
nearest neighbors, Naive Bayes,decision tree, random forest.XGBoost is kind of popular
algorithm in machinelearning classification type.So let's talk about the
classification algorithms.Support vector machine is
one of the most popularclassification models, both
binary and multiplicationclassifications.And especially for the complex,
but small and mid-sized dataset.It works best for the
binary classification, whichmeans yes or no, or
class 1 and class 2as you see in this graph.And it is actually
easy to train.And then what it tries to find
in machine learning is linesor hyper-plane that divides
two or three classifications--2 classes.This hyperparameter
is a corner--either linear or
polynomials, and also gammaand the margin, which is
how far is it different.And usually, it tries to
find about the function thatdetermines the separation
between two or three classes.And just see this graph--in the two dimensions, it's
going to be a linear lineor something polynomial.If it's three dimensions
which is three data points,then it could be
three-dimensional or plane.So once the model is trained,
if data point is find on this--let's say was something the
machine finds and then trains--so it actually finds out about
this hyper-plane functions.If theta point is going to
be fine on this area, whichis left side of
this hyper-plane,then it belongs to this class.If it's right side,
then it belongsto this class-- maybe
class 2 and class 1.So something about the
support vector machineis they really work
well with a clear marginof separations, and then also
very high dimensional species.High dimensional
space means that thereis a lot of data points--x1 to maybe x100,
or things like that.And this can mean also
about support vector machineis that it works well even
though number of dimensionsis greater than number of the
sample, which is not alwaysthe case in most other
machine learning algorithms.It's weakness is
that it actuallyrequires a lot of
computational power,so it doesn't work
well with large data.So big data applications,
SVM is not the ideal machinelearning algorithm.And then it doesn't
perform well whendata has a lot of noise,
which means unclean data.It doesn't work well
with unclean data.Actually, SAS also
has machine learningpackage, such as SAS Visual,
data mining, and machinelearning.It provides data learning
pre-processing, and supervisedand also supervised
machine learning,and also post-processing.It's kind of easy to use,
and click, and click machinelearning algorithms interface.Such as there is a
data mining process--either anomaly detections or
clustering, feature extractionsand filtering, imputations.So within programming,
you could kind ofprovide how to prepare
some of the input dataso that you could apply it
to the machine learning.And then machine learning
has Bayesian networks,and decision tree, the
forest, gradient boosting,linear regressions,
logistic regressions,artificial neural network,
and support vector machine.So here's a couple of the
machine learning algorithmsthat we can actually
apply into our model.So here's the support
vector machine.And then once you collect
this-- once you actuallyprepare your data, and then you
can click the support vectormachine, and then it
will open little box,and the data coming in.And you can set up some
of the hyper-parameter,such as we talk
about the kernelsWe can make linears, or
polynomials, and so on.And then once you've got any--there's this run button
that it will run this model,and it provides the
result of this model.K nearest neighbor is one of
actually very useful machinelearning algorithms.It is very simple and
easy to implement.Then you could also apply to the
classification and regressionproblems, which
means that you couldapply to distinct output,
and also continuous output.We actually call this
the lazy algorithm.I think it's kind of funny
that we call this lazy.But what happens is
that we don't reallytrain knn machine learning,
because knn remembersthe trained data point, and then
it will calculate and memorizethat data point, and it
will apply the data pointwith our test data.Let me give you an example.In this example, let's
say you do have about--we have a training data.And then we actually
run this training datainto knn algorithms, and then
knn remembers each data point.So when new test data comes
in, such as this area,then it finds the
nearest neighbors.We setup k as 3, so we find
about 3 nearest neighbors.So one from this side,
2 from this side.So I just see that for this
data point, here's a one--group one.And here's a 2--group 2-- group b.So based on the result that we
got from the nearest neighbor,this test data is
going to be class b.However, what's
interesting is that wehave a situation
that when k equals 3,provide more class b, but
larger k, which is k equals 7,provides more class a.So it is very important for knn
to determine what is optimal.Test optimal k so that you could
provide the best predictions.The benefit of knn is it could
train really, really fast.It is very simple and easy to
implement, as you see this.And that it is very useful
for a lot of linear data.However, here's the weakness.It could be slower and
costly for the timeand memory in predictions.And then it might be
ideal for large data,because it has to memorize
all the training data points,and then it has to calculate
in the predictions.Usually, all other algorithms
do calculations in the trainingdata, so test data doesn't
need a lot of calculation.But knn requires a lot of
calculations in the predictionphase.So it doesn't work
well if you needto have a quick and fast
prediction or time requirement.Decision tree is one of the
most popular machine learningalgorithms, because it provides
visual representations,and is easy to explain
to other people.So it could be applied for both
classifications and regression,even though it is much more
popular in classifications.If we can little bit discuss
about its architectures.And here's a root node.And then internal node, and
then branches such as herewarm and cold, and yes or no.And also a splitting
based on is conditionsand then leaf node
is such as decisions,which is non-mammal, mammal.That is a leaf node which
is making decisions.Before we actually provide
that the one decision tree,we could assess some
of the hyper-parameterssuch as what is the
maximum number of features,such as variable,
when you use, and alsothe depth of the branches.How many branches?Here in this case, 3, but
you could go to 4, 5, and 6.And also criteria-- the measure
of quality of the split,which is gini or entropy.The algorithm that
decision tree usesis pretty much gini
and also entropy.Gini is the measurement
of the impurity,and entropy is the
measurement of disorder.So the whole point is
that you try to minimizegini impurity and also entropy.So it kinda use cases.So for example, let's say that
a problem we'd like to solveis what is the conditions
of the patient who survive?And then we actually
input a feature--it's going to be sex, age, and
whether they're treated or not.And also the label is
whether patients die or not.So in this example--it's just hypothetical example,
but if the patient is a male--if the patient is not male,
then everybody is alive.And if it's yes,
and some is die.So you look at the
next feature whichis age, where the age
is greater than 65,and then the patients die.And if it's no, then
patients still alive.But whether they're
treated or not--if you treat it, then
patients are alive.But if not, then patients die.So as you see, it gives
a clear conditional casesabout each feature.How each feature
impacts the output.So decision tree is
very useful to findabout how some of the results
were impacted from the inputvariables.And then also the SAS Visual
data mining and machinelearning interface or package
provides decision tree.You state decision
tree with the data,and then you could actually set
some of the hyper-parameters.And then you could run
this in your programs.And that will actually provide
the result. As you see,it will actually provide
the result. How manybranches it has, and what was
the condition of each branches,and what's the output
of a condition,and how it's going
to impact the result,or the ultimate output
of the data coming in.Random forest is another popular
machine learning algorithm.It is a so-called
ensemble decisiontree, which is basically using
the multiple decision tree.What happens is that--lets say you have
thousands of patients.And then we're actually
splitting the 1,000 patientsinto maybe about 300 patients--about 20 samples.So that those 20 times,
we have 20 decision trees.And then it actually provides
the result about the classa, and class b, and class c.And it will actually take
the majority vote the output.So random forest is more about
the aggregation or averageof a decision tree.And it actually provides
more general or bettergeneralizations of
performance of--and it is less subject
to overfitting.So random forest is a
very popular machinelearning algorithm that is used
for a lot of implementations.SAS Visual data mining and
machine learning packagealso have these options.So you actually click the
random forest in your interface,and then it will provide
the hyperparameters option.You could actually select the
option, and you could run this,and it will provide the
result about random forest.Another popular machine learning
algorithm from the decisiontree is XGBoost.Rather than prior machine
learning of the random forest,it just go into boosting, more
of a sequential trainings.In the random forest,
we notice as the output,we actually take the average
or the most vote of the output.However, in the
XGBoost, you actuallytrain from output again--with another output that we
actually train from the outputagain.So it actually usually
provides better performancecompared to random forest.So it is another advanced
ensemble decision treethat provides better performance
in terms of machine learningperspectives.And then it's actually
extremely fastcompared to parallel
computations,and very efficient.It is very versatile.Actually, you can use to
extract the variable importance,and does not require
feature engineering.And it also has a regularization
to prevent overfittings.However, it only works
with numeric features,and doesn't work
well with outliers.And it is difficult to set
up because it is dependencyof the previous trainings.But you're probably going to
see a lot of XGBoost machinelearning implementations
in recent and more advancedmachine learning algorithms.And also SAS Visual data mining
and machine learning packagealso has this gradient
boosting as one of the options.Now we'll talk about
logistic regressions.Logistic regression is probably
the most popular machinelearning classification, which
is binary output algorithms.Because it can take continuous
and categorical variableas input features
into variables,and then it works really
well in a lot of situations.For example, when you try to
predict if a tumor is malignantor not based on tumor size,
at a certain point of size,it becomes highly likely the
tumor wouldn't be benign.So basically,
logistic regressionis a combination
of two functions--logistic plus regressions.Regressions such
as theta x plus b.And then logistic is pretty
much a sigmoid function.If you look at
these architectures,we get input variable times it's
slope, or parameters, like x1times theta 1, 1x times theta
2, and x3 times theta 3,and so on.Xn times theta 1
summation, plus biases,which is in a
mathmatics we're justgoing to go theta 0 times x0.So we actually adding
all these together.After that, we actually
apply this into sigmoid.So basically, apply the value
to that e to the minus z.And then our output is
going to be from 0 to 1.Let's say, for example, if I
have a slope as 2, 1, 5, 6,0.5.And if you plot this number,
and z we got all the summation.After all this calculation,
we've got 13.5 as z.And we're going to plot that
information into sigmoid,and then we've got 0.99998.So it's close to 1.Also the SAS Visual data mining
and machine learning platformalso has logistic regressions as
a part of the machine learningpackage.Regression-- yeah, I
think we kind of all knowwhat regression is.Regression is pretty much
about the continuous output.So we actually predict
the continuous output.It could be any number from
minus infinity to infinity.An example is you could
predict the house pricesper square foot, and so on.And it has linear
regressions, like this graph,or polynomial regressions.And decision tree
or random forestalso provides regression,
even though they aremore fit into classifications.So in many cases, in
the regressions problem,we usually using about
linear regressionsor polynomial regression models.And it's hypothesis function is
very simple, like theta timesx, plus weight.So like y equals 10 plus 2x is a
simple linear regression model.And then it's cost
function usually usefulfor the accuracy of
regression model.Such as square errors, mean
square errors, and rootmean square errors, and absolute
error, and things like that.SAS Visual data mining and
also machine learning packagealso has regression
as an option.We actually talk about
supervised machinelearning so far.And now there is unsupervised
machine learning,which means that non-label
input data, which meansno y-- no correct answers.Usually, it is done for
exploratory analysis.And then basically,
the clusteringis the most popular
one, which meansassign the set of observations
into subset which is cluster.So that the data
in the same clusterare more similar to
each other than thosefrom different clusters.So basically, we find
these three clustersfrom this data point.Most popular one is
the k-means cluster.And it is quite
popular in industrynowadays because not all
the data has the answers,or labels.So this is the one that we're
using for a lot of big datawhich we do not
have an answer for,such as grouping
of documentations,or grouping of music,
or grouping of movies,or finding customers that
share similar interests basedon common purchase behaviors.So basically, about the
documentation segmentation,or customer clustering
segmentation, imagesegmentation, and then
recommendation enginesare the popular uses
for k-means clustering.So k-means clustering
is one of the mostpopular in unsupervised
machine learning.It's the simplest and
probably the most popularunsupervised machine
learning algorithm.As we discussed before,
it groups similar datapoints together, and
discovers underlying patterns.So we kind of find what
is k using this data.So basically, the
k-means clusteringalgorithms identify k
number of a centroids,and then allocate every data
point to nearest clusterswhile keeping centroids
as small as possible.So in this example, we try
to find two clusterings.And then the
clustering algorithmstry to calculate
every data point,and then try to find
the optimal k usingand also tries to find the--the k is the centroid
of these k values.However, sometimes, you have
to kind of find optimal point.And a lot of times,
machine learningis the combination
between programmingand also statistics or model.So if you require a
lot of programming,it is recommended--not use too much
computing power.So we try to find optimal data
point using the cost functionsand also k-means clusters.And SAS Visual
data mining, and wesaw machine learning package
has this option as well.So you could pick
the clusterings,and then you set some
of the hyperparameters,and then run this one.Then it will find about
clustering analysis.So now once you actually
train your model,one of the most
important things ishow to validate or verify
the training model, whichmeans how accurate it is.I've been working
with stat programmers.So many times, if you actually
develop some of the reportand also the data
set, there alsoneeds to be second programming
to verify whether whatI produce is correct or not.Kind of the same thing.Once you train your model
with the training data,you also need to verify
the trained modelwith the test data.So in many cases, a lot of you
probably know about the SDLC--System Develop Lifecycle.And part of the validation
process is actually we do--in SDLC, we actually
do validate the outputsuch as based on
your requirementsor based on functional
requirements.Machine learning also
probably similar way.For example, let's say
you do X-ray diagnosis.You build the model
that looks at the X-ray,and it predicts whether the
patient has cancer or not.And then let's say you have
your model performing about 90%,and then using
another data point,you have to validate whether
this model performed 90%or not.So in this machine
learning algorithms,when you prepare data, you
prepare data into training dataand test data.The training data
is one that we'regoing to train our
machine learning models.And then we're actually
using the test datato model evaluations.So we actually set the sum of
the threshold for the modelevaluation, such as
accuracy is going to be 80%.So one person-- let's
say-- look at itthis way-- one person
actually trains your model.Another person uses that test
data to evaluate the model.So this test data never changes.We keep it as it is,
and then this personcan now see this test data.Once you build the
model, train the model.We pass onto the evaluators,
and then they evaluate it.And it should go
over 80%, otherwise,it's going to keep
training and training.And then if evaluation
doesn't pass,then a lot of
times, the trainersuse different algorithms,
or use more data,or they change some of
the hyperparameters.So in many cases, we measure the
accuracy, precision, or recallfor the classification metrics.But sometimes, for the
regression, we use MSE or MAE.Or ranking metrics
and statistic metricscorrelations, and computer
vision, and also NLP metricsthat we're going to talk
about a little bit laterin our courses.And one of the most popular
classification metricis the confusion metrics.We talk about-- you actually
evaluate accuracy of the model,and precision of the
model, recall of the model,F1 score of the model.A lot of times,
that F1 score is oneof the most popular
classificationmetric evaluation.And some cases, we're
also using the ROC as a--AUC sensitivity and
specificity as a partof the metrics evaluations.We also could setup the model
comparisons or evaluation usingthe F score here in the
SAS global data miningmachine learning package
result. And it actuallyshows some of the
output that we have.Hyperparameter tuning is
one of the important partswhen you actually try to make
your model perform better.Hyperparameter is a
setting of the algorithmthat could be adjusted to
optimize the performance.So a lot of times,
the hyperparametersare not really
easy to determine.So usually, we tune after
the model is trained,and we actually continue
to train the model usinghyperparameter tunings as well.Artificial neural
networks, otherwise ANN.It is the most powerful
machine learning algorithm,and it is a game changer.The interesting
thing is it worksvery similar to human brain--human neural networks.And the main type is
deep neural networks,convolutional neural network,
recurrent neural networks.And its architecture
has artificial neuronslike this, input layers, hidden
layers, and then output layers.So let's compare between
human neural networksversus artificial
neural networks.As you see with human
neural networks, it has--or you see neurotransmitter
from other networks.And then you get
multiple signalsfrom other neural networks,
and then it's processed,and also it's sent out to the
next human neural network.And then it has about 100
billion human neural networks.Artificial neural
networks work verysimilar to human
neural networks.It receives the data and input,
and then it processes it.And it also going
to other process,and then it sends it out to the
next artificial neural network.Something like this.It receives it, processes
it, and sends itto the next artificial neurons.So if you look at
its architectures,the formula of
artificial neuronsworks something like this.It has an input variable
from x1, x2, x3, to xn.It is a numeric value that goes
into artificial neural model.And then it can be
multiplied by weight--actually, corresponding weight.y1 for x1, y2 for x2, y3 for
x3, and so on up to yn to x1.And then it also has a bias.So if you do multiplications
and then summations--so it works something like this.The z1 from here is summation of
the multiplications of an inputvariable and its
parameters, such as x1 timesy1, plus x2 times y2,
x3 times y3, and go on--times x1, times y1, plus biases.Once we do summations
of these input variablesto the z1 output, and it also
goes into another function,such as activation functions.And then we get output as a1.We're going to talk in more
detail about this activationfunctions, and it works
something like this.Let's look at the example.Let's say our weight is going
to be something like this.We have four input variables,
and then four weights,and one bias.And if you actually plot
all this information,such as input
variables 255, 231,up to 122, times weight,
plus biases, we get 0.98371.And we plot this number
into activation function,which is in k sigmoid.Then we get 0.727844 as output.I think we've seen this
one in previous algorithms.It is very similar to
logistic regression.So let us ask a question--
what the artificial neuronin this case is?As we discussed before,
the ultimate goalof machine learning
training is to find or learnoptimal parameters.In this case, w1, w2,
w3, w4, and biases.So the reason that we do all
those calculations and trainingis we try to find about y1,
the weight variables, and thenbias variable,
which is parameters.So the way artificial
learns is exactly the sameas we actually talked
about before in machinelearning training.It calculates the
cost function, whichis difference between actual
output and predicted values.And then reusing
the cost functionsand grouping descent to
find about the parameterssuch as weight and biases.So let's look at the artificial
neural network architecture.As you see in this example,
we have input layers,and we have two hidden layers,
and we have one output layers.And input layers, we
have three features,which means x1, x2, x3.First hidden layer, we have
four artificial neurons.One, two, three, four.And second, we
have two neurons--one, two in second hidden layer.And then output layer,
we have two outputs.So in terms of weight
wise, for hidden areas,we have 12 weight.Let's calculate.For this hidden layer,
artificial neuron, hidden layerone is that we have
three input, whichmeans there's three weight.And then second, neuron in
hidden layer two is we alsohave a three.So if you times 3
times 4, then wehave a 12 weight in
hidden layer one.What about hidden layer two?You have two neurons,
and each neuronreceives 1, 2, 3, 4, which
means that we have 4 weight.So 2 times 4 is going to be 8.And then in output layers,
we've got two neurons,and it receives two inputs,
so it's going to be 2 weight,so it's going to be 4.And then biases for
hidden layer oneis that one, two, three, four.So all together, the parameter
that we have to find--this example about 32
parameters we have to find,or 32 parameters we will train.We briefly talked about
activation functionsin previous artificial
neural network.So there are a couple examples
about activation functions.So activation function
is pretty mucha node that's added to the
output of neural networks.Sometimes, it could be added
between two neural networkstoo.And its purpose is to provide
the boundary of output.There are a couple
activation functions,and the first one
is linear functions,which means that it provides
exactly the same thing as comesin.And the sigmoid or
logistic functionis that it limits from 0 to 1.So if the data comes in, all
the summations of those datacomes in as 2, then it's
going to be about 0.8.And if you go by 7, then it
pretty much goes into 1--same as the other way.Hyperbolic tangent
function is goingto range from minus 1 to 1.So it is mainly used for the
classifications of two classes.ReLU is probably one of the most
popular activation functions,and it is pretty much
going to be everythingbelow 0 is going to be 0.And then other thing, it's
going to be straightforward.So it is used between
the neural networks,and it is very popular
for CNN and DNN.And its main purpose is to
speed up the calculations.Softmax is another
function that'sreally popular that will turn
numbers into probabilitiesthat sum to 1.Let's say that we have four
outputs such as very happy,unhappy, happy, and very happy.And you see that soft function
provides the probability.Saying that the data provides
10% very unhappy, 20% unhappy,and 55% happy, and
0.15 very happy.So activation-- this
softmax activationfunction will provide
output such as happyas a final output.And it's a very popular
activation functionfor multiple classifications
for neural networks,recurrent neural
networks, and then NLP--Natural Language Processing.So let's look at how it's
going to be turned outinto activation functions.We assume that all those
inputs, such as x1, x2, x3, x4,and weight, and
bias are the same.And then we use different
activation functions.Let's say z equals 13.5.If linear function is activation
function, then we've got 13.5.In sigmoid, we got 0.9999.And tangent, we've got almost 1.And then ReLU, we've got 13.5.However, if z is minus
0.4, our linear functionis going to be 0.04, and
then sigmoid, we've got 0.4.The tangent, we've got minus
0.37, and ReLU, we've got 0.So as you see, however input
that we're going to have,activation function determines
the boundary of the output,and also changed
the output as itfit the purpose of the
machine learning algorithms.We actually talk about--one of the most the concept such
as hypothesis functions, costfunction, and gradient descent.Let's go into more
detail about itin the next three
or four sections.First, loss function.Loss function is pretty much
the same as cost function.It is a method that evaluates
how well your algorithms fityour data set.If your predictions of
the model is off a lot,the loss function will
provide higher numbers.If the prediction
of model is good,then loss function perform
or provides lower numbers.There's two types
of loss functions.It is pretty much based on the
output or label of your data.One is regressive
loss functions,and another is classification
loss functions.Regressive loss function is one
for continuous target variable.And then usually,
activation functionis going to be regression
that we talk about,or provide the first example.And usually, it
provides two types--or three types actually--the mean square errors,
and mean absolute errors,and smooth absolute errors.And it's determined by the data
of the uses of your output.The classification loss function
is for the classificationtarget variable.Usually, there are two types.One is for the
binary, and anotheris a multi-classifications.Binary is pretty much
about whether it's goingto be 1 or 0, or yes or no.And then popular one is sigmoid.Also binary cross
entropy loss function isone of the most popular ones.For multi-classification, it is
basically for the multi-target,such as 0 to 9--happy, unhappy, very happy,
very unhappy, things like that.And then multi-cross
entropy loss functionis one of the most popular ones.So when you're looking
about the] lossfunction-- when
you determine lossfunction for your algorithms,
then usually for binarycross entropy loss function
for binary classifications,and multi-cross entropy loss
function for multi-target.Optimizer in machine learning
is similar to gradient descent.It's goal is to minimize
cost or loss functions.It will calculate the change
the weight or parameterof the model so that error
for the cost functionwill get lower and lower at
each iteration of new data set.It's kind of like walking down
the mountain to find the goal--locating the value.And it's main parameter
is learning rate.How much you're going to
walk down for each step,or how much machine will
learn each iteration.So this is an
overview of optimizersthat you could actually
looking about the batch.Gradient descent, or
stochastic gradient descent,and adaptive learning
rate, and Adams.Usually, Adam is most popular
because it is adaptive--it provides adaptive
learning rate.What I mean is
that when initial--the first set of
iterations, learning rateis going to be high.However, it's going to go near
the optimal cost function--then it will become less.So your learning
rate kind of adjustsbased on your cost functions.So these are the really good
machine learning algorithmsthat we could probably use a
lot in our machine learningimplementations.It is called deep
neural networks.Another one is
called deep learning.So deep neural network is
pretty much about aggregateof individual
artificial neurons.It is a type of the
artificial neural network,and that we consider
it a deep whenhidden layers go more than two.So as you talk about
artificial network,it's basic architecture is
input layers, hidden layers,and output layers.And each layer is made up of
a set of neurons such as the 4neurons in the first layers.And then each neuron
is fully connectedto all the neurons in the layer
before, and sometimes after.So an example of
deep neural networkis detect whether the patient
will have cancer or not.So input variables are going
to be age, sex, race, weight,height, family
history, and outcomeis going to be yes or no.And we can also detect number
from 0 to 9 from images.Kinda input variable is going
to be image data, such as 28by 20 pixel images.The output is going to be
0, 1, 2, 3, 4, 5, 6, 8, 9.And also for
pharmaceutical industry,you can also detect AE
events from text messages.So input variable is
going to be the text,comment from social media,
or report from the siteor hospital, and
the outcome is goingto be whether those texts
or messages are going to beevery event related or not.And also you can
detect sensitivityfrom text message,
such as we couldhave an input
variable as a blog,comment from social media--the outcome is that you'll be
very happy, unhappy, happy,very happy.SAS also provides the
neural network packagein Visual data mining and
much learning package.And you see that there is an
option for neural networks.You actually click it
and drag into the dataafter that preparations.And you could actually
connect this one.You could setup the layer, such
as you setup to three hiddenlayers.And then the number
of neurons per layeris going to be setup to 10.And also activation function
Tahn and things like that.So you could have
an option to set upthe hyperparameters, and
also the architectureof the deep neural network
using SAS Visual data miningand machine learning package.This is another hyperparameter
you could set up--all this area.You see that SGD is a Stochastic
Gradient Descent, whichis the optimizers.And the iterations-- how many
times you're going to try.And so you could actually
setup all those optionswhich is hyperparameters using
that SAS Visual data miningand machine learning.One of the good things
about this packageis that it is very easy to
use and also very intuitive.And with some of the things
that you could alreadysetup as a default.And you could also see
what is a hyperparameteryou can use for these
machine learning algorithms.So let's also talk about
how we could improvedeep neural networks
based on the biasesand also variance
and regulations.Once you train your model,
there is a situationyou kind of notice-- is
it could be underfitting,or it could be overfitting.Underfitting is more about
your model doesn't reallyrepresent you well.Remember sometimes, you
actually wear clothesthat it's not really about you--it doesn't really
tell about yourself.It's kind of like that.And overfitting is that it
looks really good at the moment,but not really all the time.It only fits into
certain situations.For machine learning is only
for certain data points.For example, after
you model your dataand you train your
model, underfittingis that-- as you see that your
data point represents somethinglike this, but you actually get
the model in a straight line.And it is probably not
really good performancein terms of accuracy, and also
it doesn't represent your dataproperly.And overfitting is that it
fits too much like this one.So what you really want when
you build deep neural networksor machine learning
algorithms, youwant to have a some
kind of balance.You don't want to
have a underfit--you don't want your model
underfit compared to your data.And you don't want
it to overfit.You want to have
something just right.How can you do that?Now the reason is that you want
to have an underfit/overfitis that a lot of
times, if you have--for example, there's
a high bias--means underfit.And then overfit means
it's high variances.And then if you
have high variance,then it also provides
a lot of errors.If you have a high biases, it
also provides a lot of errors.So you don't have something
just in the middle.So that is kind of why you have
machine learning algorithms.And then when you
have overfitting,one of the ways that you could
actually minimize overfittingis called a regularization.There is two types
of regularizations--L1 and L2.And its main goal
is discourage someof the more complex
or flexible modelso that you could avoid
the risk of overfitting.So pretty much, it will
minimize the learning rate.So also general procedures--
a way to remove high varianceunderfitting--[LAUGHS] I mean high bias on
the fitting, or high varianceis overfitting.High bias means
that your model isunderperforming, which
means accuracy is notup to the threshold.The way that you could
actually improve your modelis that you either have more
features with more inputvariables, or maybe
you could train longer.Like rather than doing
one-time training,maybe you could do
about 10 trainings.And also you could use
a more advanced model.If you have a high variance,
it means that your modelis OK with the trained data.But in the test data, your
model doesn't perform well,which means that your model fits
too much on the training data.In the cases, you
have to use more dataor apply regularizations
and maybe try less features.So machine learning training
is kind of art in science.Using the science
such as machinelearning, but at
the same time, youhave to find the right fit--art.So let's talk about
convolutional neural networks.There are actually two, I would
say, really big advancementsin machine learning.One is image recognition.Second is natural
language processing.That really helps or actually
make all the things--it's really, really advanced.There isn't a whole lot of
difference between machinelearning and
statistics up to whenwe talk about image recognition
and natural languageprocessing.For image recognition,
convolutional neural networkis the best way for image
recognition problems.So CNN uses recorded
image-specific artificialneural networks, such as
image search recognition, facerecognition, or face
verification, self-driving car,and more.And the concept is coming
from brain visual contextwhere many neurons have a
small region of visual field.We talk about features in
neurons in our presentations,so you'll see more detail.The name of a CNN actually
comes from convolutions,one of the most
important operationsand calculations in CNN.We also show in our
training data slide.The first CNN
network is LeNet-5,which can classify digits
from hand-written numbers.We will see more about transfer
learning-- is probably the mosteffective way to apply CNN.The advantage of
CNN compared to DNNis that CNN could read the data
as two or three-dimensional.Compared to DNN-- it's
only one dimension.So for image recognition, it
becomes much more powerfulbecause we see the image as two
dimensions or three dimensions.And then it will
provide less parameters,which means much less computing
power to train image data.So let's talk about image data.Let's say when the computer
sees this image, actually,it sees the picture value.This PNG data, if you
actually import that datainto computing numbers, it
equals something like this.And you see that you can
kind of see the same shape,and 0 is probably blank.And then high numbers,
such as 255, are dark.And then low numbers such as
6, or like something dot 7,it becomes very small numbers.So now this picture is
represented by 28 by 28 pixels.So this is not really
a good quality picture.However, if you have a
good quality picture,then this pixel
becomes a lot bigger.It goes up to like
a couple thousand.But the key here is
that your image data--any kind of picture could
change into the numbers.And you see that if you
create this picture,and then go to Property
in your screen,you could kind of see that 1,300
by 1,300, which means that ithas 1,300 x and 1,300 y pixels.It's a huge data point.When you process image data
on deep neural networks,what happens is that-- or
any other model in machinelearning--you actually change this two
dimension to the one dimension.Such as 20 by 20 pixel data
goes into one dimension, whichmeans 784 input data points.And imagine how much weight
you have to carry in.So if you do image data training
with a DNN or other machinealgorithms besides
CNN, it requiresa lot of computational power.When you actually stay
as two dimensions,it becomes a lot
easier to train.That is the reason CNN
has became very popular.For example, this data--this picture has about--like 2,356 and 2,304.And then this 3 means that--when you see that
image data, youcould see either two
dimensions or three dimensions.Two dimensions is
actually black and white,and three dimensions
is RGB color.So usually, you've
got three dimensions.So if you actually convert
this data into one dimension,and then it becomes 30 million
features going into DNN.This is going to
take almost foreverto train the image data.So what CNN does
is that it actuallylooks at data in either two
dimensions or three dimensions.If you look at the
architecture of CNN--and we go one by one
as a more information--here's input layers, which
is about four dimensionswith a three--28 by 20 times 103,
where the 1 is color,One is black and white, and
3 is actually on colors.And then we actually--
or rather, the first oneis going to be the number
of the picture that'sgoing to go into training.So input layers,
convolution layers,and pooling layers, and another
convolution and pooling,and then flatten layer, and
then fully connected layer,such as deep neural networks,
and then reaches optimizer.Compared to DNN which is input
layers, and hidden layers,and output layers, CNN has
more complicated architecture.So input layers.Usually about that--
so three dimensions.If there is a one to the end.And the first and second is
going to be the pixel size.And then one is going to be--if it's a 1, it's [INAUDIBLE]
dimension, which means black.If it's 3, it means color.And if you actually process
about 60,000 images,and it's going to
be four dimensions.So CNN, the input data set--feature is going to be
always four dimensions.DNN is going to be always one
dimension-- two dimensions.So convolution-- pretty much
a mathematical combinationof the two functions to
produce the third function.CNN has features.We're going to talk in more
detail about these featuresas we actually multiply
this feature into these 3by 3 matrix, and
we got the value.So 1 by--1 times 3, 1 times 1, 2 times
1, and 0 times 0, 0 times 5,0 times 7, minus 1 times 1,
minus 1 times 8, minus 72 ,then you've got a minus 5 value.And you go on.So this is image data, and
this is a feature of a CNN--3 by 3 features.So you've got all these numbers.And you see that minus
5 is coming from there.And then you've got
the oldest number,and then you notice
that 8 is the largestone, which means that this area
represents these things well.So feature could be
many different format,but also filter is representing
image feature identifier.If you actually change
this into the image,it's going to be straight line.This is going to be-- is a
horizontal straight line.The vertical shape-- then it's
going to be kind of shape.So when you've got a high
number in certain feature,such as this case, which
means image in this areaare more likely
into straight line.So convolution kind
of works this way.The filter goes into
each area, and thenfinds what it will represent.So it finds about slope--in this kind of slope
in these features.And this kind of slope
in these features.And this kind of slope
in these features.So convolution is
kind of filter--finding best filter
in image perspectives.And then pooling is
just to making surethat you could reduce some
spatial size so that youcould actually compute faster.So there is a max pooling,
then there's average pooling.Max pooling is more popular
than average pooling.What it does is that if you are
pooling with a filter 2 by 2,it means that it goes by 2 by
2, and then finds the highestnumber.In this case, minus
2 from this 2 by 2.In this case, 8
from this 2 by 2.And it's going to be
0 for these 2 by 2.And it's going to be
minus 3, and 2 by 2.Flatten layer is
that it is actuallyconverting all those
kind of 4 or 5--well, actually, 4
dimensional pooling layersinto one dimension so you
can actually process thingsmuch faster.And then fully connected layer
is kind of deep neural networksbasically.Here's the flatten
layers, and thencouple of the hidden layers,
such as fully connected layers,and then you connect
to the output.So the fully connected layer
is working like neuronsin fully connected
layer to detecta certain feature of images,
like nose, and mouth, and eyes.So in this case, this neuron
represents maybe nose,this neuron represents certain
types-- nose 1, nose 2, nose 3.And it could represent
eye 1, eye 2, eye 3.So in this output, it
probably gets more valuefor nose type 2.And in this case, it could be
more value from eye type 2,and things like that.So output layer is pretty
much output representations.So if you get a 10 output, which
means there's about 10 output.Whether it's going to 0 to--probably 0 to 9, it could
be [INAUDIBLE] 10 output.And then the interesting
about this outputlayer is that the resulting
vector for digit classificationprograms, such as--let's say it's going to
be 0 up to there-- thenas you see that it represents
0, 1, 2, 3, 4, 5, 6, 7, 8, 9.Then that means that the
10% probability, thatis going to be 1.And 10% probability is
going to be 1, and then 2.And then 75% probability,
that image is going to be 5--0, 1, 2, 3, 4, 5--so that you actually are
looking about-- the 5is the most popular answer.Machine learning most of the
time is about the probability.If there's about 10 output
classifications, and thenyou pick the one which
has the most probability.And the softmax function
provides that information.So as you see, for CNN
model, input dimensionis going to be 4 dimension
such as 20,000 images.And each image represents by
1,400 times 1,400 pictures,and the colors.And then output is 2, which
means about 20,000 dimensionsto 20,000 and then 10 output.So this 20,000 is going to be
number of the records, whichis output.And then second dimension is the
number of the classification.Such as 0 to 9, it could
be car, and truck, and van,and bicycle, and
things like that.Actually, every year,
there is a competitionabout image recognition.And then one of the most popular
ones is the LeNet developedabout 1998 to 2000's.And then the one that's
really the most popular oneis the AlexNet, which
is developed about 2012,because it uses CNN,
and then it actuallybeat other competitions
more than half.It's about 85% accuracy
for the image recognition.And then the thing is this
gets better and better whenwe apply more CNN algorithms.The ResNet which is
competition winner in 2015is less than 4% error
rate, which is amazing.We're going to talk more
detail about this one lateron in transfer
learning, because wewill use some of the
pre-trained modelsto any other machine
learning implementations.CNN implementation is one of
the most popular deep neuralnetworks, and it is
pretty much go-to modelfor all those image related
problems, such as imageclassifications, X-ray
diagnosis, face recognition,and face verification,
and object recognition,and autonomous vehicles.I'm actually very exciting
about our next topic--recurrent neural network and
natural language processing.Actually, this is
the one that I'mworking on at this moment
in my current positions.Recurrent neural network
is the model thatuses sequential information.The reason that the recurrent
neural network became popularis that it is very useful
when our input are dependent.When you actually talk
about other machine learningalgorithms, all those
input data-- input featuresare pretty much
independent of each other.However, in the same--or in the situation that
the data becomes very muchdependent, such as
sequential data in RNN--recurrent neural network--
has the most effective machinelearning algorithm for
those kinds of data,because this RNN has
a memory feature thatcaptures previous
information about whathas been calculated so far.Now let's look at an example.The basic RNN structures and
algorithms are shown below.As you see, there's an input
such as x1 goes into the model.And it gets process, and it
provides the output model.And when x2 was input into
the model, what happenedis that these RNN
neurons also haveinformation from previous
input, such as x1.And x3 in the same situation
also received the informationfrom x1, x2.So now when you talk about
this y3 as an output,it has information from
x1, x2, x3, and so on.The basic structures of our
RNN model has two units.Gated recurrent unit, LSTM,
or Long Short-Term Memory.The gated recurrent unit
is a gating mechanismin recurrent neural network.And it has been shown to exhibit
better performance on smallerdata sets.And then GRU actually has
fewer parameters than LSTMso it can actually train faster.However, LSTM which is Long
Short-Term Memory unit,becomes more effective when
you talk about RNN and alsonatural language
processing implementations.It has 4 gates, such as input,
forget, gate, and output.LSTM-- remember, the values
over arbitrary times intervals,and 3 gates regulate the flow
of information into and also outof LSTM unit.The basic popular
implementation is that--such as text notation or
filling in the missing output.So actually same
number of inputsand also same number of outputs.When you see the situation
when there is a missing wordor vocabulary in a
sentence, then this oneis able to fill in that
missing information.Another implementation is
a sentimental analysis,such as PV signal.When you read some
of the text, and thenyou could actually analyze
whether this text meansor represents some information.Whether this text is about AE
event or other information.Or when you read
this text, you couldsee that, oh, it
sounds happy, or thissounds positive, or negative.And then another
implementation iskind of picture descriptions.When the picture is
read, then outputis going to be the information
about the pictures.And also machine
learning translations.Input is going to be
English, and output isgoing to be French or Spanish.Or in chatbot development,
or any other things.RNN is actually a very
useful implementationfor natural language processing.The natural language
processing is pretty muchin the area of
artificial intelligence,of a human language
interactions,and implementations.So basically, the computer
reads some of the language,and also it provides
descriptions and analysisabout those language
text information.So our input data will
be some text language,and output data is going to
be descriptions or other textin the language.Let's look at some of
the sentimental analysis.So input data is--we actually implement
the embedding process,which means represent each
word to the vectors or numbers.[Going by actually read the
input data as a language--some of the vocabulary-- and
we convert that vocabularyinto numbers.For example, if we go into
words such as ""I am happy.""Here's a completely, and it's
converting to the embeddingprocess, and then
it becomes e1852 ,which is about 50 vectors
of that information.And the output data could be
representation of the softmaxoutput, such as 0.07,
0.13, 0.1, and 0.70,which represent very
unhappy, unhappy, very,and very unhappy.So using that as output,
NLP could predictthe output of this
texts is goingto be very happy because it
has highest probability of 70%.And you see this one.So if you go into more detail
about this information--for example, this input
data is ""I am smiling.""So now, in NLP process,
this ""I""-- the vocabularyis going to be converting
to some other numbers,and then it will go
into LSTM process.And then ""am"" is also
going to change into numberusing the embedding process--it goes into the LSTM.And then ""smiling"" also goes
into the embedding process,and converts to the vectors--numbers-- and then
it goes into LSTM.So this part has a
memory from ""I am.""Also a combination of smiling.And using this process in
natural language process,it also provides the output.In this case, very happy.Recurrent neural
network is applieda lot in many
industries, and it isone of the most
popular implementationsfor machine learning.And there will be more
implementations youcan see in the future, such
as speech recognition, musicgeneration, sentiment
classifications, motiontranslations, video activity
recognition, and name entityrecognition, DNA sequence
analysis, and also chatbots.I'm also very excited
about our next topic--about transfer learning.It is one that I'm
currently working on.But at the same time, it
is actually making machinelearning implementations
and trainingmuch easier than before.Let me tell a story.Let's say we try to train
SAS programming people.You have two options.One is experienced programmers.The other option is
novice programmer whichhas never programmed before.So in general, you
can ask the question,who will learn faster,
and who will be much moreeffective or cheaper to train?Yes, usually, the experienced
programmer who knows about Javaor Python are non-SAS--program much faster than people
who never programmed before.Exactly the same way.Transfer learning
is that we alreadytrained our model
with some information,and then we actually convert--or using that already
pre-trained modelwith other implementations.So in a way, the
machine learningmethod where a
pre-trained model isused as a starting point
of the model developmentfor another or a similar task.So you might wonder how
it could be applied.If we go in that direction,
let's talk a little bitabout some of the
characteristics of the machinelearning model.For, let's say, image
recognition model--and we actually
train the model--in all the layers cannot
detect about edge detections.And when you go into middle
layers, it starts shaping.Each neuron represents
some shape of the images.And then when you
go to later layers,and then each neuron represents
more high level featuredetections.So as the model progresses, the
layer or neurons from the layermodel--later layers actually represent
more high level features.So let's kind of think about
con pre-trained models,such as VGG.VGG model is the
most popular one--it is also free,
available in the public.And here's the
two architectures.It's either 16
layers or 19 layers.And it is a CNN
model for image data,and then it only trained with
14,000 images with 10,000 classobjects.And it has the parameters,
such as weight and bias,almost 140 million parameters.And then if you
want to train again,it will take about a week.And it costs a lot of money.You need a GPU to process
and train this model.However, all those parameters
in this pre-trained model VGGare available in public.So let's see how we could
actually implement transferlearning in our new problems.First, we select the
pre-trained model.Let's say you select the VGG.And then we import pre-trained
model into our programs.So this is VGG is
already trainedwith 14 million images--about thousands classifications.So you import this
already pre-trained modelinto our programs, and
then we could actuallydetermine which layer
we're going to train again,or which layer we're
not going to train.Right now, let's say you
have about 6 layers here,but you could then decide to
train maybe last 2 probablydon't train more.And then we decide more
layers of the importedpre-trained model.Right now, there is
only 1 layer in our sideand I replaced with
our new layers here.And then we define the
output differently.Let's say you're going to
have a 2 on the output.And then we actually
prepare new data.Right now, there is about all
the images we trained with,but right now, we're going
to focus on certain images,such as X-rays.And then we
determine what outputis going to be of these layers.And we compile this,
and then train against.In terms of VGG, we need
about 14 million images.But in a pre-trained model,
setting about the proper outputlayers, we might need
about a couple thousand.So it is actually saving a
lot of money and saving time.So you wonder why
transfer learning?We actually talk about it--we could train
models much faster,so it becomes cost effective.In many cases-- sometimes,
you don't have a choice.Because finding the right
data is sometimes very hardand very difficult.So lack of data
decides that we'regoing to go with pre-trained
model in other cases.As we mentioned, there are
a lot of pre-trained models.For image data, there is
Xception, Inception model,ImageNet, ResNet, VGG model,
and also ResNet model,and things like that.For language data, there is
Google word2vec, and GloVeas the embedding process.And then UMLFit or BERT.ELMo is one of the most popular
language models out there.So when you do some of the
high end implementations,in many cases, we start
with a pre-trained model,and then we prepare
data accordingly.And we actually set
up some of the modelso that we could actually
apply it to other problems.So let's talk about
how machine learning AIis impacting our lives.First, it's probably
the most prevalentin our implementation--
voice recognition.I have young children,
and from time to time,I notice that they're
actually talking to Siri.So such as Alexa--Siri is one of the most
popular NLP implementations.So it is the voice recognition.And then Amazon, and
Netflix, or Spotifyusing machine learning for
their recommendation systems.The chatbot is probably one
of the most popular onesin terms of customer services.So when you open
some of the websites,they actually pop up
a small window saying,how can I help you?Most of the time, those
kinds of pop up windowsare actually the chatbot,
and you are initially talkingto the machine, not a human.A good thing is
that is chatbot--once you setup this
chatbot, it willprovide quick and easy
communication between customer,and it will work 24/7.I think about last year,
Google introduced AI Assistant.It's a system that's
accomplishing real worldtasks over the phone.It's amazing what you
could do in the future.And also imagine-- introduce the
Amazon Go cashierless grocerystore.What happens is
that you actuallyscan your phone
when you walk in,and then you actually pick
up your items into your bag.And what happens is they
monitor you using the CNN imagerecognition.And then when they
grab it, they notice.When you return it,
they also notice too.So they charge based on
what you actually selected.The funny thing is
there's no line--cashier-- but there is a line
outside waiting to get in.It happened about
a couple years ago,and IBM built AlphaGO
to beat the bestGo player in the world.We think that Go is so complex,
so machines will probablynever beat humans,
but AlphaGO actuallybeat the best Go players with
a short time of training.Whenever I talk about
machine learning,when I say about Terminator,
then people always got it.There is a debate about how
much machine learning shouldbe applied or implemented
in certain areas,such as military.There's a lot of people
actually against it--about this part-- something
that we should regulate,and some of the machine
implementation in certain area.But when you talk about
AI machine learning,then if you kind of
hint about Terminator,then people usually get
it about the machinelearning implementation.So how the machine learning
implementation in industry?Let's talk about some tools.There's some machine learning
programs such as SAS, and R,and Python.And then SAS also has
a great interface tool,such as SAS Visual data mining
and machine learning interface.And you could actually do
a lot of visual and easyto use machine learning
implementations.And also if you look
about the SAS Viya,it also has some of the
machine learning proceduresthat you can use.And R and Python are probably
the popular machine learningimplementations, and it's
also available on the website,and also there are a lot of good
machine learning packages too.And then there's also
very interactive, easyto use tools such as SAS
Visual data mining and machinelearning package, or IBM Watson,
and Microsoft Azure, and GoogleCloud, in H2O.So let's talk about why AI
machine learning is so popular.And it is actually
very innovative.It actually helps
to solve problemsthat we weren't able
to solve before.And it actually provides new
business revenue and alsodevelopments.And it is actually
cost effective.It could automate a
lot of tasks and work,and it might have a chance
to replace or enhancesome human resources.Andrew Ng is one of the most
popular AI implementation--he's a co-founder
of Coursera, and hewas leading Google
Brain, and was Alibabafor the last couple of years.Right now, he's actually
doing some AI machinelearning based on the future--I think the investment funds.But he kind of says that
pretty much anythingthat a person could do or
think in less than second,we could now automate with AI.So there will be a
lot of implementationin terms of innovations and
cost effective purposes.So there will be more--and a lot of AI
implementations in the future.And in some cases--not all, but it could be more
accurate than general humansin some of applications
such X-ray,or diagnosing, things like.A lot of people actually
ask me about how they couldstart with machine learning.Whether we like it or
not, machine learningis going to change a
lot of different things.So the first step
is we have to admitthat machine learning is there,
and it will change many things.And then I recommend that
reading or learning machinelearning either take courses.There is a lot of online
courses, free videos,and schools.And then as a programmer, I
love to do hands-on programming.So the best way to learn
is hands-on programming.So do exercises.You could participate can
have competitions with others.And then start
following other people.There are a lot of good machine
earning materials and alsomachine learning teachers--even there's a
lot them are free.So try to follow them, and
try to keep up with them.And then start implementing
in your organizationeven if it is very
small and simple.That way that you could actually
start learning about it,but at the same time, you can
actually evangelize machinelearning for your department
and your organization's.The hardest part
is any innovationin any new technology or new
idea is actually starting.And I applaud you
starting machine learning,and also taking these courses.So let's kind of keep it up.And then if you
have any questions,and if you have any comments,
then please let me know.You could find me--
you can send an emailto my personal email address.And also you could actually
send any information or requeststo LinkedIn.I'm very active on
LinkedIn-- social media.So you could actually
contact me through that area.And please let me
know how you're doing,and also let me know if
you have any questions.Once again, thank you
all for attending machinelearning pre-seminar
conference at SAS Global 2020.And I look forward to seeing
you all for next SAS Globalconference.Thank you.Please let me know if
you have any questions.Thanks.Bye."
48,"JOHN PRENDERGAST: Hi there.Welcome to this user session.My name is John
Prendergast, and I'm hereat my colleague Tomasz,
who will shortlybring you through the detail of
this particular user session.We are both part of
Accenture's Finance AnalyticCenter of Excellence, which
has offices in Dublin, whichis my own office, Poland, where
Tomasz is based, and in the USalso.Our practice's main
areas of expertiseare split into
four main pillars--digital finance
transformation, financial crimeand regulatory compliance,
fraud, and the pillarfrom which this particular
use case sits, risk.Since 2011, we've
been helping customersacross industries and use
cases shape and designsolutions to help them
address their pressingproblems within these areas,
some examples of whichwould include the providing
of an in-depth actionable costand profitability
based insights to CFOs,the insuring of an
efficient adherenceto regulatory criteria
within banking,the provision of
the capabilitiesto holistically prevent and
detect fraud at an enterprisescale, or providing high
value, data-driven approachesto addressing credit risk,
bad debt, and our collectionstrategies.There are many offerings
within these pillarswhere we have been delighted
to partner with SAS.The powerful and flexible SAS
solutions and technology stackcan both enable and enhance
our analytics methodologiesand operating model
for the enterprise,allowing us to apply our
own IP as well as leveragethe edge of the box
functionality and powerof a SAS-driven solution.What we would like to bring you
through in this user sessionis a methodology we have applied
within the telco industryto help minimize the bad debt
that can be accrued by a telcooperator's customers when
additional third-party servicesare made available to them
to purchase using the accountbalances of their
existing subscriptions.This double-edged
sword for telcosintroduces new revenue streams
but also opens the doorto additional credit risk
and greater opportunityto incur bad debt, both of which
increase the cost to serve.We want to address
this efficientlyand with minimal impact
to the customer journey.We recognize that the market
is swamped with technologiesand solutions that claim to
address these challenges,but often at a significant cost.We want to highlight
to SAS customersthat you can maximize your
ROI from your existing SASinvestment and tackle the
root cause of challengessuch as this rapidly
without introducingnew overhead for systems,
resources, and processes.So Tomasz, please,
if you wouldn'tmind stepping us through this
customer-proven solution.TOMASZ RYTTER: Thank you, John.And hi everyone.Over the next part
of this presentation,I will be diving deeper
into the solutionand how we can use SAS to
solve many of the problemsJohn just described.Before we go into the technical
[elements of the logic expects,let me elaborate on the
challenges we have observedand what kind of benefits
we expect to bringby implementing this solution.To purchase content and services
using existing subscriptions,customers can have different
spend allowances allocatedthat are relevant to them.This is to protect
clients from incurringbad debt from
non-paying customersand ensure relevant
allowances are assigned.We found clients' challenges
when using ad hoc analysisand reporting those best sets
of information which could bescattered to doors and areas.And there can be no
automatic analytics frameworkto support with ongoing
prediction of bad debtor to recalibrate
predictive models.And the process of
setting allowancesfor subscriber segments
might not be automated.This introduces a significant
amount of manual workand the incorporation
of business stressinto the process.That can lead to a limited
realization of revenue,increase bad debt, and
reduce customer experience.Implementation of
solution using SASmanages to solve many
of these problems.The data required
for analytics iscentralized of all data
integrated in one place.An analytics framework
is put in placeto enable generation
of up-to-date modelspredicting bad debt and then to
optimize allowance allocation.Debt data driven solution can
bring significant benefitswith expected excess of up
to 40% of bad debt productionand expected increase
in revenue of up to 30%.Also due to the automated
nature of solution,a substantial reduction
in manual effortcan be achieved, expecting up to
60% of manual effort reduction.Their solution is based on
three key fundamental conceptsthe data, integration of data
from various data servicesand establishing processes
to monitor data quality.And this is coupled with
having data flowing,processed, and transformed into
the solution automatically.And with analytics having debt
predictions based on the USdata without each being manual
or token analytical activitiesor involving additional
debt from thereto conduct data science
activities, alsoto have automated
recommendation of the subscriberallowance using optimization
engines, and finally,the ability to consume the
outputs by business use of CCthrough provided call
information visuallyin one place, including
portfolio analysis, projectionof revenue and bad debt across
small data source scenarios.And this allows for
streamlined decisionmaking throughout
the client basedon their analytical outputs.And based on those
underlying fundamentals,we have created a
reference architecture.Their solution consists of
five integrated components.And over the next slide, I
will go into some aspectsof individual components.But first let me show how
this solution was implementedusing selection of SAS tools.The first one being data
ingestion and processing,where we have used Base
SAS with developmentdone using SAS Enterprise Guide.The module covers
pipelines for ingestingof data feeds, processing and
transformation of the data,and also preliminary
aggregation of data,which is coupled with automated
analysis of the data qualityto detect any issues
or unexpected variancesin the data.We have leveraged
SAS/ACCESS Interfacesthat enable to tap into
databases of different vendorsand sites.We have found using a mixture
of pass-through functionalityusing SAS/ACCESS and processing
data in SAS using 4GL work verywell.Large volumes of data
can be processed nativelyor aggregated before
bringing into the solutionfor further processing.The second key component is
the customer analytical record,are in short, CAR.Your CAR is a fundamental
data structure for analyticsand predictive modeling.It is a wide table with hundreds
or even thousands of variablesaggregated on customer
or a subscriber level.CAR is there to using
advanced data transformationand aggregation, often
times on multiple levelsand in multiple stages.And here, again, we
have used the SAS 4GLto process the data and
all the necessary statusand the aggregation levels.The third component is the
bad debt prediction component,which is to build predictive
models for targetingthe bad debt.We have used predictive
algorithms from SAS/STATand that also use SAS
Interface [? INAUDIBLE ?]for a preliminary model's build
and discovery of the trendsin the data.The algorithms were embedded
in automated framework allowingothers to retrain themselves.And the framework was
created using SAS EnterpriseGuide, using SAS 4GL.And the algorithm
from SAS/STAT wereembedded into the framework.And why you have observed very
good results with the standardalgorithms just
available in SAS/STAT.More advanced algorithms
from Enterprise Minercan be further introduced
into the solutionto enrich and further refine
the analytical outputs.And the fourth
component is the enginefor optimization of the spend
allowance for subscribers.Again, we have just basis here.And we have developed
the custom grid searchalgorithm for this purpose.The algorithm enables to search
for optimal allowance scenariosacross various subscriber
our customer segmentsand leveraging
predictive models.And here, we were able to
optimize the calculationprocess through extensive
use of SAS 4GL language.This process also
generates artifactsthat are needed to enable
""what if"" allowancesscenario simulation.And the visualization
component, the solutionprovides a set of actionable
dashboards with informationfeeding from the other
components of the solutionto visualize or require the
outputs such as predictivemodels logic, and models
performance, interpretationof risk segments, and chance
that leads to bad debt,projection of
revenue, and bad debt,and the use of the
simulation functionality.Importantly, this solution is
embedded within an automatedframework.Our components run and propagate
the information automaticallythrough the components.And this was achieved by
extensive use of SAS MacroLanguage to automate
elements such as SAS codesolution, parametrization
of the solution, executionand propagation
of the informationautomatically for the solution.For example, the
predictive modelsbuild uses Macro to look
for multiple hyperparametersettings, select best
performance models by lookingfor different test sets
and propagate the modellogic to the next status.And once you cannot serve, this
solution can be implementedjust by using Standard & Poor's
commonly available in manyexisting clients
SAS environments,while additional tools can
further enrich the solutionto refine the outputs.Over the next slides, I will
go through a bit of detailon the aspects of
the components.The first step in the
solution is the integrationof the data from the sources
for automated processingand preparation to
further the foundationfor the analytics, optimization,
and the visualization purposes.There are four key elements.Data ingestion and integration,
it captures and integrates datafrom different sources.New incoming data is
captured automaticallyand incorporated
into the solution.The second, onboarding
of the new categoriesinto the solution and then
automatically propagating thosethrough the solution components.The third one, data
quality analysis,automatic detection of
deteriorating data quality.And I think all that
is any anomaliesare found or
detected in the data.And this is important
to prevent solutionfrom generating outputs based
on inaccurate or potentiallyincomplete data.And fourth, it's the advanced
transformation and aggregationto generate structures for
optimization, visualization,and CAR.And the CAR, the CAR is the
products of advanced dataprocessing and can consist
of hundreds or even thousandsof features to provide
comprehensive view of customeror subscriber.It can cover different
informational area,such as product history,
content usage and consumption,core network usage, or the
external all-tech party data.And since CAR build, this is
usually an iterative process.It is important to initially
understand which data to use.This can be achieved
through preliminary analysisof the data, like
understanding correlationbetween data
elements and bad debtthrough business expertise,
existing cuts client, biophysicnow and hypothetical factors,
or by leveraging referencepast designs based on previous
implementation experience.Next, let's jump into
the automated frameworkfor building predictive
models targeting bad debt.Their solution leverages
CAR are the foundationfor a predictive model training.And the CAR is automatically
rebuilt on most up-to-date dataon a given frequency.Additional calculations
and transformationssuch a third target
feature calculation,it's also part of this stage.The models are for training.Different algorithms can be
used to train models startingfrom more traditional
methods like decision treesor regression models to more
complex ones like trading,boosting or neural networks.We have observed
very good resultsby even using the
traditional methods.Another consideration
is if you justuse for the modeling
self-algorithms reflectthe best set of features
around training procedure.But feature space can
be reduced upfrontbefore introducing
into the frameworkthrough methods like variable
clustering or correlationanalysis.This may limit their
ability for the modelsto pick up new emerging factors,
but also reduce the abilityto compare models and
logic change over time,which can potentially reduce
interpretability of the models.And on the interpretability, the
model performance is comparedand the best models
are selected basedon the statistical performance
measures such as the Gini indexor k-statistic.And for this, the
custom testing metricwas developed to ensure that
performance is thoroughlytested by training
and testing modelson multiple partitions
of the data.The model logic is visualized
along with all the informationto understand model
performance and to providesegments of different risk.Also but then some
trends are visualizedto provide insights into key
indicators leading to bad debt.And then this kind of
level of informationis needed to give confidence
in the models and results thatare produced even more
importantly consideringthe self-training nature
of the modeling process.The last analytical component
is the optimization engine.And optimization, enables
the best allowancesfor customer or
subscriber segmentsin terms of optimizing
the revenue and bad debt.The engine consists
of two stages.The models that are built
using the automated frameworkare used to derive segments
of varying levels of bad debt.Then additional level of
the segmentation basedon other features is applied.And then the solution
upon new categories, thiswould be automatically used
within the segmentationand optimization process.And this stage also
includes calculationof the projected revenue,
bad debt across the segmentsand allowance scenarios.And for this, we have used a
custom implemented methodology.We have also implemented the
custom grid search algorithmto select the best
recommended allowancescenario across the segments.Due to the potential for a
significant number of segmentsand scenarios to
improve the scenarios,to improve the
computation time, wehave integrated the protection
of the revenue and optimizationprocedure into custom
methodology implementation.This also enabled to
generate the artifactsfor performing what-if
simulation at the same timewhile it's running the
optimization procedure.And lastly, the visualization
providing all outputfrom solution
components into a singleplace to enable an informed
decision through a setof actionable dashboards.And this includes
the visualizationof the logic of the
model performance,and performance of the models.Interpretation of the risk
segments, trends, and keyfactors leading to bad debt
also enables the projectto impact on bad
debt and revenuefrom using
newly-produced models.It gives said the
simulation functionalityto enable what-if
analysis of scenariosand comparing them
with optimal scenarioand the current portfolio view.And this provides
the transparencyto streamline the decision
making and having confidencein the analytical outcomes
generated by the solution.And this concludes the
walkthrough of the solution.And thank you for taking
time into learning about it.Now, let me hand it over back
to John for the final wrap up.John-- John, if
you wouldn't mindgiving us some final thoughts.JOHN PRENDERGAST: Great.Thanks a lot, Tomasz,
very insightful.I think it's really
interesting to seehow you can use this approach to
move along the entire analyticslifecycle from initial
assessment and problemqualification which can help
us see the size of the prizeto discovery where we
can build a business caseand estimate the
complexity and feasibilityof different approaches all
the way through to provingthe value and, of course,
then industrialization.I think coupled with agile
working and the utilizationof an analytics
pod team, so a teamof cross-skilled
team members, thiscan ensure that we can either
fail fast or continuallyprogress and build value.So a lot of what
you described thereresonates with me from
my own experiences,as I have used similar
methodologies and approachesfor other business problems.In fact, this can be applied
to other bad debt use cases,also to many other
fraud scenariossuch as the fraudulent use
of authorized push payments,also more recently and
very currently relevant,for identifying, monitoring, and
simulating coronavirus impacts,including assigning customers to
relevant bandings or categorieswith associated
treatment journeys,whether this be to
simulate the impactsof the sudden surge in digital
customers on fraud systems.Or the impact of the spike
in payment diversions,for example, in banking, the
increasing non-performing loanrates, et cetera, or
to assess and predictthe vulnerability across
different customersegments to the huge increase
in fraud scams and cyber attacksduring the pandemic.An approach such as what
you have shown, I think,can really help accelerate
getting insights in these areasand bring that all the
way through to the valuerealization of
industrialization.So in summary,
during this session,we have shown how by using
a simple SAS architecture,we can build a full deployment
of self-retraining modelsand optimization as a solution
to a particular bad debtoptimization-related problem.This model allows telco
organizations to effectivelyminimize losses due to
bad debt, reduce the costto manage that bad debt,
and increase overall ROIin this lucrative
revenue stream.By leveraging existing
SAS investment coupledwith taking this data-driven
analytics approach,an organization can
apply similar methodologyto accelerate to the
insights requiredto make decisions and address
many business problems.So that is the end
of this user session.I hope that you've
learned somethingor found this interesting.From myself and Tomasz,
many thanks for your time,and have a great
rest of the day.Thank you."
49,"Hello, and welcome to lesson
one of High-Performance DataManipulation with SAS DS2.In this lesson, we'll introduce
the basic DS2 concepts,we'll introduce the SAS
via cloud analytic servicewith its parallel
processing capability,and then we'll set up
our virtual lab image,so that we'll be ready to
do demos and practices laterin the course.So let's get to it.So just what is DS2?DS2 is a relatively new
programming languageof Base SAS.It has a syntax that combines
the best of the data stepprocess and language with SQL.DS2 is easily extensible,
and it providesa kind of object-oriented
approach to programming,if you desire, that can lead to
some advanced data processingtechniques.If you compare the two programs
that you see on the screen,you can clearly see the bones
of the traditional Base SAS DATAstep still exist in
the DS2 DATA program.DS2 does provide us
with some superpowers.And the first one is that DS2
can handle most of the ANSIdata types natively
without havingto have them down-converted
to double precision,numeric or fixed,
with character.In addition, DS2 provides
a simple mechanismfor extending the language.This means that you can write
your own methods for DS2,package them together,
and reuse them very easilyin subsequent programs.Now, the traditional Base SAS
DATA step is single threaded.And that means that it
reads from a single thread,and computes on a single
thread on your computer.Most modern computers have
multiple CPUs available,which means that they
were capable of handlingmultiple threads.And DS2 sees this
as an opportunityto process more than
one row in parallel.So on a traditional Base SAS
platform with no other softwareinstalled other
than SAS, you canuse DS2 to process more
than one row of datasimultaneously in
multiple compute threads.You will note that there is
still only a single read,write thread in this
particular mode.However, DS2 also has the
capability of porting itselfto other larger platforms.For example, there is a product
called the DS2 SAS In-DatabaseCode Accelerator,
which allows usto take our SAS code, the
DS2 code that we've written,and execute that threaded,
fully distributedacross a massively
parallel processingplatform such as Hadoop,
Teradata, or Pivotal Greenplum.This produces multiple
compute threads,and also multiple read, write
threads, dramatically improvingprocess throughput.But SAS Viya also provides a
massively parallel processingplatform with this twist, the
data is loaded once into RAMand then it can be processed
again and again in memorywithout having to
load it back to disk.This means that we do our
massively parallel processingon memory resident data,
and the throughput on thisis really impressive.Now, you write a DS2 program in
many ways, but in this class,we're going to use Base SAS
as our client, if you will,for writing DS2 programs.And to do that, we'll use
PROC DS2 in a traditional SASprogramming environment.DS2 programs can be executed
right on the SAS computeserver and single machine mode.In a single machine
mode you can stillhave multiple threads of
compute process going,but there will be a
single rewrite thread.This is called symmetric
multiple processing.In-database processing
allows us to take the codeand run it where the data
lives, rather than havingto bring the data
to the SAS platformas in a traditional SAS access.This allows us to run our code
on that massively parallelprocessing platform and
reap all of the benefitsof parallel compute
and parallel I/O.Using SAS Cloud
Analytic Services,we can do that same
MPP type processingon memory resident data,
improving the throughput evenfurther.Now, you may realize that the
SAS LIBNAME engines that weuse to access data
in SAS always convertthe data into fixed
width character,or double precision numeric.We'll be wanting
to avoid that whenwe're processing
in DS2, because DS2has the capability of handling
the rich data pallet thatare found in most
relational database systems.In order to accomplish this,
DS2 uses a separate driverto access its data.And this means that while
we have drivers writtenfor many data sources,
not every datasource that is
supported by SAS Accesshas a driver written for it yet.Now, the list continues to
grow with every maintenancerelease of DS2, so keep
your eye on the online docto see if yours is supported.And if your data source
is not support, of course,we have an ODBC-compliant
driver for accessingODBC-compliant databases that
don't have an individual driverof their own.So when should we use DS2?I like to call this the
shiny hammer syndrome.Once you find a shiny
hammer, a new hammer,everything starts
to look like a nail.But it's not true.You don't want to put
the hinges into your doorwith a shiny hammer, a
screwdriver is better.And for DS2, we want to
think of it this way,can my program
make good advantageof this parallel
processing capability?So, for instance, if I'm running
straight up on the Base SASplatform, on the
SAS compute server,does my data processing
have a lot of CPU involved?Because if the process has CPU
involved, even though there'sstill only a single
read, write thread,we could benefit from
parallel compute threads.Or perhaps my data resides
in Teradata, or Hadoop,or Greenplum, where I have
the SAS In-Database CodeAccelerator installed.In that case, it's almost
certain to improve throughputif I write my code in DS2.And if my data is
in CAS, the DS2is an excellent
method of providingcontrolled parallel
processing inthis massively parallel
environment on memory residentdata.It's not always about
performance though.For instance, if you're
calculating the USnational debt today, a
traditional double precisionnumeric does not have
enough digits of precisionto calculate the debt
to the nearest penny.If you're doing
accounting, you maywant to use DS2 just for the
accuracy that you can achieve,or the precision.And as you use DS2
more or more, youmay have accumulated
a large libraryof user defined modules.And with these
packages available,you can easily build
larger and more complexprograms in DS2 by simply
reusing the modules of codeyou've already written.In this case, computer
programmer efficiencymay be the key.But if your process doesn't
meet any of these criteria,it's unlikely to benefit from
being converted into DS2,and it may actually run slower.For instance, the traditional
Base SAS DATA stepis a very mature and
well optimized process.And if your code is I/O bound,
spreading that process in DS2will probably degrade
the performance,because the DATA step only has
to manage one thread while DS2will manage multiple
threads without the benefitof any speed on the compute
end, where DS2 will haveto manage multiple threads.And because the
compute threads are notthe bottleneck in
this case, performancewill actually be worse in that.For example, the
traditional SAS DATA stepis a mature technology
that is welloptimized for its environment.If the process that you're
using is not CPU boundbut is I/O bound,
instead, the DATA stepis more than likely
the most efficient wayof processing that.Using multiple DS2 compute
threads on an I/O bound processonly adds to the
complexity of havingto manage those multiple
threads without giving youany additional speed
in the bargain.It may actually reduce the
throughput of your data.Now, in this class,
some of our processingwill be done in SAS Viya on the
open, cloud-enabled, analyticrun-time environment we refer
to as the SAS Cloud AnalyticServices, or CAS.CAS provides an amazing
platform for dealingwith data that's
preloaded in the memoryand can stay in memory
while you process itwith subsequent steps
without having to beloaded to disk in between.This makes your processing
run extremely fast.And SAS Viya, or CAS,
includes technologythat allows you to
handle data of any sizeeven if it's larger
than the memoryyou have available to you.SAS Viya and SAS 9
are both vital partsof our SAS platform.SAS Viya makes a lot of data
sources available to youthrough this CAS.You can store those data
offline, on-premises,or in the cloud.The data can be structured,
or non-structured data.You can have SAS data sets.You can save your SAS data in
a special file called SASHDATon the server side, which makes
the loading into RAM extremelyfast also.Now, Cloud Analytic
Services can bearranged in a couple
of different formats.The first and most
powerful formatis the massively parallel
processing format.And this allows you to use
multiple machines in concerttogether to provide one
giant compute engine.As a general rule, there's
a server controller,and several server
worker machines,and on each of these
machines, for instance,a server controller would spawn
a session controller process,which would control worker
sessions on each of the workernodes.You're collected to
the session controllerfrom your client,
whatever that may be,in this class we're going to
be using SAS Studio to connectand talk to this, but you
could do this from Python,or many of the
other things, Java.And we'll submit our code
to the session controller.And a session controller
will distribute the workto the worker nodes where it
will be parallel processed.Another Configuration for CAS
is single machine configurationwe refer to as symmetric
multi-processing.In this case, the server
controller, and the sessioncontroller, with
its workers, alllive on the same physical
hardware machine,but they utilize multiple
CPUs on that machineto provide multi-threaded
processing.Once again, this is a lot faster
than a typical single-threadedprocess.So that you could use
many different programminginterfaces to get
to SAS Viya, youcould use SAS Studio, Enterprise
Guide, the SAS windowingenvironment, depending
on the version of 9.But there are many
other coding clientsthat can be used to run
code in Viya on CAS.No matter how you do this, you
have to start a CAS session.And then you'll
access a CAS librarythrough that CAS session.It's referred to as
the active library.And there's only one active
at a time, which you can shiftaround as much as you like.And then you load files into
memory in that CAS libraryand do your processing.You can process as
many steps as you want.You can choose to leave
them in memory tablesin memory, which is a
very safe procedure hereas the system is set
up to be redundant,or you can save those in
memory tables back to diskanytime you want, and
then, when you're done,you terminate your CAS session.In our classroom
environment, we'regoing to have
basically two machines,one is a client machine, which
is hosting Windows, your CASstudio, and all the other is
the SAS Viya environment, whichincludes the session
controller, anda symmetric multi-processing
version of CAS.Hello there.So now that we've completed
setting up our virtual labsfor the rest of the
class, let's go aheadand take a deep dive, or deeper
dive, into the DS2 itself.We'll take a look
at basic DS2 syntax,we'll compare DS2 data programs
to traditional data steps,and then we'll finally try
converting an existing DATAstep program into
a DS2 DATA program.And all that coming right up.Now, DS2 is a little
bit more structuredthan your traditional
Base SAS DATA step.Like most new
languages in SAS, DS2is implemented in a procedure.So we see the PROC
DS2 statement hereand the QUIT that
terminates processingthe code as if it was DS2.Within the DS2
procedure step, wehave program blocks that
define what type of programwe're writing.They all begin with a
keyword, and end with a endof that same keyword.For example, a PACKAGE program
begins with a PACKAGE statementand ends with an
ENDPACKAGE statement.A THREAD program begins
with a THREAD statementand ends with an
ENDTHREAD statement.A DATA program begins
with a DATA statementand ends with an
ENDDATA statement.You'll notice that each
one of the program blocksis followed by a RUN statement,
which is required syntax.PROC DS2 uses
RUN-group processing,which means that you can write
multiple programs before youquit the procedure, but every
program step that you writemust be followed by a RUN,
or it will not execute.So as we said before,
the DS2 DATA programbegins with a DATA
statement, and endswith an ENDDATA statement,
and requires a RUN to execute.Inside of a DATA program
there are methods.Method blocks are named
executable blocks of code.This method is called INIT.The method statement
names the method,and the end statement terminates
the method definition.In DS2, every
executable statementthat you write in your code must
be part of a method definition,or you will get a syntax error.Now, DS2 includes
some system methods,system methods are
executed automatically,and so are not executed
when you call their name.The INIT method runs
once at the startof your execution of this
DATA step, or DATA program.And the RUN method runs once for
every row of an input data set.And in this case, the banks
data set in the DS2_sas libraryhas three rows of data, that
means that the run methodcode will execute three times.And finally, there's
the term method.The TERM method executes
once and only onceat the end of your data
program processing.Only the RUN method
includes an implicit output.And so traditional
DATA step programmerswill be most comfortable
with the RUN method,as it operates almost exactly
like a traditional DATA stepprogram would.Now, because these methods
execute automatically,every DS2 data program must
include all three methods,but you don't have to write
code for every method.If you don't provide code for
a particular system method,the compiler inserts a
NULL method on your behalf.And that means, basically,
the method statement,defining the method, and an
end statement with nothingin between.This is required, because
the INIT method must be thereto start execution when the
data program starts execution.And the end statement
of the INIT method,not only terminates INIT
but it calls the RUN method.And the RUN method
needs to be there,because it's going to be
called by the INIT method.And it starts when
called by INIT.At the end, you get an implicit
OUTPUT, and an implicit RETURNto the top of the RUN method.And this will loop until
there's no more input data.And when it's all out of data,
that END statement in the RUNmethod will call TERM.Now, the TERM method has
to be there because it'sgoing to be called by RUN.It starts only when
called by the RUN method,executes the statements
contained within itonce and only once.And at END, it terminates
the DATA program processing.So you can see why all
methods are required.And for ease of
coding, the compilerwill supply you a NULL method
for any method you don'twrite code for in these three.Now, if you write a DS2 DATA
program that contains noneof these method, that
program cannot execute,because there's nothing
to start the process off.All right, to see if everybody
was paying attention,this is the INIT method, it
has the SET statement that'sreading the BANKS data set.We already said that BANKS
has three observations in it,so how many times would
that method be executed?Think about that.Remember that this
is the INIT method.And so the INIT method
executes once and only once.And SET statement will
execute, of course,and then it will read
in the first observationfrom the banks data set.But then, it will never execute
again, because this is INIT.Now, besides the
system methods, youcan write user-defined methods.And user-defined methods
can have arguments.Notice that in the
parentheses here,we have defined an argument, a
double precision numeric valuecalled TC that we'll use
as input to this method.And it can also return a value.You'll notice that
the definitionsays it's going to return
a double precision number.And then down here in the code,
the return statement actuallydoes that, it sends back the
computed Fahrenheit valueafter the method has
processed the input.Now, user-defined
methods only executewhen they're referenced by
name, but you can reference themas many times as you'd like.So in this DO loop, we are
calling the C to F methodseveral times.And you can see that it executed
once for each call of the DOloop routine.Now, we've talked about
executable statements,and we said how all
executable statements haveto be part of a method
definition, that leavesthe declarative statements.Well, what about declaratives?Well, declarative
statements in SAS,if you ever took the
programming one class,we usually talk about these as
the compile time statements,or ones that only matter
during compile time, right?And the declarative
statements areused to setup the process
before the processing starts.In DS2, there's two types
of declarative statement.There's a global declarative
that affects the entire dataprogram.And there is a local
declarative thatonly affects the method
in which it's defined.So any global
declarative statementsmust be in the DATA program
before the very first methodstatement, before you
start defining any methods.And if they're
located anywhere else,you'll get a syntax error.For local declaratives,
they must be in the methodbefore the first executable
statement is seen, if not,you'll get a syntax error.And to be honest
with you, these twoare what we call a
DECLARE statement.And within a method, the only
declarative statement allowedis the DECLARE statement itself.So let's take a quicker, or
a deeper look at the declarestatement.The declare statement
allows us to declarevariables, or temporary arrays.The DECLARE statement assigns
a data type, a length,and any other attributes
that we care to assign.So you can see that
the DECLARE statementcan be written out as DECLARE,
or abbreviated as DCL.Being a man with very fat
fingers, as far as typingis concerned, I prefer the DCL.And notice that the very
next thing you have to sayis the data type.Those of us who are
old-time SQL programmersare going to have a little
trouble with this at first.We're used to saying the
name of the variable followedby the data type.But in DS2, the data
type comes first,because you can then follow it
by a name of several variables,not just one that
you want to createwith that single
declare statement.Optionally, you can
add a HAVING clause.And the HAVING clause allows you
to apply labels, and formats,and informats to the
variables declaredin that particular
DECLARE statement.Here are some examples.A DECLARE statement that
declares three double precisionnumeric variables would be
the same as a SAS numeric,if you will.And using the HAVING clause,
we've applied the dollar12.2format to all three
of those variables.Here's a different
declare statement,declaring a
high-precision fixed pointnumeric value, a decimal type,
with 35 digits of precision,5 reserved for the
decimal portion.And we're only making
one of these called VAR1.And finally, a DECLARE statement
for a fixed-width charactervariable, char 25, named
VAR1, with a label, my text.The DECLARE statement is
used to declare variables,or temporary arrays, and
assigns the type, length,and attributes.So a DECLARE statement used in
the global declarative spacedeclares a variable
with global scope.A DECLARE statement
within a METHOD definitiondeclares a variable
with local scope.Although I don't
highly recommenddoing this in your
actual program,you'll notice that we have
two variables called MyVar.One is a double precision
floating point, numeric,the other is a fixed-width
character variable.In a traditional
DATA step program,this would be impossible.But in DS2, because
this one is global,available to other methods, and
this is local available onlyto this methods, these two
variables with the same namecan peacefully coexist.And then there's the problem
with undeclared variables.Now, what is an
undeclared variable?Well, it's neither read
in via a SET statementnor defined in a
DECLARE statement.That makes an
undeclared variable.Undeclared variables by
default have global scope,and they produce
warnings in the SAS log.If the program cannot tell
from the context what typeof variable it should be, it
will make it a double precisionnumeric by default, just as it
would have in a Base SAS DATAstep.If the context
indicates that thereshould be a character
variable, itwill be fixed-width character.Again, much like would have
happened in a Base SAS DATAstep.Local variables are declared
local to the INIT method here,and are available only to INIT.Global declared variables
and undeclared variablesboth have global scope.And that means that the
values for those variableswill be available anywhere
in the DATA program.You can see an
undeclared variable here,and a declared
global variable here.Locally declared variables are
available only to the methodin which they are declared.And so they are local in scope.This has other implications.Global variables will appear
in the Program DATA Vector,and therefore, by
default, they'llbe included in the results set.Local variables are never
in the Program DATA Vector,and so are never included
in any results that areproduced by a DS2 DATA program.And as we have
previously noted, thereare many statements in DS2 that
have the same function as theydid in the BASE SAS DATA step.For example, you can
use a SET statementto read in one or more data sets
into your DATA program in DS2.Well, the BY statement,
By-group processing,works very much like it did
in traditional DATA step,including First.Last.processing.DO groups and DO loops work like
they did in the base SAS DATAstep, including the DO I= with a
discrete list of values, which,as far as I know, is
unique to SAS programming.Loop flow control statements
like CONTINUE, LEAVE, and END.And all the program flow
statements like RETURN, OUTPUT,GOTO, STOP, all work
the way you wouldexpect them to work in DS2.Declarative statements
like DROP, KEEP, and RETAINalso work like they did in the
traditional Base SAS DATA step,but remember that declarative
statements like these in DS2are global statements,
and must come before youbegin your method definitions.Now, one thing that looks
a lot the same, but workssignificantly differently is
the MERGE statement in DS2.Like in the Base
SAS DATA step, youcan merge two or more tables
using a MERGE statement.But unlike a base
SAS DATA step, youdon't need to sort
the tables first.And also, unlike a
traditional DATA step,the BY statement
is required in DS2.The lack of a BY statement
in a merge, situationwill result in the syntax error.Now, initially,
the MERGE statementworks significantly
different from DATA step.It also worked different
from an SQL outer join.Recent additions
to the DS2 languagehave included the RETAIN option
for the DS2 MERGE statementthat makes it work just
like the DATA step MERGEstatement worked.In this demonstration,
we're goingto be taking a look
at how the DS2 MERGEstatement works versus a
traditional DATA step MERGE.We're also going to compare
it to an SQL full outer join,just so you can see the
difference between that,the DATA step, and
the DATA program.In order to do this, we're
going to use these fourtemporary tables.You notice that
table A and table Bhave unique values for
the rows in the ID column.So joining table A to table
B will result in a oneto one type merge.Table C has multiple rows
for the ID, number two.So joining either A
or B to the table Cwould produce a one to
many type situation.And finally, table
D has multiple rowsfor a couple of the IDs.And, therefore, joining table
C and D, or merging them,will produce a many to
many type situation.So let's go back into the
code, and take a quick lookat how we might do this.The first one is
DATA step merge.We're going to do a one to one
MERGE using tables A and B.We're using the IN= variables,
as we learned in programmingtwo, so that we can tell whether
there is a MERGE for thisparticular row of data or not.And rather than selecting
just those rows that merge,I'm going to actually write
the values of those variablesinto two new variables
called IN_A and B,so we can see how the
IN= variables are workingin the DATA step MERGE.We're going to do the
same thing in DS2.I'm going to do a MERGE on
the IN_A, B tables by ID.I'm going to write out the
values of the IN variables.And when we're done,
I'm going to dothat again using the RETAIN
option on the MERGE statement.This is the one that is intended
to make the DS2 MERGE work justlike a DATA step MERGE.I'm going to go
ahead and run those.And we'll see that we
ran without any errors.Back in the code, I'm going to
use PROC FSQL to do reporting.I'm going to report on a DATA
step MERGE, and the DS2 MERGE,but I'm also going to conduct an
SQL FULL JOIN at the same time,so that in the report, we'll
see the results for the twodifferent types of merges,
and for the SQL JOIN.So we're using
the DATA step kindof as our baseline
comparison to seehow everything else compares.You'll see that
the SQL FULL JOIN,when we try to mimic
the IN_A and IN_B,we get pretty much
the same resultas we did with the DATA step.And the truth is that in
DS2, the default MERGEproduced the same exact
output as the DATA step.So on a one to one
situation, all threeof these processes
produce the same result.If we go back into this
and try this with a oneto many situation,
that's A and C,we're going to run basically
the same programs doingthe merge for DS2, SQL,
and Base SAS DATA step.We'll take a look at the
comparison of those results.And here we'll see that the
DATA step, and the SQL FULLJOIN, again, produced
the same result set.So a one to many, same
result, but DS2 did not.And here's where the DS2
MERGE starts to diverge.You can see that
for the BY groups,even though there's a
match, the value for table Ais not carried down.So unless there's a second row
involved for both data sets,then the value
from the table thatdidn't have an additional
row of data in it,that value does not copy down.That produces quite a
different result setthan we might have expected.Now, we can go back into DS2 and
run this in a one to many MERGEwith a RETAIN option,
and run that same report.And when we do that, we should
see that this makes the resultsthe same with the RETAIN
as they were in DATAstep, and in the SQL FULL JOIN.So that RETAIN
option is importantif you want the output to
be similar to the output youwould have got in a
traditional base SAS DATA step.And finally, there's the
many to many situation.Now, if you all remember,
in a many to many situation,this is where SQL stops
producing the same result setas a traditional
Base SAS DATA step,because of the Cartesian way
that SQL approaches joins.So in a many to many
situation, thereare multiple outputs
for those rows thatmatched, where in
a DATA step there'sonly one row for the
two rows that match.In DS2, with the RETAIN
option, we get the same outputas the DATA step did.So once again, that
RETAIN option is critical.If you use RETAIN, and
you run a DS2 MERGE,you'll get the same output as a
traditional base SAS DATA step.And finally, just to show
you that without the retainthings are radically
different, youcan see that the IN variables
are not working the same.And, again, that copy down
isn't working the same.And this default output
matches neither the DATA stepnor the SQL FULL JOIN.So things to carry
away from thisis it's important to use the
RETAIN on the MERGE statementin DS2 if you want to get
the same result as youdid in a traditional
Base SAS DATA step.I have one more thing
I'd like to show you.And that is, what
happens to the rowswithin our data that are not
part of the BY statement?So I'm going to produce
another couple of tablesthat we're going
to use in a Merge.We're going to go
ahead and mergethese things with a retain
option in DS2 and DATA step.And then we'll look at the
source tables and the resultsof those merges as we go out.And over here you can see
that these tables havea many to many situation
involved with them,but within the rows
of the original table,say, for all the
items in ID 2 group,they were in order,
one, two, three.And for the items
in the three group,they were in order,
one, two, three.And in the table F, within their
group, they were in order A,B, C, and so on.In a traditional
Base SAS DATA step,the row order is maintained
when you do this merge.And so you can see
that within the groups,the order one, two, three is
maintained as well as the A, B,C. And because there was not
three rows of data for the IDnumber three, the
B is the value thatgot copied down to the last
row of the three group,for example.In DS2 with Retain,
there is no guaranteefor row order other
than the ID variablesthat you listed for
the BY statement.So you'll notice that when
DS2 retrieved your data to dothe merge for the threes, the
order of that second column,TableE was not even
looked at, we don't care.And in this case, they came out
in a different order, as wellas the rows of table
F. You'll noticethat this one, the last
of the two to read inwas an A, so the A got carried
down for the second one.And this is an
important thing to note.It's a difference in the way
that DS2 acquires its data.And there is no
option to force DS2to look at variables
not on the BY statementwhen it's doing a
MERGE, so I justwant you to be aware of that.And that's what we have
for you on the DS2 MERGE.So this brings us
to the point whereI'd like to discuss what
I call the Subtle SevenDissimilarities
between a DS2 DATAprogram and a traditional
BASE SAS DATA step.And the first one is that
all executable code mustbe part of a METHOD definition.Data sets are not
overwritten by default.You'll need the overwrite=
option to make that work.New variables are
expected to be declared,or you're going
to get a warning.And remember that
undeclared variablesare global in scope become part
of the Program DATA Vector.DS2 MERGE requires
a RETAIN optionto produce results that are
similar to the DATA step merge,but you don't have to sort your
data before you do your MERGE.In DS2, the PUT
statement doesn'thave all those fancy
line and column controls,so you can use format
to position thingsas you would when you
write stuff to the log,but don't expect the @ signs
and slashes to work as they didin a traditional DS2 DATA step.And keywords are
reserved words in DS2.It doesn't mean you can't name
your DATA set DATA anymore,it just means that
you'll have to indicateit's an identifier
by surrounding itwith double quotes.Which brings us to number seven,
that ANSI quoting standard,right?That double quotes indicate
the name of something,or an identifier,
and single quotesare used to delineate
constant text.And as a result, we
have a little troublewhen we're resolving macro
variables into DS2 codeas a text literal.So we use that %Tslit macro
function to resolve the textand then put real
single quotes around it,so that it resolves properly and
works properly in our DS2 code.You'll notice several,
less subtle differencesbetween traditional Base
SAS DATA step and DS2 dataprograms.And the more salient
differences include the factthat you'll find several
functions and statementsavailable to you in a Base SAS
DATA step that are not foundin the documentation for DS2.And similarly, you'll
find several functionsand statements in DS2 that don't
have a DATA step counterpart.So the truth is that most of the
functionality of the DATA stephas been retained in DS2, but
the syntax could be differentbecause it's been
implemented differently.So some of the first
things to addressare some of the functions
that we use a lot.If you're used to generating
random numbers in a Base SASDATA step using any of
these RAND functions,as of SAS 9 and
4, those functionsare no longer supported in
DS2, just the RAND function.And the RAND
function can generateall of those
distributions and moreusing a much more
sophisticated-- a random numbergenerator.So that's the one
that's supported in DS2.If you're a fan of regular
expressions like me,you're used to using the
PRX functions in DATA stepto manipulate funky text.These functions were in the
initial implementation of DS2,but are being deprecated
as of SAS 9, 4, and 5.And the reason for that
is the DS2 geniuseshave produced two new
packages, the PCRXFIND packageand PCRXREPLACE package
that ship with DS2,and they provide all
the functionalityyou'll need for finding,
matching, and doingreplacements of
patterns of text,but they work almost
10 times as fastas the original functions did.So use the PCRXFIND and
PCRXREPLACE packages instead.And finally, for those of us
who are missing the DIF and LAGfunctions in DS2, they
were added to the languagein the M4 release of DS2.But there are still things
that are not there at all.So the WHERE
statement, when I firsttried to write a WHERE statement
in DS2, and it didn't work,I almost freak right out.Interestingly enough, we have
a fascinating replacementfor this in DS2.Now, notice here that rather
than write a WHERE statement,you can actually write in curly
braces a complete FedSQL query.And that query will
execute, includingany of the clauses that
you wish to contain,and the result set is
processed directly in DS2without having to be
written to disk in between.You can do things
like joins, grouping,anything you can do with
a select type query,you can do here on
a SET statement,and pass the results directly
to the DATA step type languageof DS2 for further processing.That's a pretty
potent superpower.In DS2, I'm used to using
UPDATE and MODIFY statementsto modify values of an
existing data set with valuesfrom a different data set.If you're an ANSI
SQL user, you mayrecognize the fact that's a very
difficult thing to do in SQL.Generally, it involves
a correlated sub-query,which is horribly inefficient
as far as resources go.And in SAS, this can be
even more of a problemas one of the data
sets may resideon a platform that's
different from the datathat you're using
for the update.For example, your
update values maybe coming from an
Excel spreadsheet,and being produced
in a Teradata table.This has caused us to do all
sorts of interesting thingsin the past, like read
in the Excel spreadsheet,then push it up into the
next Teradata table, thendo the join in Teradata.So we don't have
to do that in DS2.We have a thing called
an SQL STATEMENT package.And I don't want you to get
hung up on the syntax righthere, because we're
going to spend quitesome time with this later on.But basically, using the
SQL STATEMENT package,we can read in data into
the Program DATA Vectorhere, and then just
execute an UPDATE statementspecifically for each of those
records in the other database.And this avoids all that cross
library overhead, and does notrequire any kind of a
correlated sub-query,so it runs really, really fast.Other statements
that are missingare the ATTRIB, LABEL, LENGTH,
FORMAT, INFORMAT statements.All of these are
designed to modifythe metadata of variables that
we're processing in a DATA stepin DS2.We have none of those, but we
do have the DECLARE statement.Remember, the
DECLARE statement canuse the HAVING clause to change
formats, informats, labels,that type of thing.And so while the LENGTH, LABELS,
ATTRIBUTES, FORMAT, INFORMATstatements all have
differing syntaxes, in DS2we only have one statement,
the DECLARE statement,and the syntax is
always the same.The next thing I
discovered missing in DS2was the ARRAY statement.I use ARRAYS extensively in
my DATA step programming.And I was very
disappointed to thinkthat I would not have ARRAYS
available to me in DS2,but it's not true.If you think about
things, in Base SAS,we do two significantly
different thingswith the same statement.This ARRAY is an ARRAY
of PDV variables.It's pointing at a series of
variables in the program DATAvector named N1,
N2, N3, N4, so on.Now, this ARRAY is
a temporary ARRAY,and it's not associated with
program DATA vectors at all.It's just using system space to
store some information for usin an ARRAY kind of format.In DS2, we approach this with
two different approaches.The first one, ARRAY
of PDV variablesis implemented in DS2
with a VARARRAY statement.Notice that, really,
the only differencebetween this and
the ARRAY statementwas the requirement to
specify a data type.So VARARRAY DOUBLE and then N5.This will produce a series of
variables in the program DATAvector, N1, 2, 3, 4, 5.And it'll work just
like an ARRAY reworkedin a traditional DATA step.For the temporary ARRAY, those
ARRAYS that have elementsnot at the program DATA vector,
we had to specify the type.If you remember,
when we did this,and we had to specify
TEMPORARY, and then the systemtook care of putting that
out in the system space.In DS2, you just declare a
multidimensional structure.So here I'm declaring the C
ARRAY as a five element onedimensional ARRAY.And it's character
one, and I getfive elements stored
in system space,just like a temporary ARRAY
in the traditional DATA step.And like the temporary ARRAY
in the traditional DATAstep, whose elements did not
appear in the program DATAvector, because they are not
associated with variables,ARRAYS declared with a
DECLARE class statement,whether they're global
or local in DS2,their elements are never
associated with PDV variables,so they don't appear
in a PDV either.And that's quite
handy when you'reusing ARRAYS as look-up tables.Now, there are some things
that are missing from the DATAstep that are missing in DS2.And so these are all statements
associated with processing textfiles.And as of this course
writing, DS2 stillonly reads and
writes from tables.So it does not read text files.And that's why those
statements are notin the DS2 documentation.Then there's another
series of statementsthat we use in Base SAS to
control the SAS session itself.Rather than the program, we're
feeding back, and controllingthe SAS session.We can actually turn
the SAS session off,affect the color of the log
with DISPLAY, MANAGER commands,those types of things.We can also execute operating
system commands from DS2.DS2 actually runs in a separate
process from the SAS processthat spawned it.And as a result,
it gets informationfrom the SAS system, and
it sends back your log.But it executes in a
completely separate process,so it doesn't have any hooks
to control the SAS session thatspawned it, or to interact
with the operating systemthat SAS is operating in.We're going to see later on that
the DS2 that the SAS startedoff, might not even be
running on the same platform,it might be running
up in CAS, itmight be running in
Hadoop, or in Teradata.And so I don't expect to see any
of this type of functionalityever built into DS2.Hi, it's Mark again.In lesson three of
High-Performance DataManipulation with SAS
DS2, we'll take a lookat program structure
in more detail.Also, we'll take a deeper dive
into the different data typesthat DS2 can process.And finally, we'll look at some
interesting functions that canbe useful on our DS2 programs.So let's get going.Before we dive too deeply into
the program blocks themselves,I'd like to talk about a
particular global statementfor the DS2 procedure.Now, DS2 procedure supports
very few global statements thatgo outside of program blocks.And they're not
global in the natureof the way we think about a
global statement in the SASsystem, for instance.A global statement in PROC DS2
goes before a program block,affects the behavior of
processing for that block,and that block alone.And as soon as the
subsequent program blockhas finished
processing, the optiongoes back to its
default behavior.So we're going to look
at DS2 options, whichallow us to change some things
that make it helpful when we'reprocessing in DS2 for overcoming
things like division by zero,and for troubleshooting.So the first one
is the DIVBYZERO=.Now, the default behavior
for DIVBYZERO is an error.In the old days, a
traditional SAS DATA step,when you were
running your program,and everything was fine,
and then all of a suddena variable came into play
with a zero in it thatwas the divisor in a
problem, the programused to throw an error
and stop processing.We called these data errors.In subsequent versions of SAS,
they throw a note in the log,but they continue to process,
just produce a missing value.Now, DS2 is very ANSI compliant.And ANSI systems don't
allow division by zero.And so normally, if you
get into a situationwhere the divisor is zero, then
DS2 is going to throw an errorand stop processing.If you wish to make this more
like a traditional SAS DATAstep, then you can set DS2
option DIVBYZERO=IGNORE,in which case you won't
even get a note in the log,a null or missing
value will be produced,and processing will continue.But the one that's most
useful is type one.As you can imagine, we'd
said that DS2 supported17 different data types.And those data
types are invariablygoing to be used in
multiple expressions thatcause data type conversions
to happen automaticallyin the background all the time.If we run a log error,
or note, every timewe did an automatic data
type conversion for you,your log would
fill up so quickly,you wouldn't be able
to see anything else.So by default, automatic
data type conversionsare ignored by DS2, no note or
warning is written in the log,processing just continues.But there'll be times when the
automatic conversion causesissues with the way that your
program is calculating results.And you'll be
wanting to see that.So you can set this DS2
option TYPEWARN rightbefore you run that
particular step.And then every time an
automatic conversion happens,you do get a warning
in a log, and thatcan help you narrow down where
the problem is occurring.Now, we'll move into the
program blocks themselves.And in the program block, we
have that global declarationspace that goes right before the
first method definition starts.These statements
are not valid insideof the method themselves, and
they include RETAIN, DROP,KEEP, RENAME, and VARARRAY.The next thing we'll
talk about is data types.And we said that DS2 could
define and manipulateabout 17 different ANSI
data types, straight up,without any type of
data conversion beingrequired from the data source.This is true.And you can create those data
types right in a DS2 DATAstep also, but where you save
the output, or the result set,to will control what
you can actually write.So for instance, we may
be running a DS2 DATAprogram in a pure
SAS environment,and we have decimal
data types, and VARCHAR,and all those fancy ANSI
types as we process themin a program DATA vector.But if we are writing that
result set out to a SAS 9 DATAset, then before they
can be written to disk,all of those data
types will haveto be down-converted
by the engineto fixed-width character,
or double precision numeric,because that's all that a
SAS 9 DATA set can store.Now, for example, in Hadoop, if
we're processing Hadoop data,Hive doesn't have an
ANSI time data type.So if you're storing date,
time, or date values,you'll get ANSI date, and ANSI
timestamp values in Hadoop,but if you store a time
value, it comes outVARCHAR on the Hadoop side.And many databases have
differences like this.For example, Oracle stores
date, time, and timestamp value,it stores them all as timestamp.So while we can
process them, that'sfine until you write them up.And then what can
be written out isdetermined by the target data
source that you're writing to.First, let's talk about
character data types.DS2 can define and manipulate
four different character types.This CHAR and NCHAR
are both fixed-width.And the VARCHAR and NVARCHAR are
both variable-length per row.In all cases, the N
indicates the numberof characters maximum that can
be stored in the column whenit's defined.In CHAR and VARCHAR, it's
one bite per character.But for unicode
characters, particularlyare used in languages that
use one character per word,for example, like
Chinese, or Japanese,the codes are much more complex,
and there's a lot more of them.So generally it's 2 to 4
bytes per character code.So N still defines the number
of characters to store,but it may result in
a larger byte on thisthan you're used to.However, DS2 can handle CHAR
and NCHAR in the same session,no matter what
session encoding is.Interestingly enough, we've just
talked about character data,we've already talked
about more data typesthan a traditional SAS
DATA set could handle.All told.Next data type I'll
like to talk aboutis floating-point numeric.Now, floating-point numeric
data types store assignedbut approximate value
with a floating point.These come in
basically two flavors,with a FLOAT kind
of and in between.Double precision numeric is
the standard SAS numeric value,and in DS2, it's
the standard valuefor an undeclared variable that
you can't tell what type itis from the context.Double precision
floating-point valuesare stored in eight bytes.Now, this means
that you're goingto get a maximum of about
16 digits of precision.Now, the magnitude you
could store is enormous,but the precision will
top out about 16 digits.A smaller version
of this, the REAL,is only 4 bytes, which tops
out at about seven digitsof precision.And then there's FLOAT.Basically, you tell the FLOAT
what precision you need,and it'll store the result in
either four or eight bytes,or either DOUBLE or REAL.Now, DS2 has a different
type of numericinvolved with fractions.This is a DECIMAL data type.This is an exact
fractional type.Whereas the floating-point
data types onlypromise you an approximation
of the number that you stored,the decimal data type
promises you the exact numberthat you stored.Of course, under
the covers, it'sgoing to use more
bytes than the eightbytes in your double
precision numeric,if you're storing some
very high precision data.You can have up to 52
digits of precisionoverall in a decimal number.When you think about this, this
is significantly more precisionthan you could have got with
a floating-point number, only16 digits.But because you're limited
to 52 digits total,you're going to see
that the magnitudelimit on a decimal number
is much lower than youcan get with a floating-point.I like to say that scientists
and astronomers lovefloating-point
numerics, because youcan have numbers of
enormous magnitude stored,and they rarely need more
than 16 digits of precision,whereas decimal data types
are loved by accountants,who like everything down
to the nearest penny,and don't want any of
this approximate stuff.But wait, there's more.There's integer numeric types.These store numbers, but have
no room to store a fraction.And as a result, they
store it in fewer bytesthan a similarly
sized floating-point,or decimal type.These range in size all the
way from a TINYINT, whichis only one byte,
and can only storea number between a negative
128, and a positive 127,all the way up to a BIGINT,
which can store up to 19 digitsmax.INTEGER types always
return an exact valueif the number stored in
them falls within the rangethat they're capable of storing.In ANSI systems, we also have
DATE, TIME, and TIMESTAMPdata types.These should never be confused
with SAS dates, times,and datetime values.In SAS, all of those values
are stored as double precisionfloating-point numerics.And the only thing
that distinguishes themfrom any other number is
the format that you apply.So if you have the number 365,
that could be my weekly rent,it could be the number
of days in a year,it could actually
be a date, right?And we wouldn't know unless
the appropriate formatwas applied to that number.In ANSI systems, a date
can only hold a date.It can't hold anything else.And a time can only hold a time.And a timestamp,
only a timestamp.And so they're not
related in that way.And finally, DS2 supports
two binary data types.Binary types are normally
used to store thingslike videos, and images, and
sometimes binary flag typedata.These come in two types,
fixed-length binary data,where N is the total
number of bytesyou're going to make available,
or VARBINARY binary, whichis variable per row, where N
is the maximum number of bytesthat can be stored in
that particular column.In a VARBINARY
column, of course,only the actual
number of bytes neededto store the value you
store is used for each row.So for a quick review of
traditional numeric valueprocessing in BASE SAS.In a traditional
Base SAS DATA stepall numeric values are double
precision floating-pointnumerics, this
means that they areassigned approximate values.They use a maximum of 8
bytes, and provide a maxof 16 digits of precision.You can see this when we try
to stick a very large numberinto a SAS numeric,
the digits all come outthe same up to about
the 16th place,and then when you
retrieve the number,the values are kind
of randomly different.In DS2, when you write
a numeric constant,because the default
for any numeric typeis double precision,
when we writea numeric constant
in our code, itcompiles as double
precision numeric.We can see this when we
print the values out,because once again,
we're getting that--pretty much a maximum of
16 digits of precision.Now, the constants get
their data type assignedwhen you compile your code.So we need some way of
flagging this numberand telling the
compiler, look, don'tcompile this
constant, which isn'tgoing to change
during the processingas a double precision numeric.I want this compiled
as a decimal value,so I can have that higher
precision if I want it.And in DS2, this is done by
following the constant valuewith an N-- no quotes, or
anything required-- justfollow the constant
value with an N.And that will be compiled
as a decimal value insteadof double precision.And that can resolve the issue.Now you can see that we
could put these largernumbers into our value,
and because we'vedefined the
variables as decimal,and we've defined the
constant as decimal,the value is stored
precisely as we wrote it.Now, if you write a numeric
constant that does nothave a fractional
value, it compilesas bigint by default, which is
a nice safe way of compiling it,unless, of course,
the integer constanthas more than 19
significant digits.In which case, we need
to think about that.How would you compile an
integer constant with morethan 19 significant
digits without losingany of that precision?Well, remember that when
we define a decimal value,we can tell it how many
digits of precision we need.And we don't have to reserve
any of those for the scale,or fractional portion.So if you just use the N suffix
and that large constant value,it will compile
as a decimal valuewithout any fractional portion.And you can store
that up to 52 digitswithout losing precision.Next thing is we
went to great lengthsto explain how date, time,
and timestamp values in ANSIsystems are significantly
different than SAS date, time,and date time values.So how do you write a constant
for an ANSI date, time,or timestamp?Well, you simply prefix the
value with a word, date, time,or a timestamp, so that
the compiler has a clue,and then you follow
that immediatelywith a quoted string.And the values have to
be precisely formattedin this way.A date is always a four digit
year, a dash, a two digitmonth, a dash, and
a two digit day.Time is always formatted
as a two digit hourusing a 24 hour clock, a colon,
a two digit minute, a colon,and two digit seconds.You can add a decimal point and
up to five digits of fraction,if you wish, to a time value.Timestamps are
basically a combinationof the two formats
above with a spacein between them, the date, in
the ANSI date format, a space,and the time in the
ANSI time format.And once again, if you
have fractional seconds,you can add a decimal and up
to five digits of fractionfor your seconds.You could see that when
we write them this way,they come out perfectly well
understood in our DS2 program.So let's have a look at how
this compiling of constantscan affect the outcomes in
our SAS programs in waysthat we might not anticipate
as traditional DATA stepprogrammers.Right here I have a
typical DATA NULL step.I have declared three decimal
variables, A, B, and C.I have assigned values to them.And I'm printing them
out in an attemptto see how well the
values were stored,or how close the stored
values were to the valuesthat I put in this constant.And when I run this code,
as you might have expected,because the constants were
not defined as decimal,they compiled as a
floating-point numeric.And as a result, we only got
pretty close to 16 digitsof precision.And after that, everything
kind of went haywire.So you might think,
well, we'll just fix thisby adding the numeric
indicator here,compiling them all as
decimal, and then that shouldresolve the problem.And when we go ahead
and rerun this code,we can quickly see
that it didn't resolvethe problem as expected.Now, this is concerning, right?We've got a defined
decimal variable,we have a defined decimal
value that we stuck into it,and we have kind of
every right to expectthat the value
stored there wouldbe a decimal type
with full precision.So somewhere in here,
and automatic conversionis going on, that I'm not
noticing, that I don't detect,and it's causing me to lose
the precision inadvertently.We discussed earlier that
during automatic conversion,DS2 doesn't automatically
put anything in the log.So we could add our
DS2 options, TYPEWARN.And this will help reveal where
the automatic conversion isoccurring.When we look at
this, it says, look,you're getting implicit
conversion of decimalto double in statements
91, 92, and 93.And if you look at those, those
are the put statements thatare writing the values out.Say, well, how could
a put statementbe converting my value?Well, if we look at the
code, we notice that wehave specified a format.Now, this is a thing
that you might notthink of as a programmer,
we're used to applying formatsat will whenever we want.But if you think about what
is a format in Base SAS,a format is instructions
on how to display a number.You say, well, what
kind of a number?We've never had to
ask that questionbefore in traditional DATA
step programming, right?But what kind of a number?Well, a Base SAS numeric.What are Base SAS numerics?They're double
precision numbers.So before I can apply
this format to the value,I'm going to have to convert
it to double precision.So now I'm not
losing the precisionwhen I assign the
value to the variable,I'm losing the precision when
I display the value in the log.If this rationale
holds true, I shouldbe able to go back to my
code, remove the formats,and then rerun that code.And without the formats, it
should print the actual valuethat I expected.So let's test my theory.And voila.Notice that even though I left
the TYPEWARN global statementfunctional, I ran
it again, thereare no more warnings about
automatic data type conversion.And we can see that
the values storedin the variables actually
were the values we expected.The down-conversion was
happening during the displayof the variable.So that's kind of
one unexpected resultthat we can get when
dealing with multiple datatypes for numerics in DS2.All right, we talked about
how we can manipulateall sorts of different
types of data,but when we store the data, it
might be something different.And we talked about
thinking about whatwould happen if I use a numeric
value in a character context,and in all of that
type of thing.So what we're going
to do is we'regoing to do some
manipulations with data in DS2to show you that it's capable
of dealing with all these datatypes and handling most
automatic conversions prettyrationally.So I've declared three numeric
variables, DECIMAL, DOUBLE,and REAL.And I've declared three
character variablesto hold the actual
values in there,so we can compare the
expected to the actual.And as I assign
those values, then Iwrite them out in the log.And hopefully, because
all of the valuesare within the expected
capacity of the data typethat we're using, everything
should come out as expected.And this means that,
even when I assigneddecimal types to a real value,
it all worked out as expected.As long as you keep the
value within the rangethat the data type can
handles, you pretty muchget the expected results.And the last thing
we said, well, wecan handle these data types in
Pure Data Manipulation in DS2,but when we write them out
to a data store somewhere,the data store may or may not
be able to handle those datatypes.So here we're going
to take and writethese out to two different
data sets, one SAS DATAset, and one Oracle DATA table.We're going to declare those
same values, REAL, DOUBLE,and DECIMAL.We're going to write
them out concurrentlyto both of those
output data types.And then when we're done, we're
going to take a look at the logand see from the notes
what actually happened.And you can see here that,
although the data was DECIMAL,DOUBLE, and REAL
during the processing,as indicated by the
notes that we've printed,when we wrote this data
out to a SAS DATA set,the base driver said, hey, we
can't store real data in a SASDATA set, it has to be
converted to double precision.Well, that's nice,
because that's actuallyhigher precision than the
real value was, right?Conversely, the decimal
column can't be storedin a SAS DATA set either.And that was stored out
as a double precision.And in that case,
some of the precisionwould have been lost
during the conversion.All right, if we look
at the data types,we can see the actual
values of the data typeby reading the information
from the database tableand compare that to the actual
value in the SAS data type--making a little report--we see that the
Oracle table values,the desired and actual
values were all the same.But as we had suspected,
the real valuebeing up-converted to double in
SAS didn't cause any problems.But the decimal version
being basically downgradedto a double precision
numeric causeda loss of precision right
around the 16th digit.And that, in a nutshell,
covers numeric precisionin manipulation and
in storage in DS2.So as we discussed, because
SAS dates, times, and timestampvalues are stored as
floating-point numbers,they don't look like
anything specialif you just look at them
the way they're stored.So how do we know what date,
time, or datetime valuethis might represent in SAS?Well, in SAS we apply a format.And the format makes the
floating point value looklike a date, a time,
or a datetime valuethat's human readable for us.When we store these values
out in a relational database,in a traditional
SAS environment,something has to
happen, because youcan't store SAS dates,
basically, out in the databaseand have them make any sense.In the database,
you have ANSI dates.So when you read ANSI date,
time, or timestamp valuesinto SAS from a
LIBNAME engine, theyare converted to the correct
floating point numeric value,and assigned an appropriate
format automatically.And when you save
data to a database,if you have a floating point
numeric value, a SAS numeric,that has a date, time, or
datetime format applied to it,when SAS writes that value
back out to the database,it automatically converts it
to the ANSI date, ANSI time,or ANSI timestamp value that
corresponds to that SAS datenumeric.And this has to happen,
because SAS DATA stephas no way of processing
these ANSI date, time,or timestamp values.It needs the floating
point numeric to process,but the floating point number
would make no sense at allas a date, time,
or timestamp valuein the relational database.Now, in DS2 programs,
we're capable of processingANSI and SAS values
at the same time.So when we read an
ANSI date, time,our timestamp values from
a relational database,we read them straight
up as the ANSI type.DS2 is perfectly capable of
understanding and manipulatingthat type.When we read floating point
numeric values from SAS DATAsets that have a date, time,
or datetime format attachedto them, much like
writing that valueinto a relational
database, DS2 automaticallyconverts those values into
ANSI date, time, and timestampvalues.So if you read in
a SAS date, time,or datetime
formatted value, thenDS2 automatically converts it to
ANSI date, time, or timestamp.But what about a
SAS numeric valuethat didn't have a
format attached to it,and yet it did represent a date?Well, we provide
explicit functionsto convert the data
from SAS floating pointto an actual ANSI date, time,
or timestamp, and the other wayaround.If you want to
explicitly convertan ANSI time, or
timestamp, or datevalue into a SAS
floating point numeric,we have a function
that can do that.And using these functions,
and understandinghow things work when
you read the data,you can process both ANSI
and SAS datetime valuesin the same program.So the functions involved for
these explicit conversionsare these four here.And they're required,
because if you recall,ANSI date, time, and timestamp
values are non-coercible.That mean you can't just--for instance, if you wanted to
use one of the interval timefunctions in SAS, like
INTCHECK, or INTNXyou need to provide a
double precision numeric.If you read that value
into DS2 from a SAS DATAset that had a date
format attached to it,then what you have is an
actual ANSI date value.You can't feed that to the
INTNX or INTCHECK function,so you'd need to feed it to
the TO_DOUBLE function thatwould change it back into
the appropriate SAS date,or datetime value.And conversely, if you
have a SAS double precisionnumeric that didn't
have a format attached,and you want it as an ANSI
date, time, or timestamp,you can use the TO_DATE,
TO_TIME, or TO_TIME,or TO_TIMESTAMP function to
convert SAS values to ANSIexplicitly.And here's what that looks like.The TO_DATE function
accepts a single argument,which must resolve to
A SAS numeric value,and it returns to the
appropriate ANSI datevalue for that.You can see here how that works.The TO_TIME function
accepts a single argument,which is a floating point
representation of time in SAS,and it returns the appropriate
ANSI time value for that.And TO_TIMESTAMP,
again, receivesa single numeric argument,
it's a floating point argumentrepresenting a SAS
datetime value,and that is converted into
the appropriate ANSI timestampvalue.Going the other way, we
take any ANSI date, time,or datetime value, and feed
it to the TO_DOUBLE function.And TO_DOUBLE will pump out
the appropriate floating pointvalue that represents
that date, time,or datetime value in Base SAS.Hi, there.Mark again.In Lesson four of
High-Performance DataManipulation with SAS DS2,
we'll take one last lookat converting traditional DATA
step programs into DS2 DATAprograms.And then we'll leave
that world behind,and we'll dive into
the things that DS2does that the traditional
DATA step does not.We'll take a look
at user definedmethods, and storing them
in packages for easy reuse.We'll take a look at
storing DATA step typeprocessing in thread
programs, and thenreusing those thread programs
from a DS2 DATA programto process multiple rows
of data in parallel.And finally, we'll look at
running our DS2 programsin CAS, SAS' amazing massively
parallel processing platformworking with data
that's memory resident,and is lightning fast.So I hope you enjoy the ride.In lesson two we first discussed
translating existing DATA stepprograms into DS2 DATA programs.Now, we introduced you
to DSTODS2 procedure.We're going to use
that same procedure nowto convert a slightly
different data programand go through the
steps of makingthat program functional in DS2.So the program we're
going to be working withhas an ARRAY statement.We know that there is no
ARRAY statement in DS2.There's a WHERE statement.We know there's none
of those in DS2.Has a new variable,
obviously, that we'regoing to need to
think about declaring,and the same with the Q, right?There's a KEEP statement
here that's probably in placedue to us reading in a bunch of
variables we don't need, We'rejust going to run this program
to make sure that it works,and take a quick look
at the output datato see that at the end
for each employee IDwe have total contribution
for that employee.Now, if we were to
process this in D2,we might start out by converting
the program with PROC DSTODS2.And with that
program completed, werun into the same
issues we saw before.We need to add the PROC
DS2 statement in the front.We need to end with
a RUN, and a QUIT.And we had this empty
set of statementshere that we didn't like.And, say, that looks a
lot more interesting.Now, notice, even though there's
no ARRAY statement in DS2,PROC DSTODS2 successfully
translated thatinto a VARARRAY statement,
which was kind of nice.Also, put it in
the global space.It moved the KEEP statement
out of the end of the programup into the global space
of our output program.And, again, we're
going to see that we'rereading in the
employee donations,but the WHERE clause
got commented out.So we can fix this by using
a FedSQL query insteadof a WHERE statement, and
a SET statement separately.We also mentioned
that we're goingto need to declare this total,
so let's go ahead and add that.And honestly, there's
this Q variable down here,we don't really want that
in the output anyway.So I'm going to go ahead and
declare that local to the RUNmethod.Remember, data type first,
then the variable name.And we already expect
to be out there,because we ran the data steps.So let's go ahead and add the
OVERRIDE=YES while we're here.And that seems to have worked.Let's have a quick look
at the output data set.We have an Employee_ID and
TOTAL for every employee.That's nice.And nothing in there, otherwise,
than the QTR variables.And we could leave
this code as is.Now, because I already have
a select statement here,if you think about it, right?If I just selected only the
columns I wanted to deal with,I wouldn't actually need
a KEEP statement up there.So instead of reading in extra
columns, let's just select.We're not going to need
that KEEP statement at all.So let's go ahead and
come out that out, justsee how that works.And looking at our
CONTRIB output,we can see that we do
have total Employee_ID,and only the quarter
variables were kept.So by a little bit
of DS2 only magic,we've been able to declare the
index variable for our DO looplocal, and not even worry
about bringing it out,the result set.And if we're going to
write a select statement,why not only read in
the data that you need?And so this will
be my final answer.I'll save this.And that would be my
production in DS2.So in an earlier
lesson we lookedat a small, little user-defined
method, the C to F method.Let's take a deeper dive into
user-defined methods now.Remember that the method
statement gives your methoda name, and it allows you
to specify parameters thatcan be handled much like locally
declared variables insideof the method code.Your method can return a value.Now, you should say returns if
it's going to return a value,and specify the data type.And then, in the
method code, you'llhave to explicitly code
a return statement thatreturns the value in question.But your method could also
modify some of its parameters.In object oriented
programming, we frequentlyrefer to these
types of parametersas being passed by reference.And what that means is,
instead of giving mea value in the slot,
I want you to give methe name of a variable.From the name of the
variable, the methodcan figure out where in
memory that value is stored,and then it can access the
value directly to read it,but it can also write
back into that area.So while a method
that returns a valuecan only return one
value, that same methodmay have several
IN_OUT parametersthat allow us to modify
several values at once.And you can either return a
value, or do IN_OUT parameters,or do a combination of both.For example, you
may have a methodthat modifies several parameters
and then returns a return codeto indicate how the
processing went.Now, these standard
parameters canbe passed in either
as a constant value,or as an expression that
resolves to a value.That expression could be
the name of a variable,or it could even be a
formula of some sort thatproduces the value.And these work a lot like
DATA step function parameters.Now, the value of one of
these standard parameters,not modifiable.In other words, you
could take the value in,but you can't pass it
back out, because itcould have been a constant,
you can't write to a constant.And if you're going
to return a value,you'll only be able to return
one value with a returnstatement.Now, IN_OUT parameters
are different.You're not allowed to
pass in a constant valueto an IN_OUT parameter.The IN_OUT parameter does
not want to know the value,it wants to know where to
find the value in memory.So we have to pass an IN_OUT
parameter of variable name.Once we have the
variable name, then wecan retrieve the value
from that variable,but we can also change the
value of that variable.So this works a lot like
traditional DATA step CALLroutine.When you pass in parameters,
they can either be scalarparameters, or ARRAYS, but
because parameters are alltreated internal to the
method, as if they were locallydeclared variables, you
cannot pass in a VARARRAY.A VARARRAY is a global
only type construct.So how might we
use this concept?It's traditionally used to
standardize business practicesthroughout an enterprise.So, for example, maybe
we want to ensurethat everybody calculates annual
interest the exact same way.There's a lot of different
ways of approaching this.We want to define
a way of doing itthat works for everybody
in the company,and it's easily explained to
our regulators, for instance.So we might look at
this kind of formula,the amount we have at the
end is equal to the sumof the original
amount, plus the amounttimes the annual interest rate.When we design this, we need to
think about all the informationwe need to get in.So we need to get in an
amount, a number of years,and an interest rate.In that case, our method's
going to need three parameters.If we pass on the
value of amountby reference as a variable,
then that variable value couldbe modified directly in
the variable we pass in.And so that's the
first way we'regoing to design our
INTEREST method,we're going to use it
with an IN_OUT parameterfor the amount.Other parameters are not
going to be modified.We can pass them in as
standard parameters.So the definition for
the INTEREST methodmight look something like this.I'm defining the INTEREST method
with an IN_OUT parameter that'sa double precision numeric.Now, this is going to have a
variable name on the outside,but internal to the method,
we're going to call it amount.And that requires two
other parameters as well,a double precision
parameter named RATE,and an integer parameter named
YEARS for the number of years.Internal to the
method, we're goingto do a DO loop that cycles
through our calculation oncefor every year of the period the
amount is on deposit with us.So we're going to declare
a local variable I that wecan use as the index variable.And it's very important that
you declare your index variableslocal when you make
user-defined methods like this,otherwise, they
can inadvertentlywind up in the
Program DATA Vector,and if there's another
variable named I out there,it can cause all
sorts of conflicts.But the ability to
declare a local variablemakes this a very safe process.So now we have declared
the INTEREST method,all that remains is to use it.Remember that when we
use the INTEREST method,the first parameter
cannot be hardcoded,because that's an
IN_OUT parameter.And so we're going to set
up a variable called TOTAL,and another one called DURATION.The TOTAL variable will give
it the initial amount, 5,000.The rate is a
standard parameter.We're going to
hardcode that at 4%.And the duration, we're going
to get from our DO loop.So this is going to go for
one, three, and five years.And each time we do
this, we'll write outthe duration, the total for that
duration, as a dollar amount.When we execute
this code, we cansee that our INTEREST
method is working well,and that holding my money for
one year gets me 4% and $5,000as my interest.And holding it
for five years, wecan see the effect of
compounding that interestover a five year period.Now, management wants us to
use this interest calculationfor all of the programmers
in the company.How can we best deploy
this, so that other peoplewould be inclined to use it?You know this goes,
there's only somuch you can do by laying
down the law to programmers.You can tell them, you
have to do it this way,and you better do it this
way, but programmers area gnarly little bunch, right?What's the way to a
programmer's heart?Is to make it easy,
make it so easyto do it the way you
want them to do itthat it would be too much
work to do it any other way.Now, in the Base SAS, we might
have thought of encapsulatingthis calculation method
in maybe a %INCLUDE file,where they could draw in the
code that did the calculation.But that involves
variable names and thingsthat won't be transparent in
the code that we're using it in,so that might be problematic.We could also do
this maybe as a macrothat brought in the
DATA step code requiredto do this calculation.But once again, that
can be problematic,because it also involves
variable names that are nottransparent to the user.And finally, we might
think of using PROC FCMP.If you're not
familiar with that,PROC FCMP is a
function compiler whichallows us to write our own
functions for Base SAS use.It seems to me that there
must be a better way of doingthis that doesn't involve--including variables
in my programDATA vector that
I'm not aware of.So let me introduce
you to the conceptof user-defined packages.User-defined packages
and user-defined methodsgo together like peanut
butter and chocolate.This is the perfect combination.So a PACKAGE program allows
you to name a package that youstore out in any SAS library,
encapsulate any numberof methods that
you'd like, and whenyou run this PACKAGE
program, that packageis stored in the library
with all of the methodsthat you've predefined in it.And then to use that
PACKAGE in subsequent codebecomes a very, very
simple task without havingto recreate any of the code
for all of those methods.Just remember, if you store
a package in the work librarytoday, and shut down
your SAS, tomorrowdon't expect to see
the package there.This library is still
the work library,and it will be deleted at
the end of your SAS session.So better to store these
in permanent librariesif you want to use them
in a separate session.So a quick review of
PACKAGE program block.It begins with a
PACKAGE statement,it ends with an
ENDPACKAGE statement.Global declaratives in a package
work a little bit differentlythan global declaratives
in a DATA program,in that global declaratives are
still private to the package.For those of you who
are object-oriented,you'll recognize these
global variables in a packageas a way of keeping
status of what'sgoing on in a particular
package's instance.You can think of a package
as the class, if you will,for DS2 if you're doing
object-oriented programming.Those global variables are
available to any methodwithin that package, but they
don't affect anything outsideof the package itself.And then, of course, as many
methods, as you would like,can be stored
inside that Package.Those methods may have
local variables of their ownthat are private to
that method only,and are not available
to any other methodwithin the package.So instead of putting the
INTEREST method code directlyinto our program, let's
put it in a package.And the last time we
wrote this, we wrote itwith an IN_OUT
parameter, and maybethat caused a little grief
when people were using it.So we're going to recreate
this so it returns a value justlike a function would do.And that means that none of
the parameters are IN_OUT.See, there's the BASIS,
the RATE, and the YEARS.Now, we're going to return
a double precision value.And here's the RETURN
statement that does it,otherwise, the
calculations in the codeare pretty much the same.Now, we've put this in
a PACKAGE statement,so when we run this code,
it will create the packageand store it in the
DS2_SAS library,and the element in there
will be called MY_METHODS.So the package hasn't
actually done any of the--hasn't executed any of
the code for the methodsthat we stored in it.How would we use this package
in a DS2 DATA program?Well, here's the beauty of it.A simple DECLARE
PACKAGE statementmakes all of those methods
available to my DS2 DATAprogram.So here I have a DECLARE
PACKAGE statement.See, this is the place
we stored our package.And we give the instance,
or this copy of the packagethat we're going to use in
this DATA statement, a name.We're calling it M.Now, packages can have
constructor arguments.This package has
nothing that youhave to give it to get ready.And so we just use the
open and close parensto complete the syntax and
make a copy of the packagewe stored out here ready to
use in this DATA program,and that copy, or
instance, is named M. Sowhat do I do in order to
use the interest method thatwas stored in my package M?Down here I
reference that methodby saying M. and then the
name of the method inside.I give it the parameters,
just like I did before.Remember that this is
acting like a function,so return a value.So we'll use this on the right
hand side of an assignmentstatement.And we'll assign the results
to our variable total.And we can see that this is
working exactly the same as itdid before, but we didn't
have to write the methodcode into this DATA statement,
we just got all of that codestraight from the package.Everybody is using our package.They're doing their interest
calculations the way we desire.And our pointy-haired boss
comes along, and says,that's a great job,
Mark, but now wehave to worry about Carolina
Bank and Trust Companydown the road, they're
offering their customersinterest that's compounded
quarterly, or weekly, or evendaily, and as a result,
all of our customersare abandoning us and
going down the street.So we need you to modify that
interest calculation method,so that we can compound
it multiple times a yearif necessary.So we talked about
how that might look.And in this case, we'll
take that same calculationthat we're doing,
but we're doing itfor multiple periods in a year.You only take a fraction
of the rate then.So if we were doing it for
12 periods, once a month,then you'd take the
rate divided by 12.And we'll do that for
every period in the year.And we'll do that whole
thing for every yearthat we want to keep the money.And that will give us a nice
way of compounding the interestmore than once a year.Now, the problem is we
have a package out therewith an INTEREST method in it,
and that INTEREST method onlyrequires three parameters.Our new method is
going to require four.And user-defined
methods can't accepta variable number of arguments.So if we change
the INTEREST methodto require four, everybody who
is using our package out there,they're going to crash
and burn when theyrun their code the next time.If we don't change
it to be able to usefour arguments, of course,
our boss is going to be angry,maybe we'll get fired.So what can we do?You starting to
feel the pressure,starting to feel a
little bit overloaded?OK, I'm sorry for the joke.I don't know who gave the
term overloading to this,but it sounds like something
horrible, doesn't it?Overloading a method
sounds like you'reabout to break something.But it's not horrible, it's
actually a wonderful idea.Basically, when we call a
method, we call it by name,but I was kind of playing fast
and loose with that concept.When you call a method, you
call a method by its signature,and the signature consists
of the name of the method,plus an ordered list
of the data typesthat it uses as parameters.We can clearly see that
the method TEST with onereal parameter is
different than the methodtest with two real parameters.So this is the concept
of overloading methods,writing two methods
with the same namebut distinctive signatures.Now, we talked about data
type, but it's really closerto a data class type thing.For instance, DS2 won't be
able to tell the differencebetween a real and a double.They're floating point types.So if you made one with a
single parameter that was real,and one with a single
parameter that was double,you might have a little
trouble distinguishing that.So think of this as signature,
test, floating point, test,floating point, floating point.If you get two or
more same-name methodswith the same signature, we get
an ambiguous method definition,and that'll throw an error
when we try to call the method.When we call an
overloaded method,the first thing that
happens is the signaturesare compared to the number of
parameters in each method thathas the same name.And if there is no match
by number, say we had--go back to our original
method here, saywe had test with one
real, test with two reals,and then we call a
test with three reals,there is no definition
that has three in it,and so that would fail, and
that would be an unknown methoddefinition.But if the number of
parameters matches,then we look for the
closest match by data type.And if there's an exact
match, that's great,the method is called, the values
are fed to it, and it executes.If there is no exact
match for data types,then the parameter data
types are evaluated,and the input data is evaluated
to see if they're coercible.And if there are
coercible data types,then the values are coerced
to the appropriate datatype for that method, and
then the method executes.So an error will occur whenever
the number of parametersdoesn't match any
of the signatures,or when the method
signatures are ambiguous,or when the data types that
you call the method withare not coercible.So, for instance,
with our methodthat had one real
parameter, if we called itwith a date, an ANSI date
input, dates are not coercible,and so you would have an error.So this package contains two
definitions for the INTERESTmethod, one with a
traditional three parametersof input for the
annual compounding,and one that allows
four parameterinput for a multiple
compound per year.And so we can then recompile,
or recreate the MY_METHODSpackage.Now, there's actually two
interest methods in there,and you can use either
one by merely calling itwith the right
number of parameters.So let's take a peek at a
slightly more complex algorithmthat we might benefit
from putting in a package.This is a DS2 DATA
step that is designedto score our campaign table.And basically, we're going to
declare some variables here.The SCORE variable,
which will give usthe score for the particular
individual that we're scoring.And ACCOUNT, so we can keep
track of how many recordshave been scored.Now, there's some
documentation herethat basically tells
us what variablesto input, which are the
inputs to the scoring method?And you'll see the beginning
of the METHOD definitionhere with all the inputs in.This returns a double
precision value for the answer.And you're going to see that
there's a lot of code here.This code was generated
by PROC HPLOGISTIC.So without further ado,
let's go down to the bottomand see the end of that method.And so here's the end of the
SCORE method and the RETURNstatement that returns
the scoring value.Now, the instructions
gave us informationon which variables
in the campaign dataset we should use as input.And so we're going to
use those values as inputto the SCORE method
when we call it.I'm going to keep track
of the number of rowsthat we score using
this COUNT variable.And at the end, we're going
to write out some statistics.Now, this is a DS2 DATA program,
so it is single-threaded,so we don't expect anything
odd or unusual going on here.But the THREADID is
an automatic variablethat identifies
the thread runningthis particular piece of code.For the DS2 DATA program,
we expect that to be zero.NTHREADS indicates how many
threads were available to DS2as the process was running,
whether or not we use them.And _COUNT is that count that
we've been using to count upthe number of rows we've scored.So if we run this
program, we shouldbe able to score this
campaign data setusing our scoring algorithm.We see that we processed
106,000 rows pretty quickly.The DATA program is writing the
results out to DS2 SAS SCORED.So let's see if we can
find the SCORED table.Going to take a
quick peek to seethat we are getting a
different score variablefor each ID in the data set.So that seems to have
worked pretty well.Now, what if we wanted
to use this algorithmto score this data
set, or otherslike it, many different times?Writing all that code
into the data programwould be kind of a
pain in the neck.I would prefer to do
something like this,I have a package with a
scoring algorithm in it,and then just call the package--the SCORE method
from that package.So in order to do
that, we're goingto need a package program.And what I've done
is I've extractedthe code for the SCORING
method from the DATA stepthat we used before
and I'm putting itinto this PACKAGE statement.I'm going to store this
out in a DS2PACKAGE_SCORINGin that same SAS library.And you'll notice
that I've actuallywritten a couple of versions
of the SCORING method.So this package contains
the following method,this is a CONTENTS method.It doesn't take any parameters.It's just going to write
some stuff in the log.And I do this with every
package that I make,I make a method called CONTENTS,
and I keep it up to dateby hand.So as I add methods to
the package in the future,I'll be able to tell
what's in the package.There's no automatic
method for gettingthe contents of a package out.And then the SCORE method
itself, I've overloaded.I have one version
of the method thatdoesn't require any parameters.And all it does is write
syntax help to the log.This is actually for
the Campaign1 data set.And so if I call this METHOD
SCORE without any parameters,it will tell me how
to use it in the log.And then, finally,
is the second versionof that SCORE method that
has the actual inputs,and does the actual scoring.And that's the end
of the package.I'm going to go ahead
and create this package.You see, that didn't
take much time,because the package really
isn't doing any work yet.And if you look at
the package, you'llsee that it doesn't appear
to be very interesting.Actually, when you
open it, you'regoing to be
disappointed, if you'retrying to decode what's
going on in there.This is just
encrypted source code.So that's how it's stored.And this here is a
CRC that makes surethat nobody fiddles with
the encrypted source codeto change anything.So it's a nice secure way of
being able to share algorithmswith other users
without them necessarilybeing able to decode
how it was done inside.All right, now, to use that
package, I can go back,and in DS2 DATA
step, I just DECLAREan instance of a package.Now, let's say this is
several weeks from now,and I don't remember what's
in this scoring package.So I could just
DECLARE an instance,and call CONTENTS to
see what's in there.And because I've built the
package to be self-documentingwith the contents, in
the log it tells me,hey, there's only
two methods in here,the contents method
that you just called,and a SCORE method.And I say, well, I wonder
how that SCORE method works.So I'll run another
quick DATA stepthat calls the SCORE method
without any parameters.Remember, I had made
that particular overload,so I could get information out.And I run that, and it gives
me in the log, all the syntaxfor using the SCORE method.So for Campaign1, it
tells me the namesof the variables
that were built.So that's good.So SCORE CAMPAIGN1.And all I would do is DECLARE
an instance to the package,DECLARE my score variables,
like I did before,and call the SCORE
method from that package.So let's go ahead
and rerun this again.And just like that,
we're scored again.Go down and check the
SCORED data to makesure it all looks kosher still.And so without having to
rewrite all of that code,I've been able to
score the data setin much more compact manner.Now, what would happen if I was
scoring a different data set?Let's take a look at
the campaign data set.And you'll notice that it has
different named variables.It still has an ID,
but the input variablesare named differently.Let's see, there's one, two,
three, four, five, six, seven.One, two, three, four,
five, six, seven.That looks about right.So let's go ahead and feed
those variables in instead.And we'll change this to the
scoring the campaign data set.And we'll call this
SCORED_CAMPAIGN.Now, the campaign data set is
a lot bigger than the others.You can see that it has
over a million rows.But that seems to
have gotten scoredjust like we had hoped, right?And if we look at the
scored campaign data set,it has the same types of output
that we're used to seeing.So I've been able to take
that scoring algorithm thatwas actually written
for one data set,and use it on a
similar data set,even with different variable
names, to conduct scoring.And that's a pretty useful
thing to be able to do.In the scoring
demo, the processinglooked a little intense,
and perhaps thatmight benefit from
a little threading.Remember that there are two
basic types of bottlenecksthat we encounter
in our processing.An input, output
bottleneck, and thisis when the rate at which
the data can be read inis slower than the rate at
which it can be computed.And this means that
the CPU is often idlewhile it waits for
the next recordto be presented to
it for processing.This is inefficient
use of the CPU.And it's quite common
in single machinesthat you have at home, because
they have a single disk.And the throughput has
then improved radicallyover the years, but it's still
a lot slower than moving dataaround in memory.On the other hand, the
things that we do to our datais becoming more
and more complex.And so it's not unusual
nowadays to run into a process--not again-- that
the calculationsare so intense that they can't
be completed before the I/Osystem could have presented
you with another row.This is a process that
we refer to as CPU-bound,because the CPU is still
crunching on the one rowwhile we have plenty of
other rows available.CPU-bound process is
inefficient in I/O.Now, modern computers
have multiple CPUs.And most of those
CPUs themselvesare capable of handling more
than one process at a time.So each of these processes that
the CPU can pay full attentionto is called a thread.We can use threading
with I/O, particularlyif we have more than
one disk, in orderto read data in in parallel,
so that the data movesinto RAM more quickly.And this allows the CPU to
process more continually.And as we use threaded I/O, it
becomes more and more likelythat the process
will be CPU-bound.Now, with a threaded
I/O, there wasnothing I had to change
about my program.My program was still reading and
running from a single thread.It was just being moved
into memory in parallel.When I have a CPU-bound
process, this is different.If I want to process more than
one row at the same time usingseparate threads, I will have to
rewrite my application in orderto take advantage of
all those multiple CPUs.But if I do this, it can allow
the I/O to move more smoothly.SAS has been working on this
threaded processing for years.All the way back
into version 8 of SASwe were doing things like
PROC SORT, PROC MEANS,using multiple threads for
things that are CPU intensive.PROC REPORT and many
others are, indeed,multi-threaded
processes, but the DATAstep itself is single
threaded, except for whenyou sort the data for indexing.And if you think about it,
that kind of makes sense.The better step is
instructions on whatto do to an individual
row of data.You'd expect that it would
process single-threaded.And a single-threaded
environment,anything that we do
that's compute intensivecan easily become CPU-bound.So we might wonder,
what kind of programswould tend to be CPU-bound
and benefit from threadingon the SAS Compute platform?And more importantly,
even if we suspect it,how can we tell by
looking at the SAS logafter we've run our code whether
or not that process is, indeed,CPU-bound, I/O bound?Well, the quickest way to tell
if your process is I/O boundor CPU-bound is to take
a look at the SAS logafter your program is run.In this program, we can
note that the real timeis 50.34 seconds, while the
CPU time is only 4.06 seconds.These two numbers differ
by an order of magnitude,because the real time is
greater than the CPU time,it's pretty obvious that
this process is I/O bound.And unlikely to benefit from
threading on the SAS Computeplatform.Now, this process has a
real time of 5.23 seconds,and a CPU time of 5.52 seconds.Because the real time and CPU
time are so close together,there's a good indication
that this process is, indeed,CPU-bound, and would
benefit from threadingon the SAS Compute platform.DS2 was designed for
threading from the ground up.Writing DS2 threads is easy.And when you use
DS2 threads, youcan process multiple rows in
parallel from a data program.Thread programs are a lot
like DS2 DATA programs.They can declare
and use packages.They have to have the INIT
term and RUN methods in them.They can use a SET
statement to read an input.They're capable of BY-group
processing, includingFirst., Last.And any global variables
declared in a threadwill be in the Program DATA
Vector of the DATA programthat utilizes the thread.The DATA program is required
to make the threads execute,because the threads are just
a stored program, so to speak.And the thread itself can't
make another thread execute.There's a special
statement called SET FROMthat we use to execute threads.And that SET FROM
statement is notallowed in a thread
program itself.So in many ways, threads
are similar to packages,though, because we talked about
them storing in SAS libraries,right?They can also accept
parameters, and includeuser-defined methods.And a thread stored to
permanent library, of course,can be reused in
a future session.So this is another way
of storing complex codefor easy reuse.The benefit of using a
thread to do so, of course,is that if you store
your code on a thread,you can execute multiple copies
of that thread in parallel.So this is what a thread
program looks like.It begins with a
THREAD statement,and it ends with an
ENDTHREAD statement.It contains global declaratives.And like a DATA
program, any variablesdeclared globally
here will appearin the Program DATA Vector,
and in the result set produced.Can include system methods,
or user-defined methodsinternally.You can have local declarative
statements for locallydeclared variables, private
to that particular method.So very, very much
like data program.Now, when we run
the thread program,it stores the thread
in a SAS library,much like the
PACKAGE program did.So the question is, then, how
do we get the thread to execute?We do this with as simple
DS2 DATA program thatdeclares an instance
of that thread,and then executes multiple
copies using a SET FROMstatement.Now, while we're threading
on the compute server,we'll use the THREADS= option
to specify the number of threadsto execute.Then, the threads execute.And as each thread
processes row,it returns that data
to the DATA program.And the DATA program
controls the processof writing the rows
to the results set.Now, this is a DATA
program, so youcould conduct additional
processing on each row of dataas it came back from the thread.It's perfectly
legal, but you shouldremember that any processing
you do in a DATA programwill be single-threaded.Now, the one thing
that it takes a lotof getting used to when you
start threaded processingis the idea that when you're
running multiple processes thatprocess your rows of
data, you actuallyhave no control over the order
in which the rows are returnedto the DATA program.As soon as the thread
finishes computations,it returns the row
to the DATA program.And this means
that every time yourun a multi-threaded
process, the rowswill probably come back
in a different order.In a previous demo, we
wrote a DS2 DATA programto score the CAMPAIGN data
set using a scoring algorithmthat we had placed in a package.When we executed
that program, wefound that the real time
was very, very closeto the CPU time.This indicates that this process
is more than likely CPU-boundand could benefit from threaded
processing on the SAS Computeserver.So we're going to take
that DS2 DATA programand convert it to a thread.We've changed the DATA
statement to a THREAD statement.We've changed the
ENDDATA statementto an ENDTHREAD statement.We're going to execute
this piece of codeand create the thread.You'll notice that that
ran very, very quickly.And that's because the thread
has stored the program,it really hasn't done
any of the work yet.In order to get our thread
program to actually scorethe data set, we need to execute
it from DS2 DATA program.So here is the data
program, once again,running out to that
scored data set.Now, all of the code
required to scorethat data set, though, is
encapsulated in the thread,including the PACKAGE instance
that we're going to use.So all we need to do is DECLARE
an instance of the thread.I'm going to call it TH.And use that special
SET FROM statementto execute the thread.Now, when I run it
this way, I haven'tspecified a number of threads,
so this runs single-threaded.And the real time and CPU
time are very, very closeto what we used when we ran it
as a straight up DATA program.So no benefit yet.But because we have
this stored as a thread,now we can execute that in
multiple threads at once.Let's try running it in
two separate threads withthe THREADS= option specified.And you'll notice
that when we ranthat in two separate
threads, the real time is now0.35 seconds, that's cut
the real time almost in halffor executing this program.And if we work with
this a little further,we can boost the
number of threads.Let's try 8 threads, and
see what that gets us.And, man, that cut the
real time even further downto 0.19 seconds.You'll notice that
as we do this,the CPU time goes up, as
there is extra CPU requiredto monitor the processing
of all these threads,but the throughput has
dramatically improved.And all of this, on the
SAS Compute platform,without even thinking
about massivelyparallel processing it.Well, turning on the SAS Compute
platform is all well and good,but if you have available a
massively parallel processingenvironment, wouldn't it pay
to try your code up there?Fortunately for us, we have SAS
Viya with the Cloud AnalyticsServices available to us.Now, CAS consists of
a series of servicesthat allow you to process data
that is resident in memory.In order to process data in
CAS, place a data set that'sstored in offline
storage in memory once,and then process
it, and process it,and analyze it, and
do as many thingsas you want to it without
ever having to unload ituntil you're ready.This means that the processing
at CAS is extremely fast.And because of the way
CAS is architected,you can use data of any size.It doesn't matter if
the data is largerthan the amount of memory
that you have available.Now, you can work
with CAS from a numberof programming interfaces.In this class, of course, we're
using SAS Studio on a Viyainstallation, but you
could use SAS Studioover modern
implementation of SAS 9.4.If you're using SAS 9.4
to interface with Viya,you might be using Enterprise
Guide as your client, or eventhe SAS windowing environment.Maybe you're not even using
a SAS interface at all.You can do this from our
studio, for Jupyter Notebook,and numerous other ways.CAS is normally configured
to run on multiple machines.We have the server
controller, whichwill have a series of session
controller instances runningon it, and several server
worker nodes, and each of thesewill have worker
sessions running on them.We communicate with our session
controller from our client.In our case, we'll be
using SAS Studio and PROCDS2 to run DS2 code up
on the CAS environment.From the client, we
can load data into RAMfrom offline storage.If you load this from
an external source,it's passed to the
session controller,which distributes that data
across the session workers.In other configurations, you
can run CAS on a single machine.We call this symmetric
multi-processing.And in this case, the server
controller, and the sessioncontroller all reside in
the same physical hardware,and we get multiple
sessions runningacross the multiple threads
available to us on the computerwhere it's installed.So, for example, on
my laptop, I havefour CPUs, which
are each capableof handling two
threads, so it wouldbe possible to run eight
threads worth of processingon my single laptop computer
in symmetric multi-processingmode.In our classroom environment,
you have a Windows boxwith your SAS
Studio on it, and wehave a Linux server
that is runningCAS in symmetric
multi-processing mode.SAS libraries and CAS
libraries operate a little bitdifferently.In the SAS library--we refer only to the
physical offline storage--when you reference a table
with a LIBREF.TABLENAME,the SAS system automatically
loads that table into memory,say, in a SET statement,
and processes that data.And when the data
is done processing,it unloads it from
memory automatically.A Caslib consists of a
combination of in-memory spaceand offline storage.We can load data very quickly
into them, in-memory space,from any of the locally
stored data sources.And the data sources
don't have to bein just SAS data set format.They can be in comma separated
values, in SASHDAT files,Excel files, all
sorts of things.In fact, in the
data sources, youcan actually store
access and credentialsfor reading data in from
a relational database,like Oracle or Teradata.But before you can
process the data,you have to lift
that data source upinto the in-memory space.And once the data
is in memory, itstays there until
you write it back outto offline storage explicitly.So the data source area
stores connection informationand access controls, more
than just physical files.You could store all
of this informationthere, and how to get access
to predefined data sourcesfor this particular Caslib.So when you're
running DS2 and CAS,it works kind of like this.First, you have to make sure
that the requisite resourcesare going to be in CAS,
because we want everythingto run in CAS, and
that includes if youhave packages, or
thread programsthat you're going to use.Those need to be up
in a CAS in memorybefore you try to execute
a DATA program thattakes advantage of them.So, for example, we
want to run in memory.We also need to read from
an in-memory table, whichmeans that our original thread,
which was probably writtento read from a
traditional SAS librarywill have to be rewritten to
read from the Caslib instead.And then if you want the
process to run entirely in CAS,you'll want to write the
data out to CAS also.This will provide the fastest
throughput for your processing.And once you're ready to roll,
on the PROC DS2 statement,you use the SESSREF= option to
point to the CAS session whereyou want to run your code.So to do our processing up on
the Cloud Analytic Services,or CAS, we need to make
a connection to CAS,and establish a connection
to the CAS librarythat we want to work with.Now, this was all
really handled for youalready in your
libnames.sasprogram.If you look at the
code in libnames.sas,and it already set
up connection to CAS,named CASAUTO, and had
designated this DS2 CAS libraryas the active Caslib
in the CAS session.In addition, down here, we
actually assigned a SAS LIBNAMEto that, so you could
see the things that wereavailable to you in the Caslib.And currently, we can
see that there's nothingloaded in the memory portion.I remember, in CAS, the only
data that you can processis the data that's already
loaded in the memory.So our goal, remember,
we had to make surethat all the resources we
needed were loaded into memoryover in CAS before we processed
it with a DS2 DATA programrunning up in there.So those two
sections have alreadybeen taken care of by
our LIBNAME program.Now, here we're going
to load data into CAS.Now, there's a lot of different
ways of loading data into CAS.You can write a DATA statement,
and run right up to that DS2library that we were
talking about here,and the data would write
into CAS in memory for youautomatically.But we have saved the
campaign data sets out therein SASHDAT format.Now, this is a
really nice formatto work with in CAS, because
it saves it in such a waythat it can be distributed
across the different nodesin CAS, and when
you load the table,they all load up in parallel.So we're going to load
the campaign tablesfrom the SASHDAT files
associated with the Caslibout there.But the SCORING package, I don't
want to have to rebuild that.And the SCORING package
is already available to mehere in SAS, so I'm just going
to load that up from the SASside up into the CAS library.Let's go ahead and
run the PROC CAUTIL.And that loaded that up for me.And now you can see
that in my CAS libraryI have campaign data
sets available to me,and my package for SCORING.So I have the resources I
need up in the CAS library.And all that remains
is to make surethat my thread is reading
from the CAS table,and my DATA program
writes to the CAS table,and that we specify that the
session should run in CAS.In order to run things in CAS,
we had to say SESSREF=CASAUTO,or the name of the CAS session.We called this one CASAUTO.And I'm writing my
thread program upstraight into DS2 CAS.It's reading, this time, from
the CAS data set Campaign1rather than the SAS data set.And just like before, when we
run the thread program itself,shouldn't take a lot of time.It's just storing the
code right in the thread.It's not really
executing that code yet.All right, so now that we have
the threads stored in CAS,we have the data up in
CAS, we have the packagewe need up in CAS, we can
execute the DS2 DATA programin CAS.Once again, using
the SESSREF=AUTO.And to make
everything go faster,we're going to write
the output to CAS also.OK, that ran pretty fast.You'll notice that there's
an awful lot of nodesavailable to us in CAS,
and that by default, CASmakes use of them all.Without the THREADS=
option, rather than runsingle-threaded, when we run in
a massively parallel platformlike this, then we take
advantage of all the nodes.Now, you can limit the
number of nodes that you usefor processing by using
a THREADS= option.But if you do so, that's
going to slow things down,because it's going to
require that chunks of databe moved from node to node, and
so that all the processing canbe conducted on only the
number of nodes you specified.So I just want to demonstrate
that if you do use the THREADS=option in CAS, then CAS
will only use two threads.And you'll notice that last
time it took 0.19 seconds,this took a little
bit longer to execute.Now, if we look at the old
scoring in threads on the SASCompute platform,
the real time therewas also about 0.19 seconds.So the original scoring
was about the same timeon this massively
parallel platform.And that's because the
data isn't very huge.But what if the data
was a lot larger?That last data set
that we processedhad about 100,000 rows in it.What if we had a data set
that was much bigger, say,over a million rows of data?And I actually happen
to have a data setlike that available to us.It's called Campaign.It's also stored up there
in the SASHDAT that format.So we should be able to load all
million rows of that data tablepretty darn fast.And just like that, they're
loaded up into memory.Wow.That didn't take long
at all, for sure.Now that we have the big
data set up in there,we're going to use
the same processto score that campaign data.We're going to
rewrite the threads,so it's reading the
larger data set now.And then when the
thread's re-written,we're going to go ahead and
re-execute that same dataprogram, scoring over
a million rows of data.And that's impressive, right?In about 1.13 seconds.And this will vary a little
bit each time you run it.But it's always going
to be pretty darn fast,because we have all of that
lovely parallel hardwareworking on our behalf.That's a million rows more
than the original datathat we were processing.And it only took about
three times as long to run.All right.And so, with that all
done, all that's leftis for me to clean
the system up.And just so you can see
how that would be done,you can get rid of
tables from memoryjust by using a
DROPTABLE statement.There are all sorts of
little snippets over in herethat you can play with for
Cloud Analytic Services.They have starter code for you
to load a data table into CAS,delete one, to save a table
up to the CAS library.So if you're interested
in playing with that.But we're just going
to clean this upto set it in the condition
in which we started.And when we go back, and
look at the libraries now,you'll see that we have
removed all of that stufffrom our CAS library.And that wraps up
our demonstration.Well, congratulations.You've made it all the
way through the lecturesfor High-Performance
Data Manipulation on DS2.And now it's your turn.If you can get access
to your virtual lab,go to section five
of your course notesPDF, and follow the
instructions thereto practice some of the
things that you've seendemonstrated in these videos.As you begin your
practices, make surethat you do all of the
steps of the first practicecompletely, as these are the
steps that set your lab upfor success in all of
the other endeavors.And then there should
be one practice sessionfor each of the
lessons that you'veseen in order to
help you assimilatehow it feels to program in DS2.I hope you've enjoyed this
course as much as I haveenjoyed presenting it to you.You have a great day."
50,"Hi.Thank you for your interest
in my presentation.My name is Julie Zhuo, I am a
statistical programmer workingin the pharmaceutical industry.Today we're going
to talk about how wecan write SAS to generate SAS.There are three
code-generating techniquesfor many automation solutions.We're going to talk about
three SAS techniques.We all know that SAS has a
lot of automation techniques,including, for
example, SAS macros,SAS arrays and do loops, If you
use Enterprise Guide also hasautomation specific to the EG.Today we're going to focus
on SAS code generators.Because it does
not always becomeutility for SAS programmers
to think in terms of,maybe I can write some
SAS code, which willgenerate a lot of SAS code.So then we can employ efficiency
and also maybe accomplishautomations that cannot
be accomplished otherwise.The three SAS code generators we
are going to talk about are...First, we can create and
resolve of macro variable.It doesn't really
matter which methodyou use to generate
a macro variable,you can use the proc
SQL select into clause.Or if you prefer,
also the call symput.The second technique is
the call execute routine.The syntax is very
simple as shown here.The argument in the syntax can
be any SAS code or any variablefrom a SAS data set.And the last technique
is to use PUT statementto write SAS code to
an associated file.All the three techniques utilize
some kind of a driver dataset to drive the program.We are going to demonstrate
the code generatorsusing three examples.First, we are going to create
variable labels automaticallyout of a specification
file that'sin the format of an
Excel spreadsheet.Second, we are going to convert
variable types and attributesat the same time.The driver data
set in this exampleis one of the stationary tables.The last example, we
are going to transposea number of variables,
and this time, I'mgoing to use external data
to drive the programming.The driver data set is from
the input data set itself.Now without further ado, that's
look at our first example.SAS programmers create SAS
data sets all the time.We all know that in order for
the SAS data set to be useful,it's important that we add a
variable label to the variablesthat we created.The traditional approach of
generating variable labelsis to type a lot of
labels statements.It's time consuming,
and you may alsointroduce human errors that
may compromise quality.Now an example, we have
a specific patient filethat's in the format of
an Excel spreadsheet.It contains a lot of
variables includinga column for variable name, and
a column for variable labels.It would be really
nice if we canuse the variable labels that's
already in the spreadsheetto create the variable labels of
this SAS data set we generated.To do this, firstly we
need a driver data set.We use proc import to import
the specification fileinto SAS data set.In the SAS data set, we
only keep two variables,V name for variable name, and
V label for variable label.Now let's try to do the job
using our first technique,a macro variable technique.Here is how it works.After we create the
driver data set,we interact with the driver
data set to create a macrovariable to hold the SAS code.And finally, we resolve
the macro variable upto a label statement
in the DATA step.Here is the code.Firstly, we use proc sql to
read the driver data set,and we use the select intercross
to retrieve the variables outof the driver data set
to build the SAS code.Install the SAS code we've
built inside a macro variable.In the next block, we
resolve the macro variableafter a label statement
within a DATA step.Here is the log after we've
run the code, and also outputfor our contents procedure.In the log, because we turned
on this important option,we will be able to see the SAS
code restored inside the macrovariable.In the proc contexts
output, we'llbe able to see the variable
labels we just generated.Now, let's try to do the same
job using the second technique,it's the call execute routine.Two steps to do the job.After we create the
driver data set,we use call execute routine
to build the SAS codeand execute the generated SAS
code in only one DATA step.And here is the code.As you can see, there's
only one DATA step.We use the DATA step to
read the driver data set,and then we use three
call executed routinesto build the SAS code.The first analyzed call
execute routine onlycontains simple SAS code
that will open and then closethe DATA step.And the code execute
routine in the middlehas the meat of the sandwich.It actually retrieve
the variables outof the driver data set,
and the build the SAS codeinteractively.After we run the DATA step, here
is the log in the proc contentsoutput.In the log, we can see
that it will automaticallyprint the SAS code generated
by the code sql routines.In the proc contents
output, we willbe able to verify that
we have successfullygenerated the labels.Now, let's try to do the same
job using the third technique,the PUT statements.This is the only technique
that would actually generatea physical SAS program.It involves four steps.After we create a
driver data set,we associate a name
to a SAS program,then we use PUT statement
to build the SAS code,and finally, we include
the generated SAS programinto our SAS code for execution.Here is the code.Firstly, we use filename
statement to associate a nameto a SAS program, then we use a
DATA step to build a SAS code.There are a number
of PUT statements,similar to the previous
call execute routine.The first and the
last PUT statementsonly output simple SAS code.And the PUT statement
in the middlewill receive the information
out of the driver data set,and build the SAS
code to activate.After we run this
code, it will actuallygenerate a physical SAS program.We open up the
SAS program, wherewe will be to see the
SAS code generatedby the PUT statements.And, lastly, of course,
we need use percentinclude to include
the SAS programinto the code for execution.After execution of
the SAS program,if we do a proc
contents, we willbe able to verify that
we have successfullygenerated the labels.Now, in the first
example, we are kind oflucky to have a
specification file we canuse to drive the programming.In this second
example, we are goingto show that the SAS stationary
tables are also great resourcesfor our driver data set.The programming
task in this exampleis that we have a SAS
data set that containsa number of dates variables.These are the traditional
numeric date variables,formatted by the
day to night format.Now for some reason,
we need to convert themto the international
standard format.These are character variables.In addition, we also need to
update the variable attributes.For both variable name
and variable label,we added this capital letter
C to indicate that now theseare character variables.To do this, firstly, we
need a driver data set,as we mentioned earlier.It's one of the SAS
stationary tablesis the dictionary dot columns.This data table contains
a lot of informationabout the input data set.For our purpose, we are
going to use only twoof them, variable name
and variable label.All the three techniques
we demonstrated previouslyin the last example, will
be able to do the job.For our presentation,
I'm going to only pickthe first technique, the
macro variable technique.Because, in this
technique, we can use procsql to directly access
the SAS stationary tables.Here is the code.Firstly, we use proc sql to read
the driver data set, dictionarydot columns, then we use
select into cross to retrievethe information out
of the driver data setand build the SAS code.And install the SAS code
inside a new macro variable,we call it convert code.After that, because we want
to look at the SAS codein the log, and we turn
on the symbolgen option.And finally, we resolve
the macro variableafter reading a DATA step.After we've drawn the
code, here is the log.In the log, we'll be able to
see that the macro variablewe generated resolved to three
attribute statements thatwill update the attributes.And three assignment
statements, as well, thatwill convert the data type.Now, in the first and
the second example,we both use the
external data sourcesto drive the programming.In this last example, we are not
going to use any external datasources.Instead, we are
going to show youthat it's also possible to
use the input data set itselfto drive the programming.The programming
task in this exampleis that we have an
input data set thatcontains a number of subject and
their clinical adverse effects.The structure of the
data set is vertical.It contains one record
per subject PUT event.Now we need to
transpose this variableinto the horizontal
format, which containsonly one record per subject.To do this, first, we
need a driver data set,as I mentioned earlier.The driver data set,
in this example,is from the input
data set itself.All we need to do is to add
this new counting variable.It counts number of
events per subject.After that, we use proc
sort, no dup key option,to obtain a unique
list of those columns.All the three
techniques, again, wedemonstrated in
the first examplewill be able to do the job.For our presentation, I'm
going to pick the secondand the third technique.Now, if we use the
second technique,the call execute routine,
here is the code.We use a DATA step to
read the driver data set.And then we use a
number of call executeroutines to build the SAS code.Again, the first and
the last call execute.The argument there only
contains simple SAS code,that will open and
then close a DATA step.And the code execute
routine in the middlehas the meat of the sandwich.It actually retrieves
the counting variable outof the driver data set,
and builds the SAS codeinteractively.After we run this code,
we look at the log,and we will be able to
see the SAS code generatedby the call execute routines.The first and the last
lines are generatedby the first and the last
call execute routines,and those in the
middle generateda call execute
routine in the middle.If we use the third technique,
the PUT statement technique,here is the code.Firstly, we use
filename statementto associate a name,
TC, to a SAS program.Second, we use a DATA
step to build a SAS code.We read the driver data set, we
prepare the counting variable,we use a file statement
to specify the output filelocation.And then finally, we use PUT
statements to build SAS code.The first and the
last PUT statementswill only output simple SAS code
that will open and then closea DATA step.And the PUT statement in the
middle has the meat of sandwichretrieve the counting variable
out of the driver data setand build the SAS
code interactively.After that, if we
navigate into the folderwhere that's already specified
in the filename statementsearlier, we will be able to see
that there is a new SAS programthat we generated.It's called a transpose
underscore code.If I open up this new
SAS program we generated,we'll be able to
see the code that'swritten by the PUT statements.The first and the
last line were writtenby the first and
last PUT statements.And those in the middle were
written by the PUT statementsin the middle.And finally, of
course, we need to usepercent include to include this
new SAS program into the SAScode for execution.This is the end of the examples.Now if we do a comparison
of the three techniques,we will be able to see that the
first and the second techniquesrequire less amount of coding,
compared to the PUT statementtechnique.If we use the SAS stationary
table as the driver data set,then the first technique
requires the least amountof coding, because it can use
proc sql to direct the accessto the driver data set.In terms of process,
the call executeroutine is the most simple,
because it has the abilityto generate SAS code and then
execute the generated SAScode in only one DATA step.In terms of flexibility,
the PUT statement methodis flexible, because it's
the only technique thatwould actually generate a
physical SAS program, whichyou could open up and modify.The macro variable
technique is less flexible,because there's restriction
on the maximum numberof characters a macro
variable is able to hold.Now, in terms of
system efficiency,if we run the code
in a batch mode,the call execute routine
is the most efficient.If we run the code in
the interactive mode,the macro variable
technique is most efficient.Here is how we measure
system efficiency.For each of the techniques,
we run them 20 times,in interactive mode,
and then in batch mode.Then we compare their
real run time in seconds.As you can see from this table,
if it's interactive mode,the macro variable technique
will require the least amountof real run time.If it's batch mode,
the call executeroutine requires the
least amount of run time.Today we demonstrated
the code generators,using three simple examples.For demonstration purposes, we
only included field variables,or field data records.I want to point out
that, in reality,when we have to deal
with a lot of variables,the code generators will save
us a lot of time and effort.I also want to point out
that, in reality, the codegenerators can be applied to a
variety of programming tasks.For example, I have seen people
use it to import a varietyof raw data files, to perform
automatic AE toxicity checks,to perform specification
driven lab grading,to perform meta data driven
mapping for standard SAS datasets, or to assign variable
values without writing hundredsof IF...THEN statements.To conclude, today
we have demonstratedthat SAS code generators are a
reliable and efficient methodthat would decrease programming
time and improve quality.All accomplish the
same results, with eachhaving its own advantages
and disadvantages.And there are a variety of data
sources for driver data set.It could even be the
input data set itself.The code generators can
be applied to a wide rangeof programming tasks.This is the end of
my presentation.Again, thank you very
much for your interest.Here is my contact information,
and I welcome any questionsor comments.Thank you."
51,"Hey, everyone.My name is Luna, and
welcome to your tutorialon getting started with
SAS Enterprise Guide.So the first question is,
what is SAS Enterprise Guide?SAS Enterprise Guide is it easy
to use Microsoft Windows clientapplication that provides a
visual and intuitive interfaceto the power of SAS.So in this tutorial,
I will be highlightingmany of the capabilities that
SAS Enterprise Guide offers.We'll take a look at some
things like, how do weaccess different types of data?How do we prepare and
manipulate that data?We'll also take a
look at several pointand click tasks and wizards
that Enterprise Guide makesavailable.These are going to help
us analyze our dataand also generate
some reports as well.But we'll also look at the
programming side as well.Enterprise Guide has a lot
of programming featuresavailable, so we'll take
a look at some of those.We'll also look at
process flows and projectsto help organize the work that
we do an Enterprise Guide,and we'll also look
at some projectfeatures that are available to
help customize the work that wedo.So but that said, let's go
ahead and jump right in.The first question
I have for you allis, what is your experience
level with SAS EnterpriseGuide?Maybe you have never really
heard of Enterprise Guidebefore.You saw this video
decided to click on it,check out what Enterprise
Guide is about.Or maybe you've heard of it, but
you haven't used it yourself.You've seen other people use it.No worries at all if you have no
experience with SAS EnterpriseGuide.This is called Getting Started
with SAS Enterprise Guide.So I will be starting
with square oneand then building up from there.So again, if you have no
experience, no worries at all.Maybe on the other hand,
you do have experience,whether it's just a little
bit you use it here and there,or maybe it's something
that you use all the time.Even if you do
have experience, Ithink that you'll still
find this tutorial helpful.Again, I'll be highlighting
many of the capabilitiesthat SAS Enterprise
Guide offers,and there's probably something
in there that's new to you.So make sure to stick around.All right, so with
that in mind, let's gobehind the scenes a little bit.Let's see what goes on behind
the scenes in Enterprise Guideand then talk about
what is it that we'regoing to be working with.So it's important to keep
in mind the EnterpriseGuide is an interface to SAS.SAS itself may be
on your local PC,or it might be on
a remote server.In either configuration
that you have,Enterprise Guide is going
to prepare your code.Now this code might be something
that you wrote yourselfthrough a SAS program,
or it might be somethingthat was generated for you.Our Enterprise Guide point
and click tasks and wizardsare generating SAS
code behind the scenes.But again, either way,
you have code ready to go.That code is going to be
sent over to SAS for itto be processed and executed.Once that has been executed
and the results are ready,the results will
be delivered backto Enterprise Guide
for you to view.So that's just a little bit of
what goes on behind the scenes.Now for us, when we're
working at Enterprise Guide,we're going to be focused on
this concept called a project.Now a project is
really a collectionof the different types
of files that you'llbe working with in
Enterprise Guide.These files can include
shortcuts to data.It can be sounds,
programs, and logs.It can be your
tasks and wizards.It could even be the results
for your SAS programsand your tasks and wizards.Even more so, it could also
be some informational notesthat get added in for
documentation purposes.Again, in a project, all
of these different itemsare going to be packaged
into one single file calleda project, and that
project is goingto have the extension
dot EGP, which standsfor Enterprise Guide project.Now projects are really nice
not only to package everythinginto one file, but there's
a lot of features thatcome along with using projects.You can control the
contents of the project.You can control the sequencing.You can control the
updating, so projectsprovide a lot of flexibility.Now in this course or in the
tutorial, for the first part,we'll mainly be working
with our tasks and wizards,and tasks and wizards
do require projects.So the bulk of our work will
be done in an Enterprise Guideproject.However, starting with
Enterprise Guide 8.1,if you're a SAS programmer
and you really just wantto, open up data, view the
data, and maybe write some SASprograms, you can
actually choose to do thatwithout using a project.So again, you have
some flexibility whenit comes to these projects.But for the first
portion, we willbe working with the project
because, tasks and wizardsrequire them.And when we go over into
the programming aspect,I'll show you how we can
work with Enterprise Guidewithout using a project.Either way we go, as we talk
duties different conceptsthat I'll be covering
in this tutorial,we are going to be
working with this datafrom a company called Orion
Star Sports and Outdoors.This is a fictitious
global company.They're really a
sporting goods store.So we have some data
about Orion Stars'products, their orders,
their employees,and then also their customers.Now how this is going to
work in this tutorial isfor most sections and
for most demonstrationsthat we'll be
going through, I'llpresent some sort
of business scenariothat relates to
Orion Stars' data,again, whether that's about
their products or the customersorder employees.This is going to be retail
data, and it might not reallybe similar to the data that
you work with day to day,but you can probably think
of a business problem that'ssimilar to our scenario that
relates to your specific data.So as you follow
along in the tutorial,I encourage you to think of
relatable business problems.Let's go ahead and get started
on our first demonstrationhere.I'm going to show you how to
navigate through EnterpriseGuide, and then we'll go
through a typical workflow.So we'll see how can we
add data to a project.We'll analyze the
data using a task,and then we'll see how we can
navigate through the contentsof our project.So let's go ahead
and dive right in.Here I have Enterprise
Guide opened up.And I specifically have
Enterprise Guide 8.2.If you want to know which
version of Enterprise Guideyou have, you can go to
the Help menu at the top.And then at the bottom, click
about SAS Enterprise Guide.Next to version, you see I
have 8.2, but if you have 8.1,8.1 and 8.2 will
look really similar.I'll go ahead and click OK
to close out of this window,and let's go ahead and
talk about the layoutof the Enterprise
Guide interface.At the top, I have my
main menus and my toolbar.On the right-hand
side here, whichtakes up most of the application
window, is your work area.And then on the left-hand side,
I have my navigation area.Let's go ahead and
talk about thisin a little bit more detail.First, let's talk
about the work area.Now, by default,
in the work area,I see the Start page open up.The Start page lets
me quickly starta new SAS program, a project,
maybe open up an existing file,or access some tutorials,
trainings, and videos about SASEnterprise Guide.In addition, as we
open up different filesin Enterprise Guide,
we'll actuallysee that on the start page.We'll see a list of the recent
items that we have access,and we can even pin them
to the Start page as well.Now again, by default, we have
to Start page in the work area.But as I start to open up
different data source programs.Or I start running tests.I will see tabs appear in
the work area for each objectthat I open up.So this is going to be
a tab-based interface.Now that's the Start
page in the work area.Let's go ahead and move over
into the Navigation area.The Navigation area
consists of several panes,and I'm just going to quickly
describe each of these paneshere.At the very top, by default,
I see the Project pane.And if I do decide to work in
a project in Enterprise Guide,I will see the contents of
the active project listed outin the Project pane.Below the Project Pane icon,
is the Open Items icon.The Open Items is a
way to be able to seeall the items that are currently
open up in the work area.So right now, I just
have the Start page open.So I see the Start page
listed in my Open Items pane.This is a quick way to see
what's open in the work areaand manage it in the sense that
I can save all the items thatare opened up.I can close out of
all of those tabs.Again, it's a good way to manage
everything in the work area.Below the Open
Items pane icon, Ihave the Git
repositories pane iconhere and the Git repository
pane provides accessto basic Git
features, which allowsus to track changes
and manage versioncontrol among multiple users.The Git repositories
pane is a new pane,starting with Enterprise 8.2.So if you have
Enterprise Guide 8.1,you'll not see this pane there.That's what's by default
on the top portion.And let's come down to
the bottom portion here.I see these Servers pane.The Servers pane will show
a list of servers thatare known to Enterprise Guide.Below the Servers Pane icon
is SAS Folders pane icon.It will display a list of SAS
folders that I can access.Below that is the
Task Pane icon,and this is going
to display a listof those pointed quick
tasks that we canuse to help analyze our data.And the very last
pane here is goingto be the Prompt Manager pane.We're going to use this to
create, edit, and deleteprompts, which allow
for user input and tasksin SAS programs.So that's going to be our
navigation area there.Now because what I want to do is
use a task to analyze my data,I know I'm going
to need a project.So there's many ways
to start a project.I'm going to go to File,
New, and then select Projectsfrom this menu here.Now as soon as I
start a project,you'll notice a process flow
is automatically created.To back up a little
bit, I like to thinkof my project as my entire
story because, again, it'sgoing to save all the
contests that we're workingwith in Enterprise Guide.But within a story,
you have chapters,and that's what I like to
think of process flows as.It's a way to break
up all the workthat we are doing inside of our
whole umbrella of a project.So by default, we
have our chapter 1,which is this process flow that
is currently opened up here.Now as I actually start opening
up items in my project tablesand running tasks, we will see
those listed in this processflow tab here,
and we'll actuallydisplay the relationship
among those items.We'll see that once we
actually run a task.But now that I have a
process flow ready to go,let's go ahead and add a
table to this process flowor to this project.Again, many ways to do this--I'll go to File, Open.But there's one little trick
I want to point out here.You'll notice the
icon next to open.It's the yellow folder icon.That same icon is
also on my toolbar.So that's a shortcut.So let me go ahead to
use the Open a File iconto go to the open window.I'll click My
Computer, Browse, and Iwill be working with
the Customers table.Notice the extension, .sas7b.--that is the extension
used for SAS tables.So I'll select it
and click Open.Now first thing you'll notice
is that the table actuallyopens up for me to
view, and you'llnotice that it is going to
be a tab in that work area.If I go over into
the Project pane,you'll notice that, by
default, when I open upa file, like a table, it's going
to be added into my projectand to that active process flow.I just have one process flow,
so Customers is added there.Now one unique feature about
Enterprise Guide 8.1 and 8.2is the ability to move
around our tabs in our panesfreely within the
application windowand even outside of the
application as well.So to do this, I can really
just take my Customers taband drag it, and right
now, it is free floating.I can place this
wherever I like.Again, I can move it off
to a separate monitorif you have multiple monitors,
or as I'm dragging it,you'll notice a
layout guide thatpops up in the middle in blue.You can use it to dock this
in maybe the bottom portionof the work area, maybe even
the right side of the work area.So definitely take
advantage of that.But for now, I'm just going
to go ahead and move it backto its original location
but know that not just tab.So even the panes
can be freely movedaround inside and outside of
our Enterprise Skype interface.One point with that is if
you move your panes aroundand you'd like to go back
to the default setting,you can go to the View menu,
select Reset to Default Layout,and then click restore.That'll put everything back
to its original location.So if you move
things too much, youcan go ahead and take
advantage of that.But now that I have a
table added to my project,I can actually use a
task to analyze it.So with my Customers
table opened up,I'm going to go over
to my Task pane.And the task that
I want to use iscalled characterized
data wizard,and that's going to be under
the described category.So let me go ahead and expand
describe and then double-clickcharacterized data.Now the Characterized
Data wizardis going to create
reports, graphs,and output tables that describe
the main characteristicsof my table.Now because it is a
wizard, it's goingto guide us through
a couple of stepsto be able to create those
report graphs and outputtables.The Characterize Data
Wizard specificallyhas three really quick
steps that will go through.Now because I had the Customers
table opened up into work areawhen I was starting
the task, youwill see that Customers
was my active data source,and it's used as the input
table for this wizard.So step one of
the wizard is justverifying if that is our
input table, the table that wewant to analyze.That is.So step one is good to go.I'll go ahead and click Next.Step two is selecting
our report options.The characterize data wizard
can create reports, graphs,and SAS tables.And by default, all
three are selected,and I'm going to leave it as-is.I want all of my
report options here.Now looking at my SAS data
sets, my SAS tables that getcreated, take a look
at their table names.They have free car
frequency for Customers.It's a generic and long name.To make that a little bit
shorter, a little bit moredescriptive, in the Frequency
Data pane, I will click Browse,and I'll go ahead
and change a nameto customercounts, one word.And I'll go ahead
and click Save.You can do the same thing
for our unit area data table,but I'm good to
go with step two.So I'll go ahead and click Next.Step three allows us
to limit the numberof unique categorical
values thatwill be reported per variable.30 seems a little
bit high, so I'mgoing to go ahead and
bring that down to 15.That is it for the
Characterize Data Wizard.I'm going to go ahead
and click Finish.Behind-the-scenes is generated.That'll be submitted
over to SAS.Then it's just going to
take a couple of secondsto go ahead and execute.As this was executing,
although it is done now,in the bottom right-hand
corner of Enterprise Guide,there is a Gear icon, which is
our submission status button.And you can use this to see how
long it took a specific taskto run, the task status--in this case, there was a
warning and when it occurred,for example.So I see that this has
successfully completed.I'll go ahead and close the
Submission Status window,and let's take a look
at our results here.You'll notice that I have
a new Characterized Datatab in my work area.That allows me to
see the results thatwere generated from this task.To see the results
a little bit easier,I'm going to go ahead and
hold down my Control keyand scroll up on my mouse to
zoom in on the results justto make it a little
bit easier to look at.All right, so by
default, you'll noticethat on the Characterized
Data tab there are four tabs--code, log, results,
and output data.And by default, I am
on the Results tab.This is going to show me
my summary report and alsomy graphs.So for all of my categorical
or character columns,I'm going to see
frequency count.So the number of times
a particular age groupoccurred in the table--frequency counts there--
customer country.If you look at something
like customer first name,there are a lot of unique
values for first names.So because of that last third
step in the Characterized DataWizard where I limited
the number to 15,I will only see the
top 15 names listed,and everything else gets grouped
into this all other valuescategory.So we have a couple more
character columns here.I scroll down a
little bit further.I come to my numeric columns.And with my numeric columns,
I see some summary statistics,for example, the average
age, the median age,minimum-maximum
age, and so forth.Finally, at the bottom I have
graphical representationsof this information--so again, looking at something
like customer age group,looking at the
frequency distributionin a graphical manner--so again, a really,
really quick wayto get some of that
information about my data,including frequency counts
and some summary statisticsas well.Now aside from the
report in the graphs,I also requested
some output data.To check that out, I'm going to
click on the Output Data tab.Then this is essentially
storing the statisticsthat we saw into an
actual SAS table.I can use that same technique.I'm going to hold
down my Control key,and I can scroll up to
zoom in a little bit more.Here's one table that
contains so summarystatistics on numeric columns.And if I double-click
the other tab--this is for the
frequency counts--I get those frequency values
in a separate SAS table.Because this information is
stored in an actual SAS table,I could then take these
tables, place theminto a different task, and
even further analyze the datathat I have.All right, so let's go ahead
and take a look at our projectagain.Let's take a step back and
look at the big picture.If you look over into
my Project pane here,you will notice that the
Characterize Data Wizard wasadded below the Customers table,
which indicates its associationbecause the Customers
table was the inputto this characterized
data wizard.Let me go ahead and close
out of the CharacterizedData and Customers
tab, and let'sgo look at that process flow.The Project pane gives me
a nice high-level overviewof the contents of my project.But the Process Flow Tab is
where I can see more details.So here I can see that the
Customers table was inputto the Characterized
Data Wizard,and it generated three
different pieces of output.I have two tables--those are my two
SAS output tables--and then the report.That's the report plus the
graphs all saved into one filehere--all right, so again,
a lot more informationabout the relationship
among the itemsthat we have in our project.Before I explore that
Characterized DataWizard results further, let's
go ahead and save our project.What I'll do is I'll go to File.I'll go to Save
Project, project,and then I'm just going to go
ahead and save it in a folderhere for this course.And I'm going to go ahead
and call this Orion's starbecause, again, we're
analyzing Orion's star data.And I'll go ahead
and click Save.Now let's go back to
that Characterized Datawizard of results.I've closed out of the tabs, but
in order to bring the tab back,what I can do is double-click
on Characterized Dataeither in the process flow
tab or in the project pane.So let me go ahead
and double-click,Characterized Data.There's that tab again.And I can go ahead and
further explore my results.Let's take a look
at the other tabsthat we haven't had a
chance to look at yet.I'm going to start
with the Code tab.Remember that Enterprise Guide
task wizards are generatingSAS code behind the scenes.And I can actually take a look
at that code on the Code tabhere.And there is a lot of
code that was generated.All I had to do was go through
quick three steps in the wizardto get the results.But again, all that code
is here for you to view.If you'd like, you can take
this code, make a copy of itin a SAS program and maybe
make some modificationsif you'd like.So that's the Code tab.The one last tab we have
is the Log tab here.The Log tab is going
to display messagesthat were returned
from SAS, maybehow long it took to execute a
particular part of our code,maybe if there were
any warnings or errors.Those would be listed out
to us in the log here.At the top portion of my log,
it's called the Log Summary.And it's a quick way to
see the notes, warnings,and errors that were returned.I see that I have a
couple of warnings.But I have a lot of
notes in here as well.To quickly locate my
warnings, what I can dois go ahead and click on
the Notes tab to go aheadand deselect it.And now I have just Errors
and Warnings selected.And this makes it easier
to identify it as warnings.If I want more information about
a warning, I can click on it.And in the log, it'll
actually jump meto where that log message
actually occurred.So I can get more
information there.All right, so that was
a Characterized Datawizard and exploring
those four different tabsthat we got as a result of it.Let me go ahead and close my
Characterized Data wizard tab.And let's go back to the
idea of process flows.Now again, I mentioned that
process flows are like chapterswithin our project.And so what I'm
going to do is I'mgoing to go ahead and
create several processflows for the different topics
that we'll be talking about.For this first one
right here, I'mgoing to start by renaming it.So to go ahead and
rename it, there'sseveral ways to do this.But in my project pane, I'll
go ahead and right-clickwhere it says Process
Flow, select Rename,and I'm going to call
this one Lesson 1.To create a separate
process flowfor Lesson 2, what I can
do is go to File, New,and then Process Flow.This will open up a
another process flow tab.And I can go ahead and
rename this as well.Another way to rename is to
click on our View and SetProperties button on the
process flow toolbar.And let's go ahead and
call this one Lesson 2.And go ahead and click OK.So now you see in my project,
I have multiple process flow--Lesson 1 and Lesson 2.So now when you open up
items in your project,you have to be mindful
about which processflow it's being added to.To select your
active process flow,you can either make sure that
a particular process flowtab is opened, or
you can go aheadand select a particular process
flow in the project pane.Whatever is going to
be your active processflow is where your items are
going to be added when you openthem up inside of your project.So that was a quick overview
at the layout of our EnterpriseGuide interface and going
through the typical workflowof adding data to a
project and quicklyanalyzing it using
a task specificallyto Characterized Data wizard.I'm done with this
demonstration.So I'll go ahead
and save my project.So far in this tutorial, we
have worked with SAS tables.So we saw how we could open
up the SAS table in EnterpriseGuide, add that
table to our project,and then run some tasks
against that table to getsome analysis about our data.But sometimes our data isn't
contained in a SAS table.Our data may be contained in
something like a MicrosoftExcel workbook.Or it may be in a text file.You can think of
things like CSV,which are common
delimited files, or maybetab delimited files as well.So the question is, how
can we take the data that'scontained in these
different types of filesand analyze them using our
Enterprise Guide tasks?Let's start by taking a
look at this activity here.What we're going to do is we're
going to open up the ProductsExcel workbook from
inside of Enterprise Guideand we're going to
see what happens.So let me go into my
Enterprise Guide session.I'm going to first make sure
that my Lesson 1 process flowtab is open so that Lesson 1
is the active process flow.Then I'm going to
click on to open a fileicon to open the open window
and select My Computer.Now, my data is contained in the
data folder, which I've alreadyaccessed, so you'll see that
listed as a recent locationin the open window.So I'll go ahead and
select data from hereand then locate my
products Excel workbook.Now, just to remind
you, previously,if I was opening up a SAS
table, then what happenedwas that SAS table would open
up as tab in the work areaand it would be
added to my projectand I could see that
listed in the Project page.Now with an Excel workbook,
let's see what happens.I'll go ahead and click Open.And we'll see that actually
Excel is opening up.So instead of seeing that data
in a separate tab in the workarea, the Products
Excel workbookhas opened up for
me to view the datafrom directly inside of Excel.Now, if I close
out of Excel here,you'll actually see that
the Products Excel workbookwas added to my
project, specificallyin my Lesson 1 process flow.But I still can't
use it in a taskas is right now, because
if I try to open it,it just opens directly
inside of Excel.So let's go back
into the slides hereand talk about what
we can do to beable to use our Microsoft
Excel data or our text files,for example.As is, a lot of these
text files, for example,can be considered as
unstructured data.They don't have the
proper structure to themfor us to be able to
use them in tasks.So what we need to do is
import our data into SAS.How this goes is we're
going to take our dataand we're going to
import them into SASusing the Import Data wizard.Being a wizard,
again, it's just goingto guide us through a
couple of quick steps,then just specify some
specifications about the importitself.What happens with the
Import Data wizard isit's essentially going to
make a copy of our data--our text file or our
Microsoft Excel workbook--but in a structured
format, in this case,specifically SAS table.So once we import our data
using the Import Data wizardto create a copy of the
data as a SAS table,we can use that SAS table in
all of our Enterprise guidetasks for further analysis.In this particular
business scenario,Orion Star would like
to analyze informationabout their products,
but this informationis currently stored
in a Microsoft Excelworkbook called Products.In order to use
the data containedin this workbook in
various analytical tasks,we need to import this
data into a SAS table.So in this demonstration,
I'll show youhow we can import data from
a Microsoft Excel workbookinto a SAS table using
the Import Data wizard.Now, I have the Orion
Star project open.And I currently have the Lesson
1 process flow tap opened up.In a previous
activity, we opened upthe products Excel
workbook, and wesaw that it would open up
in Excel instead of insideof Enterprise Guide,
and a shortcutto that Microsoft Excel workbook
gets added to our project.So I'm going to go ahead
and double-click on productsin my Lesson 1 process
flow, and we'regoing to take a quick
look at this workbookand point out a
couple of things.So the first thing
that you'll notice hereis that in my Products
Excel workbook,there are two worksheets, which
we see as tabs at the bottom--one called ProductList and
another called ProdFormat.Now for this scenario, I am only
interested in the ProductListExcel worksheet.Now, if you take a
look at this worksheet,you'll notice that
the first row containswhat we want to be
considered as column names.But the very first column
actually does not have a name.So let's keep these things
in the back of our minds.I'm going to go ahead
and close Excel.And let's go ahead actually
import this data this time.Now, instead of
double-clicking, since that'llopen the data in Excel, in
order to import the data,I'm going to
right-click on Productsand select Import Data.This will open up the
Import Data wizard.And being a wizard, it's
going to guide us through fourquick steps to import the data.Step one is some specifications
about the data itself.We need to verify that the
Products Excel workbook iswhat we want to import in.And we can specify some
properties about the output SAStable as well.You'll notice the default
output table name is Products.But I'd like to go
ahead and change that.So I'll click Browse.And I will call this
table ProductList.And then I'll go
ahead and click Save.That's it for step one.So I'll Click Next
and move on step two.Step two is where I
verify which part of thatExcel workbook I actually
want to import in.You'll notice I can choose
a particular worksheet,I can choose a range of cells,
or if there is a named range,I can go ahead and
select that here as well.Since I want to read in just a
product list Excel worksheet,I'm going to leave it
at the default settings.But note that you have
different options here.On the right hand side,
I have a couple optionsI can choose from.The first that is selected
is first row rangecontains field names, meaning
the very first row in thatExcel worksheet has what
I want to be consideredas column names.That's true, so I'm going
to leave that selected.And the second option
is rename columnsto comply with SAS
naming convention.So I'll go ahead to
select this as well.With our SAS column
names, the rulesare they can be one to
32 characters in length,start with a letter
or an underscore,and then continue with letters,
numbers, and underscores.So if these rules are
not met, this optionwill make sure that our column
names are truncated or modifiedto follow our SAS naming rules.That is it for step number two.I'll click Next.Let's move on to step three.Now, step three is where I can
specify the column attributes.So I'm going to
go ahead and makethis window a little bit
larger so we can actuallysee all the attributes.All right, so you'll notice that
a lot of this, Enterprise Guidewent ahead and took
a guess for us.It took a guess at the name,
the label, the column type,output format, and more.And you can leave this as is.However, I'd like to
make a couple of changesto the attributes here.First thing is a
very first column.You might remember in Excel
that this column did nothave a column name.In SAS, columns have to
have some sort of name,so Enterprise
Guide went with F1.Not really descriptive,
so I'd liketo go ahead and change that.So to make that change,
in the name field for F1,I will triple-click.And I'm going to call
this column Product_ID.I'm going to do something
similar for the label.Now, the label provides
more flexibility.I can have spaces and
special characters.So I'll triple-click
in the label fieldand I'll go with Product
space ID for the label.That's it for that first column.Now, the only other
change I want to make hereis to the very last column,
which is Supplier_ID.Supplier_ID only
contains numbers.So Enterprise Guide
took a guess that thisshould be a numeric column.But this contains ID values.I'm not really going
to be performingany sort of calculations
on an ID column,so I'd rather store it
as a character column.To make that change,
in the type field,I'm going to select Number.Then I'm going to select the
dropdown arrow that pops up,and I'm going to select String.String is the same
thing as character.Now, I get this new pop-up
box that comes up here.And essentially
what I need to dois tell Enterprise Guide how
the length of this new charactercolumn should be determined.Now, the length of
a character columncorresponds to
the maximum numberof characters that can
be stored in each rowfor that particular column.So I need to make sure
that it's long enoughto account for the longest
data value that I'd have.Some options I have in terms
of determining the length--I can scan all
values in the column,I can scan just
a couple of rows,or I can skip the scanning
process entirely and go aheadand type in a specific length.Because this worksheet
is relatively small,I'm going to go ahead and
scan all values in the column,click OK.And if you take a look
at the length field,you'll notice that
the link is 5,meaning that the longest data
value was five characters.Now again, I can do
more changes here.I can change the output format.I can choose to only import
a subset of the columns,using this first
column of checkboxes.But these are the only
changes I want to make.I'm done with step three,
so I'll click Next.Step number 4, I do have
some advanced optionsthat I can choose from.I will not be using any
of these options now.But if you'd like to
learn more about them,you can always click
on the Help button.The Help button is
context sensitive,so it'll take you
to the documentationpage that's specific
to step fourof that Import Data wizard.But again, I'm all done so
I'll go ahead and click Finish.So I'll also go through and
complete the import process.And now I am looking
at that SAS table.Again, essentially, I created
a copy of the Microsoft Excelworksheet, specifically.And that data is now in a
SAS table as we see here.If I take a look here, I'm
viewing the column names.And I see Product_ID
instead of the generic F1.And my last column, I turned
it into a character column,which is represented with the
pyramid with an A inside of it.By default, we are viewing
the column name specifically.If you'd like to see the labels
instead, in the data gridtoolbar, you can click this
More Options, the threevertical dots, and then
select Show labels.I only specified a
label for Product_ID,but we see that represented
here in the data grid now.I'm going to go ahead and
go back to More optionsand go back to Show
names, but justknow that you have that
option available to you.That is it for my
importing, so I'mgoing to go ahead and
close this Import Data taband take a look at my
Lesson 1 process flow.You'll see again,
that whole story.I see that I have my
Products Excel workbook.I used the Import Data wizard to
import that data into a new SAStable.You've already seen that we
can import data containedin Microsoft Excel workbooks
into a SAS table usingthe Import Data wizard.Now in this scenario,
Orion Star hasdata about payroll contained
in a common delimited CSV file.In order to use this data
in various analytical tasks,this data will also need to
be imported into a SAS table.This is another job for
the Import Data wizard.So in this demonstration,
I'll show youhow we can import data contained
in text files into a SAS table.I already have my
Orion Star projectopened up with that Lesson
1 process flow tab active.And I'm going to start by
opening up the payroll CSVfile that we want to work with.So I'll click on the Open
a file icon on the toolbar.That opens the Open window.I'll go to my computer.And from my data folder, I will
open up the payroll.csv file.So when I select it and
click Open, being a text filethis actually does open
up in a separate tab.And I'm going to go ahead
and zoom in a little bitso we can take a look at the
contents of the CSV file.Pointing out just a
couple of things here,you'll notice that the very
first row of the CSV filecontains our column names.And all of our data values,
including our column namesare separated by commas, again,
because this is a CSV file.So with those
things in mind, I'mgoing to go ahead and
close the payroll tab.We see that file added to my
Lesson 1 process flow tab.So I'm going to use the
same technique I did earlierto start that
Import Data wizard,which is right-click payroll
and select Import data.Step one should
look very familiar.We're just specifying
that payroll is the filethat we want to import in.And we can make some changes
to the name or the storagelocation of our
output SAS table.But I'm going to leave
this at the default.I'm going to leave
the name as payrolland go ahead and
move on to step two.Step two is where we're
going to see the mostdifferences in terms
of importing datafrom Microsoft Excel workbooks
versus importing datafrom text files.Now with this being
a text file, you'llnotice that I need to first
specify what type of text filethis is.Does it contain
delimited fields or doesit contain fixed columns?This is a delimited file.And the delimiter is a comma.So I can actually
leave everything as is.But if you have
different delimiters,like it's a tab delimited file
or it's delimited by spaces,you can make that
selection here.But again, I'm going
with the default comma.And I have a couple more
options on the right-hand side.The first option is file
contains field nameson record number one.And that is true.That very first row
in the is CSV filecontains what I want to be
considered as column names.So I can leave that as is.And the second
option, Data recordsstart at record
number two, again,that is true as well,
because my actual data valuesstart on that second row.So I don't need to make any
changes in step two here.I can go ahead and move on
to step three and change someof those column attributes.As I did before, I'm going to
go ahead and expand this windowso we can see all of
the attributes thatare listed here.I'm just going to make
a couple of changes.The first thing I'd like to do
is exclude the job title columnfrom this import.Maybe I'm not interested
in it, and I don't reallyneed it in my output SAS table.So to avoid importing
it in, I can go aheadand uncheck that checkbox,
which is in the INC or Includedfield.That's what INC
stands for, included.All right, so that's one
change I want to make here.Another change I want to make
is to my Employee_ID column.Enterprise Guide took a guess
that this is a date column.But this is not a date column.It does not contain
any date values.So in the Type field, I'm going
to use that dropdown arrow.And I can change it to either a
numeric or string or charactercolumn.We saw string before, so I'm
going to go ahead and change itto a numeric column.And that looks good to go.The last change is just
some name and label changes.So for the very last
column, Termination,I'm going to go ahead and
triple-click in the name field,change the name to Term_Date.This is just to match the
format of our other columns,Birth_Date, Hire_Date.And while I'm at it, I'll
also change the label.So I'll triple-click
in the Label field.I'll call this
Termination space Date.That's it for step number three.So I'll go ahead and click Next.Step four, there are some
different advanced options,just because this
is a text file.Again, if you want to
learn more about it,feel free to click
on that Help buttonto go check out
the documentation.But I am done with
the import here.I'll go ahead and click Finish.And let's take a look
at that SAS table.You'll notice first,
our job title is notincluded in the SAS
table, because Iunchecked the checkbox and
made sure it wasn't included.Employee_ID has been read in
as a numeric column, indicatedby the blue circle with the
pound sign inside of it.And here, you'll see that
I have the Termination Datelabel appear.Now, I have the option
set to show the labels.But remember, you can click
on the More Options, threevertical dots,
and you can changethis to Show names to see the
column names or Show labelsto show the column labels.So if I go back to
Show names for exampleI will see the actual column
name, which I set to Term_Date.I'll go ahead and close
the Import Data taband take a look at that Lesson
1 process flow tab again.I see my payroll.csv file
going through the ImportData wizard to create
my output SAS table.Now one extra thing
I'd like to show you,and it's something that's
a good habit to keep,is to rename your task labels.For example here,
the Import Datawizard task label is the
Import Data payroll.csv.Now, that might be fine.But if you're doing
a lot of imports,they're all going to have
these somewhat generic names.So it's a good habit to go ahead
and rename them to somethingmore concise and descriptive.To make that change, I'm going
to right-click on Import Datapayroll.csv and select Rename.I'll go ahead and call this
Import Employee Payroll.Now, I did not rename the import
that I did to the MicrosoftExcel workbook.But you can do the same
process of right-click,Rename to go ahead and
make that a little bit moreconcise and descriptive as well.So that's how you import
data from a text file.We've seen that we can use
Enterprise Guide to accessdifferent types of data files.But oftentimes, that data
might not quite be in the shapeor form that you
need it to be in.For example, you may only
be interested in a subsetof the rows.So you may want to
filter the data.You may not have all the
columns that you actually need.So you may need to create
new columns based offof existing columns.You may also have your data
stored across multiple tablesand you need to join
or combine these tablestogether for analysis.All of these data manipulation
techniques I've mentioned,and more, can be done
in the Query Builderinside of Enterprise Guide.The Query Builder enables
you to extract datafrom one or more tables, based
on criteria that you specify.And its output is going
to create a clean, new SAStable that you can use
for further analysis.Behind the scenes,
the Query Builderis creating structured query
language code or S-Q-L SQLcode.To get a feel for
everything that wecan do in the Query Builder,
let's try out this activity.I'm just going to
open up a SAS table,take a look at the Query
Builder and see what allseems to be supported by it.So in my Orion Star project,
add the Lesson 1 process flowtab opened up.And I'm going to
the open window.And I'm going to go ahead
and go into my data folderand open the Orders SAS table.Now, on this Orders
tab, you'll noticethat on the data grid toolbar,
there's a Query Builder button.If I click on the
Query Builder button,this will open up the Query
Builder with the Orders tableas input.And I just want to
take a look hereto see what all it
is that we can do.You'll notice a Select Data
tab, picking and choosingwhich columns I like to
see in the output table.There's also an option to
Select distinct rows only, havethe Filter data tab to
filter the rows, Sort Datatab to sort the data.On the left-hand side, I have
Add Tables and Join Tables,meaning I can bring in multiple
tables into the Query Builderand combine the data found
in those multiple tablesinto one table.There is Computed Columns,
so creating new columns.I have access to
the Prompt Manager.There is a button for
Preview, Tools, Options.And there's a lot more that we
can do in the Query Builder.So over the next couple
of demonstrations,we're going to take a look
at some of the capabilitiesthat the Query Builder offers.In this scenario,
Orion Star wouldlike to analyze the total
amount invoiced to customerswho place internet orders.Information about all orders is
placed in the Order SAS table.Now, to prepare
this data for inputto various analytical
tasks, we needto generate a subset from
the Orders table, which wecan do using the Query Builder.So in this demonstration,
I'll show youhow we can use a Query Builder
to manipulate our data, whichwill include selecting
columns, creating new columns,filtering rows, and also
sorting rows as well.Now in my Orion Star project,
I have my Lesson 1 process flowtab opened.And from the previous activity,
I already have the Orders tableadded to that Lesson
1 process flow.So I'm going to go ahead
and double-click on Orders.And we're just going to
take a look at this table.And I want to point
out a couple of thingsbefore we jump into
the Query Builder.Now again, I'm
interested in customersthat placed an internet order.And how I know the order type
is by this Order_Type column.This is a coded column.But an order type of
1 means that it wasa retail or in-store purchase.An order type of 2 means that
it was an internet purchase.And then an order type of 3
means that it was a phone sale.So we're only going to be
interested in the ordersof type 2 for internet sales.In addition, I am interested
in the total amount invoicedto the customer.To calculate that
amount, we are goingto sum up the total retail
price with the shipping value.So those are going
to be the main thingsthat we'll focus on.Let's go ahead and jump
into the Query Builder.So since I have the
orders tab open,I can click on that Query
Builder button on the data gridtoolbar.The first thing I'd like to do
is go ahead and give my querya more descriptive name, and the
same for my output table nameas well.So for my query name, I will
call it Internet Orders Query.And for my output table,
I'll click Change.And let's change the output
table name to Internet_Profit.And I'll go ahead
and click Save.So in the Query Builder,
now, let's startwith the Select Data tab.Any columns that I'd like to see
represented in the output tableneed to be included in
the Select Data tab.So I'm going to pick and
choose a couple of columnsfrom the Orders table to
include in my new output table.There are several ways you
could go about doing this.First I'll show you how we
can select a range of columns.So if I want everything
from Order_IDall the way to
Order_Type, what I can dois select Order_ID,
hold down my Shift key,and then select Order_Type.That will select
everything between Order_IDand Order_Type.And then I can drag
all of those columnson to the Select Data tab.In addition, I'd like to include
these Shipping and Profitcolumns as well.So other techniques you can use
is double-click on a column.So if I double-click,
Shipping is included.Or you can drag columns as well.So I'll drag Profit onto
the Select Data tab.So again, many
techniques you canuse to include columns
onto that Select Data tab.Now, the order that you
see the columns listed hereon the Select Data
tab is the orderthat you'll see the columns
in the output table.So I just want to
make one change here.I want to take
total retail priceand I want to move it to
below the Order_Type column.So you can drag
these columns around,or you can use the arrows that
are on the right-hand side.I'm going to use
the move down arrow,click on it a couple of
times, so that it goes rightbelow the Order_Type column.All right, so those
are the of columnsI want to include from
my original input table.Now again, I'm interested
in the total amount invoicedto the customers.And that's a new column
that I'll have to create.So to create a new
column, I'm goingto click on the calculator
icon on the right-hand side.And if you hover over it, it
says Add a New Computed Column.This opens up the new
Computed Column wizard.And just click four steps
to define a new column.In step one, since I'm going
to be using an arithmeticexpression to create
this new column,I'm going to select
Advanced Expression.And that's it for
step number one.I'll go ahead and click Next.In step number two, I'm going
to specify the expressionfor this new column.Now remember, the total amount
invoiced to the customeris the total retail
price added togetherwith the shipping amount.So I can go ahead and
type in that expressionhere in the expression box.But I can also point and click
to create this expression aswell.So to do this, what
I'm going to dois I'm going to expand in
the bottom left-hand cornerwhere it says Selected Columns.These are going to be the
columns that are includedon that Select Data tab.To include a column
into the expression,I can just double-click on it.So I'll first double-click
on Total Retail Price.That gets added
to my expression.And let me go ahead
and zoom in hereso we can see that
a little bit better.And I want that added
to, so I'll go aheadand you can type
a plus sign or youcan click the plus sign button.And then I'll
double-click on Shipping.Right, so total retail
price plus shipping.You'll notice that in front of
the column name, you see t1.t1 is simply a table alias.You can think of it as a
nickname for the Orders table.It doesn't hurt to
have it in here,but it's actually not
necessary in this case.So if you wanted to type
out the expression yourself,you would not need
to include t1 here.But my expression
looks good to go.So I'm going to go
ahead and click Next.And then step three
specifies someof those column attributes.First thing is a column name.I'm going to call this column
Invoice_Amt for amount.And this is going to
be a currency value.So it may be nice to apply
a format to this columnso that we can see dollar
signs and commas representedin our output table.So to apply a format, I'm
going to click on Change.Again, this is a currency value.So under the Categories
pane, I'll select Currency.We have a lot of options
for formats here,but I'm going to select
the DOLLARw.d format.And again, formats are all
about how our data values aregoing to be displayed.The dollar format will display
our values with dollar signsand commas included.Now my overall
width is the numberof spaces I'm allowing
for this formatted value.So I just need to make
sure that it's wideenough so that all of the
numbers, dollar signs, commas,and decimal places can
be properly displayed.In our case, I'm going to
bring this up to about 10.That should be enough
for our value here.I'd also like to see
some decimal places.So for decimal places, I
will bring the value up to 2to say display two
decimal values.That is it for the format.I'll click OK.I see the dollar format
represented here.And that's it for step three.I'll click Next.Step four is just a verification
page, a summary of everythingthat we specified.It all looks good, so I'll
go ahead and click Finish.You'll notice that
the invoice amountcolumn has been automatically
added to my Select Data taband it's also in the list of
columns under Computed Columns.This means that I can take
this new Computed Columnand use it to filter my data or
even store all my data as well.All right, but those are all
the columns I want to include.That looks good to go.So let's move on to the
Filter Data tab now.Now, I'm only interested
in internet orders,so I need to create a filter
on the Order_Type column.So to go ahead and
start that, I'mgoing to locate Order_Type
in my list of columnsand drag it on to
the Filter data tab.This is going to start
the New Filter wizard.And again, just two
quick steps in orderto create this filter here.I want the order
type to be equal to--and I want it to be equal
to 2 for internet orders.Now, you could type
in a value of 2 here.Or you can actually retrieve
the value from the input table.If you click on this down
arrow here to get valuesand then click on the
Get values button,Enterprise Guide will go look
at your Order_Type columnand give you a list of the
unique values in that ordertype column.So I could go ahead
as select 2 hereand they'll be added
in to my value box.Go ahead and click Next.Step two, again,
is just a summaryof the properties
I've specified.Everything looks good, so I'll
go ahead and click Finish.Last thing is to store
our data, so I'llclick on the Store Data tab.And here, I'd like to sort
on the Order_ID column,so I'll double-click Order_ID.That gets added to
my Sort Data tab.And I can actually choose
a source direction,ascending or descending,
but I'm goingto leave it at the
default descending order.That's it.I'm done with
everything I want to do.So I'm going to go
ahead and click Run.The query is executed, and here
is our brand new output table.You'll notice that every row
has an order type of 2 becauseof the Filter that I set.And I have my brand
new Invoice_Amt columnthat is a total of the Total
Retail Price and Shippingcolumns.That's it.That's some of the
capabilities and functionalitythat the Query Builder offers.We've seen that we can use
the Query Builder to performa lot of data
manipulation techniques,including creating new columns,
filtering rows, and sortingrows as well.One other common data
manipulation techniquethat you may need to
use is the abilityto join or combine
tables together.Joining tables means
that we're taking columnsfrom different tables
of multiple tablesand bringing them together
into one single table.Let's look at our
business scenario here.We have this table
called Productsand it contains information
about Orion Star's products.It includes things like the
product ID, product category,and also the product name.In addition, it contains
some informationabout the supplier
of the product,like the supplier
country, name, and ID.Now, the supplier country
column contains country codes.So it's just a
two-letter country code.If I want more information
about that country code,I would need to look at
a different table calledcountry_region_lookup.country_region_lookup
will actuallyexplain what each of
those country codes mean.And it'll even
tell me the regionthat the country falls under.So since I want more information
about my supplier country,I would want to join
these tables togetherso I have all columns from
both tables available in onesingle table for me to use.Now, when we're performing a
joint in the Query Builder,by default an inner
join will be performed.But before I go into the
different types of joins, let'stalk about what a
join really means.When you are combining
tables together,there needs to be
some sort of criteriaso that you know that Row 1
from Table A goes with Row 1from Table B. That's going
to be our joining criteria.So let's take a look at
this simple example here.I have a table called
employee_payroll,and it contains some salary
information about my employees.I have another table called
employee_organization,and this contains
department informationfor all of my employees.What I'd like to do
is join these tablestogether so that I have both
the salary and departmentinformation for my employees
in one single table.That's a join.Now, how do I know each
row from a single tablegoes with another row
from another table?In this scenario,
it's going to bebased on the values of
the employee ID column.I know that if there is a match,
then those rows belong togetherin my new output table.So for example, the
employee ID ending in 01,we see it in employee_payroll.We see it in
employee_organization.That's a match and so we'll
see that salary and departmentinformation all
in one single row.That's a join.Now, specifically
with an inner join,we are only going
to keep the matches.So 01 is in employee_payroll.01's in employee_organization.That's a match.It'll be included in
our new output table.Same goes for 02 and 03 as well.However, employee
ID 04, for example,only exists in employee_payroll.There's not a match there.Employee ID 05 only exists
in employee_organization.Again, not a match there.Because I'm performing
specifically an inner join,which is a default type of
joint in the Query Builder,only matches will be
included in my output table,which includes employee IDs
01 all the way through 03.Now, there are different
types of joins that we canperform in the Query Builder.But in this tutorial, we're
going to focus on inner joins.We're going to revisit a
previous business scenariowhere we want more
information about Orion Star'sdifferent products.Information about
their products wasimported in from the
Products Excel workbook.But now, the analysts
at Orion Starwant more information
about the supplier country.This information can be found
in a separate SAS table calledcountry_region_lookup.So to have all this information
in one table we needto join the tables together.So in this demonstration,
I'll show youhow we can use the Query
Builder to join tables together.In my Orion Star project in the
Lesson 1 process flow tab here,I previously imported the data
in from the Products Excelworkbook into a SAS table.So I'm going to start by
double-clicking on the tableicon--this is the product list
table that I created earlier--so we can take a
look at its contents.Again, our focus right now is
the Supplier_Country column.This is simply a
two-letter country code.And I want more information.I want to know what
the country code standsfor as well as a region that
this country belongs in.This information is again
found in a separate tablecalled country_region_lookup.We've been using the Open
window to access our files upuntil this point, but I want
to show you another way.What you can do is
in the Servers pane,you can go ahead
and expand servers,expand the name of your server--mine is called local--expand files.And from there, you can
follow the path to the data.So if I scroll down,
I have my data folder,which contains all the data
we need in this tutorial.Here's country_region_lookup.And previously, what happened
with the Open window iswhen you open the data,
it opens in the data gridand it automatically gets
added to the project.That is also the case when
you open data from the Serverspane, but you actually have a
little bit more flexibility.What you can do is you can
right-click on the table name.And you'll notice that there
are several options here.You can choose to simply open
the table without adding itto your project.You can open the table
and add the tableto your project, which
again, is a default behavior.Or you can simply add
the table to the projectwithout opening it up.So again, you have
a lot of flexibilityin terms of how items are added
or not added to your project.You can also change the
default behavior, as well, justto quickly show you
where to find that.If you go to the
Tools menu at the top,and then go to Options,
which is at the bottomof the Tools menu.This opens up the
Options window.Specifically, if you go to
Project and Process Flows,you can clear the
first option, whichsays when an item is
opened, automaticallyadd to the project.That way when you
open up a file,it will not be added to
your project automatically.You can then choose to manually
add items to your project.Now, I'm going to stick
with the default behavior.So I will not clear
this checkbox.But just wanted to show
you that again, youhave some flexibility
when it comes to projects.So I'll go ahead and close
out of the Options window.I would like to actually
add country_region_lookupto my project.And I want to view
it as well, so I'llgo ahead and double-click
on it so we can take a look.Now, before I perform
a join, ideally I'dlike to look at these
tables side by sideor stacked on top of each other.So I mentioned before that
you can move the tabs freelywithin or outside of
the application window.But another trick is if you
right-click on the tab--so I'll right-click on
country_region_lookup--I can select either New vertical
tab group or New horizontal tabgroup.I'll go with horizontal.And this allows me to have a
stacked view in my work areaso I can have
something on the topand then something
else on the bottom.So in my case, I have
the imported tableon top and country_region_lookup
on the bottom.Now, comparing these
two tables, again, Ihave supplier_country
with the country codesup top, which is my
product list table,and I have a similar
column but it'scalled country_code in my
country_region_lookup table.But it still contains those
two-letter country codes.What I need to do is perform
a join between country_codeand supplier_country so that
I have the full country nameand region name together
with all the informationabout my products.So that's the goal here.So in order to start
the Query Builderwith both tables
as input, I'm goingto go ahead and first close out
of both of these table tabs.And let's go back to that
Lesson 1 process flow.One way I can start the Query
Builder with both tablesis select one of the tables.I'm going to hold
down my Control keyand select the other table.With both tables selected,
I can right-clickon one of the tables and
then select Query Builder.This will start
the Query Builderwith both tables as inputs.However, you notice
that you willsee a warning message saying
a suitable join can notbe determined for a new table.So we need to do
the joins manually.That's not a problem.I'll go ahead and click OK.And let's take a look at
this tables in Joins window.I mentioned earlier that
Enterprise Guide, by default,performs an inner join.And by default, it's
going to perform and innerjoin on columns that have
the same name and arethe same type.However, we saw that
our columns thatcontain that same information
don't have the same name.One is called
supplier_country and the otheris called country_code.Because they didn't
have the same name,SAS did not automatically
perform that join.But no worries, we can go ahead
and manually perform the join.The one requirement is
that the column typesdo need to be the same.And that is satisfied here,
but it's because both of theseare character columns.So to go ahead and
perform the join manually,I'm going to start by
selecting supplier_countryin the product list table.Then I'm going to drag
supplier_country--I'm going to try one more
time-- drag supplier_countryto on top of
country_code and release.Now the join will be
formed on these columns.But I can specify what type
of join I'd like to perform.I'm going with an inner
join, so only matches,which is the default.
But again, thereare other types of
joins available for youto include non-matches in your
output table if you'd like.But again, I'm going
with the inner join.It's an equality.I want supplier_country to
be equal to country_code.I'll go ahead and click OK.I see that they are now
connected with an inner join.So I'll go ahead
and click Close.And now I'm back into
the Query Builder.The only difference
now from beforeis that now I have
two tables listed hereon the left-hand side
rather than just one.Now, all columns from
both of these tablesare available for
me to use anywherein the Query Builder,
which is really powerful.I'm going to go ahead and start
the Query Builder as I usuallydo, go ahead and start by
providing a name for my query.I'll call this
Products Info Query.And for my output table
name, I'll click Change.And I'll call it Products_Info.And go ahead and click Save.Now, I'll go ahead
and pick and choosethe columns I want to include.I want to include
almost everythingfrom the first
product list table.So as a shortcut,
what I can do istake where it says
t1PRODUCTLIST,drag that onto the
Select Data tab.And that'll include all
columns from product listson that Select Data tab.But since I've
performed a join, I'dalso like to see the columns
from country_region_lookup.Specifically, I'd like
the full country name.So double-click on country_name,
and then also the region.So I'll double click
on region since I'mgoing to have the
full country name,I don't really need that
two-letter country codes.So I'll go ahead and
select supplier_countryon the Select Data tab and then
click on the trashcan icon,which is our delete button.So supplier_country will not be
included in our output table.Now again, you can take
this a step further.You can use any of these
columns to filter all your rows.You can use any of these
columns to sort your table.But this is all I
want to do here.I'm good to go.So I'll go ahead and click Run.And let's take a look at
our output table here.So I have information
about all of my products.But now I just have
more informationabout the supplier specifically.We see that I have the
supplier's country with itnamed fully spelled out.So instead of just US,
I have United States,and also the region included
as well, so North America.Let me go ahead and
close out of this tab.And I just want to show you one
thing on the Lesson 1 processflow tab.When you perform a join,
you can actually, again,see that relationship
in the process flow tab.I see that both the table that
was imported from the ProductsExcel workbook and the
country_region_lookup tableswent into the products info
query to perform that join.And the result of the join
was a table that we were justlooking at, Products Info.So again, joins are
really powerful.It allows you to bring
columns from multiple tablestogether into one single table.So far in this
tutorial, we've seenhow we can use Enterprise
Guide to accessdifferent types of data and
then manipulate and preparethat data.Now that we have nice
clean data to workwith we are finally ready to
start analyzing this data.And we're going to do that
using Enterprise Guide Tasks.So what's a task?Tasks are point-and-click
Interfacesthat are going to guide us
through analytical and datapreparation processes.Now, there are tons of tasks
available for us to choosefrom in Enterprise Guide.And we'll go ahead and explore
some of those in just a bit.But these tasks
are actually goingto be based off of SAS
procedures or SAS code.So as you point-and-click
your way through a task,behind the scenes,
SAS code is generated.And when you run the
task and that SAS codeis submitted and
executed, as output,we're going to get
formatted results.And these results are going
to depend on the tasksthat you're using.But they could be reports,
they could be graphs,or they could even be
output tables as well.Now, before we go
ahead and take a lookat some of the tasks that are
available in Enterprise Guide,I wanted to quickly talk about
the difference between tasksand wizards.Now, so far in this tutorial,
we have seen some examplesof wizards, specifically.So we saw things
like the Import Datawizard and the
Characterized Data wizard.Now, these are called
wizards, specifically,because they guide us through
the process of creatingour results.You might remember that there
were steps one, two, three,or steps one, two, three, four.And we were able to
quickly get the resultsthat we were looking for.That's what a wizard is.However, if you take a
look at the list of tasks--which I'll again show
you in just a bit--there are going to
be some tasks thathave both a wizard version
and a task version,instead of just a wizard
version like the ImportData wizard and Characterized
Data wizard, or just taskversions.One example is the Bar Chart
wizard and the Bar Chart task.So what's the difference there?Well, again, with a wizard, or
something like the Bar Chartwizard, it is guiding
us through the processof creating our output steps--one, two, three,
four for example.And wizards are really
meant to be a very quick wayto get the results
that we're looking for.And because of
that, there's goingto be a limited number
of options that areavailable in the wizard itself.Now with a task, tasks are
going to have more optionsthat we can set more
customization options thatare going to be available.So if there are tasks that have
both the wizard and the taskversion available, and you want
to know which one you mightwant to use, well, if you just
want to quickly get results in,you don't need to
set too many options,or you don't need to customize
the results too much,then you can use the wizards,
again, quickly get the results.However, if you're looking
for more customization optionsto really customize the look of
the output that you're getting,you'd probably want to go
with the task version instead.So just kind of keep that
in the back of your mind.With that, let's go ahead
and actually look at the taskthat we have.Now, we're not going to be able
to talk about all the tasksthat we have in Enterprise
Guide because there are a lot.But I want to show you where
you can find these tasksand where you might be
able to get some moreinformation about them as well.So let me hop over
into Enterprise Guide.And I have my Orion Star
project open up here.And we want to take a look at
the task pane in our navigationarea.Now, right now, you'll
notice that my tasksare by default categorized.And if I wanted to look
at the different tasks,I could expand a
certain category,like data, for example,
and see the tasksthat fall under that category.So under Data, for example,
I have a Data, Compare Data,and more.Now, if you don't like looking
at tasks through categories,you'd rather just see a
listing of tasks, instead,you can change that.So in the Tasks pane,
there is this button.And it's our Sort button
with the up and down arrow.If you click on the Sort button,
you can choose Name instead.And this will give you
an alphabetical listingof all the tasks that
we have available to usin Enterprise Guide.So that might be
something that you prefer.In addition, because
there are a lotof tasks that are
available, you cansearch for certain tasks using
this filter to task list searchbutton.Go ahead and type
in, for example--and I'll keep going
to this example--but bar chart.And here we will see
some of the tasks thatallow us to create
these bar charts,which again, there are
a lot of options here.Let me go ahead and
click X here and go backto our list of all the
tasks that we have here.Now, to find out
more informationabout a task very
quickly, what you can dois simply hover over a task.So for example, if I
hover over Append Table,I can see some information.Let me just hover over
that one more time.I can see the category.It's in the data category.It's a SAS procedure that's
generated behind the scenes,which is going to be SQL.And then a description.It concatenates a
series of tablesto create an output dataset.And again, that's just
a quick description.But because there
are so many tasks,it's nice to be able to
just simply hover overa particular task or
wizard and quickly getsome information about it.One other thing I'd
like to mention hereis maybe there are going
to be tasks that you'dlike to use quite often.So maybe for your work,
you tend to create--let's go, I keep
using bar chart,so let's do something else.Let's say a donut
chart for example.One thing that you
can do is you canmake certain tasks a favorite.There is a little star icon
next to each of the tasks.If you select that star--again, I just did this with
a donut chart as an example--this task is now a favorite.This is going to be
nice because now youcan quickly access this task.For example, right now
the view in the Task paneis SAS Tasks, looking at all
the tasks that I have available.If I use that
drop-down menu, I canchange it to Favorites to just
quickly see my favorite tasks.And by default, the Import Data
Wizard and the Query Builderare going to be favorite tasks.Not only that, but if
you open up a table--and I'm just going to
open up a random tableI have in my project--do the Orders
table, for example--there is a task button
on a data grid toolbar.And using that task
button, you can quicklyaccess your favorite
tasks and thenalso some recent
tasks here as well.So again, if there
are tasks that you'regoing to use all the
time, I would highlyrecommend favoriting
them, because thereare so many tasks that we have
available in Enterprise Guide.With that said, again,
we don't have timeto explore all of these tasks.But we are going to
take a look at a couple.So let me hop back
over into my slidesand quickly mention
the tasks that we'llfocus on in this tutorial.So in this tutorial,
we are goingto mainly look at two tasks.We will look at the
one-way frequencytasks which will allow
us to quickly generateone-way frequency reports.And then we will also look
at the bar chart tasks, whichwill help us create bar charts.And these tasks and more can
be used to analyze your data.But even more so,
you could use themto explore your
data if you wouldchoose to do that instead.In addition, I think we'll
also take a look at a pie chartas well.So similar to a bar
chart, but again justlooking at a different
type of graphsto see how we can use
these tasks to generatedifferent types of output.Now, as we go through these
different types of tasks,there are going to be
some options that arespecific to the task itself.For example, in
a bar chart task,you specify what shape you
want the bar charts to be.But you probably wouldn't do
that in the one-way frequencytask, because you're
creating a report.however there are going
to be some optionsthat we will consistently
see among most of the tasksthat we work with.And I want to go ahead
and point those out here.For most of the tasks that
you'll see in Enterprise Guide,you'll most likely
start by selectingsome sort of input data source.Which table are we
actually analyzing?And in addition, you might
want to apply a quick filterto that table as well.In addition, you
will most likelyneed to assign your columns
to various tasks roles.And these task roles
tells us how a columnshould be used in the task.Should this column be
our analysis column?Should this column be our
grouping column, and more.There are different
types of task roles,depending on the tasks
that you're using.But all of these options
here-- so input, data source,filtering, and setting
our column rules--are going to be done in
the data panel of a task.In addition, if your task is
creating some sort of a report,then you probably want to
add in some titles, somethingthat's going to be related
to the content that'sbeing reported on.So you'll most likely
see a Titles panelwhere you can set
titles and footnotes.Another comment panel
and task is goingto be the Properties panel.And the Properties panel allows
us to rename that task label--so the label that we're seeing
represented in our project.Then we can also modify
our output format as well.So instead of seeing
a default HTML output,you may want to
choose to generatethe results as a PDF or maybe
even Excel file as well.So throughout this
tutorial, we aregoing to use this as a basis and
explore the one-way frequenciestask and also the bar chart
and pie chart tasks as well.Let's go ahead and start looking
at our first task, which isthe one-way frequencies task.Now, the one-way
frequency task isgoing to generate one-way
frequency reports.And the idea with
this task is that it'sgoing to help us address
the question of how many.Some examples using
the Orion Star datacould be how many products do I
have in each product category.How many orders were
placed in each order type?How many employees are
in each department?Or maybe even how many customers
are in each customer age group?Again, all of these
questions are about how many.And that's exactly what
the one-way frequencytask allows us to address.Before we take a deep dive into
the one-way frequencies task,I just want to take
a quick look at itto be able to address
this one question here.I mentioned earlier
that in most tasks,you will need to assign your
columns to various task rolesto specify how that column
should be used in that task.Again, is it our
analysis variable,is it our grouping
variable, and so forth.So the question here is looking
at the one-way frequency task,is there a task role that
requires a variable or columnassignment?Let's go into the
Orion Star projectto see if we can
answer this question.So what I'm going
to do is I'm goingto start by selecting
really, any table that'salready in my project.I'm just going to select
Customers in the Project paneand make that the
active data source.With it selected, I'm
going to go to my Task paneto go ahead and locate the
one-way frequency task.Now, I have it in categories,
but if you have itin the alphabetical
order instead,so just the listing
of tasks, youcan just scroll to
the O's and you'llbe able to find one-way
frequency there.But I'll go ahead and expand
the Describe category--that's where you'll find
the one-way frequency task--and then double-click
one-way frequencies.Because I had a customer
selected in the Project pane,it was my active data source,
so it is automaticallyused as the input data
source to this task.Now, if you look at
the Task Roles pane,I see that there are three
roles available in the one-wayfrequency task--Analysis variables, we
have the Frequency count,and then we also have the
Group analysis by role.And Enterprise Guide makes
it pretty easy for us.Is there one that's required?Well, probably the
analysis variables,because it says a
variable required,right underneath that task role.Then you'll notice
out right now,because I have not assigned
a column to this task role.I can't click on the Run button.So if I make, at least,
some selection here.So if I maybe take customer
age group for example,and drag it to that
analysis variables rule,now I have done that
required assignment,and just by doing that one thing
that run button is now enabled.So what's really nice about
these Enterprise Guide tasks,is that, really at
the bare minimum,you just need to select
your input data source,and assign columns to
require task roles,and you will be able to
get a default output,just by doing those two things.Now in upcoming
demonstration, we'regoing to focus more on the
other types of customizationsthat we can make in this task,
to get the report that weare looking for.In this scenario, the
how many question,that we'd like to address
is, how many productsdoes a Ryan star offer
for each product category?So in this demonstration,
I'll show youhow we can use the
one way frequenciestest, to generate
one way frequencyreports, that help us address
these how many questions.In Enterprise got to have
the Orion Sar project open,and I'll be working in
the lesson to process flowthis time.So what the lesson to
process will open and active,I'm going to use a service
pane to add data to my project.In the server's pane, I've
already expanded servers.My server name, which
is local, and files.And I've already expanded out
to the folder that containsthe data this is tutorial.Now, I'd like to use
the products table.And I don't, necessarily, need
to open and view this table.I just needed to add
it to my project,so that I can use it in task.So I'm going to use that
trick I showed you before,which is to right click
the Products table,and select Add to project.That way gets added
to my project,but I won't open
up from me to view.With the table added, and
making sure that it's selected,so that is the
active data source,I'm going to go
to the task pane,expand the describe
category, and then doubleclick on One Way frequencies.This opens up the
one way frequencytasks, with the
products table as input.Now, by default, we
are on the data panel.And this is where we can
choose our input table,apply a filter, if we'd
like to, and assignour columns to task rules.Now, I mentioned
earlier that we needto have some sort of column
assigned to the analysisvariables role.It's required.But we didn't really talk
about what that role means.So if you want more information
about these different taskroles, or really you'd want
more information about the task,in general, you're going to want
to click on the Help button,in the bottom right hand corner.So I'll go ahead
and click on Help.This is context
sensitive, so it'sgoing to take just
a second to load.But what's happening,
is I've beentaking to the documentation page
about the one week frequencytask, but it's specifically
about that data panel.So we see in this
page, I'm havingmore information about
those different task roles.I can see the analysis variable
roles, specifies variablesto be analyzed, and
for each variablethat we assign to
this role, the taskcreates a one way
frequency table.And if I wanted to assign
columns to other task roles,I could go ahead and read
about that right here.If you want more information
about other parts about the oneway frequencies task,
you can go aheadand use the help on when
you're in a different partof the task, or, now that we're
already in the documentation,I can simply navigate to
the different options,or the different portions under
do one way frequency task.Like if I wanted to learn more
about the statistics option,I can like down
this button here.And you'll notice that
there are sectionsfor all the different
tasks that we haveavailable in Enterprise Guide.So take advantage
of the Help button,in a documentation
that's available,so that you can learn more
about the different optionsand settings that
are available in allof these different tasks.But now let me know about
the analysis variables rule,I'm going to go ahead
and minimize my browser,and return back to the
one way frequencies task.Now, remember, in
this scenario, Iwould like to know
how many products,which correspond to rows, there
are for each product category.So in this case,
product categorywill be my analysis variable.So I'm going to select
product category.I could track it over to
the analysis variables rule,but I'm going to,
instead, select it.Click on the plus sign, and
then assign it specificallyto the analysis variables rule.Now, again, this is the only
task rule that is required,and, because I did
that one thing,I can actually go ahead and run
and create a report right now.I'm, actually, going
to go ahead and dothat, so we can see
the default report,then will come
back into the task,and make some
additional changes.So I'll go ahead and click Run.Here's our report.I'm going to zoom
in a little bit,so we can take a
look at what we have.The report's title, one
way frequencies, results.And you'll notice that
in my output report,I have one row for each
unique product category.And I have the frequency,
the percentage,and then, also, the cumulative
frequency and percentage,as well.So, for example, I know that
looking at just one example,here, in the Gulf
product category,there are 346 products,
which make up about 6%of all of the products
that are available.Again, of long with the
cumulative frequencyand percentages.So this is a good
starting point,and already helps us to
address our question.However, maybe I'd like to
make some modifications.Maybe I only want
frequency percentage value,then maybe it'd be nice to
make title a little bit moredescriptive.If I want to go back and
make changes to the task,I do not need to
start from scratch.What I can do, is you'll
notice on the task toolbar,there is a modified task button.When I click on this button, I
am back right into the one rayfrequency task, and
I can just continueto make additional changes.So let's go ahead and
make those changes, here.I'm going to start by
going to statistics,and the selection pane.And I'm going to focus on
frequency table options.Again, the default is to have
frequencies, percentages,and the cumulatives.I just want frequencies
and percentages.So I'll go ahead and
select that option.That's it for
Statistics, for now.How about, along with
the summary report,also creating a bar chart to
have a graphical representationof the data?Well, to do that in
the selection pane,I can select plot.I have the option of, either
a horizontal or vertical barchart.For this example, I'll go
with a horizontal bar chart.Let's take it a step further.Along with a report, a graph,
and then let's go aheadand add an output table.Let's store this
data into a table,so that we can use it
for further analysis.So I'm going to
click on Results.I'm going to go ahead and
select the checkbox thatsays Create Data set, with
frequency and percentages.And the default
name for the tableis, again, generic and long.Let's go out and change that.Click Browse, and,
for this example,I will call the
table Product Counts.Product Counts, and click Save.Now, continuing on with
the report that we have,we know that the title right
now is the default, one wayfrequencies results.I'd like to make that a
little bit more descriptive.So in the selection pane,
I'm going to select titles.Now, titles is one
of those panelsthat you will see very
often in a lot of tasks,because most house will
generate some sort of report,where you might want to
specify a specific title.So to change the
title, I'm goingto clear a checkbox that
says, use default text.I'm going to delete
the default title.And I'm going to change
the title to numberof products per category.Number of products per category.All right.Last thing that
I'd like to do hereis, go ahead and
change the tax label.If you look at the
project pane, you'llnotice that the default tax
label is one way frequencies.It's really just the
name of the task.That might be OK, but if
you use a one way frequencytask multiple times, you would
have one way frequencies, one,two, three.And it'll be hard to
distinguish between all of them.So let's go ahead and make a
task label more descriptive.I'll click on Properties,
in the selection pane.And here I'm going
to click on Edit.There's a lot of things we can
do in the Properties window.We'll see some of it later on.But for now, I'm simply going
to modify the label field,and change it to
products per category.Products per category,
and then, click OK.So I've made a lot
of adjustments.Again, to remind you, I'm going
to move the one way frequencytask window.What we have here is
the default report.All I did was assign a column
to the analysis variables rule.Now that I've made some
changes, let's give this a run.And let's see what our updated
report's going to look like.Here we go.The title, the lot more
descriptive, than just one wayfrequencies, that we had before.I'm only seeing the frequency in
percentage values in my reportnow.Then I scroll down.I have a graphical
representationof this information with
a horizontal bar chart.Again, sometimes having that
graphical representation,can help put the numbers
in some more context,give it a different perspective.Now, in addition to the output,
the report, and then the graph,I also requested
an output table.So if I take a look at
the output data tab,here's that table.It looks very similar
to my report, whereI have a count column
for the frequency count,and percent for the percentages.It's just now stored
and an actual SAS table,which I could then take,
put it into another task,and perform some additional
analysis on it if I'd like.All right.So that's looking pretty good.But there's one more change
that I'd like to make.So I'm going to go back
to the results tab.We changed our title.But you might have
noticed this extra title,that was automatically added in,
which says the FREQ procedure.Now, I did not request
for this to be added in.I did not have that in myself.This is simply
automatically added in.You might remember me
mentioning that all of our taskscorrespond to SAS
procedure, or SAS code.And the name of that
procedure, sometimesgets automatically represented,
depending on the procedureor task that's used.So in this case, we see
this actual procedure titlecalled the FREQ procedure.This may be useful.In my case I'd like to
go ahead and remove it.So to remove any of
these procedure titles,you'll want to go
to the Tools menu,and then go to
Options at the bottom.And in the options window, I'm
going to go to Task, general.So that's the general
subcategory under task.And you'll notice
this checkbox thatsays include SAS procedure,
titles, and results.I don't want that.I simply need to clear this
checkbox, and then click OK.Now, I made that change, but I'm
still seeing the FREQ proceduretitle represented.All I need to do, is just
run this task one moretime, to get the most
up to date results.We've seen that we can
use modify task, whichwill go back into the task, and
let us make additional changes.If all you need to
do is rerun the task,without making changes
to the task itself.So in our case, we change
the system setting,or maybe the input
data has updated,but you don't need to make
changes to the task itself.You can simply click
on Run, instead.So click Run.Here's our final updated
report, and now yousee that we no longer have that
procedure title represented.So that's how you can easily
make a one way frequencyreport, using the one
way frequencies task.I'd like to address a
different how many question.And in this scenario,
Orion Star wouldlike to know the number
of employees by job title.And they'd like to take
this a step further,and group the results
by department.So in this demonstration, we'll
revisit the one way frequenciestask, but I'll show you a
couple of additional options.In Enterprise Guide I have
my Orion Sar project open,and I will be doing work in the
lesson to process flow, again.I've already expanded to my
data folder into servers pane,and we'd like to work with
the employee master table.So I'll go ahead and right
click employee master,and select Add to project.Then I'll go to the task pane,
expand the describe category,and then double click
one way frequencies.Now, as we did
before, we're goingto start by assigning
columns to task roles.And remember, in
this scenario, wewant to know the number
of employees by job title.So job title will be
our analysis variable.I'll go ahead and
take job title,and drag it over to the
analysis variables rule.Since I'd like to group
my results by department,department will be my
group analysis by role.So take department and
go ahead and drag itto that group analysis by role.Now, I'm going to go
ahead and just makea couple of additional changes.I'm going to start by
going to statistics,in the selection pane.And, as we did before,
extent of includingthe cumulative frequency
and percentages,I'm going to change that to just
frequencies and percentages.Now, there's an
additional optionthat I'd like to
show you, and thatis going to be in the results
panel, in the selection pane.What happens, by
default, is that whenyou look at your
results, the rowsare ordered in
alphabetical order,by the values of the
analysis variable.But I like to change that to
be by frequency order, instead.So, at the bottom, the
order output data by option,I'm going to use
the drop down menu,and change that to
descending frequencies.From there, It does
have two more thingswhich we've seen before,
which is changing the title,and then changing
the task label.So I'm going to go to titles
in the selection pane,clear to use default
text checkbox,and go ahead and delete
the existing title.Let's change out to
employees by job title.Employees by job title.Let's go ahead and delete the
default footnote, as well.So in the section
pane, I'm goingto select footnote, clear the
use default text checkbox,and simply delete the
existing footnote.Last tab, go to
Properties, click Edit,and, instead of the default
label of one way frequencies,I'm going to change that to
jobs by depth FREQ report.So jobs by, depth,
for department,FREQ, for frequency report.And then, click OK.That's it.Let's go ahead and
give this to run,and let's take a
look at our report.I'm going to go
ahead and zoom in,so it's a little bit
easier to see the results.And you'll notice that,
unlike before, where I justhad one single table
as in my report,I now have multiple tables.I have one for each unique
value of department.So for example, my first
table is for the departmentbeing account.And within the
accounts department,I can see the unique
job titles listed here.I also set the options, so
that the rows are orderedby descending frequencies.So I see the job title
with the highest numberof employees, which is
Accountant I, on the accountsdepartment, listed
first, and then it goesin descending order from there.And, as I scroll
through this report,again, I get one report for
each unique value of department.And it looks pretty good.So those are some
additional optionsthat we have available in
the one way frequencies task.Continuing with our discussion
on analyzing our data,one great way to be able
to visually representthe information that you
have, is through graphs.Now there are tons of
graphing tasks available,and Enterprise Guide.And I have a couple of
examples, here for you,to show you the type of
graphs that you could create.You can create a bar line chart.You could create
stacked bar charts.You can create map charts,
so we're actually overlayingour data values over maps.Then you can even create
a tile chart, as well.And these are not the
only options available.There are so many tasks for
you to be able to choose from.So in this tutorial,
we are goingto focus on analyzing our
data using bar charts,and we'll also take a look
at pie charts, as well.In this scenario, the management
team for the children productline, would like to
understand the purchasinghabits of customers, that are
not in the Orion club shoppingprogram.So for this analysis,
what they'd like to do,is they'd like to see
the average profitfigures for each
individual productgroup that's within the
children product line.So in this demonstration,
I'll show youhow we can use the bar chart
task, to create bar charts.And we'll also see how we can
filter our data within a task,as well.At the end, I'll show
you one project featurewe can use to help document
the work that we do insideof the project.So in Enterprise Guide, I have
the Orion start project open.And, again, we'll
be doing our workin the lesson to process flow.In the servers
pane, I've alreadyexpanded to the data
folder for this tutorial.So I'm going to go ahead and
right click on the Orion profittable that we'll use, and
select Add to project.Then I'll go to the
task pane, expand graph.And you'll notice that we
have, both a bar chart,and a bar chart wizard.So we talked before
that a wizard versionis going to have less options,
as compared to the full barchart task.Now, specifically, if you want
to compare to two, if you wouldlike to do things, such as
customizing your referencelines, applying formats
to your columns, maybeyou like to change the shape
of the bars in the bar chart,or maybe you'd like to customize
the axis labels and values.To be able to do
those things, youwould have to use a bar chart
task, instead of the barchart wizard.And I do want to take advantage
of some of those features.So I'm going to double click
on bar chart, which, again,will be the bar chart task.But the first thing that I'll do
here, is pick what type of barchart that I'd like to create.For this example, I'm going
to go with a vertical coloredbar, which will create
a vertical bar chartwith different colored bars.Next, in the selection
pane, I'm goingto move on to data,
which will lookvery familiar to what we saw in
the one way frequencies task.We have our input data
source, we can apply a filter,and assign columns
to task roles.Now, in Orion profit, I
have profit informationabout products from
all product lines.But I'm only interested in
the children product line.So I'm going to
show you how you canapply a filter within the task.We can, of course, use the Query
Builder to create a new table,and use that new filter
table in the bar chart task.But if you just need to
apply a simple filter,this is a really
quick way to do it.So here, in the data
panel, I'm goingto click on where it says Edit.And this will allow me to bring
up the edit data and filterwindow to apply a very
simple filter to my data.In this case, what I'm going
to do is, from the first dropdown menu, select the column
that I want to filter on.I want to filter on the
product line column.The second drop
down menu is goingto be the different operators
that we have available to us.And here, I'm going
to go with equality.So I'll select equal to.In the last box, I
can type in the valuethat I want product line to be,
or I can go ahead and choosea value from the input table.So to choose it, I'm going to
click on these three dots here.This is our value
selector button.And I have the different
values for product linethat exist in the
Orion profit table.I'm going to select
children, and click OK.And then we have a filter.I will only now be analyzing
rows where the productline is equal to children.So I go ahead and click OK.I see that filter
represented in task filter.And now I can move on,
like I've done before,where I can assign different
columns to my task roles here.So, again, I want to look
at average profit figuresby product group.Because I want each of the bars
to be represented by productgroup, I will
assign product groupto the column to chart rule.Now, for the height of
the bar, again, that'sgoing to be my
average profit value.So I'm going to look for
profit, and assign profitto the sum of role.Now, in order to control
the statistic, thatis used to calculate
the bar height,I'm going to go into
my selection pane,and I'm going to go under
the appearance category,all the way down to advance.So, again, this is advance
under the appearance section.Now, here you'll
notice this dropdown menu statistic,
used to calculate bar.The default is sum, which
will be the sum of profits,but I'm going to use
the drop down menu,and change that to the
average, because, again, we'reinterested in average
profit figures.I'd like to see the average
value actually representedin my bar chart.So I'm going to select
the checkbox thatsays specify one statistical
value to show for buyers.And I'm going to go ahead and
change that to the average.Just to have a label there.All right.So now that I know
how my columns willbe used in this task, I'm going
to focus on now customizingthe look of this graph.Let's start by going to
layout, under Appearance.So by default, the bar chart
is going to be a 2D bar chart.I'll go ahead and
clear the checkbox,so that we have a 3D bar chart.I can also change
the shape here.So instead of
having it as blocks,let's go ahead and change
that to a cylinder.I'll make one last change,
here, for the order of the bars.Instead of the automatic
order, let's gowith descending bar height.All right.That's it for the overall
shape, and look of those bars.Let's move on to
another category.I'm going to go into
horizontal axis,and then the axis subcategory,
underneath horizontal axis,here.Now, here I am going to
select the values tab.And I'm going to play
around with value rotation.So the values that you
actually see as labels,on that horizontal axis.For the rotation, I'm going
to put them at 45 degrees.This will just make sure
that the values fit nicelyin our bar chart.All right.Continuing on with
customizationsI'm going to now move
onto the vertical axis.And I'm going to go to
axis, under vertical axis.Now for to label,
again, my vertical axisis going to represent
those profit values.So I'm going to go ahead
and type average profit.That'll be my axis label
for the vertical axis,While we're here, let's go
ahead and add in some referencelines, as well.So under the vertical
axis category,I'll select Reference lines.First I need to
select a checkbox thatsays use reference lines.And then I can actually
change the styleof the reference line.So for the style,
instead of a solid line,let's go ahead and
change that to dashed.And for the color,
instead of black,I'm going to go ahead
and use a gray color.I'll go with gray 40%.Last two things I'm
going to use, again,something that we've seen
in other tasks, is go aheadand changing our titles,
and also modifyingthe task properties.So I'll select titles.And let's go ahead and clear
to use default text checkbox,delete the title, and, I will
call this, I'm gonna say,children product line.I'll do that on the first line.And then I'll come to a second
line, and type average profitby product group.So average profit
by product group.Let's go ahead and change
our footnote, as well.So I select footnote, and
clear use default text.Go ahead and delete it.Never get to go there.Let's go into the properties.Go ahead and click Edit.And for the label
here, I'm goingto go with Children
Avg Profit/group.So children Avg, for
average, Profit/group.And go ahead and click OK.Now there are more
customizationsthat you can make
to the bar chart.As you can see, there
a lot of categoriesavailable in the selection pane.But I just showed you
some of the couple thingsthat are pretty
common, that we'dlike to use when
creating a bar charts.So I'll go ahead and click Run.And let's take a look
at our bar chart.All right.So looking at this
bar chart, I haveone bar for each product group.And you might remember, I
applied a value rotationto my label, so they're
at a 45 degree angle here,so that they fit nicely.The bar height represents
the average profit,which I did apply a
label to, and I actuallysee the average profit value
above each of my parts.So that looks pretty good.Now, one thing I'd like to
show you is a project feature.I did apply a filter to
the data within a task.So I'd like to make note of
that filter and my project, justfor documentation purposes.So I'm going to go ahead and
close this tab for the barchart, and come into my
lesson to process flow here.I'm going to use something
called the Note Field,to document the work that I did.So to add a note, I'm
going to go to File, New,and then select note.This opens up a new no
tab, and I can reallytype whatever I like here.Going to go ahead and
zoom in a little bitto make the text larger.And I'm simply going
to explain the filter.So I'll just say, this
report includes productsand the children product line.So this report includes products
in the children product line.I'm going to continue to keep
my good habit of renamingthe different objects
in my project.So for this note, instead of
just leaving the name at note,I'm going to click on
this properties icon.And let's go ahead and just
call this filter detail.Going to click OK.Changes a name.And then I'm going to go
ahead and close this note.Now you actually
see this note that Icreated, almost like a sticky
note within my process flowhere.And so if you wanted to
add a note, somethingthat pertains to
the entire project,you could leave
your note like this.But I'm going to make a
couple more changes here.First thing I'll do, is
I'll click this down arrow,and select Collapse.This will make
this into an icon,just like all the other objects
that are in my process flow.Now this filter only really
applies to this particular barchart.It does not apply to these
other one way frequency reportsthat I created.So I'd like to make
the connection clearbetween the bar chart,
and this filter.So in order to create that
link, what I'll do is,I'm going to right click on the
bar chart icon, select link to,and, in this link
window, I'm goingto go ahead and select
that filter detail.And click OK.And now you see
that my bar chartis tied to that filter detail.So if anyone was looking at
this project at a later point,they would be able to
know that, oh, there'sa filter in the bar chart.Take a double click
on the note icon,and see more
information about that.So that's how you
create a bar chart,using the bar chart task.As well as how you can
filter data within a task,and even use notes in a project.I'd like to show you another
example of working with graphsand Enterprise Guide.So in this scenario,
Orion's starwould like to see the
total salary by department,but they'd like to see
this in a pie chart form.So in this demonstration,
I'll show youhow we can use the pie chart
task to great pie charts,but I'm going to take
it a step further.I'm going to show you
how we can furthercustomize the look of our graph,
by adding in just a little bitof SAS code to our task.So here my Orion Star project.I'll be working with a lesson
to process flow, again.And I do want to work with the
employee master table, which isalready added into my project.So no need to add it
again, it's already here.So I'll make sure to
selected to make it active.And then in the task
pane, I've alreadyexpanded the graph category.And, again, I have
the pie chart taskversus the pie chart wizard.If you want to be able
to quickly and easily addin some code to
the task, then youwould want to go with the task.So I'm going to double
click on a pie chart,to open up the pie chart task.So the first thing I do is
specify what type of pie chartI'd like.I'm going to go with
the default, whichis just a simple pie.Let's move on to the data
panel in the selection pane.Now here, again,
we're going to specifyour input table, any
filters, and assign columnsto task roles.The first thing
I'd like to do, isgo ahead and filter
the data little bit,just as we did before.So I'll click on Edit, and,
in the edit data and filterwindow, I'm just going to
quickly go ahead and excludeone of the departments.So from the first
drop down menu,I want to apply a filter to
the department column cellselect department.And, again, I just
want to excludeone department, which is sales.So from an operator, I'm
going to go with not equal to.And then I'll go ahead and
click on the value selectedat three dots.And from the list
of departments,go ahead and click on the
Sales department, and click OK.And, again, this will just
go ahead and omit the salesdepartment from the analysis.So let's go ahead and click
OK, and create that filter.So, again, what I like
to do in this scenario,is I want to look at the
total salary by department.So to call that I would like to
chart would be, the department.So let me take
department, and drag itto the column to chart rule.That means I'll have a slice
for each department at a OrionStar.And then, again, I want that
slice size feet representingthe total salary.So I'll drag salary
to these sum of role.All right, so that looks
good there for a data panel.I've assigned my
columns to task rules,and I've also assigned
a filter, as well.Next thing I'm going to do is,
under the appearance section,I'm going to select Layout.Then here I'm just
going to customizethe labels, and the placement
of the labels, as well.What I'd like to do is have the
department name and the salarytotal outside of
each slice, and thenthe percentage of the total
salary, inside of each slice.So for name, I already
have that outside,that's going to be
the department name.And for the statistic value,
again, that's already outside.For the percentage, however,
by default it is not selected.I'm going to use
the drop down menu,and go ahead and place it on
the inside of my slices here.All right.Couple last things,
as we always do.Going to go to titles, go ahead
and clear use default text.And for my title here, I'm
going to call it total salaryby department.So total salary by department.And let's go ahead and
add a footnote, as well.So going to select footnote
in the section pane,clear to use default
text checkbox.And I'm just going
to quickly explainto filter in the
footnote this time.So I'll just say figures
exclude the sales department.So figures exclude
the sales department.Let's go ahead and change
a task label, as well.So properties, edit.And I'll go ahead and change the
label to depths salaries pie.So department
salaries pie/no sales.Again, just indicating the
Filter in the label as well.So I'll go ahead and click OK.Let's get this a run.In here, we have our pie chart.So again, each of the slices
represents the department.I have the total salary
figure representedoutside of the slice, along with
the value or the department.And the percentage that makes
up of all of the salariescombined together, inside
of my pie chart here.This looks pretty good so far.Now, there's one more
thing I'd like to do hereto take this to the next level.And, perhaps, I'd
like to emphasize,the pie chart slice for
the engineering department.And emphasize it, I'd like to
go ahead and explode or pop outthis slice from the
rest of the pie chart.This is not something that I
can do with an option that'swithin a task, but all I need
to do is add a tiny bit of codeto the task itself, to be
able to accomplish this.So let's go ahead and
hop back into the task.I'm going to click on modified
task, on the task toolbar.And, at the bottom,
you'll notice that thereis a preview code button.I'm going to go ahead and
select on that button.This opens up the code
preview for task window,and as the name
implies, it's showing usthe code that is generated
behind the scenes.Now, right now, I'm
just able to view it.But if I go ahead and select
where it says Show custom codeinsertion points, in particular
points throughout the code,I'm able to type in my own code.And, again, just just
gives me more flexibilityto further customize the pie
chart that I'm getting here.All.Right.So here I'm going to go
ahead and scroll through.I'm looking for a
particular insertion point.I'm looking for the in graph
options insertion point.So I'm going to go ahead and
scroll through a little bit.And let's go ahead and look for
InGraphOptions insertion point.Here we go.And, as you can see, in the
comment it says InGraphOptions.I'm going to click where it
says Insert custom code here,and I'm just going to
type a little bit of code.I'm going to use the
explode equals option.And in quotes, I'm going
to type engineering.What this explode
equals option will dois, it will go ahead and explode
the slice for engineering.And that's the only code I have
to add just a little bit here.So let me go ahead
and close this window.Now let's just give
it one more run.And there we have it.Now we have the same pie chart.But instead of it, able to
explode the engineering piechart slice.So, again, tasks
have a lot of optionsto be able to help customize
the look of the graphthat we're creating.But if you know a
little bit of SAS code,then you could go in to the
code for the task, add some codeand predefined places, and
you can further customizethe results, like I did here.So far in is tutorial, we've
seen several different typesof tasks.And now I'd like to talk about
customizing the task results.Now what I mean by
customizing now,is changing the output format.With the task that we use,
the default output formatwas HTML, meaning that the
reports that we were creatingwere generated in
the HTML format.Well, we actually have
a lot more options.We can choose to generate
our report in the SAS reportformat, Listing, which is
the same as text, Excel, PDF,PowerPoint or RTF,
which is word document.So we have a lot of
options, in termsof the type of formats that we
can generate through our task.We can even take
this a step further,and control the look of these
output formats, as well.So what I mean by that is, with
the different types of outputformats that we
generate, we can chooseto apply a different
type of style.A style is going to control the
color scheme of the results,the font that's used.Again, is just that
overall look and feel.So what I like to do
with this activity,is show you where you can get
a preview of the different typeof styles we have
available for you to choosefrom an Enterprise Guide.So I'm going to hop into
Enterprise Guide, here.And to see the different
types of output formats,or to styles that we can
apply to output formats are,you can go to the Tools menu,
and then select style manager.Now you'll see in
the style list,that we have a listing of
the different types of stylesthat we have.We see their names listed here.And in the style
manager, you can simplyclick through these
different style names,and get a preview of what
the results would look like.So for example, if
we select festival,we see that there is
a yellow color scheme,with a purple outline to it.And on the other hand, let's
say something like journal,for example.Looks more academic, with a
more black and white feel to it.So you can use the style manager
to get a preview of the styles,before you actually apply
them to your task results.Even further, if you'd
like, you can actuallycreate your own styles, as well.What I would recommend,
is create a copy of oneof the existing styles,
and then go aheadand modify that style
to whatever you'dlike it to look like.So keep the style
manager on handy,so you can look to all the
different styles that we have.Now I'm going to go
back to my slides here.And let's take this
one step further.Once we generate our output
in different types of outputformats, which you
might want to do is to,actually, export those files.And this is not just
for our results, either.Whenever we run a
task, different typesof output objects are generated.We get code that is
generated, which is SAS code.A log is generated.You may have some
output data, and youmay have your results, as well.I will show you, in
upcoming demonstration,how we can actually take all
these different output objects,and then go ahead
and export themto different types of files.So for example, if you
have an output table,maybe you like to export it as a
CSV file or maybe even an Excelfile.We have a lot of
options in orderto customize the
results that we have.In this scenario,
Orion Star, we'dlike to make it easier
to share their reports,by generating their reports
and different output formats.So in this demonstration,
I'll show youhow we can modify the
properties of a task,in order to generate our reports
and different output formats.And then we'll also take a look
at exporting our project items,as well.So here I have the
Orion Star project.And from the lesson
to process flow,I'd like to work with the
products for category taskthat we worked with previously.If I double click on
products for category,we can see that on the
Results tab, by default,our results were generated
in the HTML format.What I'd like to do,
is go ahead and changethis to generate our results
and different types of outputformats.So to make that change, I'm
going to click on Modify task,on the task toolbar.In the one way
frequencies task, I'mgoing to select on
properties, and thengo ahead and click Edit.Now, before in the
Properties window,all we really did was
change the task label.But, this time, in the
property selection pane,I'm going to go to
where it says results.Now again, the
default is to generatethe results in the HTML
format, specificallywith the HTML blue
style applied to it.To change that,
I'm going to clickon the button that
says customize resultformat, styles, and behavior.And now I can choose from
all of these different typesof formats.For this demonstration, I'm
going to select all of them.So SAS report, Listing, Excel,
PDF, PowerPoint and RTF.I'm going to go
with all of them.You'll notice on next
to these output formats,there's a drop down
menu for the style.I'm going to go ahead and
make a couple of changes here.So let's say for SAS report,
instead of the HTML blue style,I'm going to go with
the meadow style,just so we can see
what that looks like.And then, for Excel, I'm
going to change that to Dove.Now, of course, you can change
all of these other styles,as well, but I was going
to change to show youwhat that looks like.That's it.I'm going to go
ahead and click OK.And I'm going to
give this a run.And let's take a look
at our new results now.It's going to take
just a second to run,because there are different
types of output formats.But now that it's completed,
take a look at our results tab.Right now, I am still viewing
the HTML results, by default.But you'll notice that
under the Results tab,I have a lot of option now.I have one tab for each output
format that I have selected.So to take a look
at them, I simplyneed to double click on
the corresponding tab.So I'll double click on
SAS report, for example.And here's the
SAS report result.I did generate this
using the meadow style,and we can see how
that has affected,both the coloring and just the
overall look of this report,here.Now for a HTML SAS
report and Listing,we can view those directly
inside of Enterprise Guidebefore the others.So Excel, PowerPoint,
RTF, and PDF.We need to view those in
their respective applications.So just to show you an example,
if I double click on the Exceltab, the results will not open
up inside of Enterprise Guide,but instead Microsoft Excel
will open where I can actuallyview the results.So I'll zoom in a
little bit here.Here's a result in Excel.Again, I applied
that Dove style.I have one worksheet
for to report.And I have a separate worksheet,
which contains the graphthat I generated with the
one way frequencies task.I'll go ahead and close
out of Excel, here.But Enterprise Guide
makes it really easy.I just said to me, I just
need to make a small changeto properties of a task, and
now I can generate my output in.All sorts of different
output formats, here.I'd like to point out
one thing, though.What I did here was, I changed a
property of this specific task.Specifically this products
for category one wayfrequencies task.So if I were to run
a separate task,I would only see the
default HTML results.If you always want to
generate a specific outputformat in all the
tasks that you use,you can make a system change.So to do that, and
just really show youwhere that is, I'm going to go
to the Tools menu at the top,and go to Options.In the options window, I'm going
to select Results, general.And we're going to take a look
at the result formats pane.Again, the default is just
to generate HTML results.But if you always want
to generate your resultsin Excel, or PDF,
for example, youcan make those selections here.In addition, if you want
to customize those results,so maybe you want
to always generatethe results with
a specific style,you can go to the subsection
of that specific output format,and make those changes there.So again, a lot of flexibility.We were just focusing more on
how do we change the outputformat for a particular task.Which, again, if I close
out of the options window,is within the properties
of a particular task.All right.Let's go ahead and take
this one step further.By changing the
properties of a task,I'm able to view my results in
these different output formats,but they haven't been saved.If I wanted to do that, I
could go ahead and exportmy different result objects.And, again, that includes
the code, log, results,and output data.There are several
ways to do this.If you click on the Share
button, here is like share,and then select what it
is that you want to share.Is it the result?The output data?And then you can choose to
go ahead and export that.Another way to do
this, is to use the tabfor the specific item
that you want to export.To show you an example,
I'm going to go with Excel.So I'm going to right
click where it saysExcel, products for category.Select Share, here.And what I'll do is I'm
going to go with an export.Specifically, export
as a step in project.And I'll show you
what that means, whenwe're done finishing exporting.But let me select Export,
as a step and project.And, again, this will
allow me to actually savethis Excel file.So step one on
the expert wizard,is just making
sure that the Excelfile is what we want to export.And so I'm going to go
ahead and click Next.Then step two, I
can specify whereI'd like to save this file.So I'll go ahead
and click Browse.And I'm going to go ahead and
go to our course file location,here.So I'm going to go in.Let's save it in this location.And, for the name, I'm going to
just shorten it a little bit,and call it product kept.Go ahead and click Save.Now, there's this option
at the bottom, thatsays overwrite existing output.This is checked
by default, whichmeans, that if there already
is an Excel workbook calledproduct CAD.I'm giving Enterprise
Guide the permissionto go ahead and
overwrite that file.If I clear this option,
and a file with that namealready exists, every time
you perform the export,Enterprise Guide is going
to append the date and timeat the end.So you can have a version
history of all the exportsthat you've done.So I'm going to go ahead
and clear that option.All right.That's it for step number two.Click Next.Step number three is
just a confirmation.We're good to go.So I'm going to go
ahead and click Finish.All right.I see the log, and looks
like this was successful.So I'm going to go ahead
and close the export filetab, and then also the
products for category tab.Taking a look at
my process flow,I see that I generated my output
formats in many different typesof output formats.But because I decided to
export the Excel file,I see this extra export step.Now, I'm really
only seeing this,because I said not
just export, but exportas a step in the project.So we actually see this
represented as a stepin the project.And, what that
means is, that if Iwere to run the contents
of this process flow,or maybe even run
the entire project,because the exporting
is part of my project,it will automatically go ahead
and export this file for me,as well.We can take a look
if I double click.This is that product CAD Excel
file that I actually exported.I'll be able to, actually,
view that in Excel.And, again, this is saved
in a certain locationall my computer.All right.Now looks good.I'll go ahead and close.So you've seen how easy it
is to generate our outputformats in different
types of file format,and even to export our different
project items, as well.So far in this tutorial, we have
added different types of filesto our project.These include data or
tables, and we've alsoadded in some tasks as well.Now what I'd like
to talk about, isorganizing the different
contents of our project.And I'd like to start by
talking about how we cango about updating our results.Why might you need to
update your results?Well, maybe your input data has
changed to a particular task,or maybe the input data has been
updated to include more rows.Whatever the reason
may be, you maywant to update the results
that are in your project.And you have several
options for them.If you like, in
the project pane,you can click the
green run arrow,and this will run the entire
project, so everything thatis in your project.However, that may not be
the most efficient wayto update results, if
you really only needto run a portion
of your project.So instead, what you can do
is come to your process flow,and you'll notice the run, but
not the process flow toolbar.When you click the
down arrow next to run,you'll notice that there are
several different run options,that are available here.I like to quickly
talk about these.So let me go back to my slides.Let's talk about those
four different run options.First thing I will
mention, though,is the available run options
will depend on the objectthat you have selected
in that process flow.But the available
run options are goingto include run selected items.So if you have certain objects
selected in the process flow,run selected items will only
run those selected items.Run process flow will run
the entire process flow.Run to selected item,
will run the selected itemand everything leading up to it.And then run from selected
item, or run deselect an item,and everything following it.So although you could choose
to run the entire project,and you may need to do that,
you don't necessarily have to.You can just run
a particular task.You can run everything
surrounding a certain task,or even run the
entire process flow.So again, a lot of
different options, when itcomes to updating your results.Now let's say you choose to
run the entire process flow.If you have a lot of
different objects in there,you might be wondering, in what
order do these objects run?We'll take a look here.By default, your objects
in the process flow,will execute order of top to
bottom, and then left to right.What that means is
that objects thatare closer to the top of the
process flow, will run first.And if they're objects that
are in the same horizontalposition, objects
that are to the left,are going to run before
objects to the right.In our example process flow
here, the assigned projectlibrary wizard will
run first, because it'sat the very top of
the process flow.Second will be the
sample list SAS program,because the second
highest object.And then finally everything
in this third last rowwill be executed.Now let's talk about
these links here.If you have objects that
are linked together,whether they are automatic
links that EnterpriseGuide generated, or links
that you define yourself,we saw an example
earlier where welinked a note to a bar chart.Either way you go about
that, those links,they're going to ensure
a certain run order,meaning the links will run
based on their dependencies,regardless of the positioning
of the objects themselves.So meaning, it will follow the
direction of the arrow here.So you can take
advantage of this,to let's say, place objects
in your process flowin a certain order.Or maybe link the
objects in your processfloat in a certain
order, as well.Last thing I'd
like to talk about,are several project
organization recommendations.First is something
that we've beendoing throughout this tutorial,
is to use descriptive names.This goes for our task labels.This goes for our process flows.And it also goes to our
different output tablesthat we generate, as well.Get in the habit of renaming
those objects to somethinga little bit more
descriptive and concise.That way when your
projects get larger,and you have a lot
of items in them,you're able to distinguish
the different items.Documenting details.Take advantage of notes
that we saw earlier,and you can choose to have them
floating in your process flow,or you can link them
to particular items,if they're describing
a particular item.So definitely take
advantage of that, as well.The last two items
arranging objectsand using background
colors, I'd liketo actually show you
that in Enterprise Guide.So let me go into
Enterprise Guide, here.And I have my process flow.By default, auto
arrange is enabled.And what that means is, that
if I try to move objects aroundin the process flow, Enterprise
Guide doesn't really let me.Out of range
essentially optimizesto view the process
flow, so Enterprise Guidedetermines where every
object should go to.If you would like
to change that,you can right click anywhere
in the process flow,and you can go ahead and
clear the Auto Arrange option.And now you'll be able
to move items freelywithin your process flow, so
you have a lot more flexibility.What I would recommend, is go
ahead and turn auto arrange on.Then Enterprise Guide
optimize your process flow.And then you can turn it off
to make some small adjustmentsto the process flow.I'll go ahead and
turn it back on.The last recommendation
is background color.So in the same way I'm
going to right clickin the background
of the process flow.And then we have the
background color option.And we have several
colors that we can use.So I can use red, for example.And I would recommend picking
a different background colorfor each process
flow, so that youhave a visual distinction
between your different processflows.So those are a couple of project
organization recommendations.So far in is tutorial
we've seen howwe can use tasks
and Enterprise Guideto help us analyze our
data, and generate results.And I mentioned before that,
as you're pointing and clickingyour way through a task, behind
the scenes Enterprise Guide isactually generating SAS code.But that's not the only
way to generate SAS code.If you'd like, you can choose
to write out your own SAScode from scratch as, well.To do this, you
can simply open upa SAS program and
Enterprise Guide,and start writing your own code.Now Enterprise Guide,
actually, makescoding a little bit
easier, because it offersa lot of programming features.Just to point out
a couple of these.You may want to take advantage
of the format code feature,so the format code feature
will add in line breaks,and indent certain statements.And it's just going to make
your code a lot easier to lookat, with just one
click of a button.There is the Integrated
Syntax Help feature,so, as you are
typing out your code,you will see a little snippets
of the syntax documentation,so you can get some
help with the syntax.There is the
Autocomplete feature,so get as you're
typing up your code.The Autocomplete feature will
make suggestions on, maybe,some column names, or maybe
some options or keywordsthat you might want to
add into your program.We have these Data Set Debugger.Now the data step is mainly used
to help manipulate your data.And with a Data
Step Debugger, youcan execute the data
step code line by line.And this is really helpful
if you have any logicerrors in your program.The Data Step Debugger can
help you spot exactly wherethat logic error is occurring.We have the Program Analyzer.So the Program
Analyzer allows youto identify different
parts of your program,and see how each part is
related to each other.This is useful if you have
grid computing available,and you'd like to take
advantage of that.Or to see if there's
any internalizationissues with your code.Finally we have the
Program History,or Git Integration
available, as well.This is really useful to track
or manage version controlfor yourself, or
among multiple users.So we're going to take a
look at some of the featuresthat Enterprise Guide
makes available,to make coding a
little bit easier.In this demonstration,
I'd like to show yousome of the features that are
available in the EnterpriseGuide program editor.So we'll focus on things
like, how we can quicklyformat our code, how we can take
advantage of the autocompleteand integrate a
syntax help features,and also take a quick look at
the data set debugger, as well.Now, in Enterprise Guide I still
have the Orion start projectopened up.And I can, actually, add
programs to the project,just like I did with data
and my task, as well.However starting with
Enterprise Guide 8.1, if I justwant to write
programs or open data,I can choose to do that
without using a project.So I just want to show
you what that looks like.I'm going to go ahead
and close this project.So I'll go to the File menu,
and select closed project.So I'm completely
outside of a project now,and I'm simply going to
open up the program I'dlike to work with.So in the servers
pane, I've alreadynavigated to that folder.And this program is eg3do1.And I'm just going to zoom
in on the code, a little bit,so that it's a little
bit easier to see.Now right off the bat,
what you'll noticeis that this code is very
difficult to look at.It's all squished together,
and the formattingis not very consistent.I could go ahead, and manually
change the formatting.However, there's a much easier
way to address this issue.On the program editor toolbar,
you'll notice this button.And if you hover over, it
it's called format code.If you click on that,
now we'll go aheadand automatically format
your entire program.So we'll add in some line
breaks out logical pointsin your program, and also
do some indenting, as well.So again, a lot easier to look
at the code that we have now.So now that my code
is a little bit easierto look at, let's go ahead
and run this entire program.I'm going to click
on the Run buttonon the program editor toolbar.The code will be
executed to SAS,and now we have our results.Now, probably the
first thing you noticeis the layout of my tabs.I have the code tab
on the left hand side,and the log results and output
data on the right hand side.And you may like that, or
you may want to change that.As we saw earlier
in this tutorial,tabs can be moved around
freely within the interface,outside of the main application
window, or even dockin predefined areas.However, if you would like for
there to be a certain defaulttab layout for all
your SAS programs,you can set a certain option.So if you go to the View menu,
and go to program tab preset,you have several options.The default is this
vertical split, so the codetab on the left, and the
log results and outputdata on the right.I'm going to change
this to standard,and this looks more familiar
with what we had in our task.So again, we have the code
tab, the log tab, results,and output data, but
they're all together,instead of being split
like it was by default.All right.So that's getting
the setup there.Now let's say you're
looking at this program,and you're not very
familiar with SAS code,so you'd like to know what
some of these keywordsactually mean.Any of the words that
are in a blue color,you can hover over them to
get a quick snippet of the SASdocumentation.So for example here, if I
hover over the If keyword,I see as it says it execute the
SAS statement for observationsthat meet specific conditions.I see a little syntax
box, and I get quick linksto the documentation, samples
in SAS notes, and papers.So pretty similar
to the Help buttonthat we saw in
task, at any point,if you're not sure what a
certain keyword is doing,hover over it, and then
we'll go ahead and give yousome more information.Let's move on to
another feature, whichis the Data Step Debugger.Now I do have a data step
open in this program.It goes from data in line, 3
all the way to run, on line 11.And I'd like to see how
this data step that code isprocessing behind the scenes.To do this, I need to enable
the Data Step Debugger.So on the program
editor toolbar,I'm going to click on this
button that says debug.Has a little bug icon, here.What happens is, now anywhere
where there is data step code,you will see a green
line in the margin,along with that bug icon.Now to open up the
Data Step Debugger,I can simply click on
anywhere in that green line,or on the green button.So I'll go ahead and
click on the bug,and here we have the
Data Step Debugger.And, again, I'm
just going to zoomin on the code a little bit,
so it's a little bit easierto look at.Now, what I can do is I can
click on the step executionto next line button, and just
execute the statement, oneby one, and see how
that's affectingthe values in our columns.So for example, here, I'm
going to go ahead and clickon the step execution
to next line button.And it's going to execute
what is highlighted.So this is the SET statement.The SET statement
is going to readin the very first row
from my input table,which is employee addresses.And I see those values
represented on the right handside.Now we're going to go into
some conditional processing.So what I'm doing
here, is lookingat the value of country,
and then settinga value for country name.Now if I look at the value of
country for this first row,it is US.So this first If
statement is true.So if I go ahead and
execute it, it's true.So country name will be
set to United States.And we see that represented
on the right hand side, here.Now because I already met
a true condition with thisIf statement, the else if,
and the else statements,are entirely skipped
over, and now we'reat the end of the data step.I can repeat this process again.This time with the second
row from my input table.So I click Next.Execute as that statement.Now we have new values,
so this is the second rowfrom our input table.And we're going to go through
the conditional processing,again.This time you'll notice
my country is AU.So the first if statement
is going to be false.So if I go ahead and execute
it, we move onto the next elseif statement.In this case country this
is true, because it is AU,so country name will be
assigned a value of Australia.And, again, because we
had a true statement,it's going to skip
over to else statement,and move onto the
next iteration.So you could go ahead and
click through, line by line,to see what's going on behind
the scenes with the data stuff.And again, this
is really helpfulwhen you have any logic
errors in your program.So that you can see, where
exactly in your code,that logic error is occurring.So just to show you, I'm
going to go ahead and clickon the running man button, here.It's going to go ahead and go
finish your execution phase.Debugging is now complete.And I can go ahead and close
out of the Data Step Debugger.While I'm here, I'm also
going to go ahead and clickon that debug button, on the
program editor toolbar, again.And this will go
ahead and suppressthat green line and a bug
that was on that margin.So we kind of got
an idea of whatwas going on behind
the scenes, withthat conditional processing.I've already run this code, so
I don't need to run it again.But if we take a look at our
results tab, for example,we can see how the
results turned out.So again, the conditional
processing said,if country is US, country
name should be United States.And if it's AU, then the country
name should be Australia.And I see that that was properly
done with my results, here.All right.Show you a couple
other features.I'm going to return
to the code tab, now.And now, what I'd like to do
is add in a PROC MEANS step,to help calculate some
summary statistics.So I'm going to come to
the end of my program.And I'm going to take advantage
of the autocomplete feature,to quickly type this
PROC MEANS step.So I'm going to go ahead
and start by typing outthe key word proc, so PR.As soon as I do it, I see the
autocomplete window pops up.Proc is highlighted.So because proc is
highlighted, whenI press these spacebar
the keyword proc isadded to my program, along
with the space, as well.I'm going to do the
same thing for means.So going to type me.The word means is highlighted.So when I pressed the
spacebar, the word meansis added to the program,
along with a space, here.Now, next, I see the
autocomplete window popping up.And here I'm seeing some
valid options for a PROC MEANSstatement.And, for example, there's
a data equals option.When I hover over this option,
I get quick informationabout it, which
says it identifiesthe input SAS data set.I'd like to go ahead and use it.So I'm going to double click
on data equals, to quickly addthat into my program, here.Next, I see a list
of the librariesthat I have assigned in my
current Enterprise Guidesession.So I'm going to type in o.That highlights the Orion table.So I'm going to
press the period key.What this will do it will add
an Orion, along with the period.And then, finally, I want to
work with the employee mastertables.So I'll double click
employee master,from the autocomplete window.Let's keep going with this.I'm going to go ahead
and hit the spacebar,and, again, I see those valid
options for a PROC MEANSstatement, in here.I'm going to go ahead and take
advantage of autocomplete,and go ahead and type in
a couple of options, here.So I'm going to type in mean,
I'm going to type in min, max.And I'm also going to see max
this equals option, and setthat equal to 2.I have to fully
type all that out.I just took advantage
of autocomplete,and let autocomplete fill
in all the details for me.All right.Let's move on to
the next line, here.And I'm going to tab
over, just to keepthe formatting consistent.And I'm going to
use a var statement.Now, the var
statement is where Ispecify which numeric
columns I'd like to analyze,or get summary statistics for.And as soon as I
type var, I actuallysee a listing of all the numeric
columns in my input table.So I'd like to
analyze salary, I'mgoing to type in S,
that highlights salary,and I'm going to go and,
actually, type in a semicolon.What this will do,
it's going to addin the highlighted
keyword salary,and it's going to go ahead
and end that statementwith a semicolon.Again, I'm going to just
continue on with autocomplete,and go ahead and add in a
couple of more words, here.I'm going to use
a class statement,to go ahead and group my
results by department.And then, at the
very bottom, I'mgoing to add a run statement
to end the PROC MEANS step.Now, I really enjoy using
the integrated syntaxhelp and autocomplete
features, but if youwould like to turn
those off, thereis a way that you can do that.At the very top, you'll
want to go to program,and go ahead and select
on editor options.Now, here you want to go
to the autocomplete tab,and you can go ahead and
clear the enable autocomplete,or enable integrated
syntax help checkboxes,to go ahead and turn those off.There's a lot of
other options youcan set in this enhance
editor options window,like, for example, changing
how the formatting code featureworks, or the
indenting, for example.And you can even change
the appearance of your SASprograms, and log files as well.So a lot of options
you can set, here.I'm going to keep everything
as it is, just wantto show you where to find out.So I'll go ahead
and click Cancel,and return back to my program.Now, I've already run
everything in this program,except for the PROC MEANS.So if I'd like to run just
a portion of my code, whatI can do is highlight the
portion I'd like to run,which in this case
is PROC MEANS,and then click the Run button.Another short cut you can use
is the F3 key on your keyboard.That works as well.So click Run.That is going to run only
the highlighted portion.So I don't see the
results from before.I only see the results for the
current portion that I ran,which is the PROC MEANS step.So my results are updated,
my output data is updated.I didn't generate
any output data,so I don't see that there.And my log is updated, as well.Turned to my code.Again, those are just
some of the featuresthat we have available,
in terms of programmingin Enterprise
Guide, so definitelytake advantage of them
to make programmingjust a little bit easier.Before I finish, I'm going to
go ahead and save this program,and we're good to go.We are at the very
end of your tutorialon getting started with
SAS Enterprise Guide,and I hope you found
it very helpful.Just to recap, we saw how we
can add data to our project,including importing different
types of data files,like Microsoft
Excel, and CSV files.And then we saw how we could
prepare this data usinga robust Query Builder.Then we talked about
analyzing our data,using different types of
tasks, including the one wayfrequencies, bar chart,
and the pie chart task.We then took a look at a couple
of project features, thathelp us customize
our project results.And also organize the contents
of our project, as well.Finally, we took a look at some
of the available programmingfeatures in Enterprise
Guide, to help make codingjust a little bit easier.Here, I'd like to end up with
a couple of resources thatmight be of interest to you.If you'd like to learn more
about other Enterprise Guidetasks, and some other project
features that are available,you may want to take our SAS
Enterprise Guide 1, Queryingand Reporting course, or our
SAS Enterprise Guide 2, AdvancedTasks and Querying Course.Now I've pointed out some
of the programming featureswe have in Enterprise
Guide, but we didn't reallygo into the syntax portion.So if you're
interested in that, Iwould recommend watching this
video on writing a basic SASprogram, or taking
our SAS programming 1,essentials course.Finally if you'd like to
connect with other SAS users,you can join a SAS community.There is one that's specific
to Enterprise Guide,or you can follow our blog.Some of my favorites are
the SAS Dummy Blog, and alsothe SAS Users Blog.So again, thank you so much for
joining me in this tutorial,and I hope you found it helpful."
52,"Hello, I'm Alan Bowe and
this is my SGF presentationthat's called the HTML5
Web App using SAS.Phil Mason would've been
here, but he can't make it.But I recommend Phil's content.Phil has many years experience
building web apps in SAS.Same for myself,
built a few web appsincluding Data Controller,
which is a commercial webapp for building--for managing data which uses
the same kind of technology.So why build a web app?So why HTML5?Well, it's easy to build.This guide's everywhere,
it's easy to deploy.It's built for
interfaces and it's free.There are paid libraries, which
sometimes are worth buying.But the components are free.Why SAS as a backend?It's secure, it's
very scalable, it'sbuilt for data as we
all know, and it's free,which is true if you consider
the marginal cost of buildinga web app on SAS.Everything you have is
already there in the platform.You have a web server,
you have an integrationwith your authentication
mechanisms,you have a database.There's no additional cost
to building a web app on SAS.So the architecture of a web app
on SAS looks a bit like this.You have a client, which
could be your laptop, a webserver, so the Sas web server
is part of your infrastructure.That web server talks
to an app server,which could be a stored
process server on SAS 9or compute server inquire.And then when it comes
to writing data backyou use a database.And those are the
four componentsto your architecture.So there was a division
there between the frontendand the backend, and it
is an important division.We do see people
building web appsusing put statements
in SAS programswhich does work, but gets
very complex, very quickly.So if you're looking to
build more than streaminga simple table,
then it really isimportant to think about
scalability, compatibility.If you use the work
adapter, then youdon't have to change
your frontend if youswitch the backend out to Viya.Speed of development.So by having all of your
frontend in a frontend repo,you can use all of the
frontend build tools.Different skillsets.If you're serious about
building web apps on SAS,then you probably want to
have a dedicated frontenddeveloper who knows the
frontend, and a SAS developer.You do get SAS developers that
know frontend and frontenddevelopers that know SAS.But really, if you
want to move fast,quickly, then you get
two different people.One person who
knows the frontendand one who knows the
backend and that combinationreally works.Having standard interface
talking to SAS meansthat you are effectively
creating an API to SAS,and having a standard
interface willhelp you to document the API.And maintainability,
it's much easierto find a generic frontend guy
to maintain a frontend partif it's not wrapped
in some SAS code.So design of your startup.Do you know what
you're going to build?When it comes to
design there's a splitbetween the user experience
and the user interface.And then design frameworks
accelerating the designprocess.So user experience
is the way that younavigate through the
site and user interfaceis the way the site looks.This, at least, you can copy
the slides, you can follow them,some recommendations.When it comes to
frameworks, ratherthan creating all
your own CSS classes,it's going to save
you a lot of timeto just use the generic
frameworks throughout there.So you can just
add some class namerather than writing
your own custom CSS.So if you know what
you're going to build,the next thing to consider
up front is security.So the good news is by
the way that SAS is built,it's very hard to make
an insecure app on SASbecause it's mostly out of
your hands and preconfigured.Your authentication is
going to use SAS log on,so it's already going to
use your existing AD or LDAPintegration.Authorisation.Again, you can't access any
stored process or job executionservice without it having coming
through the SAS authorisation,who can access it.There are some things
to look out for.So for instance, on
the SAS 9, if you'reusing a stored process server
with a stored process systemaccount, it's something that
could potentially be misused.So this is why you typically
restrict stored processesto those teams that are
subject to governanceof this type of code that
they put into production.So yeah, misuse of the
SAS of the account.Code injection.So be careful about resolving
macro variables straightfrom the URL.And man-in-the-middle attacks.So this is where, if you're
using an external script,potentially somebody could
modify that JavaScriptafter you're downloading it.But there are ways
you can mitigate that.For instance, SRI hashing.So this is a hash,
which you set upwhen you are referencing
your external source.And your browser
can then compareto make sure that
the hash of this fileequals that, and
then there's no waythat this can be
subsequently changedwithout the browser
knowing about it.In terms of security,
once you startto add more services
to your web appyou want to think about
how you organize stuff.And by putting all of your
services in dedicated folders,administration becomes
easier because youcan set the permissions
at folder level.I'm going to talk about SASjs.So SASjs is a new framework for
building apps on SAS, and it--there's a few parts to it.There's a macro library,
there's a SASjs adapter,so this is the part that
handles the communicationbetween the frontend
and the backend.And there's a CLI
tool, which is goingto help with building
your content in a projectrepository and deployments.So the MacroCore library
is full of macros.It's useful macros
for building apps.So it helps you with things
like creating folders,creating services, restore
processes or job executionservices, getting lists of
users and groups, et cetera.There is a documentation site.And the macros are
split by folder.So base is macro functions,
macros that right SAS code.Metadata, so these are SAS 9
specific macros and then Viya.Then if you open any of
these, this documentationis actually driven
from the macro itself.So this is the
macro and the headeris in something
called doxygen format.So if you add these specific
styles then you get--it generates this documentation.And this bit is still worth
looking at, dependencies.So by setting your
dependencies in this format--so dependencies and the
macros that your macro uses.And you get this
navigate through.So you can click through
and see the macrosthat are in your macros.So it's quite neat,
so I recommend that.But all the MacroCore libraries
follow that convention,MacroCore macros.That's the adapter.So this is the part that you
will include as a JavaScriptlibrary in your
frontend, and thatwill handle the log on to--the SAS log on.It works well for
SAS 9 and Viya.It's well-tested and
it's easy to use.So Github here.And just to show you
the configuration.So you indicate your
setup if you're notserving from the SAS server.And this server type here
change SASVIYA to SAS 9and then go out.We'll work on a
different SAS server.And then appLoc is
either the locationin the metadata folder tree
or in the Viya file service.And then the CLI tool.So this is what will help
you set up a service--a service project and repair
your project for deployment.So it's on Github,
but you don't actuallyneed to go to get
help for this one.It's an NPM tool,
so you didn't needto install NPM, node
package manager.If you're a frontend
developer thiswill be very familiar to you,
and it's a command line utilitythat helps you easily download
your external dependencies.So I'm showing you
my terminal now.I'm changing it to
the temp directory.I'm going to install,
globally, the SASjs client.So that all download-- you
need to install this globallyin order to make
the command linetool available on your path.So now it's available,
I can go to SAS.And that will create
a folder with--in an opinionated
way, as in it willcontain the folder structure.if you use this for
all of your projects,then you will have a system--all your apps will
be consistent.So change it and then, if I look
at the SAS folder, so a treeSAS.There we go.So there's a build folder with
a service in it and a serviceterm.So these files get inserted
at the beginning and the endof all of your SAS services.There's a folder setup
to build the databaseand detail on the SAS.Then there's a macros folder.So this is where you'll put
all of your dependent macros.And then a services folder
with some example foldersand some example files.So if I then go
into that folder.So for instance-- and
I create a new table.I'm going to call a
macro and to get the--you see here this
macro is not defined.So if I go here
and I reference it.So this macro is not
in my macros folder.However, it does exist
in the MacroCore library.So the way that the
compilation will workis if the macro is not
in the macros folder,it will look into
MacroCore library.So what I've done
here is I've createda table called areas,
created a second tablecalled nobs, number
of observations,and then I'm going to send that
to the frontend as an arrayformat like so.So I've defined a service
here, a very simple one,that's going to write
some data and send it backto the frontend.And these macros
all get insertedas part of the build process.And this will open the
jSON, it's in the tables,and close it again.So now, I've modified my macro.I'm going to repair it to build.Oh, before I do that,
I'm going to configure itin the config file.Configure it for SAS 9.I'm going to in my personal
folder and that's it.So I'll save that.Going to run the
SASjs build command.And this will then compile
all my dependenciesand create a deployment script.And this deployment
script is somethingI can run in SAS Studio.You can see here, it's
got my application.And then at the end, it
creates the web service.I just run that.Oh, oops, This will go away
and create all the services.And yeah.And it will also give
me a link in the log,so I can just verify
that that worked.So the one we modified,
it was called app in it.And if I copy that
link into a new window,I get the jSON back.And if I just confirm
how they spell it, jSON.They did something else.And validate-- there
we go, valid jSON.So yep, it works.This is the process that
will create the stopprocesses for you.And as you can see it
was very, very easy.I just ran that
command, compile.You can see that all these
dependencies are included.And then you run it in SAS
Studio to create the service.So apply to all this.So at the backend, we
mentioned SAS 9 and SAS Viya.On SAS 9 the services are now
known as stored processes,and Viya uses the job
execution service.They're very similar.On SAS 9, you can run
your service using--as a stored process or
on the workspace server.Or you can run stored processes
under user credentials.And then on SAS Viya, it's
always the compute server.So on that point, if
you run a workspaceit's your client or your
user account credentials,but on the stored
process server,it's a SAS of the account or the
general shared service account.On SAS Viya, it's the--always your own credentials.But you can configure, also,
a system identity from 3.5.The big difference
on SAS 9 is that youhave metadata, which
doesn't exist in Viyabut we do have Postgres.So ...we did the demo.This is where the
SASjs tool comes in,and that integrates
in get with MacroCore.Storing code in the service.In Viya, that happens
by default in SAS 9.From 9.3 you can store
code on the file system.But we don't recommend that.By actually storing
the code in the servicemakes it easier to deploy.You don't have to
touch the file system.It's easier to debug because
all the code's in one place.And so it's more stable.I mentioned doxygen headers.So this is what the
SASjs collateraluses to determine what
the dependencies are.So if you're building
services that have macros,list them here and then they can
be compiled, not just doxygen,but also for the CLI tool.I won't go into all this,
but if you're on SAS 9and you're looking--you got load balancing
issues, some tips here.That's all backend.On the frontend, it's
a SAS presentationso I won't talk too
much about frontend.But just to say we do
recommend you use framework.When you get started,
it's very easyto just write an index HTML
and start hand coding your HTMLand remembering
your closing text.But using framework and
using a professional IDEwill accelerate the
development of your frontendbecause you get the-- not
just the auto-completion,but all of the
build tooling thatlets you, for instance, write
your code in typescript, whichis a superset of
JavaScript, and thenthat will compile down
to ES5 script, whichmeans that will be compatible
with older browsers like IE 11.So you can use all the
latest ECMAScript functionsand compile them.And then the compiler
will convert thatto older functions that
work in older browsers.So frameworks and tooling
lets you do that and thingslike being able to
change something locally,and it will spin up a web
server and do a hot refresh.So you can just make a change
and then view it straight awayin the browser.It will, again,
accelerate the speedat which you can build web apps,
and then seed apps and dev ops.So the frameworks-- the major
frameworks are angular, react.Seed apps.So we've got seed apps
for angular react,and then a minimal see app.So a seed app is a
repository on Github.Has all of the
boilerplates in it.So rather than configuring
all of the configfiles yourself, just git clone
a see app, follow the steps,and you can straight
away, in five minutes,even, have an app
up and running.Some tips and tricks.So we're working in a browser.And in order to keep fast,
the biggest thing you can dois minimize the amount
of data that you send.So as much as you can,
pre-summarize and justsend the least amount
of data possible.To a degree, it's better to
send more data in one requestthan it is to send
smaller amounts of datain multiple requests.The reason being that while
SAS is very fast processinga lot of data, it's not
the most responsive.So there is an overhead
to each request,ranging from 400
milliseconds on SAS 9to a couple of seconds on Viya.So yeah, fewer requests
are often better.Paging.So your browser can handle
quite a bit of data,but it will choke
up and stop if ithas to render that
table in the DOM.So if you are displaying
data and you couldn'tbe more than a
few hundred lines,it's better to keep the
data locally but just this--a few hundred at a time.There's a loop that the browser
does to speed up your app.So things like the caching of
JS and CSS, that's automatic.There's a local storage
facility that youcan store data locally.Obviously, this won't work
if you want to keep static.When you reuse it, it
uses different machinesor different browsers.Same with cookies, CDNs.What CDNs do is they
provide a location whichis closer to your
physical locationfor caching things
like fonts and icons.Server caching is something
that, if you have--it takes a long time to run
in SAS then what you can dois you can either
calculate yieldresults or every time they're
requested, you can store them.And what you do is you
take a hash of the datathat you're storing or hash of
the query, and the next timethat exact query comes
along you can checkto see if it exists
alongside that hashand then send the
summarized data insteadof recalculating it.So that can really
speed things upas long as your data
doesn't go stale.So put a-- yeah, this
is a really good trick.Some coding tips.There's actually
one missing there.This one, I use this a lot.If you're going to
navigate your folder tree,this is quicker than just
going to the home page.Action equals 1063.Ah, there's the other one.So if you're testing your
store process web service,so when you write code it will
go to this webout file ref.That doesn't exist
in say, SAS Studio.But if you use this filename
statement as opposedto filename webout
temp, that willbreak if you send non-standard
comment like Excel filesor PDFs.We're going to store
webout with a cache engine.This is an undocumented engine,
but it's basically a dead spacethat you can send anything
you want, including PDF files.So worth having in your toolbox.Some GOTCHAS.So I had a client who
was running my STPand they got different
results than I did,and I could not understand
it until I realizedthey were calling
the STP from Exceland Excel had a different
locale to the browser.So The browser was US, the Excel
was UK, which caused issueswith the end date time format.If you're using those
kind of formats,those locale-dependent
functionality,then you will need
to set to your localeexplicitly to avoid
this, at least in SAS 9.Memsize in the amount of
memory in your sessionis often lower on a web service.So something to look
out for if you're justtesting in SAS studio.And then multibridges In SAS 9,
the default multibridges numberis 3, which is very, very low.It means that if you have three
requests in one app or four,then it stop queuing.So multibridge is a--
think of it as a pipe.You only get so
many connections.So what they recommend is, say
five to eight per CPU core.So if you've got eight cores,
then some 40-odd multibridges.So you definitely
want to increase.Another one, if you're looking
for a way to do error handling.So you've got send
abort and you'vegot abort cancel, all
of these variations.From experience and working
in lots of environments,you have to be very
careful with thisbecause it can just hang
the multibridge session.And like we saw before,
if there's only threeand two of them hang then
everyone else is left with one.So I recommend using
endsas or abort.There's actually a really
nice trick you can use,which is to open a
macro but don't close.So what that means is that with
all of the rest of the codejust gets added to a macro
that never actually executes.So that's a very quick way
to just, wouldn't matter,how deep you are in
a macro definitionyou can end your service.Another one.Don't use usernames
in folder names.If you've got users with special
characters in their names,it can mess things up.In terms of building
documentation,so once your apps'
done you're goingto need a user guide, a config
guide, a deployment guide, evena developer guide.There's tooling you
can use for this.So markdown-to-html, this is
a command line tool that youcan--say you're read making.You can convert
that to a html file.MkDocs.So this is the documentation
tool used in Data Controller.So you write in markdown, you
can embed videos and images.Doxygen. So this
is the one that weused for the MacroCore library.And then Sphinx, which
is similar to markdownbut often used in Python.Testing.So browserstack is
a good facility.Browserstack lets you test
your code in multiple browsers.It's great if you have a Mac
and you need to test on IE.Cypress is an awesome tool.So this is a command line, but
it can also launch a browser.So you can write all your
frontend tests and load cypressand you can see
them in real time.It can even control
your frontendand you can record some tests
that it can replay for you.And it even makes videos
that you can watch.So that's really good.And then sasjs.io.So this is a website which
has a lot of content.So all of the stuff from
these slides, and thenlots of examples and
videos and sample codethat you can use for
building web apps in SAS.And then the app dev community.So there's a Slack channel,
and if you've got any questionsor you want to connect
with other users whoare building apps
on SAS in this way,then you can check
out this channel.So this isn't meant to
replace communities.sas.com,that's also a good place
to go and ask questions.And think that is all.So I hope that you've
learned something.My name's Alan, and if
you've got any questionsor you'd like to do some
training on building web apps,or you'd like some help building
web apps so I can help inwith backend, with
project management,with providing frontend
developers or any questionsat all, just contact me."
53,"Hello.This is Wendy Czika
from SAS, and today I'mgoing to be talking to you about
automation and SAS Visual DataMining and Machine Learning.According to Forbes, one of
the top artificial intelligencetrends to watch out for in
2019 was the rise of AutoML,short for Automated
Machine Learning.So what is automated
machine learning?It involves, as you
would guess, automatingthe tasks that are
required for buildinga predictive model based on
machine learning algorithms.These tasks include data
cleansing and pre-processing,feature engineering and
selection, model selection,and hyperparameter tuning.With so many possible
combinations of features modelsand parameters to
try, it can be verytedious to perform
this all manually justby trial and error.So there are now platforms that
provide this AutoML capabilitythat can empower data
analysts or citizen datascientists by giving them a
start at a machine learningworkflow, as well as
enable advanced datascientists to spend more time
on solutions to a problemrather than being bogged down
with these repetitive tasks.But the intention
is not for AutoMLto replace the work
of data scientistsbecause there can
be a need for thingslike domain specific feature
engineering to be performedby data science experts.So ideally, these systems should
allow for manual intervention.These systems also need to
expose the algorithms beingused under the covers and not
be black boxes so that userscan be aware of and
understand and thustrust the models
being generated.As you might expect, this is not
new territory for SAS software.SAS Rapid Predictive Modeler
was first released in 2010,giving users process flows
of automated data preparationand data mining tasks as
part of SAS Enterprise Minerto create predictive models.In 2017, the Model
Studio Visual Interfacefor SAS Visual Data Mining
and Machine Learningwas released, which is
a modernized versionof SAS Enterprise Miner
that is on SAS Viya.It leverages the cloud
enabled, distributed,in-memory analytics engine
of Cloud Analytics Services,or CAS.Like Enterprise
Miner, Model Studiois an interactive
pipeline environmentfor putting together the
various steps for data miningand machine learning models
from data preparationto model comparison,
and includes automationat various levels of the
model building process.And just last year, in
2019, the 8.5 releaseof SAS Visual Data Mining
and Machine Learningincluded true AutoML,
including automationof the entire machine
learning pipeline as wellas other new tools for
automation specificto particular tasks.So this talk will be focusing
on the tools for AutoML thatare new in this latest release.In Model Studio, there are
two new nodes for automation,one for feature engineering
and one for model algorithmselection and
hyperparameter tuning,as well as the ability to have
an entire machine learningpipeline generated dynamically
for your specific data.This AutoML can be accessed
through Model Studioif you want to use
a visual interface,and also via a REST API that
comes with SAS Visual DataMining and Machine
Learning for youto embed into your
own applications.So I'll talk about both of
these ways to access machinelearning automation in SAS
Visual Data Mining and MachineLearning, starting with using
the user interface ModelStudio.In the latest release
of Model Studio,we have some
automation enhancementsin the areas of data
pre-processing and featureengineering, modeling, and
full pipeline automation.For data pre processing
and feature engineering,some automation that has been
in all releases of Model Studioincludes the data
advisor that determinesif columns of your data are
inputs, IDs, or variablesto be excluded from your model,
and if they should be treatedas categorical or continuous.For transforming your
variables, instead ofpicking a single transformation
to apply to all your inputs,you could use the Best
Transformation methodin the Transformations node to
automatically determine for youwhat function, if any,
from a suite of functions,including log, square,
inverse, et cetera,is the best for each and
the individual input,based on the criterion
of your choosing.You can find more information
about these two featuresin our paper, but
today I'm goingto focus on the Feature Machine
node, our new automated featureengineering node that
came out at a top fivethat you can think of
as an extra strengthversion of the Best
Transformation.In addition to the numeric
transformations thatare performed for the
Best Transformation,this new feature
machine node can firstscreen out uninformative
inputs, perform multiple setsof transformations that
can include amputationto create new features
from each input,and extends the numeric
transformations thatare performed by the
Best Transformationto include conversions such
as binning to transforman interval input
into a class input,encoding to transform class
inputs into interval inputs.And again, based
on the criterion,it will rank the
new features createdby these sets of
transformations and returnone or more features per input
as specified by you, the user.You have control
over the data issuesthat you want to address with
Feature Engineering, shownhere, so from things like
missingness to outliersto high cardinality.What you select dictates
which transformationsare applied to inputs that
exhibit various levelsof these data issues.Here you can see the Feature
Machine node in a Model Studiopipeline with its properties
displayed on the right.And here you see
a table includedin the results of the node,
showing for the variable agethe various features
that were createdusing a set of transformations,
such as medium-based imputationfollowed by squaring or logging
the values, tree-based binning,and others along with
their rank accordingto the symmetric
uncertainty coefficient thatcan simultaneously rank
interval and nominal featureswith respect to your target.Now let's talk about modeling.Since its first
release, Model Studiohas had automatic hyperparameter
tuning or autotuningavailable and many of
its supervised learningnodes for training
a predictive model.Hyperparameters
are properties thataffect the training process
such as learning rate,regularization parameters, and
a number of trees for a forestmodel.Finding the optimal value
for the hyperparametersfor each model can involve a
lot of manual trial and error.To find them in an
automated manner,autotuning searches the
hyperparameter spaceto determine the values that
optimize the selected objectivefunction, typically a
measure of model error,like misclassification rate,
using search methods suchas Bayesian kriging,] genetic
algorithm, grid search,Latin hypercube sampling,
or random search.This model is then trained with
these hyperparameter values.The Model Composer
node takes thisto another level, an
extra strength versionof single model autotuning.In this new supervised
learning node of Model Studio,you can autotune various
model types in paralleland perform multiple
rounds of autotuning.This means after the
first round, whereautotuning evaluations
are equally allocated,subsequent rounds
allocate more evaluationsto the best
performing model type.At the end of the rounds,
the overall best modelfor your data and its optimal
set of hyperparameter,also specific to your data,
are used for training.The model selection is
intrinsically and automaticallyperformed for you as well.Here are the properties
available to youin the Model Composer node.You can choose which
model types youwant to include in the
autotuning, the numberof rounds, and
evaluations per round,which then get distributed
among the model types.Then the autotuning
properties thatare available when you
autotune a single modelcan also be specified here,
including search method,objective function, and so on.In the results, there are
several summary tables,including the hyperparameters
for the best overall model.So here you see the
values for thingslike learning and sampling
rate, lasso and ridge,and tree depth for
the gradient boostingmodel that performs the
best of all modelingtypes for this particular data.Other tables show
the fit statisticsfor the top model
of each type, numberof evaluations per model per
round, and more information.Next, we're going to talk
about complete pipelineautomation, true AutoML.Similar to rapid predictive
models for SAS Enterprise Minerthat I mentioned
at the beginning,Model Studio offers a
library of templatesthat you can use that contain
static pipelines to help youget started, from basic to
advanced pipelines for buildinga predictive model.And again, please see our
paper for more informationon these various templates
that Model Studio provides.But new and fast Visual Data
Mining and Machine Learning 8.5is an option to automatically
and intelligently generatea pipeline that is optimized
specifically for your dataand built in a dynamic fashion.This is using the REST
API under the coversthat I will be talking
about more shortly.So now in Model Studio,
when adding a new pipelineto your project, you
can select this optionto kick off this process that
takes the static templatesa step further.It attempts various
data pre-processing,feature engineering and
selection, and different modeltypes, including
ensembles of modelsto find the optimal pipeline.A pipeline with the
top five branchesthat lead up to a
supervised learning nodeis generated for you,
like the one shown here.The nodes that the
pipeline includesand the properties
that are set for themprovide details of all the
steps that are performedand the supervised learning
algorithms that are being used.It is in no way a black box.You can optionally
unlock the pipelineso you can edit it to
include your domainknowledge by adding,
deleting, or modifyingnodes in the pipeline.Alternatively, you
can run the pipelineas is to get the overall
pipeline champion model basedcompletely on
automation, as shown herein the Model Comparison node.Here, the forest
model and the branchof the pipeline that included
two sets of transformationshas the best KS statistic,
the chosen model's selectioncriterion, so it's
designated as a championmodel for this pipeline.Now we're going to talk
about it a different waythat this automation can be
accessed in Fast Visual DataMining and Machine Learning
using the REST API for MachineLearning Pipeline Automation.This API, or Application
Programming Interface,enables you to embed the
automatic generation of ModelStudio Machine
Learning pipelinesinto your own custom
applications usingthe language of your choice--Python, JavaScript,
Curl, et cetera.The machine learning
pipeline automation REST APIis a set of endpoints
that automatesthe training of
predictive modelsand that gives you control
over various parameters,such as Model Studio
project settings and more.Complete documentation
of the APIcan be found at
developer.sas.com.A request to create a project
requires just the input dataand a target variable in
order to start the pipelineautomation process.Then various end points
are available to youwith the following
request methods.You can use the
GET request methodto return an automation project,
return a list of all automationprojects, get champion model
information for a project,get the current
state of a project,or get a list of all
currently usable links.You can use a POST method to
create an automation projector score new data with
the champion modelfrom an automation project.PUT methods can be used to
update an automation project,retrain the project,
and register or publishthe champion model to an
available destination.And DELETE can be used, of
course, to delete an automationproject.You can find an end-to-end
example in Pythonto automatically generate
pipelines using this REST APIin the example1.py file in the
sas-viya-dmml-pipelines GitHubrepository for SAS software.As I mentioned, other
languages or REST clientscan also be used
in place of Python.In the example code, there
is setup for the parameters,then the code includes the
request methods to carry outthe following steps.First, it creates a
Model Studio projectfor the automated pipeline.Next, it pulls
every five secondsto wait until the project
state goes to completion.The project state initially
starts in the pending state,and then it transitions to
waiting, ready, modeling,constructing pipeline,
running pipeline, and finally,to the completed state.Once it's in that
state, the next stepis to retrieve and
print the champion modelinformation, including
assessment statistics.Next, it publishes
the champion modelto the SAS
Microanalytic Service,one of the destinations
available for publishing.The fifth step is to score new
data using the champion model.And finally, there's the
step to retrain the project.Once new training data
becomes available to you.Here's just a snippet
of the Python codeto give you an idea of how
these requests are made.So just to summarize,
today I coveredhow AutoML and SAS can be
used to streamline your modelbuilding process, using
either the Model Studiointerface or REST API for
automation, both availablein SAS Visual Data Mining
and Machine Learning.Please find our paper online
and the conference proceedings,once that is available, for
more detail on this, includinglinks to code, or feel free to
email me with any questions.Thank you."
54,"Hello, everybody.I'm Dr. Aurora Peddycord-Liu.In this tutorial,
I'm going to show youhow to open the black box of
model interpretability usingSAS Viya software.We're going to start
with some introduction,then I'm going to show you four
different techniques of openingsuch black box.First of all, why
should we care?Why is model
interpretability important?Imagine you're working
in a bank and youbuild a machine-learning
model to helpyou make the decision
whether to approve or rejectsomebody's loan request.You need to explain
to your customerswhy you make such a decision.You also need to
inform your customerswhat they can do to improve
the chance of getting approved.You need an interpretable model
so that you and your customerscan act on it.Another reason is that
it is more and moreimportant for a model
to be fair to states.You cannot include features
such as race and genderinto your loan approval
decision-making, because that,your model, will be
biased because of the biasin the historical data.We want to treat everybody
fairly despite their raceand gender, so we need to make
sure these features do notgo into our model.In highly-regulated industry,
it is extremely important.You can control your model,
and you can explain your model.So what is model
interpretability?For me, it's more-- you
can explain by your modelmake such decisions,
given the input variables.An example is regression model.A linear regression
is explainable.You can clearly
tell, hey, in orderto improve that much
in the logit (p),how much I should improve in
x1 or x2 using the weights--the w highlighted on the slide?Another example
is decision tree.Decision tree is a set of
rules so you can clearlyexplain, hey, when x2
is bigger than 0.63and when x1 is
smaller than 0.51,you end up in the yellow leaf.You can explain exactly what
are the sequence of stepsthat lead to a final
decision by your model.However, some other models
are not interpretable.An example is Neural Network.Even though Neural Network
is essentially a huge mathformula, it's made of
many, many little mathformulae that are not linear.It's extremely
difficult, or I sayit's impossible to tell how much
I can increase in x1 in orderto increase that
much in y becauseof this complex formula.Another example is Forests and
the gradient boosting, as well,because those models
are, in symbol, modelsof lots of decision trees.Let's use gradient
boosting as an example.We build one tree
to predict a target.Then we build another tree to
predict the residual, whichis a difference between the
prediction of the first treeand the real target.Then we do the third tree
to predict the residualafter using the
previous two trees.And we keep building
trees and trees--a sequence of trees to
predict our final target.Now we have a lot of trees--it is impossible to explain
why your model makessuch a decision.There are so many
trees to keep track of.So when we have to use your
network or gradient boosting,what can we do?How can we explain
what our model's doing?Before we get to
the exciting part,I want to first
introduce the data we areusing in our demonstrations.We are using this
Titanic survival data.Each observation
is a person, and wetry to model whether they
survived the Titanic tragedyor not.The input variables
are age, sex,numbers of siblings and spouse,
the cabin class, and fare.We are going to use
Model Studio in SAS Viya.We're going to view the model
pipeline with two models--a decision tree, an
interpretable model, and thenuninterpretable model--
the grading boosting.Then we're going to
apply our four techniquesto open the black box
in grading boosting.All right, so now we're
in this SAS Viya tool.It is actually a toolbox,
because there's somany components
inside this tool.Today they're only going to
focus on one little tool, whichis Build Models--SAS Model Studio.We are going to start
this Create a New Project.Let's name it Titanic.It's a data-mining,
machine-learning model type,even though there are
other types available.And we are going to load a
data, click on this Browse,click on Import, Local Files--Titanic3_ID.We can import a CSV or SAS
files into our project.We can also set up in a way to
connect our data with Teradataand other external database.In here, I click Import.This is the process of passing
your data from our computerto the CAS server in
the cloud, so the CAScan then analyze the data in
a parallel computing fashion.Then we Quick Save.Once we create a project, it's
going to open on this one--this Data tab first.In the Data tab, we're
going to first tellour machine, what is the
variable we are predicting?It is the Survived,
and we are goingto change its row
from Input to Target.Another variable we are
changing is the key_ID.We are going to change it from
ID to Key, so that later wecan refer to an individual
observation by this variable.Then we're going to reject
two variables, whichmeans don't use them.We are going to reject
Embarked and this Parch,and change the row
from Input to Rejected.All right, know that
whatever we're doing here,we have not changed
the real data.Imagine you work in this
large project but everybodyshares the same data.In here, we just
merely add metadata,which are tags of how
to handle this variablein our specific project.The original data is untouched.After we preprocess our data,
we're going to this pipeline.This is where we do
procedures of buildingmachine-learning models.We're going to right-click
on Data, add a Child Node,Supervised Learning--we're going to first
add a Decision Tree.Then we right-click
on Data, Add ChildNode, Supervised
Learning, and we are goingto add a Gradient Boosting.If you click on
any node, you cansee on the right
there are settingson how to build each model.For example, how are we
splitting and growingour Decision Tree?In Gradient Boosting
here, in the OptionsBy Default, we are
building 100 trees--a sequence of 100 trees.If you hover on
this text, it willshow you some documentation.You can also see
more documentationby clicking on this
question mark in here.It's very well-documented.For the Gradient
Boosting, we're goingto add those additional
functionality to openthe black box in the setting.Let's scroll down to the
Model Interpretabilityin the Post-Training Properties,
which is-- after training,we are going to add
this component in.In the Global
Interpretability, we'reclicking both of the
options, and later we'regoing to explain, what are they?In the Local Interpretability,
we click these three.Then all we need to
do to do the modelis click this Run Pipeline.It seems so easy, right?Every point and click is
being translated into codeat the back end.This code are CAS actions, which
is a small unit of work for CASto execute in
parallel in the Cloud.Now we can see it
already loaded our data.It's building two
models parallelly.Of course, we are
expecting Decision Treeto get viewed first,
because it is just one tree.Gradient Boosting is
finished, and those modelsare being compared
in the final node.So let's right-click on Decision
Tree and see the result.We see several tables--both Node and the Assessment.Note explains the model, and
Assessment is the predictionresult. You can see the
misclassification rategiving the numbers of leaks.And then we are going
to select the machine--has selected the subtree,
given the performanceof Validation data.If we go out and
see the Tree model,we can click this
Extend button, and youcan use your mouse to zoom in
and zoom out to see the detail.You can check it, and
for example, what Isee here is, that
for all the malethat is more than four
years old or missingage, the majority of them--80% of them-- didn't survive.Whereas, if they are less
than a four-years-old child,they have a higher
chance of surviving.So this is an
interpretable model.We can clearly explain what
happened to each individual,and what influenced
the chance of survival.Now let's close this out and
see our Gradient Boosting model.We right-click--
see the results.There's no tree anymore.We can see the
Average Squared Errorin Training Data, Validation
Data and Test Data,giving the numbers
of trees in sequence.And there's code to describe
the sequential model,but we cannot explain it.We cannot say why
somebody survived or not,because there are so many trees
to take into consideration.So next, I'm going to show
you how the button we clicked,the Model Interpretability
functionalities,helps us open this Black Box.The first technique
I'm going to show youis Partial Dependence Plots.This is a Partial
Dependence Plot,and it's showing you what is
the probability of survival,given the passenger's age.And as you can see from this
plot, the older you are,the less likely you can survive.So how do we
calculate this plot?We have our original data set,
from which we viewed our model.Then we're using
the same data set,but replacing
everybody's age to be 1.We pass this new data to
our already-viewed model,and we calculate the average
prediction as the y-value.Then when x equals 2, we
replace everybody's age to be 2.Then we put this
data into the model.We calculate the
average predictions.And this is a y value,
when x equals 2.So let's see an example
in our software.This plot is in the
Model Interpretability.We can expand it.This plot explains what
a Gradient Boostingmodel is thinking.With a Gradient Boosting
model, if you're a female,you're much likelier to
survive as compared to male.You can also check the
age and other variables.Here's the age.And as you can see, the older
you are, the less likelythat you will survive.You can see this dramatic
drop at age around 14,so females and people under
14 are more likely to survive.And this makes sense because
we had the policy of lettingwomen and children go first.So this is a PD
plot, and this ishow it helps us explain
what our Gradient Boostingmodel is thinking.The next technique is the
Individual ConditionalExpectation plot--the ICE plot.This is really similar
to the PD plot,but instead of just explaining
the whole population,we are explaining
interesting cases.This blue line represents the
average, which is everybody,just like in the PD plot.The other colors each represent
an interesting individualof our choice.It is calculated the same way
we calculate the PD plot, whichis, after we built
the model, we applythis new case using the model.We see that they're making
everything else stay the same.We just change the age
for this one observationif the model will give
a different probabilityof survival.In this example, we
see some difference.For this yellow
person, our modelwill not change too
much of the prediction.No matter how much we change
the age, given everything else,it stays the same.Whereas this purple person--based on our model, if this
purple person is younger--given everything
else stays the same,this purple person has a
higher chance of survival.So this plot, the
ICE plot, is commonlyused to do the ""what if""
kind of thinking to helpexplain interesting cases.So let's see how we use
that in our software.First, we're going to close
out this result table,click on the Gradient
Boosting, and scroll down.So we can choose the
case of our interest.In the Post-Training Properties,
Local Interpretability--if you scroll down,
you'll see here they'respecifying Random
instance to explain.We click here.We want to specify two
cases of our interest.In here, Individual
Observation 1--we're typing in number 1.So this is what we refer
to, using the key value.So this number 1
person is actuallya passenger called
Miss ElizabethWalton, who is a 29-year-old
woman who survived.We want to look
into this person--whether this person has a higher
chance of survival if she'syounger, or other cases.In the Individual
Observation 2--we're typing in number 1309.So this person is
Mr. Leo Zimmerman,who is a 29-year-old man.So they're both 29 years old,
but the first one is a womanand the second one is a man.After we've done that, we
run the Pipeline again.Because we changed the setting,
the Gradient Boosting modeldoes not have the
green check anymore.We have to run it again.This is a really good way
of controlling our procedureso we don't assume a model
is viewed on some settingseven though we never run it.After it's done, we right-click
and see the result. In here,in the Model Interpretability
tab, we scroll down.Here is the PD and
ICE overlay plot.Let's expand it, and we can see
if Mr. Zimmerman is a female,his survival rate would be 0.44.But unfortunately he is a male,
so his survival rate is 0.18.But for Miss Allen, the female--if Miss Allen is a male, given
everything else is the same,then his survival rate would
drop from 0.86 to 0.39.We can also change
the sex to age,and this is the plot we see.For our lady, no
matter how old she is,she still has a high
chance of survival,whereas for a man,
if he is younger,he has a much higher
chance of survival.And if he is older, he barely
has any chance of survival.This totally makes sense,
because remember, wewere still having this child
and women go first policy.So next we're going
to explore this Fare.And here we see, no
matter how much they pay--unless they pay
really, really lowor how much they pay high here--it doesn't have a big influence
on their survival rate.But still, as you can
see, the survival ratebetween our lady and our
male is totally different,despite the amount
of fare they paid.Next we chose the Pclass, and
here we can see a difference.For both our lady
and her male friends,if they are higher class, they
have higher chance of survival.So this is an example of how we
use these ICE plots to answerthose ""what if""
questions and helpus open the black box of
our Grading Boosting model.The third technique I'm going
to show you is the LIME plot.This is how you would
do the LIME plot.It's a plot aimed to interpret
local decision boundary.We start at this Fitting
an Uninterpretable Model--the Gradient Boosting model.Then we choose an observation
or a set of observationsof our interest.Then we create perturbed samples
around the chosen observation.Then we score this
perturbed sample,and then we fit a linear or
logistic regression modelsto the perturbed sample.So we use a simple
linear model to explainwhat happened at that spot
of a decision boundary.One way to show this
is that-- imaginewe are doing this decision
boundary between the blueand the yellow.We can see the boundary
is very, very curvy,and it's hard to
explain this curve.It's hard to explain how much
I need to increase in x1 and x2in order to increase or
decrease my chance of survival.So we choose this
observation of interest,represented as a cross.Then we create a bunch
of the samples weightedby the distance to
the observation.Then we state a regression
for this instance.So now we can explain what
happened at a local level.The linear boundary at the
local level around this crossrepresented instance.So let's go back to our software
and see how we can do that.Remember, in the
previous demo, we alreadyselected two very
interesting cases.It's Miss Allen--29-years-old first-class
lady, and this Mr. Zimmermanis a 29-year-old
third-class male.And by selecting
those two examples,the LIME plot is going
to describe them.Let's expand the LIME plot.Local instance 1--
this is our Miss Allen.For her, this a
regression model.Viewed using the perturbed
samples around her,we can see her first-class and
her being a female increaseher chance of survival.Whereas for Mr. Zimmerman,
him being a third-classand being a male decreases
chance of survival.And as you can see in
both examples, sex--the gender-- is the most
important factor in survival.So by using this
LIME plot, we areable to explain how our
Uninterpretable GradientBoosting model thinks.By creating small
linear regressionmodel using Perturb the Samples
around interesting cases.For the last techniques
I'm showing you--it's the SHAP plot--Kernel Shapley
Additive Explanations.This is very similar
to LIME plot,and it's used the same
way to try to explainlocal interesting observations.Key difference is
that SHAP computesthe values in a different way.It's calculating how
each variables contributeto the difference
between the predictionand the average prediction.Seen here, as you
can see, featuresare ordered by importance, and
the positive and negative valueshows if they positively
or negatively influencethe survival rate.So let's go into our software
to see this SHAP plot.The SHAP plot is right next
to the LIME explanation.Let's extend it.We see a similar pattern,
which is a good thing.For a Miss Allen, we can
see being a first-classand being a woman contributes
to her survival rate.Age-- maybe Miss
Allen's a little older,but it doesn't
influence that much,even though it
influences negatively.For Mr. Zimmerman, being a
male and being a third-classreally hurts his survival rate.So we see the same pattern.We see all the variables.And we know that
throughout our techniques,we learn that the
class and the genderare the most influential factors
that play into survival, givenour Grading Boosting mockup.So this is all the four
techniques I'm showing you,but remember in this
big SAS Viya toolbox there's a lot of plots and
ways for you to build the modeland play with the model results.So feel free to
explore more usingSAS Viya for Learners and
other tools available to you.In this specific
tutorial we've seenhow can we use this
tool to both explainan uninterpretable model
globally and locally.All right.Thank you for watching
this tutorial.I am Doctor Aurora
Peddycord-Liu.I wish you all stay
SAS-sy, and havefun learning more about SAS."
55,"Welcome to SAS Visual
Analytics on Viya.I'm Linda Jordan.And I'm an instructor at SAS.I've been teaching SAS
classes for over 20 years.And for the last
eight years or so,I've been solely dedicated
to Visual Analytics.Visual Analytics is
a tool that allowsfor you to analyze very, very
large volumes of data very,very quickly.So during this video, I'm going
to show you kind of the soupto nuts of Visual Analytics--how do we get to
our data, how can wecreate interesting
visualizations,how we can turn that into
an interactive report.So hang tight and
we'll just get started.So what is SAS Visual Analytics?Well, Visual Analytics
is a web-based product.So all you need is a
browser and your webaddress for Visual Analytics
for the Visual Analytics server.Once you connect, you'll
be accessing SAS'shigh performance analytics.This allows for us to reach
out to those very large volumesof data and look for
those patterns, trends,and opportunities for us to do
a deeper dive into the data.So we start with
massive amounts of data.This is what we
all struggle with.We have so much data.And what can we
do with that data?Well, we can create reports.We can explore the data.We can share those insights.But before we do any
of those functions,we need to get the data
into Visual Analytics.So we can import our data.It's quite easy.Just a little point
and click interfacewill bring that data right
into Visual Analytics.Behind the scenes,
the data's actuallygoing to be stored on
a server called CAS.This stands for our
Cloud Analytics Services.It's very quick.It is scalable, meaning
if you guys have startedwith maybe a smaller
installation,and you want to add
some machines, that'squite easy to do.It has node-to-node
communication.So there is a head
node that's in charge.And it will distribute
your data equally outamong those nodes,
the worker nodes.Those worker nodes
have a backup.Therefore, there is
a fault tolerance.That way, we can make sure
that if it would go down,we don't actually lose our data.And it is quite resistant.All of those
functionalities are includedin our multimachine
installation or the distributeddeployment of Visual Analytics.We can also do a single machine.If you have a single
machine installation,then you would not have
a distributed deployment,meaning all of the data would
reside on a single serverinstead of multiple machines.So we're talking about Viya.Viya is the platform
or the environmentthat we'll be taking a look
at Visual Analytics through.When we access Viya, we're
going to land on the SAS Drive.This is kind of
like our home page.And this gives us access to
all the other applicationsthat our credentials
have capabilities for.So within this
class, we're goingto focus on Visual Analytics.Visual Analytics
is the tool thatallows for us to do
our data discoveryand our interactive reporting.Within Visual Analytics,
you could havea variety of different users.So just depending upon
your company's makeup,you might have all
of these people.You might have some
of these people.For the purposes of
our little video,we will take on the
role of analyst.Analysts are the primary
users of Visual Analytics.We can have other users as well.So the data scientists may be
accessing Visual Statisticsor Visual Data Mining
and Machine Learningthrough Visual Analytics.Our data administrator might
be using the Data Studioapplication on Viya.And that allows for them
to clean data, make changesto the data, upload data to CAS.Our administrator's in charge of
making sure the environment ishealthy, so making sure we
have the correct access,making sure that
the users also havethe correct access to
the correct tables,so not only access to
the application, but alsoaccess to the data.And then all of us, including
the information consumer,would view reports.So the information
consumer's roleis that of just viewing reports.Might be our manager,
senior management that'sjust going to look at reports.So I'm going to
jump into a demo.Throughout this video, I
will have a few slides,and then I'll jump
into a demo hopefullyto keep you guys hanging on.So let's take a quick
look at that SAS Drive.During this demo, I'm
going to illustratehow we can sign in and
access the SAS drive.So you do need to have
some login credentials.They should be supplied
by your organization.I'm going to log in as
one of our users today.We have some standard users.And Eric has the
credentials for an analyst.So we've just logged
into SAS Drive.And I'm going to make sure
that my window is fully zoomed.There we go.So this is SAS Drive.And everybody's
SAS Drive will belinked to their user or
their login credentials.So over in the far
right hand corner,you'll see that I've
logged in as Eric.So on his SAS Drive, he
has a link to the videosor an option to drive
into the videos.If I go in the upper
left hand cornerwhere I have those
three horizontal linesand select that, this shows
me the Viya applicationsthat Eric has access to.And for the purposes
of this class,we're going to be
primarily interestedin the Explore and Visualize.If we want to
start a new report,we would select our
Explore and Visualize.And that would take us
into Visual Analytics.At the top portion
of our SAS Drive,we have this Quick Access area.And I can drag and drop
items here that maybe I'mgoing to use on a
very regular basis.Like potentially, I
could have a reportthat I'm using as a template--sort of my starting place
for subsequent reports.And I could drop that here
in the Quick Access area.If you would like a little
more real estate for the folderstructure and the data
items in the bottom,we can minimize this.So over in the far
right hand corner,you'll see Quick Access
with the double up arrow.We can select it.And now, that section is gone.A couple of things about Visual
Analytics that you'll seeare common, one of
which is the threelittle dots over in the
far right hand corner.And this is going to be
available in a varietyof different places.And sometimes, it'll say Menus.Sometimes, it'll say Options.But basically, this is
a pretty important iconthat you want to get in
the habit of looking for.So when I select
this, this shows mewhat my options are on
this particular interface.So I can manage the tabs.What are the tabs?Well, the tabs are
these items here.Shows me my projects.If I had any reports,
they would appear.If I wanted to do some SAS
coding, I could do that.I'm going to start with the
All over in the left hand sidejust to talk a little bit
about the folder structure.So your folder structure
at your organizationmight look different.It probably will.One thing that
should be consistentis that you should
always have a My Folder.The My Folder is set
up that is linkedto your login credentials.And the only other person
that could have accessto your My Folder would
be your SAS administrator.So currently, if I
select my My Folder,you can see that
I have the videos.I have some SAS
videos stored thereif I wanted to go back and
take a look at any of those.You also have your My Favorites.This is a great place to maybe
drop that starter report, sothings that you're going
to work with a lot.If you've minimized
the Quick Access area,you might want to use
the My Folders for that.So we're never going
to develop SAS code.We're not going to
do custom graphs.So I'm going to go ahead
and modify these tabs.So I'm going to go back
over to my little snowmanin the upper right hand corner.And I am going to
select Manage tabs.So now, it shows me
all of those tabs.And I'm just going
to remove them all.When I do that, the All
and the Recent are saved.They will never be removed.You can see they have
little locks next to them.So they're always
going to be there.In addition to All
and Recent, I'mgoing to go ahead and
select My Reports.And I can just move that over.And then maybe I want to
go ahead and also selectBuild Custom Graphs in case
I need to do that as well.So now, I can select OK.And my tabs will have updated.And so we will have removed
a couple of those tabsthat we really don't need.From the SAS Drive, we're going
to go ahead and take a lookat some reports that
were created for usthat we're going to use
throughout the remainingof the class.So I'm going to go
into the SAS content.Within SAS content
in the public folder,I have a couple global forum
reports that have been created.So I'd like to make sure
that Eric can see these.So I can just right
click, select Share,and then I can find someone.And I'm just going to
type in my E for Eric.Then I can locate him.And looks like I've
already shared it with him.So let's take a look at
sharing it with maybe Lynn.So now, I've got Lynn.And what do I want
to let her do?Can read and edit.So I can select Share.And now, it means that Lynn
has access to this global forumreport.OK, so that's going to wrap up
our first section on the SASDrive.So our next section will
be on analyzing our data.So in this
demonstration, I'm goingto show you how you can make
some modifications to the data.And we'll begin
analyzing our data.I need to get into
Visual Analytics.So I'm going to go to my
menu option on the SAS Drive.And Explore and Visualize
is what we're looking for.So the first thing we're
need to do is grab some data.So we'll go to the Data icon.And for this, I want to use
the customer's clean table.So it shows me that I have
21 columns, 951,000 rows.Gives you a quick little
glance at that data.We can go there.There's a sampling of it.So this is what I want.And I'm going to select OK.So when I look at the
data in the Data pane,I have all of my
categorical columns.And then I have
counts next to those.Those counts are the
number of distinct values.So I have seven different
customer types within my--what was it--
951,000 observations.And again, no distinct
counts on my measures.That just wouldn't make sense.So I'm going to go ahead
and modify some properties.For our date order
was delivered,I'm going to go in, drop down
onto the edit properties.And instead of showing
this at the day level-- sothis is being displayed
with the monthname, the day of the month,
and a four digit year.That is what date 9 shows us.So I'm going to go in and edit.And I would just like to see
the month and the year, no dayvalue.So I'll select the MMM/YYYY.That's always a tongue twister.Select OK.And I'd also like
to shorten the name.That's kind of a very
long column name.So I'm going to change
this to delivery date.And I'll select Enter
to accept those changes.Additionally, I'm going
to modify the propertiesfor the discount in percent
of normal total retail.So when I look at this,
the aggregation is sum.So it's totaling up
the discount amounts.I don't want that.I want to know the average.So I'm going to go down,
select the dropdownfor the aggregation, and
then we'll select Average.Also want to change the name
here to Average Discount.And again, Enter to
accept those changes.I'd also like to modify the
name for the date the order wasplaced by customer.Dropdown Date Order
was placed by Customer.And we're going to rename this
Order Date and accept that.So now, I want to learn a
little bit about the data.So before I can really get
into analyzing the data,I need to have kind of an idea
about what I'm looking at.I'm going to take Profit
and drag it into the center.And you see I'm still
holding my mouse down.It says Auto chart.So I've just selected a
column and dragged it overinto the canvas.By default because
it's a measure,I'm seeing a histogram.When I click on any bar, I can
find out the detailed levelinformation.So the majority of my data is
residing between 0 and $25.That can't possibly be
good for my company.Maybe I'd like to know
what percentage of my data.So if I go over to the
Roles on the right hand sideand select Frequency, I have
the option to replace that.So I'm going to select Frequency
Percent and then change that.So it told me the
number of observations.Now, it's telling me the
percentage of the data.So I'm at about 65% of
my data is 0 to $25.I don't think I'd be very happy.I can also take a look at
these values in a table form.If I go over into the
upper right hand corner,I'll see the Maximize
icon, the double arrows.When I select that,
this is showing methe breakdown of the
observations, what percentage.You can see very, very small.And then if I select one, there
it's highlighted in that grid.So 22% of my data is
between 0 and a negative 25.Again, I could not possibly
be happy about that.So let's take a look at
profit using another object.You need to restore the
size, and then we'llgo to the Objects.And we'll take Crosstab.And I'm going to drag that down
and release it to the bottom.So what am I interested in here?Well, I'd like to know the
profit by the order type.Maybe I can learn
something about the data.I want to go ahead and add
in my order type as the rows.So I'm going to use
the data roles overon the right hand side.And I'm going to
select Order Type,and then select OK
to confirm that.So what is this telling me?Well, wow.The majority of my data
is in retail sales.And this is just the frequency.We're interested in the profit.So let's swap out
Frequency for Profit.And so this doesn't
really surprisedme considering the
number of observationsthat we had as retail sale.But I do need to learn a
little bit more about this.My company wants to increase
catalog and internet sales.So I'm going to
need to figure outwhy the majority of our money
is coming from the retail sales.Maybe I'd like to see where it's
coming from as well, like whatlocations.So in the columns, I'm
going to select the Plus.And I'm going to go
grab the continent name.So we'll start by just
looking at which continentit comes from.Well, Africa is not doing well.We're losing money on our
internet sales in Africa.And interestingly, my largest
profits are coming from Europe.And the reason
that's interestingis my company is based
in North America.So we would think that
we'd be generatingmore money in North America.It might be nice to have some
totals or subtotals here.If I go to the options,
and I scroll all the wayto the bottom, you'll
see Totals and Subtotals.I'm going to select Totals.I don't really want
them to have themin both the rows
and the columns.I'm going to select
just columns.And maybe I'd like them
to stand out a little bit.So we'll select the
background color.And we'll select the light blue.So I think that's good.That makes those stand out.A lot of times, when I
create a visualization,I often wonder,
hey, I wonder whatthis would look like in a
different visualization.So we can easily go
in and change this.If we go to our three
little dots or our snowmanin the upper right hand corner
of the object, just the object,then I can change
my crosstab to--and it recommends a list table.But that's not what I want.I want to see it in a bar chart.Well, because it has two
categorical data items, ordertype and continent
name, by default,it creates this lattice
columns bar chart.I don't love this.So I'm going to
take the order type.I don't want to replace it.I just want to drag it.So I'm going to drag order type
from the lattice column rowinto the group of
roles and release it.So now, this is a more
traditional bar chart.I've got my three different bars
for the three different ordertypes by continent name.I'm going to go ahead and give
this actual observation a name.If we go to the objects,
we can expand objects.And then I might want
to give this a name.I'm going to just
call it Profit by--well, that's the title.I don't want to do it there.But oh, there we go.I'll just move it
up here to the name.Excellent.So I've just named
it behind the scenesin case I'm going to end
up creating like a PDF.That way, the name
will be on there.Let's see how this might
look as a stacked bar chart.So as I scroll through this,
I'm in the bar section now.I could change the direction.I'm going to go down
to the grouping styleand change it to a
stacked bar chart.I'd also like to
see the data labels.Some of the Select Data
labels, they're quite small.So I'm going to go and
just up them a little bit.There we go.Maybe I want to make them bold.I think that looks good.So for this section,
we're going to save this.And I'm just going to call this
Report 1_1 and select Save,and then we'll talk a little bit
more about analyzing our data.OK, we're going to continue
in analyzing our data.We just did some quick
glances at the data.And we want to
further analyze it.So coming from the perspective
of being part of the marketingteam, and our responsibility
is to increase the profits,we quickly glanced
at the profitsby continent name
for the order types.We'd like to continue
analyzing the data.But we need to create
some new columns.So we'd like to know, are you
part of the loyalty membership?And also, which customer
group would you fall into?And these items
need to be created.So we have some choices when
we're creating data items.We can create new
calculated data items.We can create custom categories.We can also duplicate
a data item.So earlier, I changed
the format of Order Date.I could have duplicated it
and kept the original formatand then just changed the
format on the duplicated item.We also have some built-in
predefined geography data itemsthat we can link to our data.We can build hierarchies.This is just a
nesting of the data.And then we can collapse
it and use it to drill in.It's a great tool for when you
have lots and lots of data.We've got so many
derived data items.There are approved written
little routines for us.So something like percent
of total is quite popular.We can do aggregated data items.So that's a
calculated data item.But it's a little
different because thisis going to sum up the data
before the calculation.So our calculated
data items by defaultwould be observation
by observation.So we have salary
times increase.And every single record is going
to go through that calculation.And the new value
will be written out.Aggregated measures
mean that we'regoing to do some summing, some
BY-group processing first.So we'll sum it
up by the countryand then do the calculation.In custom categories, we
have a couple options.We could use the point and
click side to assign it,or we can just use some
programming, some if-else logicto go ahead and create
our new custom categories.Customer age, though--
that's a little trickier.So it's a simple expression.We have our x minus
y divided by z.But here, we're going to go
ahead and use some operators.This might be new to you.The Now operator will return
the current date and time.And while I want to
calculate their age,I don't need the time.So going to use the
DatePart operator to justtake the date portion off.Because I'm in
Visual Analytics, Ineed to ask for this
to be used as a number.So we'll use the
TreatAs operator there.So now, I've got my
basic x and y set up.I'm going to do the
calculation, divide it by 365.25to accommodate for leap years.What gets returned will
have several decimal places.And I don't want to round up.Because if you're
38.7, you don'twant to be referred to as 39.So we're going to use the floor.And that will just return
the nearest integer.It will round down.And that way, we
won't be rounding upany of our customers.In addition to
calculating data items,we're also probably going
to want to filter the data.And we have some
choices here as well.So I can filter the data
at the data source level.So as the data source
comes in to the Data pane,it could be filtered, which
means that those items wouldbe updated.We also can create
geography data items.Visual Analytics comes with some
predefined geography lookupsthat we can connect to our data.We can create a variety of
different graphs doing that.So we've got country names and
codes, state and providencenames, the ID codes.We can link it to
US postal codes.We can link our data to US
state names and abbreviations.If you have something
else, but youhave the longitude
and the latitude,then we can set up a geography
data item using that.We can also do
regions or polygonsby setting up the
geographic data provider.So this lets you add in
custom polygon shapes.Hierarchies are
nesting of information.So here, I'm going to nest
the month into the quarterinto the year, and
then I can drill into expand that information.So it's just an arrangement
of categorical data items.And you want to start
at the most detailedand kind of work your way up.OK, so we're going to
do a demo that shows ushow we can create
some new data itemsand just continue working
with exploring our data.So I've got a couple
objects here on this page.I'd like a little
more real estate.So I'm just going to select
the Plus next to Page 1.And it'll give me a new page.Now, I want to go ahead
and create a couplenew distinct data items.So on my Data pane, I'm going
to look for my customer ID.And I'd like to create
a new data item that'san aggregated measure that shows
me the distinct customer IDs.So I'll right click on Customer
ID, select New calculationtowards the bottom.And then I'm going to call
this Number of Customers.And it is a distinct count.So I'll select OK.And I basically want to do
the same thing on order ID.So it's right click,
New calculation.We're going to call this one
Number of Orders and select OK.So now, I have my two new
aggregated at the measures.And I want to take a look at
those with the order type.So I'll select all
three, drag those over,and I should get an order chart.So it shows me three different
order types and then the numberof orders and the
number of customers.Remember, we're
not-- they're notgoing to share the x-axis
because the values areso different.So if we force this, we would
not really see much informationabout our number of
customers because it'sso much significantly smaller
than our number of orders.Let's go back to the
Data pane and continueto make some changes.So we have profit.And I'd like to
create average profit.So let's duplicate
this data item.Right click, and then Duplicate.So it automatically
makes me this Profit one.And then I'm going to go ahead--whoops.I'm going to go ahead
and make some changes.So let's change the aggregation
from Sum to Average.So we get the average profit
instead of the total profit.And let's give it a name.We'll call it Average Profit.Now, let's use that.I'm going to go ahead and
minimize the Properties pane.I would like to take
a look at the ordertotal, the average profit
by the continent name.And I'm going to drag these over
to the right of the existingbar chart.And it makes me a list table.Well, I'd like it
to be a bar chart.So I'm going to just use
the three little dotsin the upper right hand corner.And we want to change
our list table to--and we're going to
select bar chart.So here is that columns,
the lattice columns.And I'm not loving that.I'm going to go to the roles.And you can see that
continent name--whoops, hit cancel-- continent
name is in the lattice columns.I'm just going to drag
it up to the group role.So now, this is more of the
bar chart I was looking for.So I should see my
three different catalogsas my y-axes, the average
profit on my x-axes,and then a bar for
each continent.And we can see Africa
here is our one locationthat we're losing money.I'm just going to
go give this a name.I'll go to the options.And then instead of
Untitled, Average Profits.And we can also give
the object a name.I'm going to give this a
little bit more average.Remember, the name we don't see.That's behind the scenes.I can see it over on
the left hand side.If I select the outline,
there's that name.And I do like to name
my things, especiallyif I'm going to create a PDF.Those names would
go out into the PDF.So our goal in this section
was to create new data items.So let's go back
to the Data pane.And we're now going to
do a custom category.So currently, I have this
column named Loyalty Number.And there's two
different values.There's zeros and nines.And we want to create a
new calculated data itemor new custom category so that
we see the words yes and no.So let's go to the new data item
at the top of that data paneand select Custom category.So the first thing you want to
do is make sure you name it.I forget this on
a regular basis.So we're going to name
this Loyalty Member.And then what do I
want it based on?Well, I don't want to
based on city name.You can see all of those cities.I'm going to drop down and
locate the loyalty number.So I'm going to only
have two values here--the zeros and the nines.If they are not part
of the membership,it will be the zeros.And I want the word no.So I'm going to type
in the value label No.And then I'm going to drag over
the data value that matches.And then down here at
the bottom, click or draghere to add a value group.So we also want to do the 99s.So I'm going to drag those
over and release themon top of that information.And now, I'm going to rename
value group one to say Yes.So one more thing that
we should point outis if you see these remaining
value options at the bottom.So if we got something
else in, do wewant it to show up as it is?So if there was a typo, and
somebody typed in three nines,or they got a eight in there,
do we want it to show as is?Do we want it to
show is missing?We've decided to
group it into other.So I'm going to leave this set.Other's good.And over in the far right
hand corner, I will select OK.Now, in my data pane, I
have this new data item.And it's two distinct values.And you can see the icon
looks a little different.So this is the icon
that's used for creatingcalculated character
or categorical data.So I want to go ahead and
duplicate this bar chart,my Average Profits bar chart.I'm going to select
the three little dotsin the upper right hand corner.And I'm just going
to select Duplicate.So now, I have two.And I'm going to stack them.So I'm going to
select the move iconand just drag it down so
that it kind of splitsthe space so they're stacked.And now, I want to
change the roles.So let's select the Roles
icon on the right hand side.Instead of Order
Type, we're goingto look at the
loyalty membership.We still want to look
at average profit.But instead of looking at
them based on the continent,we're going to go ahead and look
at them based on their customertype.So I'm going to
replace continent namewith customer type name.So here are all of my
loyalty members, yes and no,and then the
different memberships.I think I might switch these.Let's see what happens.See if we like this better.I think I do.I think I like the customer
type name and then the loyalty.So each customer type
name will have two bars.I do find this a
little challengingto read because
a lot of the barsare very close to each other.So I find myself kind
of tilting my head.Let's go change the
direction of this.So we'll go to the Options icon.And then I'm going to scroll
down into the bar section.And the first
thing is Direction.So let's change
that to a vertical.And now, I think this
is much easier to see.There's a little
bit of a difference.The Orion Club
members low activityand the Orion Club gold
members medium activityhave less members
than the others.The rest of them,
the blue's higher?Let's see here.Just barely higher.Remember, if I wanted to look
at that data in a data grid,I could expand, so
maximize that object,and then we could sort
it if we wanted to.And then if we want to go
back to the original size,you need to restore it.Let's also give this a name.I'm going to go up to the top.And I'm going to name
this because it'snot Average Profits
by Order Type anymore.It is Average Profits by Loyal
Team Membership and CustomerType.In continuing to
create new data items,let's take a look at creating
a new geographic data item.So we'll go to the data icon.And we're going to convert state
name to a geography data item.One thing I'd like
to point out arethe number of distinct
values for state name.Currently, there are 272
distinct or different statenames.When I hover, it tells me that
the name in the data sourceis state underscore province.So remember that we are actually
working with global data, datafrom all around the world.We want to create
just US state names.So let's go ahead and do this.We're going to change
it from state name.Actually, let me undo.Up in the upper right
hand corner is our undo.Sorry.Let me go back.Let's change the
classification first.So from Category, I'm
going to select Geography.Now, I'm going to name
it US State Names.And then currently, only 1%
of the data is being mapped.And that's because it's trying
to map it based on the countryor region names.And that's not what
we're looking at.So let's drop down
on the last optionand change this
to US state names.So my new column is going
to be US state names.Let me add my S there.I'm using the built-in lookups.So we've got a handful of
different types that come with.These are all of the
different geographic lookupsthat are available.We want to match
it to state names.It tells me that 19%
of the data got mapped.Now, when I scroll down, it
starts to show me those values.And clearly, what I'm
seeing is non-US data.So I'm going to come
back and fix that later.But I'm OK with
this at this point.I'm OK with the 19%.So I will select OK.So now, it tells
me there are 272.But if I tried to
use it, I wouldget the little i,
the information thattells me 220 were not mapped.We're going to do the same
thing with postal code.So let's drop down, change the
classification to Geography.Call this US Postal Code.Nothing's mapped.Because again, it defaults
to the first option, whichis country or region names.And I'm looking at US zip codes.Let's get this the correct name.And 57% are now mapped.Again, we're looking
at data worldwide.So once we add a filter,
we should be good to go.I'll select OK.I would like to
create a hierarchy.So remember, that hierarchy
is nesting that allowsme to drill into the data.I'm going to use
the new data itemin the top portion of the
Data pane, New data item.The first thing is a hierarchy.US Hierarchy is what
I'm going to name this.And then the top level
should be the state.When you create a
hierarchy, you wantit to start with the data
item that has the leastnumber of distinct values.Oops, that I did not want.I just want state name
and then US zip code.There we go.So I'm going to drill into this.So first, I'll have
my US state names,and then I can drill
into the zip codes.So when you build
the hierarchy, youwant the item with the least
number of distinct valuesdown to the item that
is the most detailed.So the most granular
should be the bottom level.So for us, this would
be our zip codes.I'm going to select OK.Now, in the Data pane, I
have a whole new categorycalled Hierarchy.I have a whole new
category called Geography.So let's fix this situation
about the non-US data.We're going to put
a data source filteron the customer's clean table.So I'm going to select the
icon next to the data source.And I can change the data.I can refresh it.But the thing I want to
do is apply a data filter.So here's our data
filter window.If you note down in the
far right hand corner,it tells me how many
observations I have.In the far left hand
corner, it tellsme how many were returned.So I've not done anything
yet to filter this.Let's go find our data source.It's a character column
called Customer Country.Single click, and now, I
have a bunch of conditions.This is going to be
my filter criteria.So I'm going to select
a Customer Country in--and then where it says
None selected, I'll select.And I can start to type
to limit those values.I don't want to have to scroll
all the way to the bottom.So I want this to only pull off
those records or observationswhere the customer country
equals the United States.I'm going to select OK.So I said equals but I use in.The reason that I
use in is because Idon't have to worry about
typing this in correctly.All the way at the
bottom of the conditions,there is the equals.The x is in quotes.So if I wanted to do that equals
operator, I would select that,and then I would have to type
in United States in quotes.And it has to be case sensitive.So I like to use the in because
it gave me that dropdown list.And suppose there were multiple
iterations of the UnitedStates.Maybe somebody put USA.Somebody put US.Somebody did all lower case.I could just grab all
of those variations.Again, check the far
right hand corner--our number of
input observations.Now, we have a filter.Check the far left hand corner.And what does it tell you?It shows you the
number of observationsthat are being returned.So 232,258 observations had
a value of the United Statesfor customer country.I'm going to select OK.Now, my data updates.Those distinct counts
changed quickly.And I have 52 US states.Still not perfect,
but definitely closer.And if you pop up to the
top portion of the data,you'll see that your customer
country is equal to 1.And your continent
name is equal to 1.So we've changed those
because of that filter.They are automatically changed.I'm going to go ahead
and use a Geo map now.So let's go to our
objects and scroll downto the Geo map section.And I'm going to use
a coordinate map.I will right click
Geo coordinate--Add to new page.So I'm going to
create a new page.When I do this, it automatically
creates the next page for me.So I was at Page 2.So it automatically
creates Page 3.And then I'm going to go over to
the roles and assign the data.I would like to use that
hierarchy I created.So I click on the Add.I'll select the US hierarchy.So let's scroll over.And all we see is US data, which
is what we were wanting to see.We're going to let--we're going to add in the size
as the frequency, so the count.And the color will be profit.So the darker the blue,
the higher the profit.The bigger the circle, the
more observations there were.So California had the most
observations and the highestprofit.Note the i down in the corner.When I hover, it
tells me no matcheswere found for the supplied
geography data items PR.And I think there's a blank.And that is why we have 52.I could use a object-based
filter to get rid of thatif I was interested.Over on the right hand
side, I could filter.So this would just
filter this object.And I could filter
my United States.Say I don't want any missings.And we don't have a
match for Puerto Rico.There we go.So now, our i has gone away.Let's work with some
of these options.Let's give this a name--Profit by Location.And let's change our marker.So expand Coordinate.That's the type of map.Then we'll change our
marker to Diamonds.We could make them a little
bigger if you guys want that.I'll show a couple
of the options here.So now, they're a
little bit bigger.The one thing that I
frequent like to dois move my legend because
we have more width than wehave height of the page.So if I move my
legend to the side--and I do this in
a lot of objects--then I get a little
bit larger areato display because
I have more height.And it doesn't impact
the width for this.So again, the larger the
icon, the more observations.The darker the blue,
the higher profit.That's really not a surprise.Let's see how things work
when we use average profit.Let's go to Profit and
change it to Average.Let's just remove Frequency.So when we're no longer
looking at the countor the number of
observations, ittells a very different story.Let's go back to Options.And I'm going to change
the gradients down herein the Gradients section.It's using this first kind of
lighter blue to darker blue.I would actually like a little
bit deeper of a gradient.There we go.I just don't think they
stood out very well.So Delaware, New
Hampshire, Montana--I might need to make
that even darker.Sometimes, you have
to go into custom.There we go.That's pretty dark.OK, now, hopefully, they
stand out even more.Again, we can maximize this.Sort it.And we see New Hampshire,
Montana, Oklahoma, Delaware,Wisconsin.Those are our leaders, which
is a very different storythan when we were
looking at it usingthe counts or the frequencies.So let's restore the size here.And let's take a look at
what's going on in Texas.So this is a hierarchy.When I double click on
the icon for Texas--it shows me I'm just zooming in.It shows me all of my
zip codes for Texas.And a lot of them are obviously
laying on top of each other.Maybe I'm interested in what's
going on in the Austin area.So on my map in the upper left
hand corner, I have a pin.And this is the location pin.I'm going to type in
Austin one more time.Come on.There we go.Austin-- and it should
give me a popup of Austin.And I'm going to
pick Austin, Texas.So I'm dropping my
pin on Austin, Texas.I'd like to see all
the zip codes thatare within 50 miles
of Austin, Texas.So I'm going to expand
the pin location.And now, I have this option
for a geographic selection.Let's drill into that.And we want to see the distance.We're using miles.But instead of 5,
let's see 50, so allof the zip codes that are within
50 miles of Austin, Texas.And that's what
we're seeing here.So the other data
is still there.See all of my other
little diamonds.Maybe I would like to
just look at this data.I can right click, and then
you have this option Newfilter from selection.And I'm going to select
include only selection.So I'm filtering the data
to only see those zip codeswithin 50 miles
of Austin, Texas.Note all of those other
icons have now been removed.And if we go over
to our Filters icon,see this Selection Filter?If I hover-- just a sec.There we go.Oops.There we go.All of those 50 zip codes are
now included on that filter.OK, so I'm going to go
ahead and save this.And we'll take a look at our
next section-- performing dataanalysis.OK, we have one more
section to talk aboutin our analyzing data.And for this
section, we're goingto focus a little more on the
relationship side of thingsfor our shipping data and
then the targeted audienceor the focus group side of
things for our marketing data.So we want to know, is there
any relationship between someof these columns--delivery times, profits,
retail price, number of orders?And then is there
any standout profitsthat are going on
with our focus groups,places that we might
want to target?We have a handful of
different visualizationsthat work well
with measured data.So our bubble plot
is really cool.And that requires three measures
at a minimum and one category.So when we're doing
this, you'll seethat it's not mandatory to
have a categorical data item.But the visualization
will attemptto create a bubble for every
observation in your datasource.So if you have a very
large data source,it's going to attempt to
build a lot of bubbles.Most likely, it'll just
return an error message.Treemaps are also great when you
have a lot of data, especiallyif you've put it
into a hierarchy.This allows for you to
look at multiple measuresbased upon hierarchical data.If you just only have measures,
then the correlation matrixis great for that.You can add up to 60 different
measures in the correlationmatrix to see if there
are any relationships.Maybe you've found
a relationshipbetween two measures.You could use a scatter
plot or a heat mapto further analyze that.We also can add fit lines to
our scatter plots and our heatmaps.We've got it four fit lines.And we have best fit I
usually go with best fit.It goes ahead, looks at all
the data in the background,and picks the best
fit line and gives yousome additional
information about it.We'll take a look at that.We can also work with
time series data.So if we have some date
values, date time values,we can do that.We can also use ordinal
data, so data thathas a specific order like
small, medium, large,or youngest to oldest.In addition, we can
use forecasting.Forecasting is great if you have
good historical data so if youhave a lot of
input data, and youwant to use that input data
to make some predictionabout the future.The analysts can determine the
duration and the confidenceinterval for those
predicted values.Visual Analytics is going to
determine the forecast method.So let's jump into demo and
take a look at a few of these.So let's add a new
page, and then we'llstart with the
correlation matrix.I'll go over to the objects.And we'll select
Correlation matrixand drop that into the center.Now, you need to assign
some measures to this.So we're going to go pick a few.We want to look at the
discount, the cost,the days to deliver,
the quantity ordered,the retail price.And we'll select OK.So as I look at this, this one
blue box really stands out.And if I select it, I see that
there is a strong relationshiphere.So the closer to the number
one positive or negativeindicates a stronger
relationship.So maybe I would like
to further analyze this.If I right click on that
cell, I can take that object,New object from selection,
and put it into a heat map.I'm going to put
them side by side.It's just easier for viewing.So we can see that
as our cost goes up,so does the retail price.But I would actually like
to switch these axes.So if I take my retail price--oops.Don't want to replace it.I just want to move it.There you go.So either way, you're looking
at this positive relationship.As one goes up,
so does the other.Probably not that
big of a surprise.Let's expand this and get
some more information.So here are the actual values.And again, if I
select any one value,and I scroll through
this, it is highlighted.We've seen that.But what we haven't
seen is beingable to look at some
of the analysis.So if I select this Retail
Price, Cost analysis,it tells me that the
linear fit is the best bet.The r-squared value, the
correlation, saying there'sa strong linear relationship.Doesn't really
surprise me though.So this is just saying as
the retail price goes up,so does the cost.And it increases by $1.69.So it's a very
interesting relationshipbetween these two measures.Welcome to lesson 3.Lesson 3 will focus
on reporting--interactive reporting.So our goal here is
to create a reportthat other consumers,
information consumers,your clients, your
colleagues can consume.And while they're
consuming this report,they can make some choices
about what they're going to say.So we're going to stick
with that customer datathat we've been working with.But our focus now is to create
the report side of things.We're no longer
analyzing the data.Now, we want to
generate a report thatcan be used to identify
those customers thatmaybe would be appropriate
for our marketing campaign.And in doing so, we want to make
this look a little bit nicer.So we'll fix some
titles and labelsand add some coloring in.Within the report,
we've got some thingsthat we want to keep in mind.We want our report
to be organized.So we'll use multiple pages.We want it to be
easy to navigate.So we're going to go ahead
and add some instructions.We've got lots of different
tools that we can implement.We can use prompts
and ranks and filters.And we also want to
make it attractive.So we're going to focus
on some colors and styles.When you're creating
the report, you reallywant to think about the
audience that's goingto be consuming the report.So you want them
report to tell a story.And you want that story to
be easily understandable.So you want to pick the
most basic or simplest graphpossible to tell your story.I always say just because you
can doesn't mean you should.And that truly
applies to reporting.We've got lots of
fancy, different graphs.But if a bar chart can tell me
the story, let's just use it.We want to limit the number
of objects on any given pageand limit the number of
pages in your report.If your report is getting
up to seven or eight pages,you might want to think about
creating a second reportand then just linking those.And we'll talk about linking
reports and pages and evenan URL if you're interested.So we do have a lot
of different objectsthat we would focus on
when we're reporting.So word clouds are very popular.The donut or pie chart
is also very popular.You want to be
careful when you'reusing some objects
because certain objects,you really need to think
about the type of datathat it's going to be deploying.So if we take a look at our pie
chart or our donut chart here,you can see that the
orange and the purpleare quite close together or
the yellow and the purple.And I can tell that
the catalog sale islarger than the internet sale.But when you're using
a donut or a pie chart,you really want to make
sure that it's demonstratingor displaying values that
are very far apart so notclose values.It's difficult to always be
able to tell the differencein the slices of a pie.The same is true
for the word cloud.When I look at that and
see Madrid and London,I know Madrid is a
little bit larger.But if I'm quickly glancing
at this, maybe not.So you want to make sure that
the object that you selecttell the story.One of my favorite
objects are our dual axes.This is great when you have
measures that just don'tplay well with one y-axis.This gives me two
y-axes and allowsme to display values with a
different series of ranges.So one of the
examples is earlierwe saw the total orders
and the total customers.That would be perfect for
a dual axes bar chart.So let's take a
look at using these.So we're back in
Explore and Visualize.And we want to start
with our demo VAV report.So I'm just going to
double click to select it.And let's start by modifying
some of the settings.So if I go all the way over
in the far right hand cornerunder the E, that's our log in.I can go into Settings.And then from Settings, I'm
going to go to the generalunder the SAS Visual Analytics.And then I want to
scroll down and lookfor the Titles section.And here, it says
Automatic title.I want to change
this to Custom title.And so that's all I
really want to do here.I just want to be able
to add my own titles.So I'll select Close.I'm going to start
with a pie chart.So let's go to our objects
on the left hand sideand locate our pie chart.We're going to drag it
over and release it.And then we need to
assign some data.So we'll go to the Roles icon.And for the category, the
different slices of the pie,that's going to be driven
on my customer type name.Note there's seven
distinct customer types.So I should have seven
slices of the pie.And right now, it's using
Frequency as our measure.So let's change this
to Number of Orders.And so when I'm
looking at this, I'mimmediately noting that the
green and the orange and maybeeven the teal blue are
really close together.So maybe this isn't the
best use of the data.So we can change this.If we click on our three
little dots, our snowmanin the upper right
hand corner, wecan go in and change
our pie chart too.So maybe a bar chart
would be the best object.And you can see that's actually
the recommended object.And while I'm
working through this,maybe I've decided that
the orientation of thiswould be better served
as a vertical bar chart.I think that's the best
representation of the data.Let's go ahead and
give it a title.So in the upper left hand
corner where it says Untitled,I'm just going to double click
and type in Number of Ordersby Customer.I like to center them.And let's make it look
a little bit better.We'll bold it and
modify the text color.There we go.Oops.Typo.Much better.We might also want
to give this barchart a name just in case
we're working with multiple barcharts.There we go.That way, we can identify
this particular one.So we're going to go and
add another bar chart.Let's go to our objects.And we're going to go ahead
and grab the bar chart,and drag it to the right.And let's go to our roles.So for this, I
would like to haveOrder Month be my category.Instead of frequency, I
would like to look at profit.And then we're going to look
at the number of orders.And this is going to cause
the axes to separate.And I'll have one for
the profit and onefor the number of orders.I want to go ahead and use
my dual axes bar chart.So let's go to our
little snowman, the threedots in the upper
right hand corner,change our bar chart to--and then I'm going to go
to the dual axes bar chart.Much better.So quick glance at this.Look at my order months.I go December, June, January.So they're in
descending frequency.I want to go ahead
and modify that.I'm just going to right click
in the center of the bar chart.And I'm going to
select the sort.And instead of it being
sorted in descending frequencyby profit, I would
like to change itto being sorted in ascending
frequency by order of months.And now, we start with
January and end with December.Let's go ahead and
name this as well.We'll go to the options.And let's give this a name.And we'll also change the title.We can change the title
by double clickinginside the graph, or we
can use the side pane.Also, as I look at
this, I don't thinkI need to have the
label for order month.So let's go down and
clear that label.There we go.Also, I think I would like
to bold this, center it,and also change it.There we go.Kind of matches the
other one because nowthat we're in the
report side of things,we really want to make
this visually appealing.So I'm good.At this point, I'm
going to save this.So now that we have
our basic report set,we want to add in some
pieces that will makethis a little more interactive.OK, so in this
section, we're goingto take a look at creating
interactive reports.We want to build reports
that our businessconsumer, our client,
our colleague, our bosshas the ability to navigate
through and make some decisionson what they're seeing.So we have a variety
of different toolsthat we can use to make
our report interactive.We're going to create
a multi-page reportand add in some
prompts or selectionsso they can decide
what they want to see.We're also going to set the
user or viewer's customizationlevel.This allows the
viewer to either makejust very basic, simple edits.They can do comprehensive
edits to the report.Or they could even
swap out the datathat's being used-- the columns,
the data items that are beingchanged or used in the report.So we have three
different levelsof that viewer customization.Again, simple allows
for them to make changesthat wouldn't modify
or make any differencein the original report, and
then we have comprehensive.And that way, they
can alter the report.It will make some changes
but not to the actual data.And then data edits allow
for them to change the datafor the objects.So I could change
the data assignments.I could change filters.I could change ranks.Reports with multiple pages are
very useful because it gives usa bit more real estate.So I don't want to crowd a page.I want to have my pages
focus on a single idea,limit the number of objects.In the report itself, we want
to limit the number of pages.We do have this ability
to use a hidden page.And that would be a page that
does not appear to the viewer,unless it has been kind
of selected or opened.And basically, what
we would do hereis we would link
this to an object.And then when the user makes
a selection in the object,that particular page would open.It's very useful to hold
additional graphs or evendetailed level data.Filtering data comes in a
variety of different ways.We saw a data source
filter earlier.Now, we're going to talk about
filtering on the report side.So we can set up
prompts, which arecontrol objects that allow the
viewer to make the selection.So maybe it's a button bar,
or it's a range slider.We can also set up
actions between objectsso that when I make a
selection in one object,it could update
subsequent objects.Those are actions.And then finally,
we could do links.So links are when
I make a selection,it takes me somewhere else.So does it take me
to another page?Does it take me to a website?Does it take me
to another report?Lots of choices there.We're going to derive
these interactionsthrough control objects.So on the left hand side,
you see the list control.That's the only object that
allows for multiple selections.The range slider is also
unique because that wouldbe used for dates or measures.Otherwise, the
majority of our objectsare going to be used
on categorical data.And you want to
ask yourself, whatis the best way to
display these values?Button bars are
great if you justhave a few distinct values.Text input is
great if you've gotlots of values, so hundreds,
thousands of distinct values.You'd want to use a
text input for that.And then kind of the
in-between is the dropdown.So this is completely
subjective.Do you think in a dropdown
list, you'd want more than 15,more than 25?So you can decide
how many you thinkthat this is appropriate for.If you have too many
distinct values,then you want to use text input.We're going to talk a little bit
about report and page prompts.This is going to control
the way the report looks.You can control it at
the report level, whichis the most comprehensive,
or you can control itat the page level.So this little sample
here has a button barat the top and a dropdown.The button bar and the dropdown
are at the report level.So this means that
everything in my report,even if I have
multiple pages, willbe filtered for just Europe
and just plush toy items.Then as we come down
into the report itself,we have some more
control objects.We have a range slider.We have a list control.And we have text input.These are within the
body of the report.So this is only going to
impact what I have linked it toor set it up to work with.So you can see that
when I select Europe,I'm only given European
cities to work with,and that when I've
selected plush,I only have plush
toys to work with.Actions happen between objects.So here I have a few objects.And I've linked them.When you do a linking,
it means that whenyou make a selection
in one object,it's going to highlight that
value in subsequent objects.So it shows you all your data.It just highlights that value.Filtering, though--
that's the oneyou're probably
more familiar with.When I make a selection
on the Filter side,it's going to limit what I
see in all the other objects.When we set up
actions on the page,we can do this using the Actions
tab on the right hand side.Here, I've selected Europe.Europe is then
highlighted in my Geo map.Then when I select
it in the Geo map,it's then limiting the output
in both the bar and the listtable.Then when I select
it in the bar,it highlights it
in the list table.Let's take a look at this.Europe-- there we go.We're only seeing
European countries.Select Italy.Italy then highlights,
filters our bar chart,filters our list table.So now, we're only seeing
observations from Italy.And then when we make a
selection in the Clothes& Shoes category
on the bar chart,it's highlighting
that observationor that value in my list table.So let's jump over into demo
and take a look at this.OK, so we're back in our
report, our demo VAV.And we're going to go ahead
and first modify the usercustomization, so
how much of my reportam I going to let
my users change.I need to come over to
the far right hand side.And I'll go to Options.And note I want to be on
options for the report.If I have selected an
object, then the optionsare just for this object.I can easily select the
dropdown and move to the report.So now, I've got the
options for the report,and then we're going
to scroll down.And you should see Viewer
Customization and thenwhat's the capability level.So right now, they're set
to Comprehensive edits.We're going to
say, you know what?We'll let them change the data.So we're going to move these
options up to Data edits.And that way, if our
user wants to usea different-- some different
columns in the reportthat we've created, they can.Now, I'm going to go
add a second page.So we'll just click the
plus next to Page 1.And let's give these names.So I'm going to to right click.And I'm going to name
this Delivery Analysis.And then I'm going to
right click on Page 1.And I'm going to
rename that as well--Customer Analysis.And I'm going to go ahead
and move this to Page 2.So I'm going to right click
in the center of the graph.And you can see Move
to, and then I'mgoing to move it to the
delivery analysis page.I could create a new page.I could move it around
on the existing page.But I want to move it to
my delivery analysis page.And I'm going to
add a bubble plot.So let's go to our objects.And I'm going to
select the bubble plot.And I'm going to put that on the
left hand side of the canvas.Now, we need to assign
some data to it.So I previously have
mentioned that whenyou're doing the
bubble plot, youneed to make sure that
you have some column thatis grouping the data.Otherwise, it will
attempt to make a bubblefor every single observation.So if I have a
million records, it'sgoing to try to build
a million bubbles.And it cannot do that.So you'll get an error message.So I'm going to start
by doing the grouping.I'm going to select my group.And I want to group
it by the city name.But take a quick peek at that.We have 4,500 unique city names.That's a bit of an
issue because I'm notgoing to be able to
build 4,500 bubbles.So we'll walk through
this, and then we'lltalk about how we can
make some changes.So we'll go and add our days to
deliver, our number of orders,and then, of course, profit.We're always
interested in profit.You can see it's working.At this point, it's trying to
build all of those bubbles.And that's just crazy.That's not something
that is very useful.So we're going to go ahead
and rank this so that wecan look at the top 10 cities.Over on the far right hand
side, my very last icon is Rank.So we have to give
this a second.There we go.Then we'll go to our rank.We would like to add a new rank.Oops.Select New rank.And I do want to
rank it on city name.And I do want the top 10.But I don't want them
based upon days to deliver.I want it based upon
number of orders.So there we go.We have our top 10 cities based
upon the number of orders.And wow.Brooklyn is way up there.I'd like to look at
this over the years.So I'm going to go ahead and
add in another column to this.If I keep scrolling
down to the bottom,I can add in a
column for animation.So I'm going to click my
plus add under Animation.And I would like to animate
this by order month.So now, I have a little
Play button at the bottom.And when I click Play,
I can watch the bubbleshow they move over the year.So we could pause it.I could say, hey, you know what?I'm really interested
in Brooklyn.And that's Brooklyn's history.I can go back to playing it.And we can see how everybody
else compares to Brooklyn.If I just want to
clear this, I justneed to click into
some white space.And it will clear.I'm going to go ahead
and give this a name.And let's give it a title.We're going to call
this Top 10 Cities.And while we're here,
we will bold, center,and pick our blue.There we go.That way, they match.Let's just take a peek
at this as a viewer.So up to this point, we've
been looking at our reportsfrom the designer's perspective.I'm going to go
ahead and save this,and then I'm going
to click on my threelittle dots, my snowman in
the far upper right corner.So this is report level options.And I'm going to
view the report.So now, I'm a viewer.It doesn't really
look that different,except that I no longer
have those side panes.So this is what the
viewer would see.And remember, we set
our customization levelto allow our users
to make data edits.So let's just right
click on the dual axes.And you can see I
can replace data.So maybe I want to replace
the number of orders,and then it pops up and shows
me all of my other options.And we can say days to deliver.So this is a little weird
number over here, isn't it?Because this days to
deliver is the total.So this is adding up all
of our days to deliver.So that might not
be the best option.Look at the number of customers.There we go.So this is what the viewer
customization allows.And we have the
little Back button.So if we want to keep
going back to our original,we can easily do that.OK, let's go back
into edit mode.So we will go upper
right hand corner,report options,
grab the snowman,and select Edit Report.Let's take a look at
adding a page prompt.We'll select the
customer analysis page.And then I'm going to
add in a page prompt.In order for me
to do that, I needto expand the area for that.And we find that in
the options on the tab.So the three little
dots on the taballow for us to expand
our page controls.Now, I can drop in
a page prompt here.So I'd like to see
the order type.I'm going to go to the objects.And I'm going to
pick a button bar,and then I'm going to
add some data to it.Order Type-- so Order Type
has three unique values.So that's going to work out
perfect for a button bar.Let's go ahead and
give this a name.This is going to be the
Order Type Selector.And then I want to
give it a title.And I'm going to name it.I'm going to give
instructions to my viewer.So now, when they
make a selection,it should-- the
report should update.The majority of our
data is now beingdisplayed in our first column.You can move around and see
how catalog does, retail.And I'm going to go
ahead and add a treemap.So let's go to our options.And let's locate a treemap.And we're going to drop that
to the right of our bar chart.We created a hierarchy earlier.I'm going to add that
here so the US hierarchy.And then let's take a look.We'll leave the size, the
count, or the frequency.But let's change this
to our average profit.So now, we can see
the average profitby state for retail sales.So let's see how it
looks for catalog sales.Very different.Let's go back to our treemap
and go to the options.And let's give this a title.So now, I can drill
into any given state.Let's see.I'm going to modify
the gradient as well.I'd like it to be a
little darker blue.There we go.Oh, maybe even a tad darker.Very nice.OK, so now, if I really
want to see, I can hover.And it says South Dakota.If I double click on it, it
then shows me the zip codeswithin South Dakota.Above the treemap, I have
this little bread crumb.I can make the selection
to return to the top level.I also-- don't forget.We have the maximize
option that'llshow me the data
in a grid format.So maybe if I want to
sort it by average profit,I could do that as well.And it does show us that South
Dakota and Delaware, whichare these two little
states over here,do have the highest profit.And they're quite
low in frequency.And this is based
on catalog sale.If I want to clear this, let's
return to its original sizeand click Catalog.And that will return
to its original sizingor return all of
the data rather.So let's go over to that
delivery analysis pageand add in some actions.I would like to add a dropdown
list control to the canvas.So not in the page
prompt area, I'mnot going to expand
my page controls.I am going to put this in the
top portion of my report maybe.There we go.Just being a little picky.There we go.OK, and then I'm going to
assign the city name to it.So let's go to the Roles.And we'll go to our category.And I'm going to assign
city name to that.So I get this little i saying,
hey, that's a lot of databecause it's not using
this top 10 ranking.I need to go in and add that.So let's go to our ranks.And we'll do City Name,
top 10, Number of Orders.So now, it should only be
showing me the top 10 citynames.I want to set up some
actions so my goal hereis say I picked Chicago.I want my bar chart to update
to just show me Chicago,and then I want Chicago, which
happens to be my purple bubble,to be highlighted.So we're going to go over to the
actions icon on the right handside.And then we're going to
select our order by month.And what happens is it
filters it by default.See our little filter icon?I'm going to drop down here.And I'm going to change
it to a linked selection.So I should get all 10.But Chicago should be the
highlighted or trailed bubble.And it is.Now, I do want it to filter
on the dual axes bar chart.There we go.So let's test this.Let's pick another city.I'm going to go where it's nice
and warm and hopefully sunny.We'll go to Miami.And there we get
highlighted Miami.I want to save this, and then
let's just pop over quicklyinto our view mode.And we can take a look,
again, navigating around.There's our retail sale.Let's look for all
the retail sales.Looks like Oklahoma
is a big one.And those are all of our
zip codes within Oklahoma.We have two that don't
have a lot of frequency,but they do have very
high average profits.And again, click the retail.It will clear.And go to the delivery analysis.What happens when I pick--we'll go to San
Antonio, and then weget San Antonio highlightedOK, I like to start and
finish with the SAS Drive.So I'm just going to return
to the Share and Collaborate.This is my SAS Drive.It'll show me my recent files.And again, we could
set favorites.We could open up that
Quick Access area.And that really wraps up
our section on reporting."
56,"DAVID L. WHITE: Hello, and
welcome to the SAS Global Forum2020 for our virtual webinar.Today myself, David
White, Tim Howard,and Snowil Lopes will
be presenting our paper,Achieving Net-Zero--Forecasting Power Usage
to Improve Sustainabilityon Clemson University's Campus.Specifically, we're going
to be speaking to our effortunder the Clemson
Energy Visualizationand Analytic Center,
where we are workingto improve
sustainability on campus,primarily focused
on data analyticsand visualization
using SAS technology.And with that, I'm going to
pass off the presentation startto Snowil Lobes.SNOWIL LOPES: Hi.My name is Snowil Lobes.I'm Clemson's energy enginee
and system analytics.Just want to give you a brief
scope of our Clemson campus.So we are about 26,000 student
population, and about 5,000to 6,000 staff population.So about 31,000, 32,000
population campus or so.And we have about six
type of energy servicesacross the campus--electric, steam, chilled
water, water, waste-water,and natural gas.On Clemson campus, we have
various distribution networks.One of the primary steam
water, chilled water,and electric distributions are
a key distribution network.Just give a brief overview of
steam distribution network--we got about seven
boilers, rangingfrom 9,000 pound average to
78,000 pound average steam.We got 100% redundancy in our
steam distribution system.We got one CHP heat
recovery steam system,and our generator is 75,000
pounds per hour to that heatrecovery steam generation.And our campus peak
demand or peak steam loadis about 110,000 pound per hour.And for cooling system,
we used chilled wateras a primary centralized chilled
water distribution system.We've got about five chilled
water systems as chilled waterplants on campus, with a
loop capacity of 16,500 tons.And our peak summer load
are about 11,000 tonsof chilled water.And just to give you how that
tons are, is your average UShousehold have 2,000
to 3,000 square feet,just require two to
four tons HVAC system.In this research
paper, we are primarilyfocusing on electric
distribution.So our campus
electric distributionis served from one Duke
Energy substation, whichis 28 megawatt substation.And in 2012, 2013, our
campus exceeded 24 megawatts,a peak demand.And you can see after 2013,
we got a significant growthon campus.And now our demand level is
very close to 35 megawattsduring the peak summer load.And to maintain this
campus infrastructure,we use Schneider
Electric SCADA system.But one thing we
are always doingis monitoring every
single building.So every single building--so we take electricity on
one location at one Dukesubstation, and then we
have our own infrastructureto deliver to all
of our buildings.It's about-- roughly
about 200 buildings.And all that building,
we do have metering.We do have submetering
at some location.But we closely monitor our
power through all the meterson every single building.To connect all of
this infrastructure--metering infrastructure
all over campus--chilled water, HVAC, every
whole infrastructure,we use Johnson Control
Metasys as our primary BAS.BA stand for Building
Automation System.Metasys primarily
use Bacnet protocol.But we have some Modbus/TCP
for electric meter usage.So we run all our infrastructure
on a campus network,and everything come together
as a connective systemto our central repository
and to various SCADA systems.And here, I'll just
give you an example.We have chiller plants and
boilers connecting to Metasys,and goes to network
NCEs, what we call.And all sensors and everything,
finally it comes to our campus.It connects fiber and goes
through our main repositoryand our main SCADA
system and BAS systems,or controls and all
of the verifications.And at CEVAC, we are
trying to pull that datafrom all over BAS
and all other datasystems to various protocols.So here, you can see all of the
data we are pulling togetherfor CEVAC.We got about 4.5 billion
records in our facilities datafor a year.That's only a year
worth of data.And then we are pulling that
to building automation--BAS API, or ODBC connectors--we are creating a central
database for our whole energyand utility infrastructure.And then that's 4.5
billion records.And our system, is
we are kind of--so imagine a giant tank.And when we are filling
small different tanks.So a system we have
created called pipelines.So every pipeline will fill
data to a usable data set, whichwe want, like say, Watt
power, or Watt Center CO2,or Watt Center tank.So we are creating that
whole infrastructureto pull and automate
data integrationbetween various building,
various type of utilities,and so on.So you can see a
facilities data,we are pulling that data
to XREF and historian,data sets from BAS to SAS VA.And there are two primary
things in our data sets,our facility data sets.One is historian, and one
is real time-- our latest.So historian data sets, we
are pulling from one location.And then on latest data
set, we are pulling to API.And we're going to
combine that two data setsfor our further analytics.Why we are doing this?So we have made zero
reduction goals.We have commitment for a better
sustainability and climate CO2reduction.So we-- as a large campus, we
got about 8 million square feetof conditioned space
on campus, which is--and the electricity we use
is about 30,000 US household.So it's a big impact on
the carbon footprint,a global carbon footprint.So we want to reduce.We want to become
a net zero campus.So we have a commitment.So how are we going to achieve
those commitment to this dataand data analytics?The first thing we
want to make sureis we have visibility
to our user.Hey, well, how they are doing?How much is carbon footprint?How much energy they're using?And where are we in
terms of other institutesor other people?So we want to give
them visibility.Then we want to create tools for
efficiency and clean energy--like how we can
reduce energy, howwe can do efficient energy
practices, how we cando some performance contracts.So those are the things we can
get out of the data analytics.Then we want to do
a carbon neutrality.So we want a plan for future.How are we going to achieve?So we have a goal
of net zero by--right now we have 2030.But how are we
going to go there?What are the steps
we have to take?And that's what we want
to do with the second goalof efficiency and clean energy.And the third thing we
want to do is health.We have a commitment
to our users--a healthy Clemson,
a healthy campus.And every user spends about 40
hours a week in the building.So we want to make
sure that we give themright healthy
indoor air quality.So we want to do some analytics
with indoor air quality.So those are our primary
goal with CEVAC Energy Scope.DAVID L. WHITE: So I'm going
to provide some backgroundon how CEVAC got
started here at Clemson.And it really started here.It started at the Watt
Family Innovation Center.This is a state-of-the-art
building on the Clemson campus.It was opened for
business a few years ago.And it is loaded
with technology,with the goal of
really providingan environment for collaboration
among students, faculty,and business and government.It's envisioned as
an aggregation pointfor all kinds of ideas, and
essentially an opportunityto start programs, initiate
opportunities, and have thosespawn out of the
Watt Center, and moveon to further funding
and bigger opportunities.And CEVAC itself started
with what really seemedlike at the time a
simple question-- whatmakes a building smart?The Watt Center was--as it was being
opened, was considereda smart building in many ways.But it also was much
like other buildings,in that a lot of those
technologies weren't connected,and there wasn't a
lot of automation.And we're still not
quite there yet,but we are making
progress on that front.But when you start thinking
about smart buildings, whatare those core requirements?And one is sensing.You need a lot of sensors
in a smart building.And so as we started to dig
into the data sources thatwere available to
us, we began to findthat we had near real-time
or historical data.Every room in the Watt
Center had a temperature,and we also had some
humidity sensing.We had a lot of CO2
sensors in various roomsacross the Watt Center.We worked with Clemson
facilities with Snowil,and began to deploy
some metering.So not only did we have one
meter for the Watt Centerto measure power
usage, but we wereable to submeter the
building so that we couldget multiple measures
of different componentsby each floor and
by various systems.So for example, lighting
versus outlets versus HVAC.Additionally, we
are also beginningto monitor water
usage in the building.And one of the big opportunities
we also began to look atwas occupancy.We'll talk a bit more
about that later.Data-- we found a lot of data.So for example,
the Watt building,over two or three
years of operationwhen we started to
look at the data,had millions of data records.And the campus as
a whole right nowhas over four billion
records of data.Additionally, we
started not onlyto look at those data
sets or those datafrom the various sensing
systems for the Watt Center,we started to bring in external
data, one being weatherforecast data and
historical weather data.Additionally, we needed to build
out in information technology.And this is where SAS really
proved to be invaluable.We started working with SAS
Data Integration Studio, whichallowed us to essentially,
literally in one day,connect to our
campus data systemand begin to design data
pipelines to consume data.And not only were
we able to build outthese data pipelines using
SAS Data Integration Studio,we were able to use that tool
to set up cron jobs that thenallowed those data
pools to be automatedevery hour essentially.And so from there, after we
built those data pipelines,we were able to push the data
into SAS Visual Analytics,into the SAS LASR Server, and
start to work with SAS VisualAnalytics to visualize the
data, and really build outa seamless environment.And the SAS tools were
really invaluable.Because at that time, it
was really just Tim, myself,and Snowil working on this.Although, now we
do have a numberof undergraduate and
graduate studentsworking on the project.And so one of the
driving goals of whatwe were going after
here was to understandwhat makes a building smart, but
also to reduce carbon emissionsat a campus and building level.And when we think about
why that is important,Snowil alluded to this
earlier in the talk,and that is because
building operations reallyaccount for an exceedingly large
percentage of global carbonemissions--28%, it's estimated.And that's actually
a low estimatefor carbon emissions
for buildings worldwide.And when you throw in
construction of buildings,that increases that
footprint up to over 40%.And so how we build and manage
our buildings, and how we powerand run our buildings,
is really important.And if we could just
make single digit or even5%, 10% differences
on the Clemson campuson a building-by-building
basis, thatis going to make
a huge difference.And that's one of our
overarching goals.But when we start
thinking about someof the most advanced
smart buildingsout there, what are they?Well, if you do
smart building, oneof those that will
come up most likelyis The Edge, which is
located in Amsterdam.So for example, it
has 28,000 sensors.It has web dashboards.It's known as one
of the greenest,most efficient
buildings in the world,because it has solar
panels supportinga lot of its energy generation--
on-site energy generation.Additionally, when we
start thinking about smart,it allows the user to
customize their lightand their temperature
in their local space.And so there's a lot of
technology in this building.And that's a direction that
we'd like to see the campus--Clemson campus go, is that type
of smart building technology.So where are we now?We've developed a platform
to drive sustainabilityand improve efficiencies
at a building and campuslevel that support
interdisciplinary research.So it should be noted that we
are an academic institution.And so not only are
we approaching thisfrom a sustainability
perspective,we also want to build out
a foundation for research,and also an educational
opportunity.And this is also a partnership
between the Watt Centerand Clemson facilities.And we think this is a
very unique partnershipat a national level.And this facility,
CEVAC, is staffedby a number of students working
on information technologies,including database,
middleware, visualization,and modeling, providing great
opportunities for on-hands.We're solving real
world problems.As I mentioned, this is
a research and educationplatform.So from an education
perspective,this allows students to
work with other studentsfrom other disciplines, provides
hands-on experience solvingmodern information
technology challenges,and really making a
sustainability impact.And I think that is
one of the drivingobjectives of a number
of our students,is that they're really concerned
about global climate changeand carbon reduction.And this gives them an
opportunity to make an impact.And what are the potential
opportunities for research?Big data, sensor development,
real-time data processing,building system
automation, sustainability,carbon footprint reduction,
building efficiency, and justSmart Campus in general.There's a lot of
opportunities out therewith this type of platform
that we're working on.So when we think about from an
architecture and informationtechnology standpoint,
where is the CEVAC platformat this time?We're able to consume all kinds
of different data sources,from regular databases,
cloud-based APIsover Wi-Fi networks.And all that data is streaming
into our CEVAC databaseswhere, as Snowil was
talking about earlier,we primarily have historical
and latest data sets.And from there, we have out
alert systems, our own APIsto share data and set up tables
that support our SAS VisualAnalytics.And from there, we're
able to generatereal-time reports, alert
emails, and also web pages.And I'm going to
turn it over to Tim.TIM HOWARD: Hello.So we started looking at--having a look at the campus.So we created this
interactive map,where we could look at utilities
and indoor air quality.We can get a rough guide of
where we are across the campus.We decided to have a little
bit of fun with the colors.So athletic buildings are kind
of a browny, orangey color.The academic buildings
are the purple,and then the living, housing,
and dining are the green.So you can see with just
one clear shot here,where the buildings all
fall within each otheron the different things.Here, we're showing temperature.If you clicked on any
one of those buildings,you would get to the dive here.We have about 12 dives right
now for each of the 12 buildingsthat we have on campus
that we're looking at.We're moving that into 35
by the end of this year.So here we wanted to have one
dashboard to give an overviewof the building now.The students felt
it necessary to putin a picture of the building,
just because some students arenew and not so familiar
with buildings on campus.Then we have the
power, with how muchpower has been used
by the building.And then on the
bottom there, yougot the steam, the chilled
water, and the water.We wanted to have an idea
of the outdoor temperatureand humidity.And then looking
across the buildingitself, what is the hottest
room and the coldest room?Which room has the
highest humidity?Which one has the
lowest humidity?And then, again, was the CO2--which is the highest and
which is the lowest CO2.And then on the bottom there,
you'll see known issues.Those are issues where we
found anomalies in the databaseor in the data.And we've researched it, and
we know what the issue was.Maybe it was a power outage.Maybe it was a sensor that died.We write that down
there so we don'thave to keep looking for it.Every time we see that data,
we know what's going on.In the middle there,
we have alerts.Those are where--
we are startingto play with alerts to tell us
if a room is getting too warm,or if the CO2 is
getting too high,or if a sensor has
stopped reporting.Or in fact, if a CO2 is
actually reporting too low.We found that some
of the CO2 sensors.And a good example is that
in the bottom right there,you'll see the
CO2 sensor is 136.That's since been fixed.That was an issue
with the CO2 sensorneeded to be recalibrated.So we spoke with facilities.They came in,
recalibrated the sensor.And then the tickets, we're
looking at all the ticketsthat are being created
from the building.Those are tickets really
to facilities or work,whether it's maintenance,
or whether itis for corrective action.You can begin to see
what issues are going on.Because that gives
us an idea nowif we're working on something,
and we see a sensor going down,we can get a good
understanding--OK, so facilities
have taken that down.Now we know what's going on.We've taken it--
along the top there,you can see we've taken
a lot of deeper divesin a number of things--
temperature, CO2, power.Here, I wanted to show
you about the data rooms.Well, we've got an
air handler here.It's a good example of
a finding that we had.But the temperature
was about right.It was a little high, but
it wasn't out of norm,really, for a data room--certainly a modern data room.When we started
looking at it further,we noticed that the air
handler was working at 100%.That obviously, was an issue.But we put in a
ticket, and facilitiescame and investigated.And they found that the
air handler hadn't gotthe vents correctly positioned.They actually did a
further investigation,and found there
were two other datarooms with the same problem.So they corrected it.And you can see,
obviously, by the slidehere when they did that.Instead of working
100%, this air handler'snow working at 56.7%.And the temperature
is dead on now.So it's working
really, really well.It's a good example of
how these students usedthis data, researched
it, worked together,worked with facilities, worked
with the building operatorsto solve the problem.As Snowil mentioned
earlier, a carbon footprintis a big deal for us.We're really trying
to focus on that.So we really started
investigating.And we're working with our
engineering company, as well asfacilities, to using
standard populationsto figure out, or
estimate at least,our carbon footprint for
each of the buildings.And here you can see
our carbon footprintas it's related to
the energy usage--so chilled water,
electricity, or steam.It's a good guide for
us to be able to see.And obviously on the top, you
can see we can do it by year.DAVID L. WHITE: OK.So one of the
opportunities that we'vehad and developed as we've
been working on this projectis measuring occupancy.And rather than relying upon
a new deployment of occupancysensors across buildings
all across campus,we decided to try to leverage
those technologies that wehave on campus.We've settled at this time
on using our wireless accesspoints.Clemson has a great
wireless infrastructure.And we are able to essentially
datamine those wireless accesslogs, and provide a 30-minute
estimate of occupancyby floor in a building.And we're currently running
this for the Watt Centerand the campus library,
which is a very large space.These two figures here
that we're showing actuallyshow visitor counts by day
over a given time period,and kind of a hotspot
analysis for time, telling uswhen the Watt
Center is most busy.And so this is--these data here
are an aggregationof those 30-minute observations.We're really excited
by this technology.Finally, our power
forecasting tool--one of our goals
has been to beginto develop a forecast
model of power usagefor every building that
begins to enter into the CEVACportfolio.Right now, the only one that
we have for is the Watt Center.We do have a student working
on one for the library.So why are we interested
in power forecasting?One, we're looking at how we can
improve building efficiencies.It can lead to an understanding
of how different buildingsand HVAC systems perform
when compared among others.Because all these buildings have
been built at different timesand have different
infrastructure.And detect and diagnose
building system faults--so for example, that issue
that Tim was just discussing.Although, that was
a visual diagnostic,we envision that we might
be able to use modelingto help us better understand
building performance.And if a building
is not performingat its expected
performance, thatmay be an indication that we
need to dive into that buildinga little bit more to identify
where there might be issues.So we're using a deep
learning neural network.This currently is
not being run in SAS.It's being run in Python.The factors that
we're using in thatinclude total power consumption,
the time of the month,the hour of the day,
day of the week,cloud coverage-- so we're
bringing in cloud coverageand forecasted
temperature and humidity.Additionally, we plan
to move on to occupancy.And so right now
what we have is wehave a 24-hour forecast model.And in this model in the slide
here, the actual data usageis in red.The most accurate
model at this timeis probably the
hybrid model scaler.That's in the kind
of baby blue here.As you can see,
for the most part,we're able to track
trends pretty well.But we do miss weekends.That's the dip that you
see here in the middleof this screenshot, is it is
a weekend observation wherethe building goes into
a no-occupancy mode.So finally, what we
wanted to talk aboutis another really
exciting opportunitythat we've been
pursuing, and that'sthe idea of a CEVAC
Virtual Operation Center.And so we've been working
with a small company.And that company
essentially has a modelthat allows us to build out
a virtual operation center.And so what you see in
these two slides are,one, in the
background, we've takenthe SAS Visual Analytics
dashboards, and essentiallypasted them.And so as a viewer
wearing goggles,wearing virtual reality goggles,
you step into this space.And you can literally
walk around the roomand see all these
dashboard slides.Now unfortunately, at this
time these aren't real-time.So we can't make these
slices and paste theminto the room in real-time.But we can envision
an opportunitythat we could pursue
that, where every hourwe're updating this virtual
operation center with allthese visualizations.Secondly, in this second graphic
here that we have on the right,not only do we
have the backgroundmaterial in this
operation center,but we have real-time data
streaming into a table locatedin the center of the room.This data happens to be from
our power forecasting model,where we're looking
at total power usageand absolute error over time.And as a kind of a
virtual user of this room,the concept is is that you
could have multiple peoplefrom on campus from
off campus, allleveraging this space
to identify issues,to discuss
opportunities, and justto experience this concept of
a virtual operations center.And so we're really
excited about wherewe've taken this technology
up to this point.Here's our contact information.We thank you for your time.And please reach out to me
if you have any questionsor suggestions.Look forward to
hearing from you.Thank you."
57,"MATT SIMPSON: OK.I'm going to talk
today about howto incorporate auxiliary
information into your modelusing Bayesian methods.So I want to start
with a question.How many times have
you fit a modeland then checked to see if the
parameter estimates made sense?Maybe if that slope
coefficient is 10, sure, fine.That could happen.But 100 or a million,
something went wrong.Maybe I need to fit
a different model,or maybe the optimizer
got messed up.That tells you something.You know something that
your analysis is nottaking into account, and
so that's an opportunity.You can improve it.So that's the pitch
for Bayesian methods,taking into account this
additional information that'snot explicitly in your model
through the prior distribution.But, admittedly,
this is not easy.Sometimes we'd like to think
it is, really explain itas if it is, but this is
a very difficult problemtranslating this information.So my solution is going
to be, essentially, thinkreal hard about the problem.But I'm going to
guide you in thinkingabout this problem using a
conceptual framework, hopefullymaking it a lot easier.So from here, the game plan
is to start with the Bayesianstory.In other words, what
we tell ourselveswe're doing when we do
Bayesian statistics.From there, we'll
use that to framehow to think about the prior,
which will then help us thinkabout how to select the prior.And we'll have an example.But, of course, see the paper
for more detail and moreexamples.So the Bayesian story.This starts with a
philosophical argumenttrying to conclude
that uncertaintyis subject to the
laws of probability.And there's several
different argumentsthat potentially get you there.But the point is you really
start with this to getBayesian inference to fall out.Uncertainty is subject to
the laws of probability.Probability represents
degrees of belief.Then you can use the
laws of probabilityto constrain how your degrees
of belief must behave.Particular, using Bayes'
rule theorem of probability,you get a consistent
inferential frameworkthat tells you how to update
your beliefs about whatever,about reality, in response
to new information.This is great.It makes philosophers
very happy, for example.The problem, of course,
is a very common criticismof Bayesian statistics, is that,
in practice, when you use it,this prior seems made up, kind
of just pull it from somewhere.What is it?Well, there is a glib
Bayesian responseto this, best represented by
redacted Darth Vader here.Apparently, Disney
would not give usthe rights to use this image.I guess they don't care about
Bayesian statistics very much.Know what?Sucks for them, I guess.In any case, the
glib response isthat the prior comes from the
same place as the likelihood.They're both made up.And that's true, I think,
but it's not necessarilya helpful response.Because you still have
to choose the priorat the end of the day.So how do you do that?So I'm going to try
to help you do that.We have to operationalize this.We don't come pre-formed
with the mathematical priordistribution in our head.We really have to think
hard about this in orderto get something to plug
into these equations.So how do you think about it?The first step is
to understand that,in the Bayesian context,
the prior and the likelihoodtogether, jointly, form a
model of your uncertaintyabout the problem.You have a lot of experience
with models in the sciences.What do we do with models?I'll apply them to the
problem, see what they say,try alternatives,
try to break them,try to think about what
their assumptions are.This is something
we do all the time,whether we're a statistician,
an epidemiologist, a physicist,an economist, everywhere.So it's no different
for a prior.A prior is part of
a model, but it'sa model of this weird sort of
abstract thing, uncertaintyand not reality.So it's a little more
difficult to think about.So let me help you
think about it.There are a couple of
tricks you can use,and the big ones
are really crucial.First is to focus on the
distribution of observables.Think about the data, basically.It's a lot easier to have
strong, valuable intuitionabout what the data should look
like than it is parameters.And then you can
convert that intuitioninto intuition about parameters.But that conversion is
less than straightforward,so how do you do that?Well, transform the model.Transform everything
you can in the modelto make it easier
to think about.Transform data,
transform parameters,transform covariates,
everything.Now, we'll get into an example.We'll show very concretely how
to make these transformations.But it really
depends on the model,so it's hard to
give general rules.But once you have
these transformations,well, it's time to pick a prior.What do you do?What I'm going to say is you
start with a reasonable defaultprior.What do I mean by default?Well, something that's
weakly informative--so spread out but not too much--questions that you care about.And that last part is key.You can't have
something that's weaklyinformative or uninformative
for literally everything.That's impossible.You can be weakly informative
for the question you're asking.What's the likely value
of this parameter?What's the forecast
that I'm interested in?That sort of thing.So spread out but not too much.Let's operationalize that,
and let's just operationalizethe whole prior.So let's just use a
normal distributionas our default price
for any parameter.We're going to choose the
mean and standard deviationof that normal distribution
judiciously in orderto make it work.So the mean should be
the value you expect,or whatever you would set
as the null hypothesisfor that parameter, depending
on your inferential rules.For forecasts, you might just
use the value you expect.If you're trying to learn
about that parameter,you might use the null
hypothesis method,because you want to sort
of APES a hypothesis test.The standard
deviation, you shouldset so that the
mean plus or minusthe standard deviation is the
most extreme value you thinkis realistically possible
for that parameter.And what this implies
is that there'sa 32% chance in the prior
that the parameter willbe more extreme than
you think is possible.So it allows for crazy
values to happen,but it does lightly
constrain them.Insane, yes, maybe not too much.So that's what I mean by
spread out but not too much.It allows insane
things to happen,insane according to your
expectations, but not a lot.So let's apply
this to an example.So I have a hypothetical
example here.We have a network of
100 car dealerships.They're selling 4
by 4 pickup trucks.I cannot tell you what type of
pickup truck they are selling,but I can say that the
company or organization whichmanufactures these
pickup trucks,they do use SAS somewhere.So, in any case, this network,
they're looking to expand.They can construct
a new dealershipin a bunch of different
potential locations.They want to predict
that dealership salesusing price, and
climate variables,and demographic variables.You can see them here.So what are they going to do?They're going to start--they're going to fit
a regression model.They're really
interested in price,because that's the
variable they can control.They can't control the
weather yet, unfortunately.So let's think about
priors for regression modelin this context.We're going to start
with the fall price.We're going to start with
the slope coefficient.So how do we handle
these slope coefficients?Well, we need to make them
easier to think about.We need to transform them.So, first things first.If we have positive
constrained covariates,and our response is
positive constrained,which it is, because it's
sales, we can take logs.And then the coefficient
is an elasticity.So we can interpret that
data so that a 1% changein the original
untransformed covariateis associated with
a beta percentchange in the original
untransformed response,in this case sales.This is great because
it's unitless.It's a lot easier
to think about.So how do we choose the default
prior for this parameter?Well, the null hypothesis
is pretty easy, right?Slope coefficient, null
hypothesis, no impact.What about the most
extreme value possible?Well, you can argue
with me on this one,but I'm going to
say plus or minus 4.So that would say a 4%
change in sales in responseto a 1% change in the covariate.That would be pretty surprising
in an economic context.Maybe not in a physics context.But for economics elasticities,
I think this is reasonable.But if you want to disagree
and go a little bigger,a little smaller, go for it.We put those into the formula,
get this prior, normal 0,4 squared.And now, also, I
want to emphasizethese priors we're
going to specifyare independent of each other.So you can put a dependent
structure in your priorif you would like.However, that's not
really where you wantto start for a default prior.This is a lot harder to think
about, and beyond the scopeof this talk.Now, what about covariates
which are constrained--or not constrained, sorry.So you can't take a log
of a negative value.So how do you think about these
covariates and the associatedcoefficients?We can standardize them.Subtract the sample mean,
divide by the sample standarddeviation, and then think in
terms of standard deviationunit changes in the
covariate and the response.So these are a lot easier to
think about, so let's use that.So what's the null hypothesis
for this sort of coefficient?Again, 0.We expect no impact.The most extreme possible value?Well, I'm going to use
the sample standarddeviation of the response
to frame my thought hereand say, a 4-standard
deviation changein log sales in response
to a 1-standard deviationchange in the
covariate in questionwould be pretty surprising.Anything beyond that
is probably impossible,which, again, the
prior that you see herewill allow this impossible
thing to happen about 32%of the time.Now, some people
start getting nervouswhen they start using data in
order to choose your prior.And it's true that if you
look at the equations,here's the golden snow.In Bayesian context, the
data does not and should notimpact the prior.Unfortunately, we are humans.We don't know our
prior, and using datato help construct our prior
is really, really useful,because it's a very
difficult problemto operationalize that prior.So I think it's fine to do that,
but you do have to be careful.So, anyway, we've done
our continuous covariates.What about a class variable,
categorical variable,covariate?So we have all these,
dummies, right?So the basic method
here is to thinkabout an intuitive
base case, based group,and say, what happens when
we move to a different group?And, really, this
idea generalizesto more complicated
models where it's harderto apply the other tricks.Start with the base case, and
deviate from that base case,and think about how that impacts
the response, a nonlinearmodel where the impact
of the covariatedepends on the other covariates.This is very useful.So in this case for just
a categorical variablein a linear regression
model, a changefrom the base group
to a different groupis associated with a beta
change in the log salesresponse variable.So we interpret it.So how do we choose a
prior for that beta?The default is to
say, well, expect itto be about the same as a
1-standard deviation changein a continuous covariate.So we'll use the
same default prior,independent of each other.Now, you might
disagree with this.Maybe group membership
for your group variableis way more important
for your problem.OK, fine.Choose a different prior there.Maybe it's way less important.Same idea.But this gives you the sort of
way to think about the problem,right?You start with the
continuous covariate,and use that to
frame your thoughtson the categorical covariate.Now, we have one more
regression coefficient,and that's the intercept.It's a lot harder to think
about because, well, there'sthis quagmire of
interpretation dependingon how you transform things,
what the values will be,what the parameters are.So to sort of cut the Gordian
knot and make this easy,then do something sort of like
an empirical base type idea,if you're familiar with that.The basic idea here
is to center the prioron the OLS estimate
of the intercept,and then set that prior
standard deviationvery large so that it
doesn't really havemuch impact for that parameter.Now, if you cared a lot
about this parameterfor some inferential
question you have,you'd want to do something
a little more principled.But as a nuisance
parameter, this is fine.And so the prior I choose
is centered on 8.88,because that's the OLS estimate.It turns out, of that intercept.And this prior
standard deviationis 100, which is about
2 orders of magnitudelarger than 4, the largest
prior slope standard deviation.Finally, you have the
error standard deviation,the last parameter of the
model, which is the prior 4.And I want to emphasize
error standard deviationin that variance.Variances are bad.Very difficult to think
about because their units aresquared.Standard deviation's units
are the same as the response.Much, much easier
to think about.There are also some
problems with commonly usedvariance priors having
pathological problems,but that's less of an
issue because you can justuse different priors.So think in terms of
standard deviation.How do we apply the default
prior to a standard deviationif the standard
deviation is constrained?It can't be negative.Well, constrain the prior.You get a positive truncated
normal distribution.And to pick m and s, m equals
0 is a good default choice.You can choose
something else if youwant to kind of center
that prior a little bit.And then for s, you set that to
the sample standard deviationof the response.That's really the most
extreme value that's possible.You remember partitioning
sum of the squares.In OLS estimate,
it's always boundedfrom above by that
sample standard deviationof the response.This isn't true in
a Bayesian contextbecause you can have arbitrarily
weird priors making that thingdo all sorts of weird stuff.But it does require
arbitrarily weird priors,and we're trying not to do
something arbitrarily weird.We're trying to do something
that's the default prior.So it's a good upper
bound that we thinkis constraining this parameter.And then, of course,
we don't actuallyconstrain the parameter.We give it a 32% chance
of going above thatbound using our default prior.So that's a default prior.We can go ahead
and fit that model,get results,
interpret them, startrunning with things, right?That's great.But we were interested
in price, and wewant to move beyond
the default price.Let's talk about
informative priorsand focus on this
price coefficient.Do we think that
coefficient can be positive?No, probably not.As Vizzini would say, redacted
Vizzini from The PrincessBride, it's inconceivable.So you haven't seen this movie,
please do yourself a favor.Go watch The Princess
Bride tonight.Something to do
during the quarantine.Classic.Vizzini, if you have seen
it, says that this isinconceivable, cannot happen.So what we do with inconceivable
things in the prior?We force them to never happen.So we can use a negative
truncated normal distributionas the prior for that data,
and then the posteriorwill never be negative.That's great.We can do that.But maybe it's not a good idea.Vizzini's partner in
crime, Inigo Montoya,would say, you know,
that's not really whatthe word inconceivable means.Really, we think it's unlikely.So how could increasing
the price of the truckcause more sales of the truck?That's really the question here.There's this
theoretical curiosity,mostly from economics,
called a Giffen good.Probably not a truck.This is a very weird
situation for that to happen.But there are other
possibilities.Maybe consumers
are using the priceof the truck as a
signal of quality.Or they're using the price
to signal to each other.The more expensive the truck,
the more rich they are.It gives them social status.These are possibilities.Maybe these effects
aren't large enoughto make increasing the
price increase sales,but it could happen.So you want to allow
for the possibilityin the prior, if not letting it
be a very likely possibility.Some better choices for these
priors, I have listed here,and these are based on some
outside empirical studieson the price elasticity of
demand for pickup trucks.And you get estimates that
go from about negative 1to negative 2 depending
on your estimation method.And the standard
deviation of these priorsis chosen so that,
really, a positive priceelasticity of demand
is extremely unlikely,but it could happen.If the data screams, actually,
that this is happening,OK, fine.We're going to allow
for it in the posterior.So we chose some priors.We went through some examples.I'd like to sort of summarize
the basic ideas here outsideof the examples.The main ideas for how
to think about the priorare focus on the distribution
of the observables, because datais easier to think
about than parameters.Then use transformations
to help translatethese intuitions about data into
intuitions about parameters.Transform everything.Transform everything you can.Now, I want to emphasize these
are tricks, not theorems.So if they don't seem to apply
for your case, don't use them.Maybe use them as an inspiration
to come up with tricksto help you think
about your problem,but don't think that
they must apply.We came up with this default
prior for any parameter,normal distribution with the
mean and the standard deviationchosen in this particular way.But I want to emphasize that you
should try alternative priors.Try informative priors.Try alternative means of
specifying the default prior.Because, well, if the prior has
a large impact on your results,well, you want to know that.You want to know what the
crucial points of your analysisare.So I also want to take a
minute to show how you wouldfit this model in PROC QLIM.So PROC QLIM is an
econometrics procedurethat does Quantitative Limited
Independent variable Modeling,but part of that, it
does linear regression.You can specify that, like this.You can see my mouse right here.So you just basically
have this base statementand these prior
statements to controlthe prior distributions.And this prior on the
standard deviation parameteris automatically truncated.But I mainly want to show this
because I want to show youa teaser of what's coming up.So PROC CQLIM, which is a
CAS-enabled version of QLIM,currently exists but does not
have Bayesian features in it.Well, those are coming soon.They are in the
works, and you can seewhat that syntax may look like.Look, you can
specify that normalprior in terms of the standard
deviation if you would like,or the variance.You can also explicitly specify
the lower and upper boundsfor truncated priors for any
parameter, which is useful.And a little bit different
syntax elsewhere.And because it's
CAS-enabled, youcan call the CAS action that
underlies this QLIM actionfrom any client, not just SAS.For example, here's how
you would call in Python.You could also call
in R or whatever.So if you're interested
in that, well,make sure you pay attention
to the upcoming releases.With that, I'll stop.And thank you for your time.If you have any
questions, please do nothesitate to shoot me an email."
58,"I'm Peter Flom.Welcome to this rather
unique SAS Global Forum.I'm an independent
statistical consultant.I work for me.You hire me, I'll work for you.Most of my clients are graduate
students and researchersin the social, medical,
and behavioral sciences.And I've been using
SAS for a long time.And my co-author is
Deanna Schreiber-Gregory,and you can see her info there.And she's been using
SAS for about 10 years.So OK, we'll have
an introduction.I'll talk about quantile
regression and PROC QUANTREG,multiple adaptive repression
splines, and ADAPTIVEREG,transforming
variables to TRANSREG.Then some general thoughts and
a summary and contact info.So if you have a continuous
dependent variable,the usual thing for regression
is ordinary least squares.This was developed early and
it's mathematically tractable.And it's used a lot
because Alan Turingwas born after Ronald Fisher.So Ronald Fisher developed
a lot of this stuffin the early 20th century,
and there were no computers.So you had to do stuff
that you could do by hand.And at that time,
computer meant peoplewho sat in big computations.And a little later,
they were usingwhat used to be called
adding machines.But, oh well, let's make
some strong assumptions,principally that the
residuals are normallyindependent
identically distributedwith zero mean and constant
standard deviation.And it also only
examines the mean.I don't think any
of you would onlyexamine the mean
if you were lookingat a continuous variable.At least look at the
standard deviation.Is it a range?In the quantile range, do
a frequency distribution,or something, or a
density plot or something.And then the more recent methods
that reduce the assumptions,provide additional
insight are nowadaysreasonably fast and
available, and Ithink they should
be more widely used.So first quantile regression.So in addition to reducing or
really eliminating the need--the assumptions
about the residuals,there's three other
motivations for quantile reg.The dependent variable might
be bimodal or multi-modal.When you have a
dependent variable that'sshaped like that,
you usually don'twant to use the
mean or maybe notany measure of span
of central tendency.If you have a highly skewed
dependent variable like incomeor sales or things like that,
pretty much anything with moneyand also some other
things, well, we alwayslook at median income.Almost always.You don't hear mean income
talked about as muchas the median.So why do we model the mean if
we're interested in the median?Or substantive interest
might be in the quantiles.In the example I'm going
to use a little bit,we're talking about birth rate.Another example would
be where I used to work,we did research into
HIV and other sexuallytransmitted diseases.And we were very interested
in the people whohad a lot of sexual partners.And not so interested
in the peoplewho had one or two in a year.Or another example, if
you own a retail business,you may have customers
who some of themmay spend a great deal more
than almost anybody else.You might be interested
in what motivatesthem to come to your store
rather than somebody else,and that might be different
than the average customer.So now let's get into it.This is just a little
bit of the syntax.This talk started a--oh, it's had a
lot of iterations.It was a 50 minute talk.It was a three hour training.It was a 90-minute how, and now
it's a 20 minute presentation.A lot of the details
are in the text.So one of the key differences
from the usual syntaxis on the model statement,
you have a QUANTILE option,and that number list can be any
kind of list that SAS accepts.Or you can use PROCESS.PROCESS uses every
possible QUANTILE.So it's every distinct value
of the dependent variable.That could be a lot.If you have a lot of
subjects for observationsand you don't have a
rounded dependent variable,that could be a lot.We're going to talk
about graphics.I would say the
graphics are alwaysimportant in any kind
of model evaluation,including regular regression.But in quantile regression, you
often have many regressions.If you do, as I'm going
to do, 10 regressionson the first rate, then you'll
have 10 times as much output.You need graphics not
just about assumptions, asin the usual models, but to
summarize what you've got.So birthweight.Doctors are interested in
predicting low or sometimeshigh birthweight.Low birthweight babies
are known to haveall sorts of additional risks
but high birthweight babiescan cause problems in
delivery and labor and so on.There's two usual
approaches to this.One is OLS regression.This isn't good, because
different factors mightbe predicting the
mean the quantiles,and what we want to predict
is the low quantiles.We'll see that is indeed
the case in this case.What is often done is to
categorize birthweightinto low birthweight and
normal, or sometimes very lowbirthweight, low,
birthweight and normal,often at a cutoff
of 2,500 grams.And I will tell
doctors that thisis treating a baby who is 2,499
grams as very different onewho's 2,501 grams, but
is identical to onewho's really, really tiny
at 1,500 or 1,000 grams.And I think, does that
make any medical sense?And they say, no.And I say, well, there is a
better method called this.It works.And that's very interesting.Do it with logistic
regression, because that'show we always do it.Oh, well.I think doctors make their
clients-- because you know,we don't listen to them when
they tell us what to do.So maybe they don't listen to
us when we tell them what to do.So there's a data set
in SAS called bweight.And it has a program and
a reasonably full setof variables.I'm going to start with just
a very simple one very--a very simple model with only
one variable, m_wtgain, OK?That's maternal weight gain,
and it's centered on the mean.And we can have quantiles
from 0.05 to 0.95 by 0.5.And we're going to have
the quant plot whichwe'll see in a minute.When we do this, we
get these two plots.What these are is the
parameter estimatesat all the different
quantiles, from 0.05 to 0.95.And SAS nicely
smooths them for us.On the left, we have intercept.That's not very interesting.It goes up as the
quantiles go up.No surprise.Bigger babies, higher intercept.However on the
left, on the rightwe have the parameter estimate
for maternal weight gain.And it's higher at
the low end, whichmeans that while women
who gain more weighttend to have heavier
babies at every quantile,the tendency is stronger
at the lowest quantiles,and it's stronger,
it's twice as much.Over on the extreme
left it's almost 15.On the right it's
about 7 and 1/2.That's a big difference.There's another graph
called the fit plot.It's a wonderful thing.But you can only do it for
a single continuous IV.Here it is for this.So what we see here
is from this model,these are the predicted
values for each.Each line is a
different quantile,from 0.05 at the
bottom 0.95 at the top.Here we can see
for the women whogained much less
weight than average,5% had babies under 2,000
grams and about 15%--that's the third line here
--had babies under 2,500 grams.That's a lot.We can also see that the
lines are not parallel.That is the gap between
the lowest quantileand the highest is bigger at
the low levels of weight gainthan at the high.Now, we can make a much
fuller model includingall these things, the
child's sex, the mother'smarital status, age with
a quadratic, education,all these things, and weight
gain with a quadratic.And again, age and weight gain
was centered on the means.So that gives us
a bunch of plots.On the top left,
we have intercept.They're not very interesting.To the right we
have black mother.Black women have lighter
babies than white women,and the difference is
greater at the low end.And it's actually fairly large.The low end it's
almost 300 grams.In a baby who's going to
weigh 1,500 or 2,000 grams,that's a big difference.On the bottom left, married
women versus others.There was a slight difference.Married women had
heavier babies.But the differences
doesn't vary too much.You have baby boys are heavier
than baby girls on the lowerright.But that difference
isn't very big.And it's actually smaller
at the lower end, whichmay be interesting to people.And we have stuff
about prenatal visits.And we can see top left
is first trimester,top right is last trimester.And the bottom
left is no visits.And the reference group is,
I think, second trimester.So you have to be careful.Look at the axes.They're different.Mothers who didn't
have any visitshad much lighter babies than
mothers who had a visit.And that difference was huge
at the lowest quanitiles.It was 400 grams.That's gigantic.Bottom right, we have
educational level.This made almost no difference.We got more educational
level things on this slide.Different levels of education.Smoking women on the bottom
left had lighter babies.But the difference
was fairly constant.Number of cigarettes
on the bottom rightalso made a bit of a difference.Then we get to age.The top two were age here.And older women
had heavier babies,because these are
all above zero.And this made a bigger
difference at the high end.That's about 10 grams per year.So that can be a somewhat
large difference.But mother's age
squared was negative.And I'll show how to look
at the quadratic effectin a little bit,
because these canbe a little hard to decipher.In the bottom, we have
pregnancy weight gain.And this is really
hard to decipher,because not only are the
numbers very different,but the shapes are
completely different.So I came up--we'll see how I managed
this in just a minute.So what I did, and
again, all of the detailsare in the actual paper.All the code and everything.Get the predicted weights
for different maternal weightgain holding the other variables
constant at their means.Subset this to get only the
cases where the other valuesare at their means.Sort by the weight gain and
graph with a series plot.And if you don't know
about series plots,they're an option on SG plot.They're really cool.What we get is this.So this is showing the effects
of maternal weight gainon predicted birth weight,
holding the other variablesconstant at their
means or modes,for the modes for the
categorical variables.And some remarkable
stuff is going on here.First at the bottom,
the bottom line,first percentile
is very non-linear.It's not even monotonic.And it's showing that at
the lowest end, more than 5%of women had babies
under 2 kilograms.It's also showing that the
women who gained the most weighthad a fairly high percentage
of low birth weight babies.If you look at the
10th percentile line,it's now more or less
monotonic, but still, 10%were clearly under 2,500
grams at the low end.And if we look at the top
line, the 95th percentile,we see that at the
high end, oh, 95%of women at the very high
end were having babies whoweighed well over 5 kilograms.That's a very big baby.And that could clearly
cause some problems.So to summarize this,
the extreme quantilesare where the quadratics matter.You couldn't find this with
ordinary least squares.And this confirms mothers--this confirms medical opinion.We did the same for age.And here there's nothing
really very interesting.The percentiles are
roughly evenly spread,and a little bit of a
quadratic at the lowest end.But it didn't really
matter that much.We can also look and see how
accurate are the predictions.So from OLS on the left
and quantile regressionon the right, we had
the predicted value.So at the top row,
the 5th percentile,OLS predicted that they would
weigh about 3 kilograms.QUANTREG predicted
about 2.5 kilograms.And QUANTREG was
off by 13 grams.That's not much.Whereas OLS was off
by half a kilogram.And the same at the high end
in the opposite direction.And in the middle, the 50th
percentile, or the median,these were very similar.Next is MARS, multivariate
adaptive regressionsplines with PROC ADAPTIVEREG.So SAS has for a long
time offered a few optionsfor non-parametric regressions,
including TPSSPLINE, thatonly goes with low dimensions.LOESS, only low dimensions.And GAM, which has
a long computationand isn't guaranteed
to converge.ADAPTIVEREG
implements MARS, whichwere developed by Jerome
Friedman in the early '90s.And this is a prime
example of computershaving to catch up to
methods, because MARS involvesa multiply nested
loop with a verycomplicated mathematical
function in the innermost loop.So until recently,
this would work,but it took a very
long time to run.You can view a MARS model as a
generalization of a regressiontree.Regression trees
have some drawbacks.They're discontinuous.That is, the predicted
value of the PVwill jump at particular levels
of the independent variables.And this rarely makes sense.They don't create a purely
additive model, thatis one where OLS would work.They don't work remarkably,
trees don't work very well.They don't work well where
there's no interaction or oneor two very dominant.And you can see details of this
in a paper by Friedman in 1991.MARS models are more
useful and most usefulin high dimensional spaces
and lots of variables,or whether it's a literal
substantive reasonto assume linearity or a
low-level polynomial fit.Now, in some places
there is strong reasonto assume linearity.I would guess especially
in chemistry or physics,there could be a strong
reason to assume linearity,or you may have a
very good reasonfor there to be a quadratic
or something like that.I work in medicine,
psychology, social sciences,behavioral sciences, things
like that, and is usually,personally there's usually
a lot of variables,and there's usually
not a lot of reasonto assume that things are linear
or even necessarily monotonic.So they allow very flexible
fitting of the relationship.Their selection
methods that can reducethe dimension of
the model by a lot,and SAS implementation
of these--I'm going to show it for
a continuous variable,but it covers the entire
exponential family.So you can do these fairly
wide range of topics.There are some disadvantages.There is overfitting.But Friedman shows
that this isn'tas big of a problem with MARS
models as you might think.And I'll show one
way to look at it.And I guess the other one
is that the model itself,the output as we'll
see in a few slides,is to put it mildly
not intuitive.Unfamiliar options on
the MODEL statement.And again, like I said,
it's a abbreviated,but all of the stuff
is in my paper.ADDITIVE, that specifies a
model with no interactions.ALPHA, that controls
how many knots you have.And the knots, when
you have a spline fit,you will allow the curve to
bend in all sorts of waysand it appears in knots where
the different bends can happen.KEEP allows you to force certain
variables into the model.I think this is a very
useful thing to have.There are some variables
that you want in the modelregardless of whether they
improve things or not.If your hypothesis is
entirely about one IV,you want that IV in the model.And you know people
say look, it'sat a very small effect size.MAXBASIS and MAXORDER, these
control various options.The PARTITION, which
lets you divide thingsinto training, test,
and validation.And the SCORE statement,
which lets youapply it to new models--new data, sorry.So here I analyzed
automobile data,which are available from
2005, which are people at SAS.And the data originally,
so Asuncion and Newman.These are old data.So the models here aren't
going to be interestingsubstantively.This is just to
show how this works.A few cars had 3 or 5 cylinders
and they were strange,so I deleted them.I consider it an additive
model and a modelwith two-way interaction,
but the two-way interactiondidn't do only a
tiny bit better.So I used the simpler model.All right.So here is the code.proc, adaptivereg, model mpg
equals these various things.output, I'm going to
send some output out.These are residuals and
the predicted values.And I partitioned it
into training and test,with test having 30%.DETAILS equals BASES gives
output about the bases.I don't cover that in
this abbreviated talk,but it's in my paper, and
the ADDITIVE I talked about.So we've got R square of 0.91.That's very good.Four variables were included.First two are by far
the most important.Year had 4 knots
at these levels.Weight had 2 knots at
the levels you see.Acceleration had 1 knot and
displacement had 1 knot.One thing here to remind
you, and in SAS I'mgoing to show, acceleration
is sort of the opposite-- it'ssort of reverse coded,
because the way they lookedat acceleration was
number of secondsto get to a certain speed.So higher numbers
on accelerationare worse acceleration.So I checked for overfitting.I had, as I said, used
the PARTITION statement,and then I looked at
the absolute valuesof the residuals
of the two models.The data validate,
I set these to--I created absolute
values of the residuals.And I did a t-test.The mean absolute residual
was 1.84 for trainingand 2.02 for test.That's pretty small.That was nonsignificant.And that's despite the fact that
we have a pretty big N, 389.All right.So here's what I
was talking about.This model, well, maybe John
Van Maanen could look at thisand say, aha, but I can't.The intercept 17.61
is pretty clear.But then we have this parameter
estimate, 0.0057, times the maxof 3,449 minus weight and 0.So if weight is more
than 3,449, this is 0.If it's less, it gets
multiplied by 0.0057.Plus 0.9 times the
max of year minus--and notice it can
be-- the variablecan be before the
constant or after.Plus this times
acceleration minus that.You know this--I do not get this.I don't understand it.You could write it out,
because people sometimeswant you to put down the model.So here it is written.But it's not intuitive.So what I did was
I generated a dataset that covered the
entire range of the dataon independent variables
that are the model.Then I scored it using the
score statement in ADAPTIVEREG.Then I sliced it on the
less important variables,and then I made a contour graph.And the contour graph is this.So it's a little hard to see,
maybe, but these are divided.In the top left,
we have the MPG.The displacement is small
and acceleration is small.So they're small engines
that have good acceleration.And we have the different
combinations here.And rather get into any
of the details of this,because these are old cars,
it's not that interesting.What's interesting to me is that
the shapes of these contoursare by no means
straight, and they'redifferent in the
different quadrants.So that could be interesting.Unfortunately here,
the MARS modeldidn't improve things that
much for the average residual.But I think it's
still interestingand I think those graphs
could be interesting to peoplein any particular field.Finally, transforming--
oh, next.I think I have one more thing.Transforming variables
and TRANSREG.It used to be that we
transformed a lot of variablesin order to force them
to fit the assumptionsof the ordinary least squares.You have Box Cox transformation
and things like this.I'm generally opposed to that.But I think, if you're going
to do a priori transformations,they should be based
on substantive reasons.Like frequently you want to
take the log of money variablesbecause we think about money
variables multiplicatively.If you're making $20,000 a year
and you get a $5,000 a yearraise, that's huge.If you're making $200,000,
you get a $5,000 a yearraise, that's small.And if you make even
more, it's even smaller.Same with prices of things.We say one house costs
double what another costs.So there a log makes sense.But there's other times
when you can do it.And sometimes you want to do
it so the model fits better.That is not that it makes
the assumptions better,but so that the
relationship is clearer.You can do many of
these in data step,but TRANSREG offers
many, many optionsand allow automation
of a bunch of tests.And some transformations
like splinesare impossible to
do in the data step.So there's a huge number
of options in TRANSREGand you can obviously read
the SAS documentation.There's linear ones for
linear transformations.Monotone ones.That's very good for
ordinal variables,and ordinal independent
variables are often tricky.Sometimes people treat
them as continuous.Sometimes people treat
them as categorical.If you use a monotone
transformation,then you're treating
it as ordinal.Msplines, which are
optimal monotone splines.Opscore is interesting.It's very good for
nominal variablesand it recodes things
to their optimal level.So we'll see an example later.But usually each level
of a categorical variableis simply compared to
the reference level.And this lets you fix things
a little better than that.So using the same mileage
data, we have this.So we have model
identity equal mpg.I'm not transforming
miles per gallon.I'm taking a spline of
displacement, weight,acceleration, opscores
for origin yearand cylinders, sending
out the output,and that's what I just said.So displacement has a
nonmonotonic relationshipwith mileage.Let's look at the things.Here at the top left, we
have miles per gallon.Well, I didn't transform
it, so it's the same.Displacement, the relationship
isn't monotonic, OK?That's weird.Acceleration has a
slightly nonmonotonicthat matters more at higher
levels, less acceleration.Let's see.Acceleration has a nonmonotonic
thing going on there.And in the next
one, we have origin.So here we have one, almost
exactly two, and three.Those are American,
European, and Japanese,I believe, or Asian.I don't remember off hand.For year, It's almost monotonic.It's sort of linear.You might want to treat
year as continuous.I treated it as categorical.Cylinders is weird.4 and 6 were about the same.And 6 was lower than
4, which is strange.8 was much higher.But on this data set the two
models did almost the same.Some general thoughts.I'm going to skip those,
because I'm running out of time.But they're in my other papers.I just forgot to delete
this outline slide.Sorry.So in summary, ordinary
least squares regressionis often useful.Good alternatives exist
that make fewer assumptionsand answer more questions.And these ought to
be more widely usedand SAS makes them available
in a straightforward manner.Model building is part art,
part science, and part craft.But you want to play
around with these things.You have more tools
in your tool box.Thank you.Here is my contact information,
my website, and my co-author'scontact info.And thank you for attending
this virtual SAS Global Forum."
59,"Hey, everyone.My name is Mark Malek.I work at SAS as
a visual designeron SAS Visual Analytics,
data visualization,and our application themes.Today, I'm going to be talking
about updates in SAS VisualAnalytics.The new features and
SAS Visual Analyticsdeliver a giant leap
on the user experiencetowards explaining data,
uncovering insights,summarizing findings,
and sharing them broadly.They make the software easier to
use for data scientists, reportbuilders, casual business users,
and pure content consumers.In this presentation,
I'm going to highlightsome of the features that make
reporting smarter at gettingstarted, smarter at
building, and smarterand sharing those insights.First, I'm going to be
talking about page templatesWhen a new page is
added, page templatesare provided in
the bottom panel.Templates allow you to
quickly put together a pageand assign data to it.With a single click, the
beautifully designed dashboardis added to the page.The focus is now on data input
and creating a compelling storywithout worrying about
the layout or the colors,and one of these templates
just shown on the screen.Next, is the View
and Edit toggle.It's now easy to flip between
the author's editing interfaceand the viewer experience.Simply click a title
in the top banner,and their report experience
is switched between editingand viewing the report.Viewing the report
this new way alleviatesallowed discrepancies
and ensuresthat the integrity of the layout
of the report remains intact.Next in the Getting
Started sectionis the Suggestions pane.Let me show you the
Suggestions Pane.Visual suggestions
extend these capabilitiesinto an all new
Suggestions Pane.This new pane automatically
generates visualizationsbased on your data set.Simply drag and drop these
objects onto the report canvas,or use these suggestions to gain
new insights into your data.Clicking Refresh in the pane
will generate more suggestionsbased on the data
set you have loaded.Next is assigning
data in context.Instead of dragging and dropping
data items onto your objectsor assigning data
using the Roles pane,you can now assign data from
the object on the canvas.Click the new Assign Data
button that automaticallyappears on all of your objects.This enhanced pop-up that
is showing on the screenis customized for the object
and presents the primary dataroles that are needed making
data role assignments a breeze.So here, you can see that we've
updated multiple roles at once,or we can show the
required roles.Next is the Options toolbar.This is our first one in
getting building reports.The new Options toolbar presents
common property settingsfor the selected objects
right on the report canvas.With the toolbar, you can
change the style of the piechart, direction of your
bars in a bar chart,or edit the marker
size in your line chartwithout leaving the
focus of the object.You can see this
toolbar on my screen.We're continuing
to work on these,so you can expect more
features in future releases,like more flexible
page controls,Page controls enable
you to add filtersthat automatically
filter individual pagesinside the page prompt area.Now, in the latest release
of this SAS Visual Analytics,you can orient these controls
on the left, right, top,or bottom of your page.So you may be wondering
why you would need this.You may want to selected the
left or right orientationfor a vertical page control.That provides more
space and allowsyou to present data
values in a list.In this case, I would be
changing this dropdownto a list to allow more
filter controls to be shown.Next is viewer customization.It's an exciting feature that
allows report authors the powerto provide report consumers with
more ways to modify the report.Three levels of
viewer customizationprovide options for
report consumersto customize content
and explore their data.For example, enabling the
more advanced data editallows report consumers to
replace the data assignedto the report objects
automatically without leavingthe viewer setting.Next are data correlations.Gaining insights into your
data is now even easier.This release of SAS
Visual Analyticspresents correlated measures
when you select the right dataitem without having to
leave the Data pane.Select a measure from your data
set, and correlated measuresare automatically
highlighted, ensuringthat you have the knowledge
to create the highest qualityvisualizations.And as you can
see on the screen,that blue dot is
indicating that those dataitems are correlated.And finally, the automated
predictions object.The new object,
automated prediction,enables you to predict the value
of a selected variable basedon the values of
underlying factorsin the data in a single click.When a response
variable is identified,the object runs several
models assuming the remainingdata and underlying variables.It then selects
a champion model,as you can see, and displays its
predictions with the underlyingfactors.The interface
allows you to adjustthese factors to see changes
in the predicted responsevariable.To wrap up, we looked at
some of the new features thatmake reporting in
SAS Visual Analyticssmarter at getting started,
smarter at buildingreports, and smarter
at sharing insights.Our ongoing work builds
on these directions.In the future, you'll see
more in-context editing,user-defined page templates,
and built-in toolstowards creating
effective reports.Join us in this journey though.Get more information
on this paper.Go to the new online SAS Visual
Analytics Learning Center,and there you can try out the
software with your own data,and share with us how you'd
like that software to grow.Thank you so much for your time."
60,"Hi.Good morning, good
afternoon, and good evening.My name is Sanjeev Heda,
and it is my pleasure todayto have the opportunity to
present to you on this topic,enabling real-time
stability monitoring usingSAS Viya and Event
Stream Processing.A little bit about myself.I am a senior
industry consultantat SAS in our internet
of things division.My primary
responsibilities includeproviding thought leadership
to drive advanced analyticssolutions and to help
customers uncover valuewith a specialized focus on
industrial and transportationsegments.Prior to joining SAS, I was
with GE power for over 12 years,where nine of those years was
in the data and analytics teamwith the primary responsibility
of enabling conditionmonitoring using analytics.So for this
discussion, to showcasehow we can enable real-time
stability monitoring,we are using the aircraft
engine degradation problem.This problem comes from NASAs
Prognostic Center of Excellenceat Ames Research Center.The aircraft engine
degradation datais generated using simulations
from a program called C-MAPSS.So within this
problem, there area total of four data sets,
where each data set includesa hundred simulated aircraft
engines, one or more operationmodes, one or more
failure modes,and each data set contains
the following information.It contains the unit
number in time and cycles,it includes three operational
parameters, and 26 sensormeasurements.For this particular
paper and presentation,we are using engine number
24 from the FD003 data set.So what does that mean?This data set has
one operation mode,and it has two failure
modes present, high pressurecompressor and fan degradation.And the motivation
to select this datais built upon a previously
published SAS paperto show how you can create a
stability monitoring model.However, that paper
did not go furtherto discuss how you can
deploy and operationalizethat model using SAS Viya and
SAS Event Stream Processing,so this is the motivation
and what we'll accomplishwith this presentation today.So our end goal,
our final outputwill be an actionable dashboard
that indicates degradationhas occurred and
warrants further actions.In this case, what
we're seeing hereis using the open
source tool, Grafana,to show a dashboard that
includes the following.We see engine parameters and
associated analytic time seriesoutputs.We can see the escalator
alerts in the tableon the right-hand side, as well
as being annotated on the timeseries charts.When we look at this from
an end user perspective,we want to know the following
pieces of information.We want to know for our
stability monitoring modelwhat was the target
variable behavingand how does that compare
with our predicted valueswith their confidence limits?We also want to look at
other engine parameter dataso that we can start to
diagnose and understand whatcould be causing the issue.So let's talk about the need
for real-time predictivemaintenance.So in this example of
an aircraft engine,a failure mode
does not typicallyhappen instantaneously.The acid condition
will degrade over time,where there can be various
precursors to indicatethat degradation has occurred.The rate of degradation can
change depending on the failuremode, the operating
of the asset,the environmental factors,
and resolving the issuecan increase in costs
over time as damagegets propagated and becomes
harder to remediate.So the question
becomes, should Ialways take an action on
the earliest indication?And it really
depends on two items.The first is, what is the
cost to remediate the issue?When we think about how do
we remove the degradation,we either have to use hardware,
we have to mobilize personnel,the asset may be down.We have to think about if
there's penalties associatedwith this because the
asset is not able to dowhat is being said to do.On the other side,
we want to balancethis with, what is the cost
of not doing anything, whereif the assets been degraded, we
might see reduced performance,lower output and yield, and
it may have a negative imageimpact to your organization.And so what real-time
allows you to dois increase your
window of opportunityto help make these
decisions quickerand to maximize the
overall asset performance.So one use case we have
is with Volvo and Macktrucks using SAS to enable
predictive maintenance.So their challenges
include, how can theyenhance their remote
diagnostics and monitoringcapabilities of critical
assets such as enginesand transmissions?How can they minimize
unplanned downtimeand improve overall vehicle
efficiency and uptime.So using SAS Analytics, Volvo is
able to achieve the following.They're able to apply these
analytics to over 250,000trucks, they're able to achieve
a reduction in diagnostic timeby 70%, and a reduction
and repair time by 25%.And they continuously are
able to leverage thousandsof sensors, creating
new analyticsto continually improve
their capabilityin providing
predictive maintenancefor their customers.So now, we'll talk about the
approach of how we enablereal-time stability monitoring.So here's an overview of what
was implemented to enablereal-time stability monitoring.More details will come
in particular areasin the following slides.Starting with data at rest,
we have historical dataof the aircraft engines
with known issues.The first thing we want to do
is load that data into our tool.In this particular case, we're
using SAS Viya with SAS Viyaand SAS visual data mining
and machine learning.Once that data is
loaded, we wantto process that data to
make it ready to be trainedusing machine learning models
and then we train our models.In this particular case, we're
using stability monitoringto predict potential degradation
of our aircraft engine.Other methods can be used, such
as neural networks, gradientboosting, to achieve
the same objective.So once that model has been
trained and ready to bedeployed, we can then generate
to ASTORE file of the model,and register that model
into SAS Model Manager,and this is a key step
for a couple of reasons.One is that now we have
governments behind that model.We know how it was
created, what wasthe audit trail, the
version history, et cetera,and we also know the
lineage so that whenwe want to deploy this model
and downstream processes,we know which processes
are consuming that model.So in this particular
case, we'readjusting this model using
SAS Event Stream Processing.So we're using SAS
Event Stream Processingfor a couple of reasons.One is that it provides
minimal latency and alerts thatcan be generated for
actions to be taken.So when we talk about real
time alerts and that windowof opportunity, Event
Stream Processinghelps in that regards.And the second reason
is when we thinkabout aircraft engines
and the amount of datathey may produce, that
may be a tremendous amountof information to try
to take and collectonto a centralized location
to be able to process this.So in the event-- in the case
of Event Stream Processing,we're able to push
this model to the edgeonto the aircraft engine itself.So we have the model linked
into Model Manager, whichprovides lineage.So if there's any
changes to this model,we'll get notifications within
our Event Stream Processingproject.And on the other side, we
have the streaming dataof the aircraft
engine coming in.So then we take that data,
prepare it to be scored,and then we score
the data in streamusing the model we
have trained at rest,but we don't stop
there because wehave a model that says
whether we think we're outof the control limits or not.But it really depends on what
is the right decision to makeand when.And so now we get into
the post-processingof the analytic of
the results to beable to understand
which alerts shouldbe potentially escalated.So the Event Stream
Processing analyticis producing these
analytic outputs,and these outputs get pushed
into an external data system.In this particular case, we
have a time series databaseand an alert database,
and then these outputsare visualized and
consumed throughan operational dashboard.So this is an important
feature because no valueis realized until an output
of an analytic is consumedand decisions are being made.So how do we generate the
stability monitoring model?First, we have historical data
for the given aircraft engine,and we want to select
data to be usedfor the training
and the hold out.And in this particular
case, the targeta variable that we're
using is the ratioof fuel flow to a
static pressure,as research indicates
this is the bestindication of degradation
for the aircraft engine.Next, we create multiple
models to competeto see what does the best job
in predicting the behavior.In this particular case, we
are using a mean, a regression,and ARIMA model to see which
one is the best predictor.Input variables
into these modelscan include pressure
at the fan inlet,total pressure at
the bypass duct,or corrected course speed.So once we have the models
and they've been created,we use the holdout
period to determinewhat model method performs the
best in predicting the targetvariable.In this particular
case, we're usingMAPE, the mean absolute
percentage error,to determine how well
does that model performduring the holdout period.In this particular
case, the ARIMA modelis selected as the best one
and is close to the regressionmodel created.So the best model is selected
and becomes the champion model.So once the champion
model is selected,it can be registered
into SAS Model Managerand be consumed into SAS
Event Stream Processing.This overall process was done
using Jupyter notebook thatinteracts with SAS via using
the Python Package SWAT, whichstands for a Scripting Wrapper
for Analytics Transfer.The Python package SWAT allows
users to execute CAS actionsand process all the
results using Python.Great.So now we have our model,
and now the question is,how do we get that model
to score in real time?So here we have a simple
flow to show that processwith three key steps.First is the Model
Manager, wherewe have our stability monitor
model registered, whichprovides the
governance and lineage,and that model can be used in
multiple places downstream,including SAS Events
Stream Processing.Then within SAS Event
Stream Processing,we can include the
model for scoring,we can pre and post
process data thatis coming into the analytic to
help make actionable alerts,and we do this through a
graphical user interface calledEvents Stream Processing studio
to create said analytics.Once this analytic is
created and been validated,we can publish in
version this analyticand deploy this
analytic to whereverit needs to be and monitor
status using SAS Events StreamManager.And really, this is
a continuous process,because over time, the
requirements in the modelwill change where we need
to continuously interrogateto make sure that the model
is performing as expected.And so what we see here as
the graphic change is thisis the SAS Event
Stream Processinganalytic within ESP Studio.And what we see here is that
the scoring of the modelis just one window.The results
analyzed-- results areanalyzed to determine what
decisions are needed and when.What about the analytic outputs?So value is not
realized until an actionis taken that otherwise
would not have been taken.Analytic outputs really depends
on the business requirementsand process.As you have multiple
users and individuals thatconsume this information,
that informationcould be made its workflows,
time series databases,or management, and
really, it comes downto the analytic outputs shared
a line to the organization,tools, and interfaces.We don't want folks to have
to use 1,000 tools for someoneto fulfill their
job, and we reallywant this to be as
frictionless as possible.SAS and SAS Event
Stream Processingprovides flexibility to
integrate with existing systemsand platforms.To highlight this, for this
particular presentation,we are using open
source components.For the external
data system, we'reusing a Postgres database, and
for the operational dashboard,we're using Grafana.And with that, we will
now go into our demo.For this demo, I want to
cover three key items thatare important to accomplish
in this deploymentand operationalizing these
stability monitoring models.The first is how we create that
stability monitoring model,and in this particular case,
we use a Jupyter notebookin conjunction with
the Python SWAT packageto be able to interact with
the SAS Viya and CAS system.So we see we make the
connection to the CAS system,we are able to load action sets.In this particular
case, the datathat we're using for
training is on my local PC,so we've loaded into
Pandas, and thenwe can manipulate that
data, modify it as needed.That data eventually gets
uploaded as a CAS table,and so then we start
preparing this training data,as we typically do, determining
which time series of datawe want, what's our
target variable,until we get to the core options
that are used or selectedfor creating the stability
monitoring models.So in this case, we
can select the inputs,the types of models.In this case, we're
doing a means,we're doing a regression,
we're doing an ARIMA modelto where we submit this into
CAS, we get the score outputs,and it's nice because those
results get returned backinto the Jupyter notebook,
where we can take that same dataand plot it using standard
packages like matplotlibto make a determination
of which model performedthe best in predicting
the target variable.In this particular case,
we see that the ARIMA modelis the one that performs best
with the project idea of onewith a model idea
of two, and that weare able to score
of additional datajust to get confirmation
of how it performs.And then finally, because
we have in ASTORE created,we can take that model
and then register itwithin Model Manager so
that we can consume itdownstream and other processes.So in this case, we have the
model name as smodelswat,and then the project name
as stability monitoring.And a key piece here is
that the ASTORE actuallycontains all three models.And so we need to make
sure that when we score it,the SAS code that's used
to score the ASTORE modelknows which model
we want to select,and that's getting
into step number two.We go into a Model
Manager, wherewe can see our new model
that we've created,a new project called
stability monitoring here,and we see our model
that was created.We do want to create
this as a champion,but as I mentioned before, we
need to make sure that whenwe score this model that
we are using the ARIMAmodel because that was
determined to be the champion.To make that change
all we have to dois click into this
model information,we can see the associated
files with this model,and we want to look at
the score code that loadsand be able to invoke
this model for scoring,and so in this particular case,
we added two lines right here,where we set the project ID
and model ID for the ASTOREto know which one we
want to score against.And so as we saw,
the champion modelwas project ID of one
and model ID was two.We added this year so that
when we score the model,we are using the ARIMA
model that we expected.We would save this and then go
back into the project folder,and then make sure that we have
selected this as the champion.The reason why champion
selection is importantso that one, downstream
process know which modelwould be the best one to choose.In the case of Event
Stream Processing,the model has to be a champion,
otherwise it can't be selected.So then we get into
step number three.We have our model registered,
it's ready to be consumed.How do we get that into SAS
Event Stream Processing?And so what we're seeing
here is the completed projectthat shows from ingesting
data to analytic outputsthen alerts of completing
the full process end to end.And so in this
particular case, we'rereading a CSV file of
the historical datato replay in information,
and then we'regoing to score that data with
our stability monitoring model.And so to ingest that model,
we have a calculate windowwhere we're saying the
calculation is user specified,and when we say, how is
that handler defined?We can then select us import a
module from SAS Model Manager,and so we see the
same repositoryas we saw in SAS Viya, so we
can look to see our stabilitymonitoring model project, we
can look to drill down further.And then we can see
our champion modelthat we have
created listed here,where we can look at its
various inputs and outputsand other pieces of information.So I've already incorporate this
in, and as I mentioned before,this is just one
piece of the puzzle.Downstream of that, we
then want to post processthose results to
determine when isan actionable alert required.So in this case, now
we're ready to execute.So we'll enter into test mode.Once this is completed,
we would thenpublish in version
this analytic and thenpush it deployed on servers
using Event Stream Manager.For this particular
case, we're justrunning a simple simulation
to execute that data,as I mentioned,
running the CSV data.We can start to see
the data streaming in.Outputs of this is
that the data is reallygoing into Postgres
Database, both the timeseries and the alerts,
and what we're seeingis this the same dashboard
that we saw in the PowerPoint,where we had the various time
series data being presented,both the inputs and
the analytic outputs.We can see alerts
being generated,and we can see those being
generated, both in a table aswell as annotated alerts
being shown on these graphsso that we know when those
abnormalities are occurring.Thank you for your
time today to learnhow you can enable real-time
stability monitoring usingSAS Viya and Event
Stream Processing.If you have any
questions, pleasefeel free to reach out to
me at Sanjeev.heda@SAS.com.Note the materials that we
use to create these stabilitymonitoring models to be able
to incorporate these within SASEvent Stream
Processing are includedin the get a GitHub repository
for this conference.Thank you, and have a good day."
61,"RICHARD CUTLER: Well,
greetings everyone,and welcome to this
workshop on machinelearning methods in SAS for
regression and classification.I'm going to start by
loosely defining what I meanby machine learning methods.I think many people
would agree that they'rea group of highly computational
methods for extractinginformation and
characterizing structurein high dimensional data.The term machine learning
is used almost synonymouslywith statistical learning.The term statistical
learning was firstused by computer
scientist VladmirVapnik as the title of his
book The Nature of StatisticalLearning Theory in 1995.Vapnik was the co-developer
of the support vector machinemethodology.The Elements of Statistical
Learning by Hastie, Tibshirani,and Friedman has rapidly
become the state of the artin terms of textbooks for
this kind of methodology.When people talk about
predictive analyticsor predictive
modeling, they're oftentalking about the
application of machinelearning methods to
data, particularlytwo very large data sets.Some people conflate the
terms machine learningwith artificial intelligence,
but machine learningis only a subset of
artificial intelligence.There are many aspects of
artificial intelligencewhich are not machine learning.So two very important
types of machine learningare supervised and
unsupervised learning.Unsupervised learning
includes very familiarstatistical methods for
dimension reduction,such as principal components
analysis and multidimensionalscaling.Unsupervised learning also
includes cluster analysis,so the k-means algorithm
and the many hierarchicalalgorithms under this
general umbrella.Supervised learning is the other
major area of learning theory,and that includes the
statistical methodsfor regression and
classification.Roughly speaking, the difference
between supervised learningand unsupervised learning
is whether or notyou have a response variable.Over the past 35 years, there
has been tremendous developmentof classification and
regression methodologies,and it has really been
something of a quiet revolution.The two main areas
of developmentare regularization
methods for estimationand model selection in the
multiple linear regressionmodel, and nonlinear methods
for improved productionand alternative
interpretation in regressionand classification situations.We usually date the origins
of regularization methodsto a paper by Hoerl and
Kennard in 1970, in which theyintroduced ridge
regression as a methodfor getting sensible
regression estimatesin the presence of very
extreme crude linearity.LASSO is due to Tibshirani,
first published in 1996,and it has become, in a
relatively short periodof time, the standard approach
to model selection and variableselection within regression,
and particularly with regardto very large data sets where
p-value methods are simplynot appropriate.The LASSO has been the subject
of considerable discussionand development.The group LASSO was introduced
in 2006 by Yuan and Lin.Often, we like to include
categorical variablesin a regression, and we
do that through the useof dummy variables.What the group LASSO
allows us to dois to group together
variables sothat they come into a regression
or a lever regression together,and so that's useful for
the categorical predictors.The adaptive LASSO was first
proposed by Zou in 2006and further developed by
Wang and Leng in 2007,and it has some very nice
asymptotic properties.Least angle regression,
due to Effronand a number of his colleagues
at Stanford University,is more of an algorithm than
it is a new methodology.The LAR algorithm allows one
to compute LASSO estimates veryefficiently, and other estimates
for other regularizationmethods too.Elastic net, due to
Zou and Hastie in 2005,was developed to deal
with the kind of situationwhere we have very
wide data sets, perhapsgenetic information.So we have many,
many more variablesthan we have observations.And it overcomes
some of the problemsthat other methods, p-value
methods, criterion methods,LASSO, have in selecting
variables in that ratherdifficult situation.The elastic net combines
elements of ridge regressionand LASSO, and the
elastic net estimatesmay be computed using the
least angle regressionalgorithm of Effron et al.So non-linear regression methods
and classification methodswhich have been developed
over the last 35 yearsor so include classification
and regression trees,sometimes called
decision trees, developedby Breiman and Friedman,
Olshen, and Stone,who wrote the monograph
in 1984, and providedsome of the first
software for fittingthis class of models
to real data sets.Breiman and Friedman
also developedthe alternating conditional
expectations algorithm,which is one of the
first algorithmsfor fitting
non-parametric regression.ACE was quickly superseded by
the generalized additive modelsof Hastie and Tibshirani
in 1986, which are nowextremely widely
used and implementedin almost all computer packages
for doing statistical analysis.Jerry Friedman combined elements
of generalized additive modelswith classification
regression treesand came up with the
multivariate adaptiveregression splines algorithm,
usually abbreviated to MARS,in 1991.The origins of support
vector machinesgo back to work by
Vapnik and Chervonenkisin the early 1960s,
but the algorithmwhich is widely used today was
developed by Cortes and Vapnikin the mid 1990s.There are at least two
flavors of boosted trees.Adaboost was developed by Freund
and Schapire in the mid 1990sand is still fairly widely
used for classification.One of its advantages
is it works wellwith very little tuning.Perhaps the state of
the art in boostingis the gradient boosting machine
developed by Jerry Friedmanand published in 2001.This is the algorithm behind
the tree net classificationpackage.And in many applications,
both for regression and binaryclassification, gradient
boosting machineshave been shown to be among
the most accurate classifiersor predictors, if not
the most accurate.Finally, random forests was
developed by Leo Breimanand first published in
machine learning in 2001.This has rapidly become
a very popular algorithmparticularly for classification,
but also for regression.Part of its appeal is it
requires almost no training.One can just simply
apply the algorithmstraight off the shelf and
get very sensible resultsin a vast array of
different kinds of problems.And there are
additional featureswhich make random
forest appealing,variable importance
measures, and graphsto characterize the relationship
between the predictor variablesand the response variable.As Leo Breiman
liked to say, it'speeking into the black box.So our agenda for this workshop
begins with a quick discussionof principal components
analysis, in large partbecause principal
components analysisis foundational for
methods which follow.Then we're going to talk
about regularization methodsfor multiple linear
regression, and we'llstart with principal components
regression and partial leastsquares, both of
which are very heavilydependent upon and closely
related to principal componentsanalysis.Then we'll go to the
so-called penalty methodsfor regularization,
which includeridge regression, LASSO
and some of its variations,and the elastic net.And all of this should take us
roughly half of the workshop.The second half
of the workshop isdevoted to non-linear
methods for aggressionand classification.We'll use logistic
regression as an entry point,because many
statisticians are familiarwith logistic regression.And we'll quickly move into
generalized additive model,which are an extension
of logistic regressionand related techniques to
non-linear relationshipsbetween predictor
variables and response.We'll spend quite a bit
of time on decision treesfor classification
and regression,because they find a
lot of applicationwithin business modeling.We'll talk about random
forests, and I'llgive an example
which demonstratesits extreme accuracy
for prediction.And then I'd like to finish with
just a taste of the applicationof support vector machines.First topic that I
would like to coveris that of principal
components analysis.The purpose of principal
components analysisis to reduce the
dimensionality of databy identifying
linear combinationsof the variables
that explain mostof the variability
in the variables.In some cases, the principal
components analysiscan reveal some substructure
within large complex data sets.If there is colinearity
amongst variables,then principal
components analysiscan eliminate that
core linearity.Mathematically, principal
components analysisinvolves simply computing
eigenvalues and eigenvectorsof either the correlation
matrix or the covariance matrixfor the variables
in the data set.To illustrate the
process, I have an exampleof some weather data
that forms part of oneof my ecological studies.These are monthly
average temperaturesat 840 widely spaced sites
in the Pacific Northwest.This is the correlation matrix
that you see in front of you.And if you look at the
correlations, particularlythe correlations for
consecutive months,you see that they
are relatively high.So between January and February,
the correlation is 0.99631.Between February and
March, it's 0.98670.Between March and
April, it's 0.99285.Indeed, there's a lot of
very high correlationshere, indicating very
extreme colinearityamongst the observations
and amongst the variables,and the fact that really,
most of the informationis in a lower
dimensional subspacethan the 12 dimensions
mapped out by the 12 months.So the question that
we need to answeris, can we reduce these 12
variables to a smaller number?And so I carried out principal
components analysis in SASto do this.By default, PRINCOMP uses
the correlation matrix.You can specify that it should
use the covariance matrix,but in this example, I felt the
correlation matrix was probablymore appropriate.The variables that
I've specified,tave1 through tave12, that's the
average temperature for Januarythrough the average
temperature for December.And here's the output.The largest eigenvalue is 11.1.So that first eigenvector,
that first principal component,explains over 92% of the
variability in the data.And you could be forgiven for
saying, OK, that's enough.I'm done here.We'll settle for one
principal component.The first two
principal componentsaccount for almost 99 and
1/2% of the variabilityamong these variables,
and that surely is enough.So based upon this
tabular output,we would say that at most two
principal components whichare linear combinations
of the variablesare needed to explain all of
the variability in the data.Scree plots and
variance explainedplots are two graphical
devices to helpus decide how many principal
components to keep.And we see here in the variance
explained that by the timewe've gotten to the second
principal component,we have explained nearly
all of the variability.And so this reinforces
our earlier conclusionthat only 1 to 2 principal
components are neededto completely explain the
variability within these 12monthly variables.Component profiles
are correlationsamong the principal components
with the original monthlymeasurements.You see that for the
first principal component,that correlation,
that's the blue lineat the top of the plot,
and most of those valuesare close to one.You see for the second
principal component, whichis a dashed red line,
some are negativeand then some are positive,
but for the most part,they're away from zero.And then for the remaining
principal components 3through 12, the
component profilesare hugging the zero
line, indicatingthey're essentially just noise.So this reinforces
our earlier decisionthat only one and perhaps
two principal componentsare necessary to
completely characterizethe variability within
these 12 monthly variables.So here are the eigenvectors and
these are labeled, you can see,Prin1, Prin2, Prin3, Prin4.The eigenvectors are the
principal components.They are the directions,
and geometrically andalgebraically, they
are the coefficientsfor linear combinations,
for the variablesfor the different months.And we look down--
the first columnhere, so for the first
principal componentcorresponding to that
very large eigenvalue,and these coefficients
are nearly all equal.And so that suggested
to me that Icould create an index, which
I have called AveTempAveover here on the left.And that's simply a
straight average--not a weighted average,
but a straight averageof the average monthly
temperatures for Januarythrough December.The second principal
component hasgot a mixture of positive
and negative values.The negative values are for
October, November, December,and January, February,
March, technically Apriltoo, although that one's
very close to zero.And so those seem to
me to be winter months.And for the summer months,
we've got positive coefficients.So this led me to creating
a second index, whichI've labeled AveTempDiff,
on the left hand side.I take the average of the
April through September,average temperatures,
and I subtract from thatthe average of the
average temperaturesfor January through March,
and October through December.And so that is a contrast
between the summeraverage temperatures and the
winter average temperatures.The sites in which there
is a huge differenceare qualitatively
different from siteswhere there is a very small
difference between these twosets of averages.So in that particular example,
the principal componentsanalysis seemed
to work very well.It revealed some structure.We were able to reduce the
dimensionality of the data.I started out with 11
sets of monthly variables,and so that's 132
total variables.By reducing each group
of variables to just two,that gave me 22 variables,
which was much more much moresuitable for all
subsequent analysisthat I carried out on the data.Here's another data set where I
can apply principal componentsanalysis.These are crime data
for the 50 statesand Washington DC in seven
different categories.So we've got murder, rape,
robbery, assault, burglary,larceny, and auto theft.I might have thought that the
violent crimes, murder, rape,assault, and perhaps
robbery couldbe combined into one principal
component, and larceny,auto theft, and probably
burglary into another one.And so we might be able to
reduce the dimensionalityof the data here
to two from seven,and that would also reveal
some structure in the data.You notice, these values
are very variable.So for the murder
rate in Connecticut,it is 4.2 per 100,000 people.In Alabama, it's 14.2.The larceny rate in Arizona is
4,467 per 100,000 in Arizona,and in Alabama, it's
less than half that.So there's quite a lot of
variability within these data.So I applied principal
components analysisusing PROC PRINCOMP.Here's my correlation matrix
that is part of the output.And you can see that there are
relatively high correlationsbetween murder, rape,
and assault, and perhapsto a lesser degree with
robbery and burglary.But things like
larceny and order theftdo not seem to be
correlated virtuallyat all with things like murder.So PROC PRINCOMP, I
obtained these eigenvalues.And we see that
somewhat less than 60%of the total
variability is explainedby that first eigenvector,
that first principal component.Contrast that with
the 95% or so that wegot from the weather data.The first two eigenvalues
account for about 3/4of the total variability.The first three for about 87%.And there's a real
question as to how manyprincipal components that
we would like to keepin this particular data set.Here are the scree plots and
the variance explained plot.And I'm looking at the
variance explained plot,and I see it's a
relatively smooth curvaturefrom that first one.And if I look over
at scree plot,it suggests that maybe
two would be enough.That's only explaining 75%
of the total variability,or maybe four would
be sufficient.That's explaining
quite a bit more.That's getting up into the
upper 90s in terms of percentageof explained variability.Now let's take a look at
the principal componentsthemselves.And the first
column here, so thisis the first
principal component.That looks like similar
values, coefficients,for all different variables.In the weather data,
you know that you couldargue that that made sense.For these data, crime data,
it's really hard for meto accept that we should
be lumping togethermurder with larceny, with
roughly the same coefficients.And so I have questions about
the sensibility of that veryfirst principal component.The second principal
component looksto be a contrast
between the murder rateand perhaps the auto
theft and larceny, right?Although the assault has got a
relatively large coefficient,at least in terms of magnitude.And so that detracts a bit
from the interpretability.And principal components
three and four and thereafterare increasingly
hard to interpret.So in this particular
case, the hopefor separation into
violent crimes,and non-violent crimes, and
perhaps something in betweencertainly doesn't seem to be
apparent from the principalcomponents analysis.And I would say this
is an example wherethe principal components
analysis does notreveal anything
particularly useful to us.Principal components analysis
is very commonly used,and I would say abused, within
statistics and data sciencefor reducing dimensionality
and eliminating colinearity.It certainly makes
sense when you'vegot a bunch of variables
that are highly correlatedand which are in the same units.And then you can significantly
reduce the dimensionalityof the data and reveal
interpretable combinationsof variables.That's what we saw
with the weather data.In other situations,
principal components analysismay be less effective
for dimension reductionand interpretation,
and really, itmay not reveal any
interesting structurewithin the data at all.In these cases, it simply is
not an effective data analysistool.Now, although principal
components analysisreduces the dimensionality
of the data setby reducing the
number of variables,it doesn't actually reduce the
need for all of the variables.So it doesn't effectively
select subsets of variables.All of the principal components
are linear combinationsof all of the
original variables,so you still need all of
the original variables.OK, moving on now then
to principal componentsregression.The basic idea is this.We've got some
kind of colinearitythat we're trying to
deal with, and so wewant to carry out
principal componentsanalysis on the correlation
matrix of the predictorvariables, and then select a
number of principal componentsto retain.The resulting transform
predictor variablesare going to be orthogonal,
and hence, colinearitywill be absolutely, totally,
and completely eliminated.Then we want to regress
the response variableon the transformed
predictor variables.Once we've done that, we'll back
transform to the original scaleto obtain coefficients for the
original predictor variables.So here's a motivating example
for this kind of analysis.This concerns the physical
fitness of 31 subjects,and it's a data set
within SAS PROC REG.The response variable
is oxygen uptakewhile running, which is a
measure of physical fitness,and there are six predictor
variables, the age and weightof the subjects, the time
it takes them to run 1and 1/2 miles, their rest
pulse rate, average pulse ratewhile running, and maximum
pulse rate while running.What we would like to do is
to predict the oxygen uptakeusing a multiple linear
regression model.And so I've carried
out this analysiswithin PROC REG, an
initial analysis, at least.And you'll notice that I've
circled there the optionscollin and VIF, which stand
for colinearity diagnosticsand variance inflation
factors, which are alsocolinearity diagnostics.Some sample results
from the analysis,you'll see that the
R-square value is justa smidge under
0.85, and so that'sa point of reference for
all future calculationsand analysis.And here is my initial output.You see, down in
the far right handcolumn for the variance
inflation factors,that the two largest
correspondingto run pulse and max
pulse are 8.4 and 8.77.Usually, we use a cutoff of 10
for identifying variables whichare colinear, but I
would argue that that'sgetting close enough to 10.Among the colinearity
diagnostics,we first look at the
condition indicesand see if there are
any exceptionally largevalues and jumps in
the condition indices.And I would say the
jump from 82 to 196.8is a fairly large jump.The way that we read
proportions of varianceis, you look along the row for
which you've got the large jumpand condition index, and
you see on what variablesthe proportion of
variance is really large.And you can see here that that's
on run pulse and max pulse,and in this particular
problem, that's bang on.And so that interpretation
works well in these data.If I were to compute Pearson
correlation coefficientsamongst all of the
predictor variables,we do indeed see that the
correlation between run pulseand max pulse is nearly 0.93.So that's very high and that's
where the colinearity liesin this particular data set.So just to remind you,
here's the output.And now I want to focus
on the column whichis parameter estimates.So we've got run pulse and
max pulse, both of whichare measuring roughly
the same thing,because they're very
highly correlated.And yet you see
in this regressionthat they have coefficients
which are opposite in sign.The run pulse, it's minus 0.37,
and for max pulse, it's 0.3.And that doesn't
seem very sensible.We would expect that these
two variables should havecoefficients of the same sign.This is a symptom
of colinearity.This is what we often see when
we have colinear variables,is that they will
have opposite signs,and those coefficients
will be larger in magnitudethan they should be.So I decided to carry out
principal components analysis,and then I'm
outputting the scoresfrom the principal
components analysis.And I look at the
scree plots, and Ilook at the eigenvalues
of the covariance matrix.And certainly, by the time we've
got four principal components,remembering that there
are only six variables,we have explained about 98%
of the total variabilityamong the predictor variables.One could argue that three is
enough and perhaps even two.Looking at the scree
plot and the varianceexplained, particularly the
variance explained, by the timewe get to four, that's
most of the variability,and so that's why I
chose four variables.So now what I'm going to
do is to go back and runthe regression, but instead
of the six predictor variableswe began with, I am going
to use the first fourprincipal components.And so you can see that in here,
I've got Prin1 through Prin4,and I've labeled
this model PCREG.I've carried out colinearity
and t-diagnosticsfor this new regression,
and I have to admit,this caused me to chuckle.Because the principal components
are orthogonal to each other,there's absolutely
no colinearity.And hence, the variance
inflation factors are all one,and hence, the condition
indices are all one too.So principal
components analysis--principal components
regression will 100%eliminate colinearity
in the regression.So in order to obtain
the coefficientsof the original variables,
it's easiest to usePROC PLS within SAS.That stands for
partial least squares.But if I use method
= pcr, then thatwill do principal
components regression.I'm specifying the number of
factors to be equal to four.That's what NFAC = 4 is doing.And then with the
option solution,I'm asking for the estimates
of the coefficientsfor all of the original
variables, which requiresome back transformation.And if you would turn your
attention to the right handside where we have those
parameter estimates,then I see that the coefficients
for run pulse and max pulseare now both negative,
which makes sense.The higher the pulse rate,
the less fit the person is,and so the lower
the oxygen uptake.And they're about
the same magnitude,and that magnitude
is much, much lessthan it was in the
original least squares fit.So these coefficients have
been regularized or stabilizedby the application of principal
components regression.So the next topic, which is
related to principal componentsregression, is that of
partial least squares.And unlike principal
components regression,partial least squares seeks
to explain the variationin the correlation matrix
of the predictor variables,and simultaneously,
the response variable.It works by
successively extractingoptimal linear combinations
of the predictor variables,and it's related to canonical
correlation analysis and factoranalysis, which you
may have encounteredin the context of multivariate
statistical analysis.Partial least
squares is commonlyused in chemometrics
and in the analysisof high dimensional
data in genetics.So I carried out partial
least squares within SASusing PROC PLS.This time that's
the default, so youdon't have to specify
method is equal to anything.And you can see that I have
asked for a solution, whichis coefficients, at the end,
and then I've left everythingat default settings within SAS.And this is the
output that you get.And so we're trying to
simultaneously explainthe total variability amongst
the predictor variablesand the response variable.So that's-- the model effects
is the predictor variablesand the dependent variables
is the response variable.So if we take two partial
least squares combinations,we're explaining about 80% of
the variability in the responsevariable.If you remember from
an earlier slide,the maximum R squared that we
get from ordinary least squaresis only 0.85 or a little
under, and so thisis the vast majority of the
variance in the responsevariable explained.It only explains about
64% of the variabilityin the predictor variables.If we were to move
down to, say, fournumbers of extracted
factors, then we'reexplaining well over 80% of the
variability in the predictorvariables and in predicting
the response variables.Now, on the plot on
the right hand side,these values of R squared
have been plotted.And my selection of the value
four for subsequent analysisdoesn't really have
anything to do with the factthat the two cross over there.It has more to do with the
fact that this is the smallestnumber of factors for which both
the R squared for model effectsand for the dependent
variables are above about 0.8.So now I'm going to fit partial
least squares models with fourfactors and with two factors,
these being the two candidatemodels that were thrown up
by the initial analysis,and I want to compare the
results from both of these.In particular, I want
to take a hard lookat the solution, which is the
coefficients, the estimatedcoefficients, for the
original variables.So this slide is pretty densely
packed, I'll own up to that.So if I look at the
partial least squares fitwith four factors,
that explains 84.7%of the variability
among the predictorsand 83% of the variability
in the response.And so that seems to be
doing a little betterthan the partial least squares
with only two factors, whichexplains only 64%
of the variabilityamong the predictors and
80% of the variabilityin the response.However, when we take a look
at the parameter estimates,we say that for the partial
least squares with fourfactors, the coefficients
of run pulse and max pulsehave opposite signs.They are much smaller than
the ordinary least squaresestimates, but they
are of opposite sign,and we don't think that
should be the case.Whereas if I look at the
partial least squares estimatesfor two factors, then I see
that both run pulse and maxpulse have got coefficients
which have the same sign,and that sign is negative,
which makes sensein the context of this problem.So in this problem, we would
prefer partial least squaressolution with only two
factors because it's simpler.We're really
primarily interestedin explaining the variability
in the response variable, that'sour prediction error, and
the two factor partial leastsquares does almost as
well as the four-factor.And then finally,
the coefficientsof run pulse and max pulse have
the same sign in the two factorPLS.And so that indicates
the regularizationhas been effective.So some brief conclusions about
principal components regressionand partial least squares.Both principal components
regression and partial leastsquares may be used to fix
colinearity and regressionproblems with relatively
small numbers of variables.Both of them form orthogonal
linear combinationsof the predictor variables.Principal components regression
seeks to explain variationin the predictor variables,
whereas partial least squaresseeks to explain variation in
both the predictor variablesand the response variables.Neither principal components
regression nor partial leastsquares identifies important
variables or subsetsof variables.The principal components
and underlying factorsare all linear combinations of
all of the original variables.So if you're trying to reduce
not just the dimensionalityof the regression, but actually
the number of variables,to do some model selection
or variable selection,then neither principal
components regressionnor partial least squares
will do that for you.As we continue our discussion
of regularization methodsfor multiple linear
regression, the next groupof methods that
I'd like to discussare so-called shrinkage
or penalty methods.The granddaddy of these
is ridge regression,which was developed
in 1970 and was reallyway ahead of its time in terms
of regression regularization.And so here's a brief
description of how it works.The first thing
that we need to dois to standardize
all variables to havemean zero and variance one.That includes the predictor
variables and the responsevariable.When you do that, you'll
find that the interceptterm drops out.In ordinary least
squares estimation,the way that we would
obtain estimatesof the coefficients for
the predictor variablesis to minimize the residual
sum of squares with respectto beta 1, beta 2, up to beta p.In ridge regression, we
minimize a penalized versionof the residual sum of squares.You have the residual
sum of squaresand we add on a
multiple of the sumof the squares of the
regression coefficients,so the beta j squared.As the ridge penalty,
the parameter lambdaincreases, the
parameter estimatesget pulled towards zero.Parameter estimates
that have been inflatedbecause of colinearity are
drawn disproportionatelytowards zero.The goal is to select
the value of lambdaat which all of the parameter
estimates have stabilized.The purpose of
ridge regression isto obtain good estimates of
the regression coefficientsfor all of the different
predictor variables.But in terms of
predictive accuracy,it does pretty well too.Ridge regression does
better in generalthan traditional variable
selection methods,such as stepwise methods.So I'm first fitting
ridge regressionusing PROC REG within SAS.And so you can see
that I've done PROC REGand I'm going to output
the coefficients.And I've chosen
the ridge parameterto be between zero and
one in steps of 0.4.Selection of the range
for the ridge parameteris sometimes a bit more
art than it is science,and this was not the
first range that Itried for these
particular data, but thisseems to work quite well.So this part of the process
can take two or three attempts.Of course, it runs
very quickly in SAS.And then I've done
the modeling, and I'mobtaining the variance
inflation factors, whichwe're going to take a look at.So here is the output that
you get for ridge regressionin PROC REG.And so the variance inflation
factors are up above the plot,but I'd like to turn
your attention to,is this one here, with the
standardized coefficientsin it.And you can see that all of
the regression coefficientsare drawn towards zero, but in
particular, the coefficientsfor RunPulse and MaxPulse
are disproportionatelypulled towards zero.So they are the ones exhibiting
a greater degree of change.And in fact, the
coefficient of MaxPulseactually changes sign, from
being positive to around abouthere, around about
ridge parameter0.6, to being negative and then
staying negative thereafter.Certainly, by the time we
reach lambda is equal to 0.8,it looks like all of
the parameter estimateshave stabilized,
and so that mightbe a reasonable value of the
ridge parameter to choose.And here's the
output that we get.And so I come down here
to observation 42 and 43.And so this is corresponding
to the ridge parameterbeing equal to 0.8.And I've highlighted the
coefficients for RunPulseand MaxPulse, and you
can see that both of themare negative, which is
what they should be.And they are much, much
smaller in magnitudethan the original ordinary
least squares estimates,which were of the
order of 0.396--0.369, excuse me-- and 0.3032.So the effect of pulling
in the coefficients usingthe ridge regression seems
to have been rather good.So ridge regression can
be a very effective toolfor stabilizing
regression parametersin the presence of
colinearity, whenthere are modest numbers
of predictor variablesand we don't care
about model selection.We don't want to reduce
the number of variables.But just to reiterate that
point a different way,ridge regression does not
select subsets of variablesor identify important variables.So it's not a model
selection procedure per se.It is a regularization procedure
to obtain sensible regressionparameter estimates.So the least absolute shrinkage
and selection operator,very widely known as
LASSO, is the next topic.The motivation for this actually
came from some earlier workby Leo Breiman, who liked
the idea of ridge regression,but wanted to do some
variable selection.And so he developed
a methodologythat he called the
non-negative garrote.In his paper, Rob Tibshirani
acknowledges Brian's work,and that it gave
him the motivationto develop the LASSO.So in some respects, it
is very, very similarto ridge regression.Step one, we standardize
all the variablesto have mean zero
and variance one,and the intercept
term drops out.As we said for ridge regression,
in ordinary least squares,we minimize the residual
sum of squares with respectto the unknown coefficients,
beta 1 through beta p.For the LASSO, we minimize
a penalized versionof that residual sum of squares.It's lambda times the sum
of the absolute valuesof the coefficients,
not the squares,but the absolute values
of the coefficients.So as with ridge regression,
as the LASSO penalty parameterincreases, the parameter
estimates beta j hatget pulled toward zero.But in contrast with
ridge regression,some of the coefficients
are pulled exactly to zero.And so that's
effectively deselectingthe corresponding variables
from the regression model.One can use the LASSO to select
variables and then carry outordinary least
squares estimation,or to select
variables and shrinkthe coefficients of
the selected variablesto improve the stability of
the coefficient estimates.So there are several
variations on the LASSO now.One of the things that we often
want to do in a regressionis to fit categorical
variables and to put theminto a regression.We create a bunch
of dummy variables,and we would like those
dummy variables to allbe in the model or out of
the model at the same time.And so the group LASSO
allows us to do that,and is actually more general.You can take-- specify
any group of variablesto come in and go
out at the same time.The adaptive LASSO imposes
some additional weightson the LASSO penalty term.And in doing so, it has some
very desirable asymptoticproperties.And so there are
many circumstancesin which people recommend
the adaptive LASSO over justthe ordinary traditional LASSO.So how do we fit LASSO in SAS?The appropriate procedure
is PROC GLMSELECT.And the plots that I want to
show you are of the coefficientestimates as we increase or
decrease the penalty parameter.In the model statement, the
option is selection=LASSO.And then you'll
notice here that Ihave chosen Schwarz's Bayesian
Criterion as being the thingto select the best model.We'll discuss that a little
more a little later on.So if I wanted to get
out of this analysis,the least squares coefficients
from variables selectedby the LASSO, then I add another
piece in the option down here.So in the second
bit of code, yousee LASSO, open
parentheses, LSCOEFFS.And that's saying, print
out the coefficients--least squares coefficients
for the variablesselected by the LASSO.So here is the output.And just reading the table
in the right hand sideand looking at the graphs
on the left hand side,we see that the SBC criterion,
Schwartz's Bayesian Criterion,selects a model which
has got three variables.Those variables are
RunTime, RunPulse, and Age,and therefore Weight,
RestPulse, and MaxPulse are notincluded within the model.So since we know that MaxPulse
and RunPulse are highlycorrelated, including only
one of them in the modelmakes a whole lot of sense.So in this rather congested
slide, on the left hand side,I've got the LASSO
parameter estimatesfor the variables
retained in the model.And of course, they're
slightly shrunk towards zero.And I've got the ordinary least
squares parameter estimates.And across the board, the
LASSO coefficient estimatesare smaller in magnitude, by
which I mean closer to zero,than the ordinary least
squares estimates.The difference is
small for RunTime.The differences between
the ordinary least squaresand LASSO estimates
are a little largerfor our Age and RunPulse.Recall that the ordinary
least squares coefficientfor RunPulse when MaxPulse was
in the model was minus 0.36.And here you can see it is--the ordinary least squares
estimate when there are onlythree variables is
minus 0.13, and whenwe use the LASSO parameter
estimates, minus 0.08.So much smaller in magnitude,
because those parameterestimates for
RunPulse and MaxPulsewere very substantially
inflated by the colinearityof those two variables.We could use cross validation
to select subsets of variables.But I want to point out
that cross validation ismore related to prediction error
and really, in regularization,we're more interested in
getting sensible estimatesof the parameters.And so there is
nothing to guaranteethat cross validation will
give us something particularlysensible.In this particular example,
for the oxygen uptake data,I ran the LASSO
and I used what SAScalls external cross validation,
fivefold cross validation,in order to stop
the LASSO process.And what we find
is that it choosesa model which has got all
six of the variables in.You can see that in the
right hand side here.And that the coefficients
for MaxPulse and RunPulsehave blown up at this point
and are opposite in sign.And so it would seem that
cross validation has notchosen a very sensible model
in terms of regularizationof the parameter estimates.So in this particular case,
the LASSO parameter estimatesare identical to the ordinary
least squares estimates,because all of the
variables are in the model,and I think it's fair to
conclude that in this case,cross validated error
is not a good criterionto use for model selection.No variables were
removed and notenough shrinkage took place.This has been my observation,
is that cross validationand the LASSO do not necessarily
go well with each other,and cross validation
error is notnecessarily a good criterion
for using with the LASSO.I typically use things likes
Schwarz's Bayesian Criterion,or AIC, or the corrected AIC.I'd like to give
you another example.So this is modeling the
quality of white wine,and it was published in 2009
in Decision Support Systems,and the authors used
data mining to lookat the different
physical properties.Response variable in the
quality of wine data setis a measurement
of the wine qualityon a scale from 0 to 10,
with 10 being the highest.The median value of the score of
three experts is what is used,and there are a total of nearly
5,000 observations in the dataset.The predictor variables
are chemical and physicalcharacteristics of the
wine samples, whichinclude pH, density, alcohol
content, as a percentage,chloride, sulfates,
total free sulfurdioxide, citric acid, residual
sugar, and volatile acidity.So in this case, I did
try cross validation.And it turns out,
whether you usecross validation,
or SBC, or AIC,you get essentially
the same thing.So you can see that
I've done selectionis LASSO, and cvmethod is--split is equal to 10,
which means do tenfoldcross validation.And once again, we'll look at
a plot of the coefficients.So the cross validated
error drops off dramaticallywith the addition of
the first two variables.So looking at this bottom plot
here, first two variables,and then it sort
of flattens out.The cross validated
prediction erroris minimized for a model with
nearly all of the variables,so 12 of the 13 variables.Only citric acid
is not in there.If you were to use other
criteria such as AIC and SBC,you would get the same
group of variables.So this is a data set where
there are lots of observation,as it seems like you need
most of the variables.And if I were to show
you the correlationmatrix for the
predictor variables,you would see that
there's not reallyany strong evidence of
pair wise colinearity.So here are the
coefficient estimatesthat we get, and we
can interpret those.So increased quality
is associatedwith the variables which
have positive coefficients.And so that's larger
values of alcohol,for example, and
residual sugar, and pH.Values of pH,
higher values of pH,associated with what
the experts thoughtwere better quality wines.Increased quality is associated
with smaller values of density,of chlorides, and
of volatile acidity.So the LASSO and
variations of LASSOhave quickly become
the standard methodsfor regularization and
variable subset selectionin multiple linear regression.The LASSO combines
the stabilizationby shrinkage of ridge
regression with the abilityto deselect
unimportant variables.LASSO works particularly
well in large problemswith many variables,
the kinds of problemsin which p-value and other
traditional methods may notbe helpful.The LASSO does breakdown
when the number of variablesexceeds the number
of observations.In this case, the LASSO becomes
quickly saturated and stops.But for many
applications, the LASSOis now the standard methodology.When we go into select variables
in a relatively large data set,we don't use selection
is equal to backward,and set a criterion like
alpha is 0.5 or 0.01.Usually, we will
start with the LASSO,and then we'll move from there.So the third of the
shrinkage or penaltymethods for regularization
is the elastic netdue to Zou and Hastie.And it combines ridge
regression and LASSO.The coefficients are
shrunk towards zero.And like LASSO, the elastic
net can deselect variablesby setting their
coefficients to zero.As with LASSO and ridge, we
standardize all the variablesto have mean equal to zero
and variance equal to one.And then we minimize
this residual sumof squares, which is
penalized by the sumof the absolute
coefficients, and--which is the LASSO penalty
term, and by the sumof the coefficient squared,
which is a ridge regressionpenalty term.And so it's really
interesting that youcan get a better result by
combining these two penaltyterms.It's also interesting that the
same least angled regressionalgorithm which you
can use to select the--or to estimate the
parameter estimates,the parameters, the regression
coefficients in the LASSOworks for the elastic net.So here's a particularly
nasty data set for an example.This was used in the original
paper by Zou and Hastie,and it's a SAS example,
in PROC GLMSELECT.And so there are training
and test data sets.The response variable
is the type of leukemia.So we've got type 1
and type 2, and thesehave been coded as minus one
and one for the analysis.There are 38 observations
in the training dataand 34 in the test
data set, and thereare a total of 7,129 predictor
variables, which are genes.When I first started
out as a statistician,the notion of having several
thousand variables, predictorvariables, and only 30 or 40
observations in your trainingdata set was just
laughable, and wecouldn't do anything with it.And so it is a measure of how
far we have come, and how far--how much these modern
machine learning methodshave helped us, that
we can now considerthese kinds of problems
and get sensible answers.So the unresolved issue is
the selection of the penaltyparameters, lambda 1, lambda 2.One approach which
we follow hereis to select a very
small value for lambda 2.So that's the term which
goes with the ridge penalty,the sum of the beta
j squared, and thenuse cross validation
or a validation dataset to estimate lambda 1.That's the penalty
term for LASSO.So we can fit the elastic
net in PROC GLMSELECT.And you can see that I've
specified a training dataset and a validation data
set, which determineswhen to stop the process.And I want to see plots
of the coefficients.Initially, I've
done a LASSO here,just to show you what
happens to LASSO.And then I've done
elastic net, so that'sthe second piece of code.And I've chosen the
second parameter termto be equal to 0.001.And I'm using the
validation datato select when the
process should stop.So here's what happens
with LASSO, OK?After 73 steps, the model
is completely saturated,by which I mean the residual sum
of squares has dropped to zero.At that point, the
LASSO just stops.It can't go any further.The optimal model is selected
by the average squared erroron the validation data
set, occurred at step 68.And you can see that on
the validation data set,the squared error
there was 0.1531.That is indeed the minimum.Looking at the top part of the
plot on the left hand side,you can see, the LASSO effect
as coefficients enter the model,as the penalty
term is decreased.And then the selection model,
the model which is selected,occurs towards the right
hand side of the plot.But beyond that, you can still
see that the coefficients arechanging quite
substantially in some cases,as we add and drop additional
variables from the model.So now we look at the
output for the elastic net.And so you notice that on the
right hand side of the plot,the coefficients
smooth out very much.This is a lot like what we
see in ridge regression.And so there's a great
deal of stabilityin these coefficients.The validation data
set stops after--it says that the optimal value
of asymptotic-- of averagesquared error on the
validation data setis minimized at 105 steps.But you can see that
I've gone out to 250,and that the coefficients are
just sort of smoothly changing.The traces have
sort of smoothed outsomewhere between
150 and 200 steps,and we're not seeing much
in the way of new variablescoming in and out of the model.And when they do, they don't
change the other coefficientsvery much.So other choices of validation,
cross validation, with the dataand other choices of lambda
2 yield very similar results.There's nothing
special about the plotthat I got for the
choice of parametersthat I made in this
particular example.And so I think for
these particular data,the case can easily be made that
the elastic net works betterin terms of identifying
variables whichmight be important in
stabilizing the parameterestimates than does
the LASSO or indeed,any other kind of
variable selection method.So now I'd like to move
into nonlinear methods,moving away from the
multiple linear regressionmodel to other
things, for regressionand for classification.Some of these methods are
direct generalizationsof multiple linear regression.And so included in
that category aregeneralized additive models.Others offer a completely
different approachto nonlinear regression that
allows for the inclusionof high order interactions.This includes all of
the tree-based methods.All of these nonlinear
methods have much higherpredictive accuracy than
traditional linear methodsin certain situations.And all of these methods are
available in various proceduresin SAS.Generalized additive models are
available in PROC GAM and PROCGAMPL.Decision trees, classification
or regression trees,are implemented in PROC HPSPLIT.Random forests is
implemented in PROC HPFOREST.And support vector machines
are implemented in PROC SVM--HPSVM.So I want to give an
introductory exampleand talk about visualizing
relationships usinggeneralized additive models.So a very special
case would be whenwe could assume
that the responsevariable is at least
approximately normallydistributed.And then we have the
additive regression model aslisted here.So this differs from our
ordinary multiple linearregression model in that the
functions s1, s2, up to sp,are not simply coefficients
creating a linear combinationof variables.Instead, they can be continuous,
so smooth, but highly nonlinearfunctions of the individual
predictor variables.And so for example, the kinds
of effects that we might seewould be a quadratic function
or an upside down quadraticfunction.The s's do not have
to be monotonic.We could see threshold
effects, where initiallythe value of the
response increaseswith the predictable
variable, thenit plateaus out at some
point, or the opposite,that it's plateauing at some
point and then suddenly drops.That's another kind
of a threshold effect.These are the kinds of things
that you simply cannot seeand cannot characterize in
an ordinary linear model.So the object of fitting a
generalized additive modelis to try and estimate these
functions, s1, s2, up to sp,using scatter plot smoothers,
so non-parametrically,as opposed to specifying
some parametric form,such as a cubic or a quadratic,
for these particular functions.So I thought I'd illustrate this
by the white wine quality datathat we've seen previously.And the procedure
that I'm using in SASto fit the generalized
additive model to these datais called PROC GAMPL.And I'm going to do scatter
plots of these functions s,so that's what the
plot is asking for.And I'd like to
have separate plots,so I'm going to
unpack them, and thenI can repackage them as I wish.So as it's highlighted
down here, you can see,distribution is equal to normal.It says that for this particular
generalized additive model,we want to assume that
the response variable isapproximately normal
in distribution.And then you see, around
each of the variables whichare predictor variables, which
were included in the modelstatement, I have
this function s.So s, open parenthesis
chlorides, close parenthesis.So the s tells PROC GAMPL
to consider this variablefor a nonlinear fit.Now, one of the nice
things about PROC GAMPLis that it works very
well off the shelf,and in particular, it
can make a determinationas to whether an
effect in a modelshould be just a linear
effect or whether itshould be considered as
having some curvature to it.And so we'll see
that in the output.So I want to make all
of the six variableswhich I'm including in this
model statement here candidatesfor nonlinear estimation.And here are two of
the plots that I got,and I chose these two
because in some sensethey seem to contrast.So the first one, I would
say, is almost linear.And so that is saying
that on average,the quality of the wines
as judged by the expertsseems to increase
or be associatedwith increased
amounts of alcoholin really quite a linear way.I think the thing which
surprised me about this plotwas the range of
alcohol content,from 8% all the way up to 14%.So this is a case where PROC
GAMPL selected fitting a curveto the data, but it's not really
that far from a linear fit.On the other hand, the
free sulfur dioxide,that is a long way
from a linear fit.And in particular, starting at
about 100, on whatever scalefree sulfur dioxide is
measured, the qualityseems to go down quite
rapidly with increased amountsof free sulfur dioxide.And so that seems to
suggest that the wines whichhave too much sulfur
dioxide have a taste thatcontributes an element to the
taste which the experts do notlike.Here's another example.So this is a data set for which
the LASSO works extremely well.There is data on air
pollution and mortalityfor 60 jurisdictions
within the United States.For the most part,
they are large cities.The response variable is the
mortality rate per 100,000in population.The predictor variables
include a bunchof socioeconomic
variables, percentof the population
over the age of 65,average educational level,
percent in good housing,percent minority, percent
below poverty level, and so on.There are some
climatic variables,so average temperature
in January and July,total precipitation,
and average humidity.And then there are
the variables whichwere of most interest, which are
pollution variables, measuresof hydrocarbons, nitrous
oxide, and sulfur dioxide.These variables are
very, very skewed,and so they have been
warped transformedto improve their distributions.So I do an initial fit of
these data in PROC REG,and I want to
explore colinearity.And I find that the
R-squared value that I getis approximately 0.8.That's the measure of
the quality of fits,and that's pretty good
but not outstandingfor these kinds of data.I put as x in front of each
of the predictor variables,and that allows me to use
this very compact notation.So the x: means all
of the variableswhich begin with x,
and of course, that'sall of our predictor variables.And so here are the
colinearity diagnostics,and the parameter
estimates, and so on.And so as I look
down here, I seemost of the variance
inflation factorsare not too bad until
I get to the last two,for the log of hydrocarbons
and the log of nitrous oxide,and they're up around 20.And if I come over to
the condition indices,they're sort of
gradually increasing,but then seems to start jumping
after about 12 variablesto 103, 133, 147, and then
a huge jump up to 452.So it's evident that
there's some colinearitywithin these data, and we
would like to fix that,but I would also like to
know, is the relationshipbetween the individual predictor
variables and the responsevariable linear, or would
it be better characterizedusing some nonlinear functions?So here is a scatter
plot of the relationshipbetween the log of
the hydrocarbonsand the log of
the nitrous oxide.And we can see that that's
very close to linear,with a correlation nearly
0.95, so clear evidenceof colinearity.In the previous slide--so we could go back.We would see that
most of the variableswere non-significant,
with high p-values.And so this is a
data set that wecan use the LASSO to
help us out and selectsome appropriate variables.And that's what I do.I'm using PROC GLMSELECT again.Selection is LASSO,
and I'm goingto use Schwarz's Bayesian
Criterion once moreas a stopping criterion.And this is the
output that I get.So the number of variables which
are selected by the LASSO usingthe SBC criterion is eight.And so that includes a
mixture of variables,such as non-white, percentage
of non-white peoplein the district, population
density, with nitrous oxide,and log of sulfur dioxide.And then some weather
variables, precipitationand the average
temperature in January.And there are a lot of variables
which have been removed.So there were 21 predictor
variables originally,and now there are eight.So 13 have been removed.If I look at how well
this model fits the data,up at the top right hand
table, the R-squared for thisis 0.746.If you remember, for the
multiple linear regressionmodel with all of the predictor
variables, it was about 0.8.So actually, the predictive
accuracy of this modelhas not been decreased much
by removing eight variables.And you can see in
the plot, at the top,at the left, the effect
of the colinearitywith the hydrocarbons.So as soon as both of those
variables are in the model,their coefficients go soaring
off positive and negative.And so you can see the negative
effects of colinearity there.So we can interpret the
coefficients and parameterestimates on the
right hand side.The mortality rate
is lower in placeswhich have a higher
average educational leveland which have a higher
percentage of good housing.And mortality rates are higher
with higher population densityand with higher precipitation,
interestingly enough.The variables sulfur
dioxide and nitrous oxidecome in with positive
coefficients.So high values of
these pollutantslead to higher
values of mortality.Perhaps the one that I
thought was most interestingwas that the average
temperature in Januaryhas got a negative coefficient.So the warmer it is in January,
the lower the mortality rate,and that's suggesting
that the warmerstates in the South
and the Southwesthave got lower
mortality rates relatedto the average temperature.And as someone who doesn't
like shoveling snow,I can certainly sympathize with
that particular conclusion.So now I want to take
the same data set.I've done some variable
selection using the LASSO,and it worked out, I think
we would agree, very wellon that particular data set.And what I would
like to do is takesome of the most important
variables, not all of thembut some of them, and use
PROC GAMPL to determinewhether the effect should
come in as being linear,or whether there
is some curvature,and what the nature of
the nonlinear relationshipmight be.And so you can see that I am
fitting splines as my smoother,and I get to choose maximum
degrees of freedom thatcan be considered.And rather than
the default value,I've chosen the relatively
large value of six.And then a PROC
GAMPL will go aheadand select as many
terms as needs,up to this maximum
degrees of freedom.And this is what we get.So these are the six plots of
the estimated spline functionsfor those six variables,
which I included.And if we look at education
and precipitation,they both seem to be
coming in as linear effectswith opposite
signs, so decreasingwith increased education--
mortality decreasingwith increased education,
and mortality increasingwith increased
precipitation, and they'recoming in as linear.And then I would
say that if we lookat the percentage of minorities,
so non-white, and the logof nitrous oxide, then
these two are prettyclose to being linear effects.PROC GAMPL selected
to fit curves to them,but if I look at the
confidence limits here,I think I can probably fit
a line through each of them.And so they are very close
to being linear effects.For the average
January temperature,that's a decreasing function
and it's kind of wiggly.But even that, that is not
too far from a linear effect.And for population density,
I think one can almostfit the zero line through that.So there are some data
sets for which there arevery clear non-linear effects.And PROC GAMPL is
good at identifyingthe variables for which the
relationship with the responseis nonlinear, and
then estimatingthe nature of the response.But there are many data sets
for which the relationships areat least approximately linear,
and fitting a much morecomplicated
predictive proceduressuch as generalized
additive model reallyresults in no benefit at all.And this is one of
those data sets.So this is a data set in
which the relationshipbetween the predictor variables
and the response variableare close to linear, and no
additional interpretationor predictive accuracy
is gained by fittinga generalized additive model.In many other applications, we
do see some gain in accuracyby fitting a generalized
additive model.Next topic that
I'd like to coveris that of classification
and regression trees,sometimes called decision trees.This group of
methods, I have beenusing since the
early 1990s, and Ihave found them to be
helpful in understanding dataand in predicting both
categorical and continuousresponse variables in many
different kinds of situations.And so when I get a
new data set and Iknow I'm going to be doing
classification or regression,one of the first methods
that I turn to is a tree.So regression and classification
trees are fully non-parametric.There are no
distributional assumptionson either the predictor
or the response variable.They naturally segment the data
into increasingly homogeneousgroups.Small trees are
easily interpreted.And so when I introduce
this topic to my classesand in other talks,
I often talk about,imagine that you are
going into a bar,and you meet someone that
you've never met before,and you want to explain
something about what you doand what kind of analysis
you've carried out.Well, I can tell you that
explaining a tree, which is--divide a data set
into smaller pieces,is a much easier
thing than tryingto explain logistic
regression or evenmultiple linear regression.Trees can handle complex
high order interactions.We're going to see that.They effectively
handle missing valuesthrough surrogate splits.Many statistical
procedures reallystruggle with missing values.You either have to
impute data or youhave to eliminate all of
the observations whichhave got some missing values
in some of the predictorvariables.You don't have to
do that with trees.And then in some
situations, we findthat trees are accurate
predictors of numericand classification variables.In the classification
case, sometimes treesare about the same accuracy
as logistic regression,and sometimes they
are more accuratethan logistic regression.And again, we'll see an example.So how do trees work?They work by
recursive partitioningof the space spanned by
the predictor variablesinto increasingly
homogeneous groups,with respect to numerical
or categorical variablethat is being predicted.So this schematic to
the right is supposedto illustrate the process.The two axes represent
two predictor variables,and then the
different rectanglesare subsets of the observations.And so the first split, I
can tell from this graph,occurs just before
the value sevenon the variable
which is representedon the horizontal axis.And I know this
is the first splitbecause it goes all the way up
through the rectangle, whichrepresents the
entire data space.So that was the first split.And then the second
split occurredon the second variable, which
is on the y-axis, at a valueabout somewhere between
six and 1/2 and seven.It could have been the
rectangle to the left thatwas created by the first
split on the variablein the horizontal
axis, or it couldbe the horizontal line
on the second rectangle,the right hand rectangle.It doesn't really matter.And then the
partitioning continues,and the bigger rectangles
are generally subdividedinto much smaller rectangles.The numbers that you see
in each of these rectanglesrepresent the number
of observations.Firstly, they're not equal,
not even close to being equal.And secondly, they're not
proportional to the sizeof the rectangle.So at the bottom
left corner, we havea relatively large rectangle
with only 22 observations.And near the middle of
the plot, at the bottom,we have a tall, thin rectangle
which has got 33 observations,and certainly hasn't as
much area as that bottomleft rectangle.So why do we need
such a methodology?It seems very strange
to be subdividing data.It's sort of like a digital
approach to statisticswhen we're used to doing
linear and smooth things.And to partly motivate
it, take a lookat the graph on the left
hand side of this slide here.So there are two
predictor variables,which are labeled
x1 and x2, whichare here on the horizontal
and vertical axis.And this is a
classification problem.And there are two classes
coded as zero and one.In this plot here, you
can see that the ones arein the first and third
quadrants and the zerosare in the second
and fourth quadrants.If you try and apply--this is a standard problem
in computer science.It's a test problem
for new techniques,and we picked it
up in statistics.It's quite a useful way
of looking at things.So linear methods such
as logistic regressionor linear descriptive
analysis do incredibly poorlyin this problem in
terms of prediction,whereas classification trees
which are subdividing upthe space, you can kind of see
that it would work well here.And indeed, classification
trees and methodsrelated to classification
trees do extremely well.Now, I'm not suggesting
to you that thereare real data sets out there
which look like the XOR problemhere, but what I do
maintain, and whichI think we see evidence of
in one of the later examples,is that there are real
data sets out therein which the boundaries
between the different classesare really complicated, and for
which linear methods are notgoing to work so well.So to illustrate the efficacy
of trees over other methods,I want to start with an example
about credit card applicationdata.And so they concern
credit card applicationsto a bank in Australia,
and the data were firstpublished by Quinlan in 1987.The response variable
is coded as yesif the application for the
credit card was approved,and no if it was not.There are 15 predictor
variables, whichare denoted by A1 through A15.Some of those are categorical
and some numerical.For proprietary reasons,
the nature of the variablesis not available to us.We note that
variables A9 and A10are coded as t and f, which we
take to mean true and false.There are a total of 666
observations with no missingvalues, and of
those, 299 personswere approved for credit
cards and 367 were not.And so the split between
approved and not approvedis actually pretty
even in these data,and it's got a mixture of
categorical and continuouspredictor variables.And so it's quite an
interesting data set.So as a preliminary analysis,
I applied a standard technique,logistic regression.So in the class statement, I
have the categorical variablesA1, A4 through 7,
9, 10, 12, and 13.And then in the model statement,
I've got A1 through A15,so the variables which are
not in the class statementare continuous.I've chosen to do some backwards
selection of the variablesto eliminate
unimportant variables,but I'm going to do
that using backwardelimination with a significance
level to stay of 0.05.We could use LASSO, we
could use group LASSO,we could use other
methods, but Ichose to just take a
simple solution here.The ctable statement
within PROC LOGISTICproduces a classification
table for a particular cutoffor a group of cutoffs,
and part of that tableare some of the usual
metrics for whatwe use for evaluating the
accuracy of a classification,sensitivity, specificity,
percent correct, and so on.So given that there is
a relatively equal splitbetween approvals
and non approvals,I've chosen to use a
standard cutoff of 0.5 here.The ROC curve is a
good way of summarizingthe predictive accuracy
of a classification,a binary classification,
in a graphical way.And so I always
request the ROC curvewhen I am carrying out a binary
classification, as we are here.And here's some of the
output from PROC LOGISTIC.So let's start on the right
hand side with the ROC plot.Each time we remove a variable,
it recomputes the ROC curve,and we see that when it
removes eight total variables,all of those ROC curves
are practically overlaid.You can scarcely tell there are
eight different curves there.And so that suggests that
we lost absolutely nothingin explanatory power
or predictive accuracyby eliminating those eight
variables from the model.Indeed, the AUC criterion
goes from 0.9506for the model with all the
variables down to only 0.9463after we eliminate
eight variables.So there's no question that
removing those variables isa good thing to do.It simplifies the
model but doesn'taffect the fit or the
predictive power of the model.Now, in terms of accuracy,
the overall percent correctis 87.4%.So that means the
error rate is 12.6%.That's a number
worth rememberingfor a future discussion.The sensitivity
was a smidge higherthan the specificity,
90.7% versus 84.8%.Overall, that's a pretty good
fit, we would have to say.So now, the real question is,
what of the important variablesand how are they
affecting the response?And so to answer
that question, wemight look at the logistic
regression coefficients,and here they are.And the question
that I pose hereis, what can we learn from
these parameter estimates?And there are so many of them.And really, without some getting
down in amongst the weeds,it's very hard to
conclude anythingabout the effects of different
variables from this output.So now I want to fit
a classification treeto these data.And the procedure within SAS
for fitting classification treesis HPSPLIT.It's a relatively new
procedure within SAS/STAT,and it has been updated,
and it is very nice.So I want to evaluate the
accuracy of the classificationusing cross validation.And you can see here that I've
specified cvmethod=random(10),and that says, please do
tenfold cross validation.The only plot I want from
this initial step, whichis determining how big a tree
we should fit to the data,is this plot which
is labeled cvcc,and that is a plot of
cross validated areaas a function of the
size of the tree.So that allows us to choose
an appropriate tree size.The criterion by which
I want to measurehow consistent the
observations in the group areis the Gini index, and so I
specify that with grow gini.The default with an
HPSPLIT split is entropy.If you try entropy
and then you try Gini,you'll often come up with
very, very similar answers.So there's not much to
choose between these things.I just happened to select Gini.And here is the
plot that we get.And it turns out that the
minimum cross validated erroroccurs at a tree with five
terminal nodes, five leaves.These are the
subgroups of the datawhen the process is stopped.That's a pretty small tree.That's a very easy
tree to interpret.Breiman et al.'s
1-SE rule says, youdon't choose the
one for which you'vegot the minimum cross
validated error.You choose the smallest tree for
which the cross validated erroris less than or equal
to the minimum plus onestandard error.And that's a much smaller tree.That has only got two
terminal modes or two leaves.So that's really
saying, in some sense,the best tree is just
subdividing the datainto two pieces, and
that just seems kindof insane and inconceivable.Well, here's what
the tree looks like.So in the full data set,
there are 299 applicationsthat were approved.That's 45%.And there were
367 that were not,and that's the information
that you get in the node whichis labeled zero.And so the majority case is
one, which is really zero,so that is the ones
which were not approved.Then we carry out a
split on the variable A9,and 352 observations go--have t as being the
level of variable A9,and 314 have f as being
the level of variable A9.If we look at the
observations for which twas the value in variable
A9, then almost 80% of themwere approvals for
the credit card.If we look at the 314
observations, for which fwas the value in variable
A9, then almost 94% of thosewere not approved for credit.Now, we don't know what
the variable A9 is or was.I suspect it was
something like, have youhad a credit card before?And true would be yes, so
those people are peoplewho have already developed
a credit history,and would be good candidates
for a new credit card.And then those
people who've neverhad a credit card,
the bank mightdecide to be conservative
and say, you know,we're not willing to take a
risk on most of those people.It could be something
else, but that's my guessat what variable A9 is.What I'd really
love to point outis that this tree just split
the data set into two pieces,and from that, we were able to
get a pretty good idea of whatwas going on.Can we do much better in terms
of cross validated error rate?For that tree with
just two leaves,the cross validated
error is 13.74%.Remember that for the logistic
regression with seven or eightvariables, it was 12.6%.So that's not a great
deal of differencefor a very much simpler model,
just two subgroups of the data.The cross validated error,
if we increase the treeto have five leaves,
actually increases to 14.36%.And for 10 leaves, it's
about the same, at 14.44%.I applied to state
of the art classifierto these data called
random forest,and I found, about the best
you could do was 12 and 1/2%error rate, about
the same as youget for the logistic
regression, and only a tiny bitless than this very,
very simple tree.So pretty much all the
information in this dataset about whether an
application will be approvedor not is actually
in this variable A9,and it partitions the
data into just two piecesof approximately equal size.I think that's a
spectacular demonstrationof how well the tree worked.Now, we've talked about
the wine quality dataand we applied both the
LASSO and then generalizedadditive models to these data.And now I want to
apply a regression treeto the data set.Remember, the
response variable isthe quality of the wine sample
as judged by three experts.We take the median value.It's on a scale of 0 to 10, with
10 being the highest quality,but the vast majority of
values are in the mid range.There are nearly 5,000
observations here.The predictor variables
include chemical and physicalcharacteristics of the wine,
including the pH, density,alcohol content, chlorides,
sulfates, total and free sulfurdioxide, citric acid, residual
sugar, and volatile acidity.And so I get my cost
complexity plot,which is plot is of the cross
validated average squared erroron the vertical axis,
against the size of the tree.The minimum cross
validated erroroccurs at a large
tree with 57 leaves.However, the 1-SE rule
of Breiman et al.--so that's choosing
the tree whichhas cross validated error less
than the minimum plus one SE--that occurs at a tree
with just five terminalnodes, five leaves.And so that's a very small,
very interpretable tree.So now I'm going to refit a
tree with just five leaves.And so the way I do that is
prune Costcomplexity Leaves=5.You'll notice that I've circled
this intervalbins=10000 here.The original algorithm for
classification and regressiontrees said that you should
sort all the values of allthe predictor values
into ascending order.And then you would
choose as candidate cutpoints all of the
midpoints betweenconsecutive observations.The algorithm implemented in
SAS doesn't quite do that.It takes the range of
each predictor variableand splits it up into
100 bins of equal widths.I wanted to get a little closer
to the original algorithm,and one way of doing that
is to choose interval binsis equal to a very large number,
and then most of those binswill only contain a
single observation.And the results should
be very similar to whatyou would get using the original
classification tree algorithm.I'm using cost
complexity, so I'mgoing to prune back to
a tree with five leaves,because that's what the 1-SE
rule tells me I should do.And here is the tree.And it may be hard
to read these values.I'm sorry, I don't know how
to enlarge those values.But at the root node, there
are nearly 5,000 observations,and the average quality
score is about 5.88.The first split is on
alcohol at a value of 10.801.For the 3,085 wines with
alcohol less than 10.8%,the average quality
score is 5.6.Whereas for the 1,813
wines with alcoholgreater than or equal to
10.8, the average scoreis a little higher, at 6.34.For the wines with alcohol
content less than 10.8%,the next split is
on volatile acidity,at a value of about 0.25.The 1,475 wines with volatile
acidity less than 0.25had an average score of 5.8725.And the 1,610 wines with
volatile acidity greaterthan or equal to 0.25 had a
slightly lower average scoreof 5.3.This is consistent with a
negative regression coefficientfor volatile acidity.The second split
for the wines occurswith alcohol greater
than or equal to 10.8%is on free sulfur dioxide
at a value of 11.012.This is much less
interesting, because only 114out of the 1,813
observations end upin the node, the terminal
node or leaf correspondingto free sulfur dioxide,
less than 11.02.How well does the
tree fit the data,compared with the regression
models that we had earlier?If we use a tree with
just five terminal nodes,five leaves, then the cross
validated prediction errorsum of squares is 0.5892.And that compares
reasonably well with whatwe got for using the LASSO with
multiple linear regression,where the CVPRESS was 0.5679.If I were to fit a
much bigger tree,so getting closer to the minimum
cross validated error, a treewith, say, 57 leaves,
then I can decreasethe cross validated
prediction error to 0.5485,which is better than anything
we got from linear regression.There is a cost to
that improved accuracy,and that is you get a tree which
has got 57 leaves or terminalnodes, and that's a much
harder tree to interpret.I would like to do one more
example on classificationand regression trees.This is another
bank example, and ithas some elements in common
with the earlier example wedid on credit card
approval, but there are alsosome interesting differences in
the analysis of this data set,and I think it's
worth going through.So I actually want to show
you how I would analyzethese data using SAS OnDemand.I'll introduce the data
with a couple of slides,and then we'll switch
to SAS OnDemandto actually conduct
the analysis.So there was a direct
marketing campaign by a bank,and what they were
trying to do wasto encourage their
customers to take outterm deposits with them.And the campaign was a mixture
of direct contact by telephonecalls, and by direct mail,
and simply advertising on TV,and other aspects.The response variable
in this data setis coded as yes if the
customer did subscribeto a term deposit, did make a
term deposit, and no otherwise.There are 45,211 observations,
so that's quite a big data set.When I first saw
that, I thought,boy, this is going to challenge
some of the techniques,especially if we're going to
do tenfold cross validation.There are 16 predictor
variables in the data set.Some are categorical
and some are numerical.And here is a list of
those predictor variables.So the age of the
customer, the typeof job he or she has,
marital status, education,does the customer have--is the customer currently in
default on some credit line,average yearly balance,
presumably in all accounts,in euros, does the customer
have a housing loan--so I'm thinking that's
probably a mortgage--Does the customer have a
personal loan, the last contacttype, the day of the
last month of contact--I can't see why that
would be relevant,but I could be surprised--and the last month of contact--that could be interesting.There could be some months
which work better than others--duration of the last
contact, telephone contact--this is in seconds--The number of contacts with the
client in this campaign, numberof days passed
since the customerwas contacted in
the last campaignto get people to take
out term deposits,the number of contacts
with this clientprevious to this
campaign, and the outcomeof the previous campaign.So success is that
the customer took outa term deposit and failure
is that the customer did not.OK, so that's the
setup, and now let'sskip to the analysis in
SAS using SAS OnDemand.I have set this up
in SAS OnDemand,so I've already
written the code.And let's take a quick
look at the data set.So in particular, I want to
look at the outcome variable,which is yes if they took out
a term deposit and no if theydidn't.I suspect that many
more people wouldsay no than would
say yes, and so let'sactually run that and see.And yes, the answer is that
only 11.7% of the customersdid take out a term deposit.And so that means
over 88% did not.This number here, this 11.7%,
I want to keep this in memorybecause we're going to
come back and refer to it.There's a huge imbalance
in the data here.So there are almost eight
times as many peoplewho did not take out a
term deposit, who did.And this has some profound
implications for prediction.Usually, we find that the
class which is most commonis predicted very accurately and
the class which is less commonmay be predicted quite poorly.So given that it's
a large data set,it makes perfect
sense to partition itinto training and test
pieces, so that wecan get a very objective
estimate of howwell the predictions are doing.And so I'm using PROC
HPSAMPLE to do that.And so I specify all the
variables in PROC HPSAMPLE.In the class statement, we
put the categorical variables,and in the var statement,
the continuous variables.I'm then going to take the--oh, in the sampling, I'm going
to choose 70% for the trainingdata and 30% for the test data.And the way that HPSAMPLE
does this is that it createsa variable called _PARTIND_,
and there are two values,zero and one.It'll be-- 30% of the data
will have value one on PARTIND,and 70% will have value zero.And so we can split off
those two data sets,and you can see that
I have done that here.I've created a data set called
BankTrain, which is the 70%,and another data set called
BankTest, which is the 30%.So I will run this.And it's made the split.And so the 30% corresponds to
13,563 of the original 45,211observations.If I go to the log
file, I can seethat the data said BankTest has
got these 13,563 observationsand the data set BankTrain
has got 31,648 observations.So that's exactly
what we hoped for.Well, the next
step in my analysisis to fit a classification
tree and to try and determinethe appropriate
size of the tree.And so that's what the
next piece of code does.Exactly as it was in
previous examples,in the first pass
through, I justwant to find out what the
appropriate size should be.And so that requires
this CVCC plot,which is a plot of the
cross validated erroragainst the size of the tree.And in the class statement, I
put the categorical variables,in the model statement,
all of the variables.And then I'm going to use the
Gini index to grow the tree.So we run it, and it's going
to take a while because we'vegot training data set of
35,000 and we're doing tenfoldcross validation.And so that's going to be 10
data sets of about 31,000,and it just simply
takes a bit of timeto do these calculations.OK, and there it is.So that took a couple
of minutes, I think.And here are the results.This is the item that we want
to look at, the cost complexityplot.What you see is the minimum
average misclassificationrate occurs for a tree with
73 terminal nodes or leaves.That's actually a
very large tree.But if I apply Breiman
et al.'s 1-SE rule,that selects a
much smaller tree.What I do is I look
for the smallesttree for which the cross
validated area is lessthan the minimum plus
one standard error,and that's probably
five or maybesix in terms of tree size.And for future analysis, I chose
a tree with six terminal nodes.So returning to the code, this
is-- this next block of codehere is fitting a tree
with six terminal nodes.So I'm going to grow the
tree using the Gini index,as before.But now I prune using
cost complexity,and specify that there will be
six leaves or terminal nodes.The next statement with code,
file is equal to, is placing--is creating a
destination for the codethat we can use for
scoring new datasets, and in particular,
the test dataset for this analysis.So let's run the code, fit
the tree with six leavesor terminal nodes.That too is going to take
a little while becauseof the tenfold cross validation
to evaluate the error,but not that long.And what we find
amongst the summarystatistics is that the
overall cross validated errorrate is about 10%.So that means 90% is
correct classification rate.The specificity is fantastic.So that's for the no
category, and that'sat nearly 97 and 1/2%.But the sensitivity
is really, really bad.It's down at about
1/3, and really, Ithink that's completely
unacceptable.So in terms of
looking at the tree,we can certainly
interpret the node--nodes in the tree.And we'll notice
that the first splitis on a variable duration.So this is the duration
of the phone call.And the cut point takes
place at 540 seconds,which is about nine minutes.If I come down the right
hand side of this tree,then the next split
is also on duration.And so there is a group here for
which the duration of the callwas greater than or equal
to 836 seconds, whichis about 14 minutes.And that group of people, which
is quite a large number, 1,228,the majority of them,
so more than 1/2,elected to take
out a term deposit.And so the phone call was
a very good indicator.The length of the phone call
was a very good indicatoras to whether the person
takes out a term deposit.Presumably, that's
because the customeris discussing with the agent
over details about the termdeposit and making arrangements
to come in and signany paperwork that's necessary.If we take the left
hand branch here,so this would be the
duration of the call,between 540 seconds
and 836 seconds.But then we take the right
hand side down here--so p-outcome is the outcome
of the previous campaign.And so if that outcome
was successful,so if the customer had
previously opened a termdeposit, then the
customer is much morelikely to be willing
to do that again.The probability or
percentage is 0.83 here.But this is only a
tiny group of people,whereas the group up here
in duration greater than 836was 1,228 people.This group of people
here is only 83.OK, so here is the RSE
curve for these datawhen you've got such a small
number of terminal nodes.Then it tends to look this sort
of piecewise linear effect.So what we'd like to do now is
to predict onto the test dataset, to get a completely
unbiased versionof the accuracy
of our prediction.And I've chosen to
do it in two ways.Firstly, I'm going to use a 0.5
cutoff, the traditional cutoff,for the probability of
the outcome being yes.And then I'm going to
use a smaller cutoff.So this is a value of
something like 0.117,which is the number that
we saw earlier as beingthe proportion of the data
set for which the outcome isequal to yes.And then once I
get the nodes, I'mgoing to construct the confusion
matrices on the test data.So OK, so let's run
this piece of code,going from here all
the way down to here.OK, so predicting
onto the test dataset using the usual
cutoff of 0.5,we see that the
sensitivity is 34.29%.That's very poor.So that's the same as we saw
with the cross validation,whereas the
sensitivity, so that'sthe proportion of noes
who are predicted as no,is about 97 and 1/2%.If we use a cutoff of
0.117, then the situationis much better.So the sensitivity is now
over 50%, at nearly 55%,and that's entailed a small
loss in the sensitivity,from 97% down to 92%.I still would like to do
better with the sensitivity,and I'm wondering what
one could do to do better.And so it occurred
to me that I shouldlook at the predicted
values that are coming outfrom the decision tree.Because there are only six
terminal nodes or leaves, thatmeans that there
are only going to besix predicted values, and here
are those predicted values.So by changing the
cutoff from 0.5 to 0.117,I put these 842 observations
and 85 observationsinto the predicted outcome
is equal to yes category,but that's still a tiny number
compared to the 11,807 whichhave got a predicted
value of 0.6,and therefore are predicted no.And so as I think
about this, the fixhas to be to break up
that group into smallerpieces with different
predicted values.And the way that
we can do that isby fitting a tree which has got
more terminal nodes or leaves.And so that's the next piece
of code that I have down here,is I'm going to fit a tree
with 22 terminal nodes,and everything else is the
same as for the tree with sixterminal nodes.And then I'm going to create
the two confusion matrices usinga cutoff 0.5 and 0.117.And it worked really
quickly, once again.So once you prune the trees,
even with the cross validation,the fitting process
is very, very quick.So you can see in
this graph herethat all of the nodes which are
terminal nodes, which are pink,are ones where the outcome
equal yes was in the majority,and all the blue
ones are outcomeis equal to no in the majority.And I come down here to
the summary statistics,the cross validated
error rate is 9.63%,which is actually a little
less than for the tree with sixterminal nodes.But the sensitivity is
still terrible at 44%.Then the specificity is
still very high, at 96%.When you add more
terminal nodes,the ROC curve is going
to look smoother,and that's what we see here.And now we can look at
the confusion matricesfor predicting
onto the test data.And if we use the
usual cutoff of 0.5,even with a tree with
22 terminal nodes,the results are pretty terrible
in terms of the sensitivity.That's 40%, so we're only
correctly predicting the yesesat a rate of 40%, and
that's absolutely terrible.But if we change
the cutoff to 0.117,well, that makes
a huge difference.Now I see that we're correctly
predicting over 80 and 1/2%of the outcome is equal to yes,
and about 81.7% of the outcomeis equal to no.So the sensitivity and
specificity are almost the samehere.I think most of us would regard
this as being a much moresatisfactory result.
It's been carried out--the improvement in predicting
outcome yes has been--taken a toll on the accuracy
of the prediction of no.So this number here has
dropped from about 97%down to 81%, but then the two--sensitivity and specificity
are about the same.I'm sure that the
bank is much moreinterested in the
customers who said yesand the characteristics
of those customers,and they may want to further
change the cutoff so that theycould increase this number,
the accuracy of predictionof the yeses, even
more than for the noes.OK, so that's the analysis
that I carried out in SAS.And as you can see, it
didn't take very longto get some sensible
information out of the analysis.So returning to the
PowerPoint slides now,I have a summary here
of what we've just beentalking about in the slides.There are almost eight times
as many customers with outcomeas no as outcome as yes.And so we'd say the
data are imbalanced.This is a fairly common
problem in many applications.Certainly, in the
ecological statisticsthat I am very familiar with,
I encounter imbalanced dataalmost all the time.Most machine
learning classifierswill predict the more abundant
category very accuratelyand the less abundant
category much less accurately.We saw that, right?So the noes were predicted very
accurately, perhaps 97% or morecorrectly predicted, whereas
the less abundant category,the yeses, the sensitivity
was less than 0.5,and some cases, 0.33 and 0.4.So there are several
ways of fixing imbalance.One is to up sample the
less common classes,and you could use PROC
HPSAMPLE to help you with that.Another method is to down
sample the more common classes.By far, the simplest method
is to adjust the probabilitycutoff for the less
abundant class,and that's exactly what we
did when we fitted a treewith a few more terminal nodes.That seemed to work
very, very well.Here is the code for
partitioning the data setinto the 70% and 30% pieces.So in PROC HPSAMPLE, I had
specified partition, and thensamp percent to be 30, to
split it into a 70-30 split.And then I specified
all of the variables.And then the next piece of
code is splitting the datainto the training and
test pieces on the basisof the variable _PARTIND_ that
is created by PROC HPSAMPLE.And in fitting the
classification tree,you saw this code in the live
session and here it is again.I used cross validation,
tenfold cross validation.And initially, I
just want this plotof the cross
validated error rateagainst the size of
the tree, in orderto determine what an
appropriate size would be.And I used the Gini
index as a measureof homogeneity of the
observations in the leaves.And here's our plot.And we selected a
tree with six leaves.The 1-SE rule would say
five leaves perhaps,but six is only one more
leaf, and it's stilla very small and
interpretable tree.And the overall accuracy
was 90%, but as we saw,the sensitivity was terrible.And so that's when we
decided to change the cutoff,and that improved things
a bit but not enoughwith the tree with six leaves.So then we fit a
tree with 22 leaves,and for the cutoff of 0.117,
we got this confusion matrix,with the percent correct
sensitivity and specificityall about the
same, at about 81%.And so this seemed
to be much betterthan the previous
results that we got usingthe traditional cutoff of 0.5.So the reason I did this
example was twofold.Firstly, it's actually
a large data set,and so things take time,
and it can be a challengeto analyze larger data sets.The second reason was I wanted
to discuss the imbalance issueand how one might fix it.Of course, the simplest way
is just to adjust the cutoff,and that seemed to
work quite well here.I've also, in other
applications, used up sampling.I've never had good
success with down sampling.Usually, I get much
less accurate estimatesof the sensitivity
and the specificity.So in terms of the
interpretation,we talked a little
bit about this.If the phone call,
the last phone call,took more than about
14 minutes, thatwas a good sign for the bank in
terms of the customer openinga term deposit.And also, if the duration
was between 9 and 14 minutes,and the previous outcome was
that the customer made a termdeposit, that was also
a positive outcome.So finally, some conclusions
about classificationand regression trees.They are a completely different,
highly nonlinear methodfor predicting interval-valued
and categorical responsevariables.They work by recursive
partitioning of the datainto increasingly
homogeneous subgroups.The predictive accuracy of
classification and regressiontrees is comparable to,
and in some cases greater,than multiple linear regression
and logistic regression.And the fitted trees,
especially the small ones,are very easy to interpret
and can reveal structurein the data that is
not well characterizedby other linear methods.The example that
I would point tois the credit card
application data,where a single split of
the data into two subsetswas as good as
logistic regressionand much easier to understand.So random forests is
an absolutely stateof the art machine learning,
statistical learning,methodology that was originally
due to Leo Breiman in 2001.As the name suggests,
what it doesis combine predictions from many
classification or regressiontrees to construct a
more accurate predictionfor each observation.Random forest's origins
lie in some observationsmade by Breiman about
classification and regressiontrees.He noted that the trees were
very unstable with respectto the data.Small perturbations in
the data could yieldcompletely different trees.And secondly, he
was disappointedin the accuracy of
classification and regressiontrees.In some problems,
they work very wellbut in many other problems,
the accuracy of treeswas not substantially better
than that from linear methods,such as multiple
linear regressionand logistic regression.So the algorithm, the basic
algorithm, for random forestsgoes as follows.Many random samples are drawn
from the original data set.The observations in
the original dataset that are not part
of the random sampleare said to be out-of-bag
for that sample.To each sample,
a regression treeis fit without any pruning.So it's a fully grown tree
and it does indeed overfit the data.The fitted tree is then
used to make predictionsfor all the observations that
are out-of-bag for the sampletree, for the sample
that the tree is fit to.This is very much the
cross validation idea.You fit on some
portion of the dataand you predict on
the other portion.In the case of random
forests, the other portionare the observations which are
not part of the random sample.For a given observation,
the predictionsfrom the trees on
all of the samplesfor which the observation are
out-of-bag are then combined.In regression, this
is accomplishedby averaging the
out-of-bag predictions.In classification, this
is accomplished by votingthe out-of-bag predictions,
so that if class A had 50--if there were 50 predictions
of class A and 40 of class B,then we would choose
class A. Class Awould be the prediction.The devil is in the
details, so they say,and certainly, there are a lot
of details in random forests.The number of samples to
draw from the original datamay be determined by the user.The default in PROC
HPFOREST in SAS is 100.And in the vast
majority of problems,the error rate does not change
much if more samples are drawnand more trees are fit.Samples are drawn
without replacement,and the size of the
samples as a proportionof the size of the data set
may be selected by the user.In PROC HPFOREST,
the default is 0.6,and that turns out to
be a pretty good value.In the few times that I
have tried other values,0.6 or something close
to 0.6 has alwaysturned out to give the
most accurate predictions.The number of variables
available to besplit on at each node
in a tree may alsobe determined by the user.For regression, the default
is 1/3 of the total numberof predictor variables.And for classification,
the defaultis the square root of the number
of predictor variables, whichcan be a very small
number comparedto the total number of
predictor variables.The reason for restricting
the number of variablesthat are available
to split on isthat it ensures that the trees
fit to the different data setsare very different
from each other,and it's in those situations
that one derives benefitfrom combining the
predictions of many trees.So I'd like to illustrate
the use of random forestto you, combined
with other methodsthat we've talked about, with
an example from my research,and this concerns invasive
species in Lava Beds NationalMonument.The Klamath network
of national parksand national monuments
in northern Californiaand southern Oregon had
identified invasive speciesas being its most
pressing problem,and they were looking for
research help in this area.It has been noted before
in other publicationsthat the US spends
perhaps 40 billion a yeardealing with invasive species.And so it is a
financial concern,and it's very much of
ecological concern,because invasive plants and
animal species can oftendisplace the native
plants and animal species.We chose Lava Beds
National Monumentto illustrate some methodology
because they had the best data.The most common invasive species
in Lava Beds National Monumentwas something called
common mullein.That is the picture on the
left, and its scientific nameis Verbascum thapsus.So that was a primary interest.Park personnel, when they
detected growths off mullein,would go out there with bottles
of Roundup, and exterminate it.But it can take several years of
applying Roundup to fully killa patch, and at that--while that's going
on, the mulleinmay be coming in in
different places in the park,and they wanted to
know where to look.There are other invasive species
in the Lava Beds NationalMonument, that
include nettles, twoor three different
types of thistles,and something called
white horehound.So what does the data look like?Well, the response variable
is presence or absenceof mullein, coded as one
for presence and zerofor absence, on 30 meter
by 30 meters sites.So we digitized Lava Beds
National Monument into 30 meterby 30 meter sites, and we had
data from a little over 12,000of those 30 meter
by 30 meter sites.We had a lot of
predictor variables.So they include topographic
variables, such as elevation,slope, and aspect.And they include what we call
bioclimatic predictors, sotemperature, precipitation,
moisture index,potential global radiation,
vapor pressure, humidity,and degree days.And then we also
had the distancesfrom every single one of these
30 meter by 30 meter pixelsto the nearest road or trail.I've put that in red,
because that turns outto be very important
in these analyses.The expert on invasive species
who was working with ussaid that the literature
is very clear on the factthat the vectors by which
invasive species getinto national parks
and other areasis through the boots of people
who are hiking in the parksand on the wheels of the
cars and trucks which comeinto the parks and monuments.So I carried out
an initial analysisusing logistic regression
with all 31 of the predictorvariables.And then I did some variable
selection, just simple stuff,by backward elimination,
with a significance levelto stay at 0.1.And you can see the
results in the table below.They're pretty much exactly
the same for the full model,and for the model with
15 variables, so 1/2of the predictor
variables, percent correctis about 77 and 1/2 to 77.8.And the specificity
and sensitivityare of similar values.So that's an error rate of
about 22% to 22 and 1/2%.Since we've been
talking about LASSOas a method for regularization
and for model selection,I decided to apply the
LASSO to these data.You can apply LASSO to
logistic regression,just as you can to ordinary
multiple regression.The procedure in SAS which
you can use to do thisis PROC HPGENSELECT.And so I did that, and
the PROC HPGENSELECTselected just eight variables.So that's 1/2 as many, again,
as the backward elimination did.There's a cost in the
simplification of the model.The percent correct dropped
through about 77.8 to 73.8,so a drop of 4%, and
corresponding dropsin the sensitivity
and specificity.But if you're interested
in model simplicity,then eight variables is a
lot better than 15 variables.So I've done some
baseline calculations herewith logistic
regression, and now I'dlike to apply some
tree-based methodsand see if they
can do any better.So the first thing I did was
to fit a classification tree.You see, it says there,
classification treewith 214 terminal nodes.You might think, boy,
that is absolutely crazy.There's no way in the world
that you can interpret the treewith 214 terminal nodes.That's true if you want to
interpret the whole tree.Well, we can certainly look at
the first few splits on a treeand do some interesting things.And if we're interested
in prediction,then this is a tree which
gives very high accuracy.The number 214 came from
applying Breiman et al.'s1-SE rule to these data,
and following exactlythe same process
that I have donewith earlier examples, when I
was fitting regression treesor classification trees.So this classification tree its
cross validated percent correctis 87.4%.So that's an error rate of about
12.6%, and that's only a littlebit more than 1/2 of the error
rate of logistic regression.In other words, the
classification treeabsolutely smashes
logistic regression outof the park in terms of accuracy
on these particular data.So the next stop
is random forests.Can random forests yield even
more accurate predictions?So the first thing
to do is to talkabout fitting random forests,
and the procedure within SASfor doing that is PROC HPFOREST.You can see that I specified
the maximum number of treesas 500, when the
default is usually 100.I did that in part to
show you that there'svery little change in terms
of the out-of-bag accuracybetween 100 and 500 trees.Now, scoreparole=oob is probably
not an option that would havejumped into your head.So what this is saying is that
for constructing predictionsfor the observations in
the training data set,then one should use the
out-of-bag predictions,and that's exactly
what the algorithmsays that you should do.So PROC HPFOREST is
not part of SAS/STATSand it has a different syntax.To specify the
predictor variables,we use input statements.And you can see here,
I've got input, and thena list of variables,
and / level = interval.So that's saying that all
of these predictor variablesare continuous interval
valued variables.If I also had some
categorical variables,I would need a second
input statement,and I would do level = nominal
for those other variables.The way that you specify
the response variablein PROC HPFOREST is using
the target statement.So the presence or absence of
mullein, coded as one and zero,is the response variable.And I want PROC
HPFOREST to understandthat this is a classification,
not a regression,and so I need to
specify level = nominal.Finally, the score out= is
giving a name to the outputdata set, which contains the
scored values for all differentobservations.So I ran PROC HPFOREST
forest on these data.You might think
with 500 trees, thiswould take quite
considerable amount of time,but it does not.FOREST is very
fast and efficientcompared with comparable
machine learning algorithms.So it only took a few
seconds, and here issome of the output that I got.So on the left hand panel, we've
got loss reduction variableimportance.One of the features of random
forests and other tree-basedalgorithms is that they give you
estimates of the accuracy of--the estimates of the importance
of various predictor variables.And you can see here,
the column to look atis the out-of-bag Gini,
that relative humidity diffis the most important variable,
followed by MinTempAve,all the way down here.Among these variables
are DistRoad,the distance to
the nearest road,and DistRoadTrail, the distance
to the nearest road or trail.And among the
other variables aredifference in summer to
winter precipitation,and average precipitation,
average temperatures,and maximum
temperatures, and so on.Now, what we're particularly
interested in this exampleis predictive accuracy.And so the two
columns to look athave been highlighted in red.The first column on the
left hand side of this tablecontains the number of
trees, and I've heavilyedited the output table, which
includes the information for 1up to 500 trees, and
I've just done 1 to 5,and then 10, 25, 50,
100, 250, and 500.And then the out-of-bag
misclassification rateis equivalent to
cross validated error,and that's what
we're interested in.You can see by 100
trees, it's about 6.59%.If we go all the way
out to 500 trees,it only drops a little
bit more, to 6.47%.But that's 6.47% is a very
impressive error rate.That means the percent
correct is 93 and 1/2%,and that's what we see in
this summary table here.So I've added a row
for random forests,and it is a good 6% better
than classification tree, whichin turn was 10% better
than logistic regression.And across the
board, random forestsis spectacularly accurate.The last topic
for this workshop,I want to give you just
the merest introductionto this support vector machines.As stated in the introduction,
the original ideais due to Vapnik
and Chervonenkisin the early 1960s.The modern so-called
soft margin algorithmthat is in widespread use
is due to Cortes and Vapnikin the mid 1990s.And in his book The Nature of
Statistical Learning Theory,Vapnik discusses
support vector machines.Support vector machines
became very popularin the computer science
literature in the 1990s,and support vector machines are
related to logistic regression.And there are many
problems whereit is roughly as accurate
as logistic regression,but there are some
problems in whichsupport vector machines
just about blowall other classifiers
out of the water.Support vector machines can
be formulated as constrainedoptimization, and in the
textbook, Introductionto Statistical Learning by
Edwards, Witten, Hastie,and Tibshirani, there is
an excellent discussionof support vector machines.So I'll lay down
some terminologyand give you some
idea of the geometry.Suppose that we
have two classes,so that's filled-in circles
and the hollow circles,and they're
completely separable.So we can separate them by just
fitting a plane between the twodata.The optimal hyper plane
maximizes the distancebetween the two groups.That's the so-called margin.The largest distance
between the two groups,rather, is the margin.Now, the points which fall on
the boundaries of the margin,these are the support
vectors, and theseare very important
in determiningwhere the classification
boundary should go.So in practice, the
boundaries between classescan be very nonlinear, as
this picture strives to show.And so what support
vector machines doesis it takes a
situation where you'vegot really a very highly
nonlinear boundary between twoclasses.It projects the data into a
much higher dimensional space.And in that higher
dimensional space,a linear separation
of the two classesmay be possible or at least
approximately possible,in the sense that the
misclassification rate is low.Now, the trick to
projecting into the higherdimensional space is in
the choice of somethingcalled a kernel function.So fitting support vector
machines within SASis done using PROC HPSVM.As with PROC HPFOREST, this
is not part of SAS/STAT,and the syntax is
a little differentto what we are used to seeing
in SAS/STAT procedures.You can see down
at the bottom here,the response variable
is specified as mullein,and the input variables
specified using--the predictor
variables are specifiedusing input statements.The kernel, the RPF there,
stands for a radio kernel.And in the vast
majority of problemsI've ever applied support
vector machines to,a radio kernel has
outperformed other kernels.In most packages,
the radio kernelis the default because it is
usually the most accurate.And there's a parameter
that goes along with it,and you can see
that I've specifiedthat it should be equal to one.Now, with PROC HPFOREST
and with PROC HPSPLIT,we have specified tenfold
cross validation or somethinglike it, and that is specified
using the select statement.Fold is equal to
10, so it's tenfold,and the CV is equal
to random, and saying,do tenfold cross
validation here.So here are the results
for fitting support vectormachines to the Lava Beds
National Monument data.And you can see that if I just
use the default linear kernel,actually it does about the
same or even a little worsethan logistic regression.If I use the linear kernel with
the parameter equal to one,then I do a little better
than logistic regression,but not as well as a
classification treeor certainly nowhere
close to random forests.If I spent a lot
more time tryingdifferent values of k_par, I
might be able to improve this,but that's one of the costs of
using support vector machines,is that you have to tune them,
and that tuning process canbe time consuming and
potentially frustrating.In this particular example,
the support vector machinedid not do nearly as well
as random forests or evena classification
tree, but there areproblems that I have encountered
in which support vectormachines performed
the best out of allof these very good classifiers
that we have available to usnow.So some final conclusions
about this sectionon nonlinear methods
for predictionin classification and
regression models.Generalized additive models,
classification and regressiontrees, random forests, and
support vector machines,are methods that
are generally moreaccurate than traditional
linear methods,such as multiple
linear regressionand logistic regression.And they can also
provide insightinto the structure in data sets.Generalized additive
models are particularlygood for visualizing the
relationship between predictorvariables and
response variables.That's these s functions
that we graphedin a couple of examples.Classification and
regression treesoften provide very succinct
summaries of the data and yieldinsight into discontinuous
nonlinear structure.So we saw this in
particular with the exampleon credit cards.By simply dividing the
data set into two pieces,one could see exactly what
was going on in the data,and that was nearly all the
information there was aboutwhether or not people were
approved for credit cards.Random forests is one of the
most accurate classifiersin existence and is very
good for regression too.Unlike competitors such as
boosted trees, and supportvector machines,
and neural networks,random forests requires
almost no tuning.Perhaps it is for that
reason that random forestsis extremely popular,
both with SAS usersand with users of
other packages,for analyzing data in a wide
range of different scenarios.Indeed, in their book, Hastie,
Tibshirani, and Friedmandevote a chapter
to random forests,and they comment on the
fact that just pulling itoff the shelf and applying it
to data sets with no tuningvery often yielded very accurate
predictions and good results.That brings this
workshop to an end.Thank you for your attention.I hope that you have learned
something from the contenthere.And should you have any
questions or interestin pursuing things, please
don't hesitate to contact methrough my email address."
62,"Hello, everyone.My name is Michael Gibbs.I am the associate director
of technology for EnterpriseSystems and the technical
manager at the Sam M. WaltonCollege of Business at the
University of Arkansas.And today's presentation
is our paper entitled,If You Build It, They
Will Come, respondingto a growing analytics
program with SAS Viya.Before I begin, I
do want to recognizemy co-author, Dr. Ron
Freeze, also instructorat the University of
Arkansas Sam M. WaltonCollege of Business.And he is unable
to be with us todaydue to some
scheduling conflicts.But I do want to
extend my appreciationto him in this work and
in this presentation.So let's begin.Before we begin
what I want to dois kind of give you the backdrop
on who we are and what we do.And you might have heard,
Walton College of Business.And that of course is where
we are, The Sam M. WaltonCollege of Business.And I bring up
the mission to youhere, the mission of the Sam
M. Walton College of Business,as you can see there is
""To advance and disseminatebusiness knowledge using
diverse, inclusive,and global perspective
to encourage innovationin our primary
strategic endeavors--retail, data analytics,
and entrepreneurship.""I bring up data
analytics-- and Iwant to make sure
I highlight thatto just to show you that
what we've done hereat the University of Arkansas
Walton College of Businesswith the support of our
leadership through its mission,they've been very
focused on data analyticsand equipping our students
to be working in a worldwhere data and data
analytics and data baseddecisions are being made
in all organizations.And so this
presentation is to showkind of how we view
leveraged and usedSAS Viya to accommodate
and to fulfillthat mission of
the Walton College.We also have the Walton
College Enterprise Systems,of which I'm a part.This was founded many
years ago by two professorsin the Department of
Information Systems, Dr. PaulCronin and Dr. David Douglas.And what we wanted to do--what the goal of
Enterprise Systems is,is to bring together corporate
data with supplier technologyand then, of course,
the students as well.So bringing academic,
corporate, and vendor suppliertogether in a
cohesive environmentin which we can give
our students the verybest when it comes to data
and analytic and analyticplatforms.There you see most or all
of our platforms, Teradataand Microsoft, et cetera.And of course SAS is
one of those partners.And we've teamed up with
SAS for many, many years.They've been so gracious
to us and so supportiveof what we're doing
here at the University.And I'm just happy and thrilled
just to be a part of it.So there you see, that's
kind of how it's laid out.We have data that we get
from our corporations.We have platforms
and technology thatare either purchased or gifted,
depending on the supplier.And then, of course, all of
those supplier groups and thosestudent groups that
we support, not onlyhere at the University
of Arkansas,but globally worldwide.If you were with us last
year, at the last year'spresentation, this slide might
be a little familiar to you.This is-- before we start
talking about SAS Viya,I wanted to give a very quick
look back into where we've goneor where we've been.While our relationship
with SAS has really--it's started many, many
years ago, in the '90s.This timeline here kind of shows
you the last eight years or soin which we did a very,
very much improveddelivering of SAS and SAS
technology to our students.It kind of started in 2012
with the deployment of SAS 9.3.And then various upgrades
and different toolswere added to our
tool set to exposeour students with the latest
and greatest in technology.But of course, since today's
paper and today's presentationis about SAS Viya, that's
where our focus is going to be.So I'm going to be talking about
how we went about deployingSAS Viya, sort of that
process, talk about what workedand what didn't work or what
were some of the challengesand hurdles and how
were they overcome.I can tell you
that as someone whois a kind of a systems
administrator by heartthat while it did present
some challenges and, you know,there's always
challenges with workingwith any sort of technology,
the support around meand the support not only from
the college, but also with SAShas been nothing short
of extraordinary.So I appreciate them for
being on this trip with meinto SAS Viya.You'll also notice
in this slide herethat we started-- we
deployed SAS Viya in 2018,spring of 2018.And there you see
some of the toolsthat we deployed as part
of the Viya platform--Visual Analytics, Visual
Statistics, Data Studio,et cetera.And so that's what we're going
to focus our energy todayfor the remainder
of the presentation.One of the things
I'd like to-- youknow, one of the
statements that Ilike to live by
when doing my jobis to deliver technology
that meets a needor has a purpose for what
we're trying to achieve.And we were seeing in
2017, 2018, and even 2016,we saw increased growth in
our undergraduate analyticsprogram, our graduate
analytics program.And not only that, but
also students from allwalks of life, kind of
different departments--marketing, economics,
finance-- whoare very much interested in
learning about data and dataanalytics.And so we saw this
increase growth.And so we were, you
know, we kind of werescratching our heads and
trying of think about what kindof tool set or what
kind of technologycan we deliver to
our students thatspan across the entire
college, regardless of major,whether it's finance
or information systems,while at the same time providing
cutting edge technology.We always wanted to
stay out in front,kind of on the
bleeding edge, if youwill, of technology, and
at the end of the dayprovide students with a
powerful tool for analytics.And most importantly--
and this issomething I adhere
to immensely-- isan exceptional user experience.And so we were kind of
thinking through our next stepand where do we go from here.Now we have SAS 9.4.We have SAS Visual Analytics.Where's the next step?And with respect to SAS, you
know, the choice was obvious.And so we started looking at
this product, the SAS Viya,after talking with
some instructors at SASand some other folks with
the Global Academic GlobalInitiative, and very
supportive and very,very helpful in helping us
decide where we go from here.And so we went on this journey.And of course, with
anything technology wise,we know there is certainly value
in and research and developmentand testing and playing.But at the end of
the day, you know,since we are wanting to be
good stewards of our resources,of our funds, of our technology,
we needed to answer the why.Why SAS Viya?And why now?And you know, what
were we going to gainas a result of deploying
this system and this platformfor our students?And I can kind of sum it up with
these five bullet points here.The first, we saw SAS Viya
as an amazing solutionin which students could access
the tool regardless of device,location, or time.Put another way, you know,
when you have technology--or technologists like
to talk about anywhere,anytime, on any device.Well, SAS Viya meets that need.All the students have to have
to access our environment isan internet connection.If they have an
internet connection,they can access Viya
regardless of if they'reon campus, off campus,
in another state,or even in another country.And so that was huge for us.We also were very attracted
to this continuation,this idea of in-memory
data access and computing.We experienced that with
Visual Analytics and the laser.But when we went to
Viya that of coursewas paramount in us kind of
moving forward with that.At the end of the day,
what we saw Viya as doingwas providing an
end-to-end solution.And that is an environment for
which students from the dataacquisition to the data
preparation to the dataanalytics to the modeling,
it provided our studentswith an end-to-end solution
that would give themsoup to nuts beginning to end
of the data curation and dataanalytic process.And so that's another thing.And of course we're still this--this is still a
work in progress.But our compatibility
with open source tools,we've seen a need
with students whoare needing access
to using Python and Rand other open source tools.And scalable, we
wanted something thatis going to grow as we grow.And so that's another reason
why Viya was so attractive to uswith this idea of
multiple worker nodesand scalability
based on the need.So we went in.We jumped in, kind of dipped
our toe into it a little bit.And we started off the
start of something big.And our first
iteration of SAS Viyawas sort of a semi
pseudo proof of concept,but actually being
used in the classroom.We weren't-- we were-- we
were going live with it.But we understood that
this was our first pass.And so there you see
this very simple diagram.This is taken from the SAS
deployment guide from Linux.And it was a single
server, single node.We threw everything
on the server.And after a couple of
trials and tribulations,we were able to make it--
bring it online and makeit available to our students.So what worked in that
iteration, that installation?Collaboration yields success.I cannot speak
more highly than--enough or highly
enough about the peoplethat we were able to
interact with at SAS.I won't mention them by name.I don't want to--I don't want to
embarrass anyone.But I had wonderful,
wonderful peopleat SAS walking us through
this first pass at deployingSAS Viya.It also goes to--it's also known that
we also collaboratedwith our IT, our central
IT here at the University,as well, who provided resources
and guidance on a few aspectsof that deployment.It was quick to deploy.Now quick, as sort
of a subjective term.And I bring that up,
because essentially wewere putting everything
on one server, our CAScontroller, our services,
our programming interface.And so having multiple
servers, multiple nodes,having people to
talk to each otheror be secure between each
other wasn't necessary.So the path to get to
deployment was quickerthan if we had gone to a
multi-node configuration.We also virtualized
our environment,which we continue to do.I don't know of any
solution that we have thatis now running on bare metal.And so we have all of
our Viya environment runsin a virtual space, which
allows us the flexibilityto add more resources nearly
on the fly with certain--whether it be RAM or more
processors, that sort of thing.And of course, access
to the right tools.We were grateful
to have the accessthat we needed to
our active directory,to our domain
controllers et cetera,to really bring this
environment online to createthe users that we needed to
create to actually run Viya.And it was a success.There was, I think,
that the responsewe got from students
and other in our facultywere very positive, to the
point where we were using itfairly regularly on
many of our classes,especially our
capstone class, whichI may talk about here
in a little while.So that was our first pass.And it would-- I would
call it successful.But of course, as with anything
and especially a program thatis consistently
growing, growth begetsmaturity and more growth.But of course, it's not
without its challenges.So over the last
two years, we'veseen a substantial increase
in the students that are goingthrough our analytics program.One of my favorite
classes to support hereat the University of Arkansas is
our analytics practicum course,where we give students access
to real data from a business.And then that business
works alongside-- theyare stakeholders.And they work alongside the
student or student groupsto solve a business
problem using data.And one thing I've noticed with
a lot of these industry folks,the people that
bring us the data,is that the data is getting
bigger and bigger and bigger.And so there's that--there's that call of
well, where can weput the data, how do
we store the data,how do we secure the data,
how do we analyze the data.And so we had to plan.We had to think ahead
and look to see, OK,what is it going to take for
us to support this class alongwith the other analytics
courses and even other analyticscourses that are yet to come
online in the area of economicsin our program, our cross
college program in data sciencewhich incorporates arts and
sciences and engineeringand business.And so we looked at that.And we said, OK, well, we've
got this really awesomenew scalable environment.Let's build it out.Let's scale it out.And so our second
pass to SAS Viyawas what you see here, CAS
controller, some worker nodes,a programming node, a service
node, a six node SAS Viyacluster.And so we gave--we essentially threw more
resources at the Viya platform.We incorporated this
idea of multi-nodeto deliver more performance,
better performance,and the ability to
handle larger data sets,but also increase
concurrency of our users.But the process was not
without its challenges,as you can see here.And here are some of them.Probably the biggest
one for me was justhaving a consistent nodes
across the landscape.And what I mean by that
is having those usersthat you set up to actually
run SAS Viya like CAS and SAS,et cetera, that those were
consistent, accessiblefrom server to server
from node to node.Probably our biggest hurdle
was getting our CAS controllerto work with its worker nodes.We banged our head
against the wallfor a long time trying
to figure that one out.And I'll give you
the rest of the storyhere in just a moment.We were able to overcome it.And I'll tell you how we
did here in just a moment.We also have this
challenge of our usersthat exist in our
Active Directory, usersthat existed in different
organizational unitsin our Active Directory.So how do we bring those users
together with these usersand allow all the
users to use SAS Viya?And so that was a
bit of a challenge.Of course, getting to the data.We are grateful to have
wonderful data warehouseplatforms like Teradata,
Microsoft SQL Server.And we were-- one
of the challengeswas how do we get SAS Viya to
connect to that platform or to,you know, SQL Server and
more recently SAP HANA?And so it presented its own
little bit of challenge.And beyond that, of course,
time is of the essence.We had about a
two month deadlineto get this upgraded,
deployed, and readyfor the next semester.And so there was a
little bit of a--little bit of nervousness,
but also a lot of excitementto see, can we do this.Can we can we really deploy?Based on what we've learned
in the initial deployment,can we make this new
deployment happen?And I had no doubt in my
mind with the resourcesthat we had that even though
there would be hurdles,and there were, we
were able to deploy itin the time that was
acceptable for our facultyand for our students.And in fact, that's
the environmentthat we're running today.There is still obviously
more room for improvementor more room for growth.But that's kind of
where we are today.After this deployment, you know,
we took time to sort of reflectand what were the
lessons that we learnedin the secondary deployment?And really the first
one too, and I'mgoing to go over
these fairly quickly.One, you want to assess
the IT landscape.Know who your people are, your
processes, and what technologyresources you have.Knowing the people that you
need to contact and workwith at central IT to deliver
the resources that you needto do this project
successfully, knowingwhat you have with
respect to hardwareand limitations with hardware.I cannot stress this enough how
important it was to communicatewith our end users.I love the phrase, seek
first to understand,then to be understood.And I go into the lot of
those meetings with that.I want to listen to them,
listen to what they needand what they want, what they
would really like to have.And then do what you
can to make it happen.Know when to make the call.So the issue we had with the CAS
controller and that the workernodes weren't talking
to each other.We could not get it to work.And my manager who is
wonderful, she uses the phrase,you gotta know when
to call time of death.And what she means by
that is at some point,you've got to just put
your hands down and callthe experts.And so working with SAS
and their support team,we were able to resolve that
issue relatively quickly.Also of course, keeping
stakeholders in the loop.We wanted to make sure that our
faculty and even our studentsthat were testing this
environment were in the loopas to our progress,
as to where thingswere, as to
improvements or changesthat we might have made.And of course document.And I won't spend
much time on document.We know that we need to document
our processes, what worked,what didn't, and
lessons learned.And so now what?We are looking at, of
course, integratingwith more of our data platforms,
including SAP HANA, SQL Server,integrating with our open
source, with R and Python.We are looking forward into
possibly deploying Viya 3.4--or 3.5.We're currently running 3.4.at the end of the
day though, it'sabout delivering the
solution to our users.And that's what we do.Thank you."
63,"ED HUGHES: Hello, I'm
Ed Hughes, the productmanager for SAS Optimization.The solveBlackbox Action was
introduced in SAS Optimization8.5.Let's explore the ways in
which adding this CAS actionenhances the capabilities
of SAS optimization.Here's what we'll cover.We'll start with a brief review
of black-box optimizationand how it services in SAS
optimization and elsewhere.Then we'll compare
the new solveBlackboxaction and the black-box
optimization solver.In the course of
this comparison,we'll look at a few examples.Finally, some
concluding remarks.So what is black-box
optimization?Well, in some ways, it's
easier to say what black-boxoptimization isn't.It isn't traditional
optimization,which always depends on some
simplifying assumptions.For example, all linear
programming algorithms or LPalgorithms assumed that
the objective is linear--all constraints are linear--and all decision
variables are continuous.They could take on any
value between their lowerbounds, other upper bounds.Other traditional optimization
algorithms, for example,mixed-integer linear programming
or nonlinear programmingalso depend on assumptions
about the functions thatappear in the model and
the values permittedfor the decision variables.Black-box optimization
is different.It makes no assumptions at all
about the decision variablesor the functions in
a model it's solving.The only requirements
it places isthat any function
in the model canbe evaluated at any point the
solution algorithm encounters.That's all.Other than that, it's wide open.Functions don't
have to be smooth.They don't have to be
continuous, differential.That last point explains
why black-box optimizationis sometimes called
derivative-free optimization.In really extreme cases, the
function you're dealing withmight not even be a
function in the usual sense.It might not be a closed
form expression for it.You might evaluate it by
looking up the value in a table.And that's fine as well.The positive side of
black-box optimizationis that you can
use it to addressan essentially unlimited range
of problems with two caveats.First, remember that this
is a heuristic solutionmethod, meaning that
isn't guaranteedto find an optimal solution.It almost always finds a
very good solution, though.And that's often more than
good enough for practical use.Second, owing to the nature of
the algorithms in this class,you should limit
its use to problemswith no more than about
100 decision variables.The negative side is
simple but significant.When the assumptions of
traditional optimizationgo away, the limits on what
problems you can solve largelydisappear as well.But you also lose the
benefit to those assumptions.If there are no assumptions
about what kind of functionsyou're dealing with, you can't
tailor a solution algorithmto work well with
that kind of function.So no assumptions
is simultaneouslythe best thing and
the worst thingabout black-box optimization.This isn't a reason to stay away
from the black-box approach.It's just something
to be aware of.In fact, there are several
cases where you clearly shouldconsider using
black-box optimization--if your problem
doesn't fit into anyof the traditional
optimization categories,and you can't use
traditional solvers.So, for example a mixed integer
nonlinear programming problem.We'll look at one of
these later or if you'reworking with multiple objective
functions simultaneously.For these problems,
the black-box solverfinds a set of
nondominated solutionsin the sense that, for each
solution in that group,there's no other solution
that has a bettervalue for every one of the
objectives you're consideringor when you can use a
traditional optimizationsolver, but it's not
producing a good solution.The general rule here is
to use the solver that'sthe most tailored to
the problem at hand.If you have a
linear program, it'sbest to use a linear
programming solver.Choose black-box optimization
if your problem doesn'tfit with a more
specific solver orif a traditional optimization
solver isn't working well.So black-box
optimization augmentsyour toolbox of
solvers but doesn'treplace traditional solvers.The black-box
optimization solverwas formerly known as the ""Local
Search Optimization"" or LSOsolver.And that's been a part
of SAS optimizationfor a few years now.The original name
came from the factthat local search along
with global searchis an important component
of the solution approach.We renamed LSO solver
in SAS Optimization 8.5to reflect the fact that it
can deal with functions thatare essentially black boxes.It doesn't matter how the
functions are evaluated.You just have to be able
to evaluate the functionsat any point you encounter.Genetic algorithms, which
maintain a group of solutionsand use genetic
operators to evolvethe population of solutions
over multiple generationsprovide the basic framework
of black-box optimization.The global and local
search elementsoperate within this framework,
and they enhance it as well.Looking beyond SAS optimization,
black-box optimizationis also used by SAS Visual Data
Mining and Machine Learningto tune it type of parameters.The solveBlackbox action
supports the actionsin the Autotune action set.Now, let's consider how the
solveBlackbox action comparesto the black-box solver.With the black-box solver, you
use OPTMODEL syntax to set upthe entire model--index sets, parameters,
decision variables, constraints,and objective.This is true whether
you happen to beusing PROC OPTMODEL in CASL or
the RUN OPTMODEL action in CASLor any of the other languages--Python, R, Java, or Lua.With a solveBlackbox action,
you have more options and lesselaborate modeling support.You move away from
OPTMODEL syntax.And you can use the
actions parametersto declare decision variables
and linear constraints.Any nonlinear functions and
constraints or the objective--well, those you define
using a block of CASL codethat you designate with
the eval subparameter.| there's a little bit more
of programming burden here.But in return, you get a large
amount of flexibility thatOPTMODEL just
simply cannot offer.We'll see more about
that in a moment.Indexing is a big advantage
of OPTMODEL syntax,because it lets you refer to
members of vectors and arraysin a really direct
readable clear way.There's no indexing of
the solveBlackbox action.But you can use the
indexing capabilitiesof the language you're using.The paper includes
a Python example.If you're comfortable
with OPTMODEL,then keep using it and
the black-box solver.If you're more proficient in
one of the other languages,then the solveBlackbox
action is probablya better choice for you.And, in fact, here's that Python
example I was talking about.So let's look at a quick
example of indexing.If you needed a clear
eight binary decisionvariables, y1 through y8 and
you're programming in Python,this is how you could do it.That's a lot of code.Code involves a
solveBlackbox action.And it declares the entire
model, including those eightdecision variables.As you can see, the
bulk of the codeis devoted to just declaring
those eight decision variables.But if you're already
comfortable working in Python--and this is probably why
you'd be using the actionin the first place in Python.Then you could use a
simple loop to declarethe set of decision variables
before invoking solveBlackbox,like this.And you'd have a much more
compact and more easilymaintainable code.In fact, the Python code that
declares the decision variablesis just about as
compact and directas the corresponding
OPTMODEL code.And if you're
wondering in Pythonrange 1, 9 halts when i equals
9 so it creates eight decisionvariables in all.Now, let's talk about
another advantageof the solveBlackbox action--
multi-level parallelism.And this is part
of the flexibilitythat the eval equals
code block provides.In this case, let's say you
define a nonlinear objectivefunction.The nparallel
parameter enables youto define how many objective
function evaluations usingthat eval equals code block
can run simultaneously.The nSubsessionWorkers
parameter letsyou specify how
many worker nodeseach one of these evaluations
can use by itself.This means that multiple
parallel function evaluationscan each execute multiple
CAS actions in parallel.You can't achieve this
kind of parallelismwith just the RUN
OPTMODEL action.Even better, let's take
a step back and considerthis in a more basic level.You're evaluating a
nonlinear functionas part of solving an
optimization model.You can call a CAS action
to accomplish this, again,in that eval equals code block.In fact you can call CAS actions
on multiple levels becauseof this multi-level parallelism.In our model, at least, as
of SAS optimization 8.5,you can't call even one CAS
action to evaluate a nonlinearfunction.So in return for a little
more programming work,you gain a huge amount
of added flexibility.Now, let's look at an example.Let's say you're
a researcher who'screated a Gaussian
Process regression to fitthree-dimensional input data.That's three
independent variablesand one dependent variable.To do this, you use
the gpReg actionin the non-parametric Bayes'
action set in SAS visual datamining and machine learning.But before putting the
model into production use,it's probably a good
idea to stress test it.You know the upper
and lower boundsof the independent variables.And for this test
case, you know the datacomes from a three-dimensional
sine function, whichlets you compute the actual
value of the dependent variableat any point.You can get the estimated value
from the regression model.So the question is, how large
could the model error possiblybe within these bounds?Well, you can use
optimization to find that out.The objective function
here, the model error,is the absolute value
of the differencebetween the estimated
value and the actual value.To compute the
estimated value, youneed to call the score action.You'll also need to call
three other CAS actions.And as we've seen, the
solveBlackbox actionenables you to do all of this.This is the program that first
builds a regression modeland then stress test it using
the solveBlackbox action.We're going to run
the whole program.But for now, let's skip past
building the regression modeland go straight
to the stress testwith optimization that
starts right here.Don't worry.The entire program is
included in the appendixof the SAS global form paper.What you see here
is a CASL code blockthat computes the model
error for any pointas defined by the value of the
independent variables, x1, x2,and x3.Y is the dependent variable.In line 95, the
error is calculated.Now, notice the calls of
the various CAS actions.The save result
action in line 83creates a results table
that holds those x1, x2,and x3 values.The score action in line
87 calculates the estimatedy-value.The fetch action in line 90
moves the estimated y-valueto the results table.And the drop table
action is usedin line 99 cleaning
up in preparationfor the next call of this code.Here's the solveBlackbox action
call that sets up and runsthe optimization model that
maximizes the regression modelerror.In lines 112 and
113, you can seethe value of the
objective function erroris calculated by calling the
code block we just looked at.Notice nparallel
equals 20 in line 116.And nsubsection of workers
equals 2 in line 113.This means that up to 20
objective function evaluationscan run in parallel and that
each one of those subsections,one for each evaluation,
can use two worker nodes.That means up to 40
worker nodes couldbe used in parallel to how
much parallel work is actuallydone--how many nodes are
actually used in parallelis going to depend on how
many nodes happen to beavailable in your session.Let's run this program.And we'll scroll down to where
the optimization results are.And what we see here
is the maximum error,which is the optimal objective
function value here--we are trying to
maximize error--is a little more than 0.75.Keep in mind, though,
the regression modelis estimating a sine
function, which only rangesbetween minus 1 and plus 1.As a result, the
maximum error of 0.75is really a cause for concern.And that probably indicates we
should revisit the regressionmodel.Because black-box optimization
is a heuristic solution method,it isn't guaranteed to
find an optimal solution.However, it will
virtually always finda very good solution.In some cases, though, there
can be a distinct differencein the quality of the solution
that black-box optimizationreturns depending on how
it happens to be used.We'll see that the
solveBlackbox action gives youmore options in this respect.One example is this mixed
integer nonlinear program,which has a mix of binary
and continuous decisionvariables, a
nonlinear objective,and a mix of nonlinear
and linear constraints.For this problem, we happen
to know an optimal solution.And its objective
function value,the optimal objective function
value is minus 0.94347.Our first solution
attempt, we'regoing to try to solve
the entire problem usingthe black-box solver.We use broadcasts and
the RUN OPTMODEL action.Here's the block
about model codethat the RUN OPTMODEL
action executes.You'll see that it declares
the binary y variables,the continuous x variables.And it also declares the
nonlinear objective functionand all the constraints,
linear and nonlinear.Finally, it calls
the black-box solver.Let's run this
section of the programand see how this all-at-once
approach performs.Scroll down to the bottom
of the output results.And what we see is
that this approachidentifies an optimal
objective functionvalue of about minus 0.668.Well, this is
disappointing, because weknow that's nowhere close to the
true optimal objective functionvalue.Let's try something else--new approach.As we've already seen, if you
use the solveBlackbox action,instead of the
black-box solver, youhave more options
available to you.Remember that with a
solveBlackbox action,you can use a CASL code block to
evaluate an objective function.This CASL code could
include, for example,just running one of the
traditional optimizationsolvers in SAS optimization.Well, that's exactly what
we're going to do here.Here's a strategy.solveBlackbox action is going
to assign values to the binary ydecision variables.And then it's going
to call the NLPsolver to solve for the
continuous x decision variablesand evaluate the
objective function.Remember, you need values for
all of your decision variablesin order to find
out what the valueof your objective
function is at that point.The reason this works is because
once you assign values, zerosor ones, to the binary
decision variables,they drop out of the model and
is no longer a mixed integernonlinear program.It's a pure and
nonlinear program.And that's something
that NLP solveris well-suited to handle.This is the CASL code block that
receives the y variable valuesfrom the black-box optimization
and then uses the NLP solvervia the RUN OPTMODEL action
to solve for the x variablesand find the objective
function value.Here at the bottom
of the programis a call of the
solveBlackbox actionthat solves for the y
variables and then callsthe code block we just saw.It passes the y variable
values along as input.Let's see how well
this approach does.And good news.This time, we find
a solution thathas the true optimal
objective function value.Really in practice, you won't be
able to predict with certaintywhether to solveBlackbox
action using thatto segment-to-solution
method, like we've done here,is going to do better than using
the black-box solver to tacklethe entire problem.But the solveBlackbox
actually provides youwith the flexibility to attempt
this kind of an approach.If you use the black-box
solver and for some reasonyou're unsure, unsatisfied
with the solution it produces,go ahead and consider a
modified solution approachas appropriate like
we've done hereusing the solveBlackbox action.You might find a
better solution.Or you might confirm the
solution you've already foundis optimal.Either outcome is going to be
worth the investment of timeand effort.As we've seen, the
solveBlackbox actionis a valuable addition
to SAS optimization.It broadens the scope
of programming languagesyou can use to express
optimization models.And it also significantly
widens the rangeof problems you can solve.It helps you make much
more effective useof parallel
computational resourcesin solving those especially
challenging optimizationproblems.And lastly in some cases,
the solveBlackbox actioncan help you pursue innovative
solution methods thatcan find even better solutions.I hope this presentation
has been helpful.Thank you very
much for your time."
64,"Hello.Welcome to the basics
of macro programming.We have some very exciting
syntax to show you.Concepts, macro variables,
and macro programs.So you haven't seen them
before, or if you have,there are definitely
some things I'mlooking forward to covering for
you to be introduced to you.So I want to go ahead
and get started and justshare my table of
contents so you see whatwe will cover in this session.There are two lessons.The first lesson is
the macro basics.Just the first two topics will
be the straightforward basicsand then secondly the not
so straightforward basics.So we'll start here, just
a straightforward basics.First start with
a poll question.Multiple choice question.What is the overall purpose
of the macro facility?The answer, text substitution.Text substitution
with macro variables,text substitution
with macro program.You can do it conditionally.You can do it repetitively.There are two components
of the macro facility.There is the macro processor,
which part of Base SASthat does the macro
work, and then there'sthe macro language, which is a
syntax for you to put togetherto either use a macro function,
create macro variables,or define and execute
macro programs.We will explore both concepts.Here's another multiple choice.This is a test to see where you
are and what's your experience.What are the two main tools
of the macro language?Would it be data sets, macro
procedures, macro programs,macro styles, or
macro variables?The answer, macro programs
and macro variables.Macro variables, as I
mentioned, substitute text.It could be a person's name.It could be a table or a list
of tables and lists of states,list of countries that we
would do some analysis on.Macro programs also tend
to be text substitutionat a much larger degree.So you might create
a macro programthat generates data steps and
proc steps conditionally basedon data repetitively.There are two symbols that are
recognized as macro triggers.The symbol what a word
denotes a macro trigger.So what are those two symbols?pound sign, dollar sign, percent
symbol, ampersand, asterisk?An ampersand typically
precedes a macro variableto reference that macro variable
to substitute information.The percent symbol
is what you start offwith to use a macro
function, macro statement,to find a macro program.Those are the two symbols that
are regarded as macro terms.It takes some action
about a macro process.Let's look at some of
these macro triggers.We have &dsn, which is regarded
as a macro variable reference,ampersand location, month.These are all macro
variable references.So a macro facility would
expect those to exist.A percent symbol is used
typically to specify a macrostatement like %LET,
which you will see.And beginning of a
macro definition,what percent macro
you give it a nameshould be a macro function.Some of you have used
scan and substring,but there are
equivalent versionsat the macro level for
set substrate, percentscan, percent up case.There's a percent do
loop at the macro level.So there's some code you
want to run repetitively.Let's say you have a
certain path in your shareddrive or your
personal drive and youwant to repetitively convert
Excel sheets to SAS data sets.That's one of the
things you could do.You could process data
sources in the same fashion.There are a number of things
you can do with a macro do loop.Very powerful feature.Here's an example of the
substrate at the macro level.You proceeded with a
percent symbol, substrain,and then your argument
would result to a strain.Quotation marks
are not necessary.You indicate the starting
position and optionallythe imposition, how many
characters you have.If you referenced
a macro variablewithin quotation marks, you have
to use double quotation marks.Macro variables do not resolve
in single quotation marks.If you wanted to resolve
not as a literal,then you do not put
quotation marks.For example, we're what?2020 to resolve
just as a number.Even though it's stored as
text, it resolves as a number.In here, within quotes, I
want it to result to MAR.Within these
quotation marks, youhave a macro trigger
that's a percent symbol.So it might be that you're
using a macro program thatreturns some result, which
could be a data set in there.In fact, we will see
an example of that.More questions for you.Multiple choice once again.What are the three methods
of creating a macro variable?You have a list of six items.%LET can be used to assign
a value to a macro variable,especially if you know exactly
what you want it to be.CALL SYMPUTX is used
in the data set.You can use that technique to
create a macro variable whenyou really don't
necessarily know the valuelooks like if it
needs to be computed.So maybe you're
generating a statistic,but doing some kind of data
manipulation and you don'tnecessarily know what
that value looks like,but you can still have it
dynamically generated for you.The other technique
is PROC SQL INTO.One of my favorite options
for creating macro variablesand creating a series
of macro builds.There is this into clause
that you use with a colonto indicate what macro
variable you're creating,and prior to that you have
some value that you'reassigning to it.Could be a statistic.It could be a computed value.So those are your
three answers. %LET,CALL SYMPUTX and PROC SQL INTO.Let's start with %LET.%LET creates the macro
variable as follows.%LET you give it a name
and you assign it a value.That value will
be stored as text.If you bound it within
quotation marks,quotes are stored as
part of the value.The next approach
we will look atis called SYMPUTX where
you can create a macrovariable at execution time.The reason I mentioned
execution because youmight have some data
coming in that'sbeing executed and doing
some kind of calculationthat you are not aware of as
far as what value you get.And the second argument
is the actual value.So there's three
ways you can indicatethat there are a macro
variable name in the value.The third technique is
the INTO clause with SQL.And once again you
could do a calculationfor PROC SQL You might be doing
some mean statistic, mid max,or perhaps you are changing
the case of some text.Perhaps you've
concatenating some values.I mean, you want to put
it into a macro variable.Notice there's a
colon before the name.You have to put the colon.Here's an example of %LET.When I know exactly what the
value should be I can do %LETyear current equals 2020.%LET year previous equals 2019.I could hardcode
the values and haveit calculated as down below.The advantage of this technique
is next year when I'll run it,it will take care of itself.I don't have to hard code
2021, previous year 2020.Let's take a close
look at what'sto the right of the equal sign.By the way, when you create
a macro variable with %LET,you don't put an ampersand
in front of the name.If you do, it could be that it's
macro variable reference thatresolved to a legitimate
new macro variable name.So that would be acceptable.In this example, we're using
a function called %sysfunc.%sysfunc can invoke a
number of functions in Sas.Not all of them, but probably
the majority of them.In fact, for those of you who
are familiar with the todayfunction, the today
function returnsto sys day four,
whatever today is.And I'm using sysfunc to
return to invoke that functionand return that integer.Once that integers
return I'm usinga second version of
sysfunc to invoke the yearfunction to return
a four digit year.So that's your return a four
digit year for whatever todayis.And the second
one is doing math.They're using a %eval,
like evaluation,but %eval only does
integer arithmetic.It does not do
non-integer arithmetic.If you want to do non-integer
arithmetic in integer,then there's
%cysevalf you can use.In this case, we're just
subtracting 1 from &year, well,&YearCurrent.So this would end up being
2020 minus 1, which is 2019.We'd assign that
to previous year.Here's the new example.And what I'll do at
the end of this lessonis to bring the programs in to
show you the result through oneof our interfaces.Remember I mentioned %LET
called SYMPUTX and SQL INTO.This is called SYMPUTX.This data set that runs
only one time is reallyfor test purposes just to
really show you something.It could be that you're
doing a calculationthat you have no interest
in a data set being created.So this is how you would do it.You'll put data_null_.And it would not
create a data set.It will still allow you
to use the data set.As in this case, you're
creating a variable called YC.We will make equal
to the year up today.Remember you will
get the-- well,we're not using sysfunc because
this is a DATA step assignmentsdate.We will get the Sas date from
the day and the four digit yearfrom year function,
whatever the current year isand assign it to YC.Then YC minus 1
is assigned to YP.But we're trying to imply
year current, year previous.So this is named
as a new examplein creating the same macro
levels I created here.But I'm using CALL SYMPUTX
to have it generated for me.CALL SYMPUTX, the name is
first, and the value is second.Remember the syntax.The name is first,
the value second.So I'm assigning YC to the
macro variable call hereand the value of YP to connect
a variable here previous.Notice, the name goes
in quotation marks.The value, in this instance,
will be a data set variablewhose values is assigned
to these macro variablesrespectively.Now, it could be
that you're assigninga value in quotation
marks or assigninga value from an expression.Let's say you're concatenating
first name, last name,and in which that expression
finishes, it will be--that value will be passed
to the macro level the name.So actually, the
name or the valuecould be indicated
three different ways--as a hardcoded value
in quotation marks,as the value of a
variable, or the valuebuilt from the expression.In this example, we're creating
a series of macro variables.If you have not
seen an underscore,before that indicates the number
of iterations of the DATA step.This is a variable that's
created automaticallyin what we call the
program data vector.Or just known that
it is available.And what I'm trying
to construct--remember, we're
using call symputx.So the first argument
generates the name.The second argument
assigns the value.It gives us the value.So if you take a close look
at this first argument,it is actually an expression.We're doing some manipulation
that yields a value.That value, the
current value, will bethe name of my macro variable.So basically,
bottom line is, I'mconcatenating the word ""student""
with the value of underscore Nunderscore, which starts at
1 and then increments by 1and continues to go up.So the first sign, the
name we will generatewill be student one.And whatever the value
is from sashelp.classwill be assigned to student 1.Sashelp.class we all have.Each of you have access
to this data set.There are several data
sets in sashelp librarythat you can use, and this
is just one that we'reputting in this example.All right, if last.So n equals last is creating
a variable called ""last""that will indicate if we are
at the end of the data set,if we have read the last
observation, last record.The reason I'm doing
that is because Iwant to create an additional
macro variable called ""count.""And I'm assigning it whatever
the current value of underscoreN underscore, so I'll
know which iterationI was on, which, basically, will
tell me how many of the studentnames I have.It'll put that value in that
macro variable called count.So I will end up with
several macro variables--student 1, student 2,
student 3, and whateverthe count is indicating
how many iterationsI went through the data set.The sql INTO is another
option to assigna macro variable
value dynamically.We don't necessarily
know what it looks like.In this particular case,
I'm putting the maximum yearfrom a data set called prdsale.Let's say I have
several records in here.Which every year has the
highest value among the records,I want to put it out as a
four character, five characterstream.The put function writes the
numeric out as a string.I'm putting it out as a
string and then put itinto a macro variable
called YearMax.Colon precedes it.The data is coming
from work.prdsale.So if I have 10
records, whichever valueis the highest of year
among those 10 records,it will retrieve
that value as the maxand put it out as
a 4 byte stringinto that macro
variable called YearMax.For each macro verbal
I create, noticeI'm preceding it with a colon.So I have two values
on my assignment,separated with a comma.I want to put the max out as the
four-byte string into YearMaxand put the min value out as
a four-byte stream to YearMax.So I'm creating two macro
variables from prdsale,and I can use those further down
in my code somewhere-- maybethe title, maybe in
a footnote, or maybein some conditional logic.""Distinct"" or ""unique""--
either word would work.So I want to take
distinct makes of a car--like Chevrolet, BMW, Acura.In sashelp.cars, I might
have multiple recordsfor a particular
make, like Chevrolet,but I only want the
distinct values.So select distinct
make into colona macro variable
called CarMakes,separating with quote by quote.What is going on?What it will do is make a
distinct list of the makesand assign that list of
values of the make of the carsinto this one macro
variable called CarMakesseparated with a space.So if have Chevrolet,
space, Porsche, space,and Mercedes-Benz, they will
be separated in the listby a blank.So you can use that
class for validation.You could use it for filtering.There are a numberof
places you can use that.Here's an additional example.What if you want to assign
a series of macro variables,Make1 through Make38.Let's say you know your
data, and there's only 38makes, different makes of cars.So what you do is select this
distinct make into, colon,you start at Make1.So the first make in the
sashelp.cars will go to Make1.The second make of the
automobiles from sashelp.carswill go to Make2, Make3,
Make4, and so on, all the waythrough Make38.So whichever one is
last , that 38 record,whatever it contains, that
value will be assigned to Make.Let's say you don't
know how many you have.You can just put a hyphen.Very nice feature.It will count them for you.If you have 38, the last macro
variable would be Make38.If you have 50, the last
one would be Make50.Down at the bottom there's a
reference to an automatic macrovariable--not the ones you're creating,
but an automatic one that youget whenever you use PROC SQL.And you can put that to use.It's called SQLOBS.It will tell you how many
records you have in the resultsset, the result of
that PROC SQL step.So if I'm trying to do
something iterativelywith a macro do loop, I want
to know how many I have,because I can SQLOBS.In this case of that 38
makes, SQLOBS will also be 38.Its tracks the number of
rows that were produced.Here's another poll
question for you.Where are macro
variables stored?Hmm.Where are macro variables?Stores?Two places you can
store macro variables--in a local table or a
global symbol table.Local symbol table or
global symbol table.So it depends on where you
define your macro variableor variables.Local or global symbol table.When a macro
variable is created,the macro processor
asks the macro variableto the symbol table.So there's two types.There's global
and there's local.sysdate9 is an
automatic macro variablethat's put in this symbol table
automatically for you, alongwith a host of other
automatic macro variable.Now, these last
four were created--perhaps by you, or
whoever the programmer is,or analyst is who's
writing the %LET.So using CALL
SYMPUT or SQL INTO.User-defined are the last four.The first one is
an automatic onethat you get from SAS,
SAS creates for you,that you could use it if
you like or not use itGlobal symbol tables.Whenever you start
your SAS section,this table is created
automatically for you.In this global, it stays
around your entire session,so whatever variable macro
variables you put in there,you're good for
the entire session.You get the automatic ones that
SAS creates for you, automatic,and if you define macro
variables outside of a macroprogram, they're also placed
in the global symbol table--for %LET, CALL
SYMPUTX, PROC SQL INTO.So what about the rule for
the local symbol table?The local symbol
table is only createdwhen you have a macro program
that starts executing.And that local symbol
table is only goodwhile that macro
program is executed.And after it's finished
executing, gone--that table is gone.So what can you put in there?It can store macro variables
defined as local insideof the macro program.So there is a statement
called percent local.Whenver you have
macro parameters,if there are macro variables,
they are automaticallyplaced in the local table.Which statement displays the
value of a macro variablein the log?Percent put-- not PUT.Now, there is a PUT
statement in the data set.There's also a PUT function.But we're referring to
the %PUT macro statement.If you would like to see
all of your macro variables,you can type %put, put a
space, _all_ semicolon.That will show you all
of the global macrovariables plus automatic.The globals are the
ones that you created.The automatic ones are the
ones SAS set up for youwhen you started the session.You will see both.But what if you don't
want to see both?What if you only want to
see the automatic ones?Then you would do
a &put_automatic_.And what if you only wanted to
see the ones that you create?Then you could do &put_user_.So you can see them all.You can see the automatic ones.You can see the ones
that you create._user_.Or you can be explicit
and display specific macrovariables like &month and &year
with a %put statement separatedwith a space, and that's
exactly what you see here.They are separated with a space.Here's the next example.If you like to view it like
an assignment in the log--and by the way, this
is in your staff log--the %put writes to
your staff log only.%put &=.You see the & is
a macro trigger.Instead of being
in front of this L,we put an equal sign here.So we're writing in the
log like an assignment.LOCATION=District of Columbia.Notice when you do a %put,
you put the ampersand,but in the log, you don't see
them because they resolve.Here's an additional one.%put Today is &sysdate.This is an automatic
macro variablefrom when you started
your SAS session last.%put.Which statements start
and end a macro program?It's a quick review.You've written macro
programs before.It requires a %macro.You put the name of your
macro program and a %mendand the name of
your macro program.You don't have to put the
name of your macro programon the %mend statement.You can just type %MEND single.Here's an example--
%macro mydttime.That's the name of
the macro program.%mend concludes that macro
definition calling it the samename.So what's embedded
within this macro programis these four statements.It doesn't execute
it at this point.It's just defining it.If you had macro
level statements,it would check those
like a %if, %then, %do.We don't have any
of those in it.I'm going to name mydttime.When you call it, it will create
these two macro variables.Remember sysfunc?We're writing out the date.The date and the today function
give you the same exact thing.They give you the SAS for today.And we're displaying
it with a format.When you use sysfunc, you
don't have to put a format.For example, if I
put a parentheseswhere the comma is
and then a semicolon,it will just display it as the
number of days between January1, 1960 and today.You can optionally
put a second argumentwith sysfunc, which
would be a format.So I'm using format with a dot.We'll write the dot
like it were in--as it explains the date.The second one-- I'm
writing out the time.So I'm using %sysfunc to invoke
the time function like I didthe date function, and that
returns an integer for timeas the number of
seconds past midnight.And that's what you would
see unless you do a format--assign a format with sysfunc
to display this timeampm.So these two will create
the macro variableswithin the macro program.Then the %puts will write out
the values of date and time.Note it's preceded with
an & for reference,not when you create them.You can create them, you don't
have to put an ampersand.When you use them, you do.So when I call
this macro program,it will take these actions,
do all four of these things.You don't need a semicolon.In fact, depending
on where you call it,a semicolon might
cause you issues.So if you just do
something like this,typically you will
not have an issue.But a semicolon
is not necessary.It is still used to denote
the end of a SAS statementand not given.And as you can see, we get the
notes in the log because %putswrite to the SAS log.Which two methods are valid
for specifying parametersin a macro program?In other words, what are
the two kinds of parametersthat you can have?One is positional.One is keyword.We will see both,
positional, keyword.So here's the story with
positional map parameters.I would say 90-something%
of the time,the parameters will end up
being macro variables anyway.So when you define them and
they're based on position--whichever one you put
first and second--because when you
assign values to them,if you call the macro percent
%cars open parantheses,the first thing you type
must be the make comma.The next thing you type
must be the cylinder value.So when you do a
macro definitionlike this, when you
eventually call it--when you call it, you
have to be cognizantof the fact of the position.So whichever value you
put first goes here.Second value goes here.If you put them
in reverse order,you will get the incorrect
value going to this respect.Now in alternative
to positional iskeyword where you actually
assign default valueslike I'm indicating
the make be Audi--I like Audi's-- and then
the cylinder equals 6.If I was to call
the macro program,cars %cars and just
put parentheses,these would be the
default values.So typically in a
macro program, you'regoing to substitute these values
somewhere in the code that'sembedded with the macro program
which you're going to havesubmitted to be processed.Remember we talked about--I explained the global symbol
table and the local symboltables?Parameters always go to the
local symbol table for that mapor program, and
they will be createdwhen cars start to
execute, and they will beremoved when cars is finished.This is just defining cars.And we have to do a %cars as
we did here to have it execute.So from here to here
is the definition.This is a call to
have it execute.We have two
positional parameters.And if you look where
we're calling it,these values have to
be in line respectivelyin those positions.Make Ford, cylinder 8.If you reverse these
two, 8 will go to Ford--I'm sorry, 8 will go to
make, and Ford will goto cylinder-- get some errors.We will get some errors
because it will be treatingthe cylinders as if this
letter's F-O-R-D and will thinkit's a variable.So in this proc print, I'm
going against the sashelp carswhere there's a
column in sashelpcalled make = in quotation
marks because I wantit to be part of a literal,
and that will resolve the Ford,and cylinders = 8.Run statement, and we'll
send that proc print,process that proc print,
and give you a listing basedon your request.Positional parameters--
when you define them,when you call them, they
have to be in the same orderrespectively.What about keyword?Keyword you set defaults.I probably would have made
by default Mercedes then,or a Porsche, but
we'll go with the Audi.Cylinder 6.When I call this
macro program cars,I'm overriding the
default value of cylinder.And it's like, let's
go with 4 cylinder.Give me the makes of
Audi with 4 cylinders.So in the where, make
will be equal to Audi,and cylinders will
be equal to 8.So when you have
keyword parameters,you don't have to
specify any value.Just put the parentheses
when you use the defaults.And if you want to
override either of both,you certainly can, and
they don't necessarilyhave to be in any order.You could do cylinder=4,
make=Chevrolet or Audior whatever.Which two statements are
valid in a macro program?So %do and %end, they're valid.%if.%then, and &else.Those are the two
sets of statements.You can only use these inside
of a macro program, not outside.They're only relevant
within a macro program.When you're doing something
repetitively, or maybe you'resubmitting some
code conditionally.Here's a do loop, one of my
favorite features that a macrofacility.If I want to do
something repetitively,I could use a macro do loop.There are some I/O functions I
can use to open up a directory,and I could have it search
the directory lookingfor certain file types.So I can have it do a %do from
the beginning of that path allthe way through the end of that
path and just check files or dosomething repetitive.Or maybe do a proc print against
a list of tables and a library.Be PROC UNIVARIATE against a
list of tables in the library.Doing something repetitively--
check out the sentence.%do checking a macro
variable = some start value--you have to type
%to some stop value.To start and stop
should be integersof or macro variable references
that resolve to integers.You can do things
conditionally, too.Another favorite component of
the macro facility of mine.%IF some expression,
%THEN take some action.That could be use
a macro function,assign a value to
a macro variable,resolve a macro variable,
or call a macro program.%else, %if, and continue.Now, I don't have
a do block here,but I can actually %IF
some condition, %THEN, %DO.Do a set of statements as a
unit based on the true or false.So here's the example
of a macro do.Now, I happen to know
there are at least38 makes in that
sashelp cars, so I'mhaving this proc
print work 38 times,and I'm filtering one
of the makes at a time.I'm using %scan to go against
the value of the macro variablemakes and grab each
one position at a time.If you recall %scan,
%scan is very similar,and that &i is
automatically that variable.Whenever you do a %do some
variable equals something,that variable will automatically
be a macro variable.I like that.That works really great for me.So i will automatically
be a macro variable,and this value will
change for each iteration.Initially, it's one.So this will be a one,
and I'll scan makesfor the car to grab
the first one usinga tilde as the delimiter.So you may have Chevrolet tilde
Dodge tilde Porsche tilde next,and I'm putting that
same value in my titleso I know exactly
what I'm getting.And all I have to do
next is call %allcars.And it will use the %do to
process and generate 38 procprints for me.And I may be putting them
in a PDF or some other filestructure.If I know what they look like,
I can go ahead and car codevalues and check those values.
%if &mke=Chevrolet or-- and youdon't need a % symbol
here by the way--&mke=Toyota %then %do, do a proc
print filtering for that make,putting a title
in for that make,and it's not a proc print.It's a prop freq
actually, and I'musing a tables statement to do
a frequency on the cylinders.And then %end for the do block.And then further down,
you have a %mend.So the point is you can
submit code conditionallythrough a macro program.Here's the then for proc freq.The %else do is a proc print.So notice %if, %else, %then,
%do, have percent symbols,but not or.So the first call
would be for Chevrolet,and it would check ding,
ding, ding, Chevrolet.And it would do a prop print--sorry, a prop freq--for the Chevrolet.The next call would
be Toyota, and itwill check once again your
prop freq on the cylinders.And then lastly, Acura.It's not in that list so
we'll go to the else doblock and process for Acura.You don't need a semicolon.Unnecessary, it would be as if
you put two semicolons here.It would be harmless in
this case, but unnecessary.Here's another multiple
choice question.Which SAS system options
are useful for debuggingin your macro facility just in
case you make some mistakes?Syntax wise or logic wise,
and it's actually three--MLOGIC, MPRINT, and
SYMBOLGEN. SYMBOLGENis for resolving macros.MPRINT is for showing you the
code from a macro program.MLOGIC is really good because
you can see the evaluationresult of a %if type inquiry
at the macro program level.It will let you know if
it was true or false.And it also tracks
the beginning and endof your macro execution.Those three, all right.As I stated, MLOGIC specifies
whether the macro processortraces your execution
for debugging.All right.It will also give you the
result of %if, then, else.MPRINT will show
you the code that'sgenerated by the macro
program and submitit to be compiled and executed.SYMBOLGEN will show you the
resolution of macro variables.All very handy.Here's an example.We turn on all three options,
MLOGIC, MPRINT, and SYMBOLGEN.I'm calling this macro
program for Chevrolet.Watch what MLOGIC does.It shows you the
beginning of execution,and if that macro program
was calling another macro,it would show you that
beginning as well.Here's MLOGIC again.What parameter MKE
has been assigned?This case, Chevrolet.Here's SYMBOLGEN. Macro variable
MKE resolved to Chevrolet.Every time it's used, it
will have a SYMBOLGEN.So if you use make twice,
you'll get two SYMBOLGENs.Three times, you'll
get three SYMBOLGENs.And here's MLOGIC once again.As I stated, I like it because
it will indicate if an if then,%if %else will result
it to true or false.Check it out, true.All right.We called for Chevrolet.It is Chevrolet.That's the truth.MPRINT will show the
code it's generated.Here's a proc freq.Here's SYMBOLGEN again.Here's the other
part of MPRINT--the where in the table.It's showing you the
code that's beingsubmitted by the macro program.And then as I stated,
SYMBOLGEN will show youthe resolution of
the macro variableswherever they are used.And then, here's MPRINT
continued with the title.And then the last part,
which is a run statement.I'll be honest, I probably
wouldn't turn all of theseon at the same time.I oftentimes, but
MLOGIC with MPRINTif I wanted to see what's
happening with my macroprogram.And then if I wanted to see
the macro variables resolve,I'd probably use
SYMBOLGEN by Itself.That's just me.That's how I do it.All right.So that concludes
the first part,the straightforward approach,
the straightforward basics,and what I'm going to
do now is just walk youthrough some sample code
of some of the thingswe just talked about.So I'm going to pause
here for a momentand switch to my interactive
session, my virtual session,so we can look at some examples.So I have an enterprise
guide session going here.And I have some programs you
can see to the left broken downby chapter.I'm just going to go and
select my chapter onesection, that SAS programs that
work through a number of thingswe just talked about.Remember we started off
talking about macro variables.And here's the %let that I'm
using to create three macrovariables.By the way, if either
the value, whichis on the left of
the equal sign,or if the name, which is on the
left side of the equal sign,or the value, which is on the
right-- if either has macrotriggers, they would
have to resolve firstbefore the assignment is made.Then notice I'm writing out each
of these macro variables witha %put.So let me just highlight this
and run these statements.I'm going to do a Submit
and show you in the log--here are the values.dsn is the data set,
sashelp and then,location is District
of Columbia.Remember, we assigned that here.And then, month has
M-A-R, that text.And here, I put
dsn=sashelp.class,and then I can use %put to write
out the values of these macrovariables.Let's move further down.I'm going to create a data
set called work.class.The reason is I have an
automatic macro variable calledsyslast that tracks the name of
the most recently created dataset.So since I'm creating
work.class from sashelp.classor some other table, whatever
the name of this recentlycreated data set is, it
will be assigned to syslast.And I'm actually using a macro
program called dsn, but notmuch going on other than
resolving that macro variable.So I have that macro
variable referencedwhich will be resolved.So notice that %let, I
have dsn= a macro program.That macro program will return
to resolve value of thisand assign it to that
macro variable called dsn.So I've got a macro
program called dsn.I'm creating a macro variable
called dsn, but here'sthe difference in
terms of the triggers.The percent symbol is used
for this macro program.The ampersand is used for
this macro variable reference.Now, let me run this, and
it should come out as--let me run all of this.It should come
out as work.class.Open log.Here we go.It's put out as work.class.We talked about a macro do loop.You can only use it within
a macro program % doloopappropriately.You didn't have to call it that.%do year-- and remember
what I said earlier.Whatever index you
use, i, x, y, whatever,this will automatically
be a macro variable.That's a really nice feature.We're going to assign
it 2010 %to 2020.And then %put Year
value is &year.When you have a
macro doloop, youhave a percent symbol
in front of the word to.%end and then the conclusion
of it, and we'll call it.So let's see.2010 to 2020.2010 to 2020, and it writes out
each statement repetitively.So generally, what I would do--I would have a block
of code in herethat I wanted to
execute repetitively.It might be doing some analysis
for groups of data in a fileor maybe creating a data
set for each distinct valueof a category in a file like
country or maybe region.Now, I'm going to show you a
feature-- normally, the error.I'm going to put ERROR colon.Errors come out in red.This is a good way
to use this keywordto write your
message out in red.But the only catch is that
it writes the word ERROR.So see how it's written
out repetitively.If you don't want the word
ERROR to be written out--and you notice it looks up
here like it's an error--then you can put a hyphen
instead of a colon.And it will use the color
associated with that keyword,which is red.See the difference?I'm going to use this perhaps
for writing custom messages,when you want things to
stand out in your log.When writing a macro program,
you plan to have a groupof people use it, I can use
%puts to write notes when maybesomething happens
that I don't expect,and maybe this will apply
a value to a parameter Idon't expect, and I
can account for that.Another category
that we discussedis the macro function.Now, there are functions
in the data set.The macro says, hold on,
I got functions, too.%let is creating a macro
variable called day and makingit the 29 of March 2020.%let year equals the sub
string, %substr, of &date.Now, &date will resolve to
exactly what we assignedat the top.And we're going to extract
from the sixth positionbefore characters.So let's count, one,
two, three, four, five.The 2 in 2020 is
the sixth position.So we start here
before, and that'swhat we would get assigned
to the macro variable year.And if I removed the comma
4, I would get the same thingbecause there's
nothing after this.But if you don't put an ending
point for how many charactersyou want--and it's really
how many charactersyou want after the sixth--it will continue until it
graphs everything following.So it might be longer.If I wanted to be
more explicit, Iwould tell it exactly
how many characters.And once again, I just want
this to stand out in red.So I'll going to type
ERROR- and run this,and you can see the use
of this macro function.This is date, and this is the
year that was retrieved with%substr.It resolved this to text
and resolved this to text,and it started at position
six of four characters,and that's what we--so you do not need to put
quotations around this valuebecause if you do, quote
would be part of the value.Let's focus on the next example.You've actually seen
this one where we justhave a macro variable reference
within a macro program.So when I call
this macro program,it will resolve to &syslast,
which will resolve to the mostrecently created data set.So since I ran this earlier,
it is still the most recentlycreated data set, or should
be unless I missed something,but let's run this.In addition, in the
title, it will show memy recently created
data set as well.And here we go.work.class-- that's
the data set namethat we created
from sashelp.class.We'll continue down to this
section using %let and somefunctions like sysfunc to
invoke other functions.I'll make the year current,
2020, previous, 2019,and I'm putting them
out next to each other.You've seen this in the slides.You get the same
result, 2020, 2019.I'm just going through
the program now.And here's sysfunc.I have two occurrences
of it because this onewill return a sas date for
today, whatever today is.And then this sysfunc will
invoke the year functionreturn for the year.%eval and %sysevalf can do
math, and this is just a simplesubtraction.And then when I put them out
one year before the current yearis 2019.Your current, your
previous, 2019--So you can use
%eval or %sysevalf.%eval gives you the same
result. Now, we run this again--same result. Uh-oh.Forgot to take off the f.All right, so we're good.Let's move to the next section
and move on to call symputx.Remember, _null_ means I
want to use the data set,but I don't want to create a sas
data set because my purpose isto create some macro variables.Now, if you remove _null_,
it will make work.data1and work.data2 unnecessary.So we just don't want
to create a data set.These are data set variables--YC-- the year today
is the present year.I get 2020 for today.And YP=YC-1 I end up with 2019.Then I'm assigning 2020 to
a macro variable called Yearand 2019 to a macro variable
called Year Previous.Notice that on
the left side, I'mhard coding the name to
be exactly what I want.On the right side, I
have the value variablewhere I may or may not
know what they are.So remember what I said earlier.Both arguments--
and they don't haveto be assigned the same way--
but they both can be specifiedas static in quotes.They could be the
value of a variableor the value of an expression.In this case, this
was hard coded,and this is the value of
my variable for these tworespectively.Then I'll do a put.I should end it with the
same result, 2020 and 2019.The difference is here I
used sysfunc and %eval.Earlier, I just hard coded
the values and put them out.You can hard code them and
have sysfunc calculate for you,or you can use call
symputx to do this for you,and you can just
run it next time,and it will do it
automatically for you.2020, 2019.When we look at an example
of creating a series of macrovariables-- student1,
student2, student3.If I show you that
sashelp.class [INAUDIBLE]..Then go to that library and
show you the table called class,you'll get an idea
of what it's doing.We have 19 students in here.Alfred all the way through Wade.We have their gender.We got their age,
height, and weight.I just wanted to show you
what those looked like.So I should get that many
iterations of the data set,and each time this iterates, I'm
assigning _N_ with the spelledout student to make it student1,
student2, student3, and so on.And whatever the
current value of nameis that's what will be assigned.And then I'm just putting
out the first, the second,and the 19th, and
how many I have.And when I get to
the last observationon this set statement--I should have explained
this earlier--this end= option allows me to
define a variable whatever Iwant to call it.I can it last.I can call it x.So here's the deal with
this variable I'm creating,and it's a temporary
variable, which meansit will not be written out.This variable, whatever
I put, will be zerountil I get to the
final record in class.Then it will become one.So when I get to the final
one, create an additional macrovariable called count so I'll
know how many names I have.I'm going to run this.All right.So Alfred, Alice,
and William, and Ihave 19 names all together.If I do a %put _user_, you'll
be able to see what's macro[INAUDIBLE].See all the students?You see Alfred, Joyce, Ronald--they're all in the
global symbol table.Now, remember the
global symbol tableis around your entire session
so you can take advantage of it.I've shown you %let when you
know what the values are.I've shown you call symputx
when you don't necessarilyknow what the values are
that will be assigned to oneor more macro variables.Proc sql into says, hold
on, I can do the same thing.You can also create a
macro variable dynamicallyduring execution when it doesn't
count a calculation for you.We have a sashelp
table called prdsale.And this is what it looks like.We have actual
amount, predicted,country, region, division,
product, quarter, year,and month.I just wanted to give you a
view of what it looked like.I'm going to make year
equal to self +25,and I'm going to run this
to data set step to createprdsale.We've got year= this
year, year=year+25.So this is the data
set that was created,and I'm going to use
sql to create one macrovariable from the table
I've just created.And I'm going to take
the value of yearand get the maximum
value of yearthat I show you among
the data-- whichever one has the maximum value.Look like that's it.So it's going to get
that maximum value,put it out as a four byte stream
into a macro variable calledYearMax from the table
I just created, and I'mgoing to put it out in log,
and let's see what it makes.2019.So you can ask for a statistic
and put it in a macro variable.I've changed it
to the min, and itwill give me the minimum value.All right, looks like
they're just two, two values.You can create more than
one macro variable with sql.And one thing I need
to point out-- youhave to put colon in front of
the name after the phrase into.Put this value into colon
this macro variable.You have to proceed
it with a colon.Let's look at creating
more than one.These two expressions--
they're expressionsbecause they're doing some kind
of manipulation or function,for example.If I had 100, that's
a static value.I can put that in YearMax.I could put 1,000 literally
a 1 0 0 0 into thatwithout having the expression
if I knew exactly what it is.But if I don't know what
it is, this comes in handy.I'm going to put the
max year as a four bytestring and the minimum
year as a four byte string.I'm using the put function.If you haven't seen it
before, the put functionwrites a numeric
out as a string.The reason I'm
making it a stringis because macro
variables store text,so I'm assigning
it a text value.Even though when it
resolves, it willresolve as if it's numeric.Well, to what we
call a numeric toke.All right?So I'm going to run this.Let me put it out
here in red again.You probably notice I like
to highlight things this wayin the log.We can run this.And there's 2019,
2018 from that take,the minimum and the maximum.So I've shown you how to use
the sql into to create one macrovariable as in the
case, the year maxor multiple macro
variables, but you'dhave to have a value
or an expression.I could put a comma and put
another value-- for example,make it 100--and I could just put colon,
comma colon and the productnumber.So if you know a
static value, you'rewelcome to do that, put
&, and you'd run this.Let's see now.You forget to do something.Hang on.Ah, I have a period
instead of a comma.Here you go.So a quick fix to
get this going.Just a little typo.Here we go.There's the 100.All right.So you probably noticed
that it had more spaces hereand that's because he's
using some kind of defaultformat for the number,
and I could put it outmaybe as the three
byte string so we'dhave more control that way.That's why you should
put functions earlier.You see the spaces?It makes a difference.All right.What if I want to create
several or actuallycreate one macro variable
but take a list of the stateand makes and put all those
values separated with a spaceinto this one macro variable?So check it out.Select distinct, order
where unique, carmake into colon CarMakes
plural separated with a blank.Yes, there's a space
between these two keywordsthat you can use with
sql from sashelp.cars.I'm going to put them out.See what we get.Now, that's the report,
and here's the list.Here's a list of cars.But what if I don't
want that report,but I just want that list.I can do noprint, and let's see.I'll separate with an asterisk
instead of a blank this time.You may say, why would
you want a list like this?It might be that I'm doing
some data validation,and let's say that I create a
macro program with a parameter,and you're suppose to
type one of these values.What if you mistype
Chevrolet or Buick?Well, I might
validate it, and thenshow you what this looks like.What's valid?And put it out in red.A number of reasons you can
generate a validation list.What if you know you have
38 records in that table,and you want each
make to be assignedone of these respective macros?And notice the syntax--select distinct--
meaning unique--make out of sashelp.cars.Let me show you
sashelp.cars [INAUDIBLE]..And there's several records.You see Acura, Audi, BMW.The last one is Volvo,
Volkswagen, Volvo.And I want a distinct
because I don't reallywant these repetitious values.I want a distinct of
each, a distinct make,into starting at Make1--the dash means through--Make38.So you need a hyphen if you
want to put your end value.I'll get back to
that momentarily.But you have to put a colon
in front of each rangeand then at the beginning and
ending of that range of macrovariables after the word into.Distinct make into colon the
first one dash colon the endone.If i run this, I'll just see two
of the makes, Acura and Volvo.Remember I could
have done the noprintif I don't want a report.And let's say I don't know
how many makes I have.Now, this is a nice feature.I can just put a dash.Select distinct make
into colon make dash.It will count them for me.Remember what I said earlier.Whenever you use proc
sql, you automaticallyget that macro
variable called sqlobs.It will tell you how many
records were generatedfrom your last proc sql.And it'll only show
you two of the makes.I could show them
all with a macrodo if I had a macro
program, but thisis outside of a macro program,
all of these [INAUDIBLE]..So Volvo, Acura and Volvo,
there are 38 of them.But if I want to see
the fifth and the tenth,and I don't want a report so
I'm going to type in noprintthis time.And this'll show
you that this works.If you don't want
to see a report,and you're only interested
in creating macro variables,go for it.Cadillac, GMC--
we got 38 of them.To investigate your
macro variable storage,_all_ shows you everything.It will show you the automatic
macro variables plus anythat you create.So how can you tell them apart?The automatic ones are the
ones that SAS created for you.The global ones are the
ones that either you createdor the interface you're using
created or probably both.So I'm looking at some
that were automaticallycreated from Enterprise
Guide, but theseare ones that I
mostly created, whichyou see here, the students.You can be explicit and tell it
which macro variables you want.Remember, you can view
the assignment at locationequals its value.You can use automatic macro
variables or user-defined macrovariables.We talked about that.For macro programs, you begin
with %macro, give it a name,and to conclude it, you
do %mend the same name,or you can leave it on.So inside of this macro
is what I'm highlighting.That's what it's comprised of.It could be proc
steps, data steps,or macro statements
like %let, %put.It could be a call to a macro.And you really haven't
seen the full strengthof the macro facility.There's some power behind it.You can have it-- you can
create macro programs thatcould do a lot of
work on your behalfand just use it
whenever you want.So from here to
here is defining it,and this is how you execute it.A semicolon is not necessary.It would be as if you put
two here in this case.And let's put the
second one in red.All right.So I'll run this macro.This will create it, and
then, this will call it.If this is all you do,
you will not see a result.You have to call it.The current time, 5:43--what, then the current date.Then that's all this macro data.It actually gives you sysfunc
to get the date for todayand write that out
in worddate-- that'swhat worddate looks like.And then sys time to put
the time out and then ampm.And you see the ampm.Create it, display it.Here's a macro program
that has parameters,and these are
positional parameters.This make is a positional.That cylinder is positional.When you create the macro
program and use parameters,you don't put ampersands here.It's only in the
code where you wantthem used like here and here.So I'm filtering with this
where statement where makeequals the make I specify
when I call it like Ford.And then in the title,
I'll put the make and thenthe number of cylinders like 8.All right.Let's try it.If I flip these two around
and put eight and Ford,I'm in trouble.I will be guaranteed
errors pretty much.Here we go.Ford, 8 cylinders--Keyword-- I like keyword because
I can set my default values.Like if there's
one of those cars Iwanted like Mercedes, Porsche,
Audi's good, BMW is good.It's my wish list.So %cars.I have a keyword parameter.I'm assigning make,
my default Audi.All right?So we got a reunion coming up.I'm going to set that
one as my default.Cylinder equals 6, and
then my proc print,I'll filter for the make
of the car equals Audi,cylinders equals that number
of cylinders, and in the title,I'll emphasize what I selected.And here's the conclusion.It does nothing until I call it.Well, it will
actually define this.There's an option that will tell
you if it's executed or not.So I'm going to run options
mcompilenote=all just to showyou in the log that it was
executed without error.That's what that
option does for you.The default is nomcompilenote.And it's is only for
when you define it,not when you call it.So here, notice I'm calling
cars with no values.I better put that back so you
can see what the defaults are.If I don't put anything, it
will use Audi and cylinders of 6because those are the defaults--Audi, six cylinder.But I can override
any value, and Idon't have to put them all.I put cylinder 4 and
put a 4, the makewill still be by default Audi.4 cylinders--So you can have a macro program
with positional parameters,keyword parameters.You can mix the parameters up,
but the positional must comebefore the keyword .Now, I really like the
iterative capabilityin the macro facility
because I oftentimeswant to do things repetitively
against groups of dataor a series of files.I'm using sql to generate
a list of my car makesand put it into a macro
variable called makes,and I'm separating them with
a tilde from sashelp.cars,and then I'm displaying them.I'll type car makes colon,
So you can see my makes.I'm going to write
it out in red again.Here we go.And I'm going to do
this, and here we go--let's see.Here we go, and so remember,
I separated then with a tilde.Car makes, and we have
them all displayed.Here's the next one where
I have an integer do loopwithin my macro program.So that list that
I just showed you--this is part of the mix--I'm going to grab each one
of these at a time, BMW,Chevrolet, Chrysler, Mercury,
Mercedes Benz, Lincoln, Lexus,Rover.I want to grab one at a time.Let's start with Acura, and I'm
going to filter for each one.Notice I'm using a %scan.As I said, I'm going to
scan for Acura first.I'm going to do a proc
print filtering for those,the cars that have that make.Then when I go to the second
one-- it will be Audi--and I got to the third
one, BMW, then Buick,that's what I'm using
this do loop for.i will automatically
be a macro variable.I can call it whenever I want.Start at 1.So when i is 1-- think about it.I'm going in sashelp.cars where
the scan of makes in that listI just showed you, &i
will resolve the one--remember I said this is
automatically a macrovariable-- using the
tilde as the delimiter.So it will grab that
first car, which is Acura.So where make= Acura in double
quotes because macro triggersresolve, macro references and
functions resolve within doublequotes, but not
in single quotes.And then in double quotes,
I'm putting the exact makethat I'm filtering for.So I should get 38 proc prints.Acura, Audi, BMW, Buick, Volvo--now this is just a
straightforward example,but focus in here
in the do loop.It could be that I use I/O
function to point to some sharedirectory or path.can have it search for a certain
file type like Microsoft Accessor Excel and do something
with every single oneof those files that find.It might be to create a
SAS data set from each.There's a lot of power here
with these macro do loops.What I could do is make
a list of all my makesand actually put each make in
a different macro variable.I'm just putting out a few and
telling you how many I have.If we were to run this, I'll
create make1, make2, make3,make4, all the way through
38 or how ever many I have.And then, remember
I said sqlobs?You get that automatically.It will tell you how many
records were produced.So Acura and Audi--I just showed you two,
but there's a total of 38.Now, check out this alternative.In this allcars macro--remember this one?I can change it to say
%do i = 1 %to &sqlobs,and I don't have to create a
macro variable called count.I can use this
automatic one, and itworks precisely the same way.Same result as you saw
just a little while ago.Acura, Audi, BMW--The power of the
macro facility extendsbeyond iterative processing.You can do conditional
processing,which is very powerful.We have one parameter
is positional,and I'm checking the automobile
type, the automobile make.If it's Chevrolet or Toyota--remember that when we looked
at it on the slides? %then do,a proc freq for that
make, tables cylinders.It's going to do frequency on
those with a title indicatingthe make.If it's not one of the
two that I'm checking for,then just print the automobiles
from that particular make.So only if it's Chevrolet or
Toyota will I get a proc freq.For the other 36
makes or whatever I--I should say 36 because I'm
not doing it repetitively--but whichever one I specify,
if it's not one of these,I'll get a proc print.So you can see down below, I'm
going with Chevrolet first,and that's my proc freq.Frequency for [INAUDIBLE].Make is Chevrolet.And for Toyota, I will
also get a proc freq.There's Toyota, and
what about Acura?I should get a proc
print for Acura.Here's Acura.What'd we get?Now, remember some of those
options like MPRINT and MLOGIC?I could use those if I want
to so I can get a betterassessment of what's going on.And what if I mistype?What if I type
something incorrectly?Well, I didn't account for that.If I run this, neither
block of code will run.Well, it will attempt
to do the proc printagainst sashelp.cars, but the
where will fail because there'sa misspelling for the make.So it will at least
attempt to do a proc print.All right.But I mistyped it, so I
get zero observations.What if I wanted to see the
code that was generated?Remember MPRINT?I can do options.I'll type options mprint.So at least, I can take a look
at the code that we generate,and as you see, make=Aura.Oh, I made a mistake.MPRINT is very handy.MLOGIC is very handy as well.Let me do nomprint
and turn that off.All right.Let's move on down
to the next part.Here's an alternative.I can do %let mke=Chevrolet
and then %if within a macrodefinition again--make is Chevrolet or make is
Toyota %then %do and proc freq,else, do, then proc print.An alternative--
with an alternativewhat we're saying here is
an alternative to this.So if it's Chevrolet or
Toyota, it'll do a proc freq.Just do %else do proc print.So I could put that
inside of the macro,and I could check
with a % and let.But the emphasis here is that--check it out.This is not within a macro,
but it will still work.That's the point.It will still work.So I said earlier, %if %then
need to be inside a macro.That's not true.They changed that on us.You now have the ability--I forgot about it-- you
now have the abilityto do this outside
of a macro program.But to be honest, I
would probably stillhave it in an macro
program because Imay want to do
things repetitively,something along those lines.If this is all you
wanted to do, go for it.I've talked about
MLOGIC, MPRINT.And you can turn them
off when you're done.This turns them on.And I showed you
MPRINT a moment ago.You can go to the log.MLOGIC shows you the beginning.MLOGIC also shows you the
conclusion of your map.It shows you the result
of an if then else test.MLOGIC also tells you
when it's finished.MPRINT shows you the
code that was generated.And SYMBOLGEN just
shows you the resolutionof your macro variables.So to be honest, I typically
don't do all three,even though you can.Whatever works best for you.I normally do MLOGIC
and MPRINT when I justwant to see how the
macro programs work.But if I'm having an issue
with some macro variables,then I'd probably just turn on
SYMBOLGEN and possibly MLOGIC.That's how I debug.So we can see the beginning.We can see the assignment
of the parameter,and if that condition
is true, then youcan see all of this
proc freq code.So that concludes
this first lesson.Let's continue with
the second part,and that's the Not So
Straightforward Basics.So here's a multiple
choice question.Which selection is
a valid referenceto the macro variable name?&name and that dot, and the dot
disappears when it resolves.These are typical macro
variable references.If you put the period
at the end of each name,when the macro variable
resolves to the assigned value,the period disappears.So why would you
bother to use it?Well, you would need it if
you have some text followingthe name of the macro variable
that would compromise the name.For example, let's say
that after city, youwanted the word sunshine,
like AtlantasunshineChicagosunshine.If you typed the word
sunshine after the y in citywithout the dot, then
the macro processorwould interpret that macro
variable name as citysunshine.It would not exist, and you
would get a warning for that.So the period will be
required in that instance.Here are some examples.Let's say we create these
two macro variables, monthand year.When two macro
variables are adjacent,right next to each other,
it's typically not a problemin resolution In terms
of getting a warning.%put &month&year where
&month resolves to MAR,&year resolves to
2020, not a problem.Let's go to the second %put.Once again, &month
resolves to MAR, no issue.But &yeardata-- I want to
put the word data after year,and I get a warning because
it doesn't look for year,it looks for year
data as the name.And this is where the
period becomes very handy.If you have text following the
name of the macro variable thatwould compromise the
name, then you need a dot.Now, if it has something like
an asterisk where the dot is,that wouldn't be an
issue because an asteriskcan't be part of a name.So in this example, &month is
MAR, &year then the period--all of that resolves to
2020, and the text datacomes right after the zero.What if you need a
dot between text?For example, let's say that
I substitute the libraryas sashelp, and I
want class after that.Well, the problem is &lib.resolves to sashelp,
and &dsn becomes class.So if you try to
create that data set,it looks as if it's
in the work library--work.sashelpclass.The appropriate way to deal
with this is to put two dots.The first dot will be
treated like a delimiter,and the second dot would remain
there as text, and it works.How many times does SAS
scan this [iNAUDIBLE]?If you have multiple ampersands,
pairs of two become one,and there's a re-scan
process going on.While it's scanning, any
macro variable referencesthat are legitimate
will resolve.Let's take a look.Well, the answer is
two in this case.These two become one.City's carried over.&year, whatever value
it is, resolved.And proceeding that value
is the new reference.Two.Here's a perfect example.Let's say we have these four
macro variables-- year, var,city2020, location2020.Adjacent macro variables,
&city and &year,you get an apparent symbolic
reference not resolvedbecause there is no macro
variable called city.There's a city20 but no city.That's why you
received a warning.Let's try it again.I was trying to get the
year to resolve first,but it doesn't work this way.Let's put a second.Two ampersands become one.C-I-T-Y is just plain text city.&year resolved to 2020,
and we're left with & no k,&city and 2020, and that's
a legitimate macro variablereference. &city2020
is Washington.Let's try it again.And here's the rule.Multiple ampersands
force the macro processorto scan a macro variable
reference more than once.Two ampersands--
as I was explainingon the previous slide--
resolve to one, and scanningcontinues looking for legitimate
macro variable referencesand continue to reduce
any multiple ampersandslike two to one.Let's go for this example.This time we have
three ampersands.The first two make a pair.They're reduced to one.You keep going to the right.&var is location-- it resolved
to location-- so so far,we have one ampersand from
the two, &var is location.So far we have & location.Keep going to the right.&year becomes 2020, and the
reference you're left withafter the first scan
is & location 2020.And there is legitimate
macro variable referencewith & location
2020, which resolvesto District of Columbia.Every pair of two become one.The first two make a pair.That reduces to one.&var is legitimate.That's location.&year is 2020.And on the second scan,
we resolve location 2020,the District of Columbia.Here's another multiple choice.Which of the following
are macro quotingfunctions that mask special
characters or different meaningin text?What are the two macro
quoting functionsin the list of choices?There are two.%STR and %NRSTR.Those are the two.If you can see the
issue in your code,you can quote that text with
one of these quoting functions.So here's a breakdown
of what each can do.The %STR quoting function can
mask or cover up the meaningof certain tokens,
certain operators.It can mask and,
or, not, equal to,and make those operators
look like plain text.So for example, let's say you
were doing a %if & a macrovariable equal to and you wanted
to specify Nebraska or Oregonor Indiana.Well, if you don't use one
of these quoting functions,O-R would look like
the or operator,this one like the in operator,
and not equal operator.You don't want them to behave
that way in certain cases.So the %STR can make all of
these different operators looklike plain old
text that are a b.They have no significance.This quoting function
can also mask thingslike plus symbol,
tilde, the semicolon--it can make that
look like plain text.It will remove the
normal meaning.So don't be the
end of a statement,but just be a piece of
text, plain like a letter a.%NRSTR, no resolution string,
does everything %STR does,but in addition
to what %STR does,it also masks macro
triggers like ampersands,percent symbols.%STR cannot mask those, cannot
change the meaning of thoseto look like text, but
%NRSTR, no resolution, can.If you can see the
issue, for example,if you see O-R and you know
you want it to be treated liketext, or you see an ampersand,
and you want those things to betreated like text, if
you can see the issue,then use %STR or %NRSTR.In this example, I want the
macro variable step to beassigned a full data step
so I've used a %STR to quoteor mask--cover up-- the meaning of
the semicolon here and here.So what does that do?It treats the semicolons,
these first two, like text,like you typed the letter t
or b, just plain old text.So they don't have
their normal meaning.The last semicolon does.The last semicolon is the
conclusion of the %let,but the other two as if
they're just plain text.Now, things that come in pairs,
you have to take an extra step.For example, single
quote, double quotes,parentheses, things
that come in pairs.You have to do an
extra little work.Put a percent symbol in front
of that token or that item.Things that come in pairs--
percent single quote.I'm trying to make this
apostrophe Employee's Report.What about macro triggers?%STR cannot help us
with macro triggers.Again, %STR cannot quote or
change the meaning of thoseto look like text
or behave like text.But %NRSTR can.It can mask the ampersand, and
it can mask the percent symbol.So in this case, %let company=--I'm trying to specify R&D--but I know that &D would
look like a macro variablereference, and the macro
processor would go look for it.So to prevent that, to make
this ampersand behave like plaintext, like you typed a B or C or
D, plain text, you use %NRSTR,the no resolution attempt.%NRSTR can quote or change
the meaning of ampersandsand percent symbols.
%STR cannot.So if you can see the issue,
semicolon, quotation mark,ampersand, percent symbol,
you can use %STR or %NRSTR,whichever is appropriate.But what if you
can't see the value?What if you can't see the text?So for example, let's say that
in a data step called synputxor sql, we are assigning a value
to the macro variable company,and we're not sure what the
value is because we can't seeit, what you would use
instead of the two we've sawpreviously would be superq.Now the question here is which
superq has the correct syntax.It's the first one.You put the name in
a macro variable,and when the value resolves,
superq will quote or changethe meaning of any
tokens that wouldcause you an issue
like an ampersand,like a percent symbol.It would mask what is resolved.We can't see it yet.It has to resolve.%superq.In this macro check,
I'm checking a companythat we specified when
we called the macro,and I'm trying to
indicate Oregon Insurance,but there's a problem.If I had &company, and it
resolved to Oregon Insurance,that O-R would be interpreted
by the %if as an OR operator.I don't want that.So I'm using %superq so
that when this resolves,that O-R would be
treated like text.So why don't I use
& instead of superq?Because when I'm
looking at this,I cannot see what the
value of company is.Now when I call it, I
know I'm typing this is,but if the deal is this still
resolves to whatever that valueis, and when it resolves,
it will cause an issue.It would be as if you're saying
%if ABC not equal to OR--because I can put
the OR operator--Insurance.So since we're
masking whatever thisresolves to, if it's not equal
to ABC, we put not a match.Here's the end because I have
a do block, %else put Match.So the answer for this
one will be no match.You don't need to put a leading
ampersand when you use superq.You will know this
is a macro variable.Let's try a different approach.Let's say I run the
data step, but I'monly interested in creating
the company macro variableand assigning it the
value of Smith and Jones.This is a call symput example
where the name of the macrovariable's in quotes, and the
value is in quotation marks,too.%let new company--I'm making a new macro
variable-- =%superq of company.And then I do %put The new
company is &newcompany.Because of superq, when this
resolves and gets assignedhere, that ampersand is treated
like text instead of a macrotrigger.So Smith and Jones will
be assigned here as text,the entire thing,
and when it resolves,no issue with trying
to resolve &Jones.That ampersand is
treated like text.The meaning of it is changed.It no longer means
what it normallymeans because I quoted it.I changed the meaning of it
to look like text with superq.Here's another multiple choice.Which option enables
the macro processorto recognize the IN operator
for checking things in a list?It is the MINOPERATOR,
the macro inoperator.You have to turn
this option on if youwant this to behave like
a macro INOPERATOR asopposed to one that normally
works with the where statement.It looks like this.When you define a
macro program, ratheryou have parameters or not,
you have put a forward slash,put minoperator.That stands for macro inoperator
such that on the %if &,for example, type, checking the
value of this macro variableto see if it's in this list.For this in to be treated
like a macro inoperator,you have to use this op.And what it will do is check to
see if it's SUV, truck, wagon.If it's one of these,
%then, %do, whatever.No quotes needed, no commas
needed, no parentheses.And here's the second one.If it's in Sedan Sports,
then do something else.Don't forget your percent sign.Instead of the word in, you can
use a pound sign if you'd like.The pound symbol would
mean the same thing.The default
delimiter is a space,but you can also define
what the delimiter is.Here it is.Macro indelimiter.You put it in quotation
marks, and in this case,we're making it a comma.So now it reads as separate
things with commas.All right.So what I'm going to do--I'm going to switch
to our EG sessionagain and just go
through the examplesthat I showed you on a slide.I'm turning off any titles
or footnotes that are stillrunning that are global.All right.So here's the first example.Remember the one I did
with month and year?And as I stated before,
together next to each other,not an issue.They both will resolve.You get MAR 2020.Month is MAR.&year becomes
2020, not an issue.This is an issue because I want
the word data to come after2020, but the word in that
macro processor will work,it will look for &yeardata
and not just &year.So we will get a warning
because there's no such macrovariable called yeardata.Not resolved, and it
writes it out as it is.It did not resolve.Month resolved, but not yeardata
because yeardata is wrong.The period is the
delimiter wherewe can use it to mark the end
of a macro variable referencesuch that the name is not
compromised with the word data.And this time, we get 2020 data
because the period goes awaywhen year resolves.That takes care of the issue.Here we go.The period is removed
when it resolves.So the point of these two
%puts, if you want a period,you have to put two.The first one here goes away.For these two, the
first one goes away,the second one stays as text.No period because
you only had one.One period because you had two.Cool.One stays.The first one goes away.Let's look at substituting
a library reference, whichcan cause an issue
in a data set name.With one dot, you are in
trouble because &lib resolvesto sashelp, and the
period disappears.So after the P, you get &dsn
resolved in the class resultingin sashelpclass, and
it looks like one name.And it will assume it's
in the work library,and you will get an
error because thereis no such data set.There's no sashelpclass
in the work library.You meant to put a period
between sashelp and class.So to address this
issue, you use two dots.The first dot gets
resolved with the library.The second dot
remains there as text.That's your answer.You need two dots
in this situation.Think, in the log, the notes
indicate sashelp.class.So if you have inherited
programs with extra dots,or you have programs to tech
yourself, that's what it means.First one goes away.Let's move on to
multiple ampersands.The reason I would
use most ampersandsis if I want to try to build a
reference with multiple macrovariables perhaps, and I
want to delay the attemptto resolve maybe one or more
of those macro variablesto get what I want.Let's look at this example.We have four macro variables.We've got year.We've got var, city2020,
and location2020.I want to use the value
of a macro variableto help me come up with the
name of another macro variable.If we put two macro variables
adjacent one another,it's really not an issue.Well, actually, it is an issue.Let's look at this %put.We do not have a macro
variable called cit.We have one called
city20, but not city.If I use this first
one, I'm goingto get apparent symbolic
reference for city not resolvedbecause I don't have a
macro variable called city.Not resolved.Now, if I had 2020
after it, it wouldbe a legitimate reference.So here's what happens.I use multiple ampersands.Let me explain what happened.There's a scan rule when you
have multiple ampersands.From left to right, every pair
of two gets reduced to one,and while you're scanning, if
there are any legitimate macrovariable references,
they get resolved.Then another scan is
made to again, reduceany multiple ampersands,
pairs of two to one.So let's think about this.These two-- I'm just
going to type this--become one.After these two get reduced,
we'll keep going to the right.City is just a piece of text
so that's carried along.We'll keep going to the right.&year is 2020, and it resolves.And so this is now a
legitimate reference.&city2020 is Washington.So if that's true,
watch what happens.The first one we
got the warning.The second one,
we get Washingtonbecause the multiple
ampersand there.We're using this to
delay any attemptto resolve city until we get the
full part of city to the 2020.Let's look at the next one.This is how the next one works.The first two
ampersands become one.Keep going to the right.&var-- remember, I made
var=location, right--so &var resolves the
locations, and &year is 2020.So from this reference,
you get &2020,which resolves to
District of Columbia.Let's watch.If this is true, I should
get District of Columbia.Here we go.District of Columbia.We're using the value
of other macro variablesto help build a reference of
yet another macro variablewith multiple ampersands.All right.So we'll move on to
quoting functions.We talked about
them on the slidesand now here in the
program examples.%str can mask semicolons.You can mask each one.You can also run it this way.You could have actually just
used one %str and maskedthe entire data step.These two will be
treated like text.This is the legitimate
conclusion of that %let.And so when we resolve &step, it
becomes data new semicolon x=1semicolon, and the run statement
following it will make it run.And we'll get one
observation with one variablecalled x with a value of 1.You got data new, x=1,
and the run statement.Work.new has one observation
and one variable.So it worked because we masked
the meaning of the semicolonswith %str.%str can mask semicolons,
mnemonics like AND and OR, NE.Things that come
in pairs, rememberyou have to put a percent
symbol in front of it.So if I'm trying to make this
possessive, Employee's Report,I'd put %str in that percent
symbol so we get an apostrophein Employee's Report.Things that come in pairs, you
have to do a little extra workand put a percent symbol
in front of them whenyou're quoting a %str.And what about macro triggers?Remember I said that a %str
cannot mask or cover upor change the meaning of macro
triggers, but %nrstr can,the no resolution string.So I really want this to be
R&D, R and D. If I put R&D,&D would be regarded
as a macro reference,and it would go to the
global symbol table.The microprocessor
will not find in there,and you'll get apparent
symbolic reference not resolved.So to address this issue, I use
%nrstr around the ampersand.So it treats it like text.Then we get the D. This
will resolve to dateinside of that SAS section.I'm making company remember
equal to the entire thing.R&D as of that date.So because of %nrstr, there
is no attempt to resolve &D.This is treated like text.So if you can see the problem,
like I can see that this isthe problem or
this is a problem,you can use %str or %nstr,
whichever is appropriate.Here I see that the
semicolons will be a problem,so I use %str to quote them.Take a look at
this macro program.There is an issue with
what this will resolve to.Look at what we-- we would call
this positional parameter--this macro, this program
is a positional parameter.&company will resolve
to OR Insurance,and that's an issue because the
%if macro statement will behavelike this--%if ABC not equal
to OR Insurance.This would look like
the OR operator.We don't want it to
behave like an Or operate.We'd have some issues.I want O-R to be treated like
text as if it was Oregon.So if I go ahead and run it,
say well, I'll take a guess.And maybe it'll
work, maybe it won't.OK, well, it didn't.OK.It's referencing a character
there I'll use found ABC notequal to &company.It stopped.That's an issue.So to address this,
I can use %superq.Remember we talked about that?But when you specify the
name of the macro variable,you do not proceed
it with an ampersand.Superq doesn't need that.And if we rerun this--and remember, if ABC
not equal to this,put no match, else match.Just want to write it out.Let's run this
with O-R Insurance,and we should get no match.Not a match.What if we do indeed put ABC?If you want to mask
macro triggers--let's say that I define
a macro variable--we'll call symput x or call
symput, call it company,assign it Smith&Jones.So this is the hard
coded value, and thisis the-- well, the name, and
this is the hard coded value.All right?And let's see here.All right.So we do a %let
newcompany=&company,the problem is this will
resolve to Smith&Jones,and &Jones would look
like a macro trigger.Macro processor
goes to look for it.It's not there.You get apparent symbolic
reference not resolved.And the place it goes to look
is the global symbol table.So we'll run all of this,
and I expect an error.The new company is Smith&Jones.Apparent symbolic reference
Jones not resolved.So what we do is we
change this to use superq.Again, when you use
superq, you do notneed to put an ampersand in
front of the name of the macrovariable.OK, I'm going to write
this out in red again.We rerun this with superq, super
hero macro quoting function.The company is Smith&Jones.No resolution attempt on
this because this ampersandis treated like text
thanks to superq.So when should you use %nrstr
and %str versus superq?If you cannot see the resolution
of that macro variable,or if you can't
verify what it is,use superq because superq will
mask or change the meaningof whatever it resolves,
that resolve value.%str and %nrstr is when
you can see the value.You can see that is a quote.You can see that it's N-E or
O-R. You can see the issues.Let's move on to
the inoperative.Remember, we have to type on
the macro definition, keywordor parameter or no parameters
at all, / minoperator.That stands for
macro inoperator.It makes this inoperator behave
like a macro inoperator asopposed to the
one that regularlygoes with if or where.If you don't use this option,
it will treat this like textand not an operator.It would be as if you said
%if whatever the type resolveto in, and that's just
like typing the word car.So of course, if we
have this turned on,we're checking for
values in this list.No parentheses are required.The space is the
default delimiter.So if it's one of these, do a
proc print filtering for thattype, %if the type
is sedan or sports,and do a proc freq on the
make of the car for that type.So let's give it a shot.We're going to try SUV.There it is.What if we go ahead and
throw a little title in here?So we can see exactly
what type is selected,I'm adding this title above the
proc freq and the proc printso we can see exactly
what the value looks like.So let's try these three--SUV, truck, and wagon.So corporate SUV,
truck, and wagon.Let's try sedan and sports.Here's the proc
freq for the sedan--here's the make--
and also for sports.The next section is--let's see-- what if I want
to specify the delimiter?So for something other than
the blank, in this case,the comma, just
use mindelimiter.OK.The macro indelimiter
is a comma, again,so it treats this as a
separator instead of a space.All right.So that will conclude the
second part of lesson 1.All right.We are ready to start lesson 2--Doing More with Macro.2.1, we will talk about SAS
I/O or input output functions.We'll talk about
dictionary tables,the DOSUBL function, however
you want to pronounce it,and then, macro
termination and branching.So let's start with 2.1.Most of you if not all of you
have done a proc contents,and when you look at the
output from proc contents,there is some information,
metadata information, that'smore numeric versus character.For example, if you look at
the value for observations,it's the number.If you look at the
value for compressed,you see a character, yes or no.So some of the information's
character based.Some of it's numeric.There's also information
pertaining to variable name,if it has a label.The reason I'm mentioning those
is because there are functionsthat can be used in conjunction
with %sysfunc so they can beinvoked to perform some action.So for example, there's
an exist function thatcan verify the data set exists.There is an open function.If you want to get
certain informationfrom a table, especially
metadata information,you have to open up
the data set first,and when you're finished,
you should close it.There's ATTRC, attribute C
versus N where you can getcharacter-based information or
numeric-based information afteryou have opened the data set and
using %sysfunc to activate thatfunction against the open
data set to get that relativeinformation.There are also functions
pertaining to the variablewhere you can get the
label, length, and the type.Here's an example of a macro
program that references a dataset, and we are illustrating
the exist function.We have one
positional parameter,and here's the test in the
macro program-- %if %sysfunc--we are using sysfunc to
invoke the exist functionon the data set checking
to see if it's equal to 1.If that exist function
returns a value of 1,that means it was
successful in verifyingthat the data set does
exist, and if so, wewill print the data set for
the first 10 observations.%else put a note into the log
saying not a valid data set.In the second macro program,
again with the data setparameter, I'm pointing out
the use of the open function.Open, ATTRN, and Close.You have to open up the data
set before you can invokethe ATTRN, the attribute
N function or C.So we are using sysfunc to do
an open, and what happens--there will be an identifying
number associatedwith that open data
set, and it willbe assigned to
whatever variable namewe're putting in like the
data set ID, the S-I-D.Next, I check the value
of that macro variablewe create with the open
function on the table.If it's not equal to zero,
%then %do the following.So let's look down below.What happens when you open
it, if it was successful,it will return a unique
number, some integer value.If it wasn't able to
open up the table,let's say it doesn't exist,
it will return a zero.So we're saying, if it
doesn't equal to zero,not equal to zero, that
means it was successful.Then, I'm creating a macro
variable called numobs.I'm invoking the %sysfunc, which
will activate the attribute Nfunction against
that data set ID.Remember, we assigned
an ID number to it,and I'm look at that
ID number and knowthat is the data set
that it's referring to.And we will look for the
nlobs attribute, whichis the number of observations.Secondly, we use the
ATTRN function againto get a different
attribute this time--the nvars attribute,
which will tell ushow many variables, the
number of variables we have.And when we're
finished, we can closethe data set by
closing the data setID, that macro variable
that we created assigningit a specific
integer value that'sassociated with that data set.Now we're setting it up almost
like a %macro assignment.We're calling this rc
implying return code.We could call it x
or y, not a problem.Another function you
may be interested inis the vartype to figure
out what type of variableis in position 1,
position 2, and so on.So in this third example,
data 3 with one parameter,we invoke sysfunc again, check
and to verify the data setdoes exist.If it does, %then
%do the following.We will assign it an ID
with the open function,then determine a number of
variables like we did before.But what's different
this time isI want to count the number
of character variables.So I make up a variable
called char care,and make it equal to zero.The purpose is to count, use
it to accumulate the numberof character variables.I do a %do i=1, which
gives me to &numvar--and you might recall
that that willhave the number of
variables assignedto it, the number of
variables in the table--and then we want to
check each variable.So if I have five variables,
this will be from 1 %to 5.Remember that the
index variable, i,and whatever you call
it, will automaticallybe a macro variable.So let's iterate through
part of this do loop.&if %sysfunc.We are invoking
the vartype, whichremember is the variable type.Here's the data set ID that we
defined above, and ampersandi is currently 1.So we're checking
out the variable typefor the variable in the first
position from left to right.If it's equal to C-- and by the
way, quotes are not necessary--%then %do.I have in %end %let char=%eval
of the current value &char+1.So we're adding one to it.When we get to the %end, i
becomes 2, still in range.Again, I'm using
an example of 5,and we check to see if the
variable in position number 2is character.If it is, we accumulate again.If it's not, we do nothing
other than hit the endand go back up--this particular
end and go back up.All right.And when we're finished--good practice-- close
the data set ID.And we use %sysfunc to do that.We can also check
external files.Instead of open, we
use DOPEN and DCLOSE.D stands for directory.We have a lot of
capability here.We can open up a directory
and search for filesand manipulate them
in a certain way,and especially, if we
have a series of files,you can do things repetitively.We explore that in the
advanced macro course,but we will see
some features here.DOPEN will open up
a data set path--well, a directory path--and DCLOSE will
close the directory.DNUM is a function that will
tell us how many members wehave in that directory.DREAD returns the name
of the directory member.So I can determine
how many I have,how many different
items in that directory,and read each one at a time.And when I'm finished, I
can close the directory.So in this particular macro
program, I supply the path,and then I use a
file name statement--if you haven't seen it before,
you can set up a file referenceto that location, to that path--next, %let the directory ID.Just like I opened
up the data set,I also have to
open the directory.So whatever path you assign
here will be substituted here.Notice I'm just using the
name without an ampersandin front of it with this dopen.Next, the directory
number, dirnum=%sysfunc.I'm invoking the DNUM function
against that directory ID,and it will count
the number of membersand assign it to dirnum.Now I know how many I have, I
can do a %do i=1 %to howevermany I have, and I'm going to
do a %include on the path Ispecified when I called
the macro program,use sysfunc to do a directory
read on that directory IDfor the first member
of that directory.And I'm using source2 to
show notes in the log.So basically, I'm doing
a %include to reference--it's generally used to
reference a SAS programso it can be
executed, and that'swhat we're doing in this case.And we're going through the
directory one member at a timeand executing it assuming
it is a SAS program.When we're done, %sysfunc
dclose and that directory ID.We assign it to a
variable just like wedid with the open table.It could be a different name.And not only do we
close the directory,but we clear the library
reference-- well,not the library reference, the
file reference to the path--and that concludes
the macro program.The next section is
on dictionary tables,and there are several available.The dictionary files track
all kinds of informationin your SAS session.For example, they
track macro variables.It tracks the different
libraries you have access to,the data sets in those
libraries, every single columnin every single data set.You can get all
kinds of informationfrom these dictionary files.Now, you can only access
them with proc sql,but in a sashelp library,
we have some views of someof these dictionary tables.They're very useful.Some folks are not aware,
but in this course,we're making you aware of these.Two examples we will explore
are dictionary tablesand dictionary.columns.Tables contains information on
known tables at every librarythat you have access to,
works, sashelp, sasuser.Dictionary.columns,
when you use proc sql,can give you information on
every column and every table,so you probably
want to filter thatdown so that you don't
get tons of information.We are using the proc
sql describe statementto get a description of the
table called dictionary.tables.Remember I said that tables is
one of those dictionary filesthat contains a
lot of information.So in that
dictionary.tables file,you can get what's
called a libname--which has a library reference
you set up perhaps for a bitunless SAS set it
up-- the member name--which is the name of our table--and member type.So the member type
could be-- and Ishouldn't have said
the name of the tablebecause you can have
different kinds of filesin a SAS library.Tables is one of those types,
and catalog is another.[INAUDIBLE]So as we give this a try,
I want to filter down justfor the SASHELP.SHOES table.I use proc sql noprint because
I don't want to create a report.I'm actually creating
two macro variables,and I'm storing the
number of observationsand the number of
variables respectivelyin the macro variables, numobs
and numvar with the into clausethat we've talked
about previously.The information for these
two columns are coming fromdictionary.tables where
the libname is equal to--check out the %scan.We talked about that earlier--I'm %scanning the resolved
data set name SASHELP.SHOESfor the first word using
a period at the delimiter,so we end up getting SASHELP.Secondly, the second
word we assign,we've filtered for member name,
and that's the second word,for example, shoes.We have an additional dictionary
file called dictionary.columns,and as I said, it
has informationon every column in every
library in every data setin every library
in your session.In this macro
program called freq,I have two parameters,
one for the library, onefor the variables.In my proc sql, I
am concatenatingwith the delimiter dot, the
library name and the membername, for example, a library
reference and a table name.So I'm concatenating the
values of those two columnsand putting those into
a macro variable calledtables separated with a tilde.So I want, perhaps, a
series of table namesfrom dictionary.columns
where the libname isequal to some library
reference in uppercase,and the name of the column
is equal to whatevervariable you specify.So the bottom line
is this-- my goalis to look in the library for
every table library name thatcontains that variable.Perhaps I want to
join those tables.An alternative to using
these dictionary filesare views of these files.In the Sashelp library, there's
a view call Sashelp.Vtable,which is a view of this
file, and there's alsoSashelp.Vcolumn, which is a
view of the dictionary.columns.So you could use
these if you like.Here's an example, and I'm
using the data set as opposedto proc sql.So let's say you don't
want to use proc sql.You use the data set.Remember, we talked
about data null, _null_,which is using the data set
without creating a SAS dataset.Here's a %let, for an
example, dsn=SASHELP.CARS.In the data set,
sashelp.vcolumn,which is the view of
dictionary.columns,end=last so I'll know when
I get to the last record,where catx of the libname
and the member name,such as the data set name, is
equal to the data set that Ispecified up here.If the type is
character, then count+1.This is a sum statement.I'm accumulating count for
the character variables.Else if the type is num,
then numeric count+1.So I'm accumulating
a character variablecount, numeric variable count.If last-- when I reach the last
observation-- do the following.I want to create
two additional--well, actually, I want to create
two macro variables, one calledccount, which will have
the count of the charactervariables, and
then ncount, whichwill have the count of
the numeric variables,so that I can put them in the
title and use them elsewhere.And in 2.3, the
DOSUBL function--I'd like to cover that.This function enables the
immediate execution of SAS codethat contains some text
string as the parameter.That's the argument.This function can only
be used in the data set,and it needs to be
invoked with percent--well, it can be
invoked with %sysfunc.You don't have to invoke
with sysfunc, but you could.And you can use it
outside of the dataset with this sysfunc feature.If the function returns
a value of zero,the SAS code will
be able to execute.If it's non-zero, the code
will not be able to execute.This is an alternative
to the CALL EXECUTEif you've used that in the past.So let's take a look.Again, I'm doing a data null
because I'm using this functionto help submit some code.I'm making a literal,
which is essentiallya proc contents, a complete
proc contents set with a runstatement all in
quotation marks.And then, I use the DOSUBL
function against that variablethat has that text, and I'm
assigning it to a return codetype variable so I'll be able
to identify if it was successfulor not by checking that variable
value or just checking the log.Next, text2= a proc print,
data=sashelp.class for 10observation, user run statement.So that literal, which
is a legitimate set,will be the value of text2, and
again, DOSUBL will execute it.In the next example,
I literally putas the argument the text
string, which is a proc step,looks like two proc steps,
proc contents and a proc print.So the point is I
don't necessarilyhave to assign the strings
to other variables like text1and text2.I can just use that
as my argument.Not a problem.More on DOSUBL within a macro.So I'm writing a macro
program with two parameters,the library and the
data set, start.Here's the data null, and I'm
setting sashelp.vtable, whichis a view of
dictionary.table, and I'mkeeping the library and
the member name, whichis the table name, where
the libname equals whateverI specified when I called
the macro, such as sashelp.And by the way, notice I'm
moving it to uppercase with%upcase just in case you
don't type in capital letters,which we didn't yet.And memname equals the
%upcase of dsnstart pr,so all the data sets
that start with pr.You see the equal colon?Next, dosubl proc contents
against data= and I amspecifying in this literal, the
concatenation of the librarywith a dot and trim value of the
name to construct this literalas the code to
submit the DOSUBL.And then secondly, I'm doing
a proc print equal, and again,I'm concatenating the value
of the library with a dot withthe trim version of the member
or table name, in this case,and concatenated with
obs=10 in the run.So I have a legitimate
prop print statement.So we'll go against
the sashelp libraryand look for a table
that begins with pr.The data set, since we're
reading from vtablesand we're checking for
those that begin with pr,in the first iteration,
the PRDSAL2 tablewill be processed by proc
contents and proc print.In the second iteration,
PRDSAL3 and thenPRDSALE and then
PRICEDATA, PROJ4DEF.They all begin with pr.Remember, I'm looking for those
where it's equal to the start,the data set name
start, like pr.You can process a series
of tables that begin with,have the same
prefix so to speak.And then using this
within a macro--I have two macro
programs this time.The first one I'm
calling two procs,which has proc contents and proc
print against the table namethat I pass for this parameter.All right.So two procs will be proc
contents, proc print.Then I macro call
contents that referencea library the data set start,
which again what tables thatbegin with certain letters.Data null-- I'm using
the vtable, whichis, again, a view of
the dictionary.tables.I'm keeping just the library
name and the member name,which is your table name where
the libname equals whateverI specify when I call the macro,
and move it to uppercase justin case I forget to
type it that way.And the member name,
which is the table name,equal to colon means
it begins with %upcase.I'm moving whatever
I type to uppercase.Then the table will
be the concatenation--I'm using cats with my
libname dot the member name.All right.Don't have to worry
about patrolling blanks.So guess what?This function will
take care of that.And then I use the assignment
to invoke the DOSUBL,to invoke that macro
program-- you see %twoprocs--and concatenating that with
the table name and a closeparentheses.So I have a complete
call to the macro programfor every single data set
in that library that beginswith whatever text I specify.The last section, macro
termination and branching--We will take a look at the
%RETURN statement and the %GOTOstatement.The %RETURN will basically
terminate the macrothat's executing.The %GOTO will label a section
we can go to and perform sometask based on the value perhaps.Here's an example of the %RETURN
statement that will terminatemacro execution.I have three parameters, a
data set, a class, and a var.%if and I use %sysfunc again
to check for its existence.If it's equal to zero, which
means it doesn't have a dataset, we won't be able to set
up a data set identifier witha %put an error to the log.Not a valid data
set, and then return.It skips these statements
and ends the macro program.So for a table like xyz,
we will get a note saying,not a valid data set.Here it is.We can go to or branch
to a certain sectionthat we can control.Let's check this one out. %let
position class = the %sysfuncof the var number, num function,
on the data set ID for thatvariable.Figure out what number it
is, what position it's in.%if the position of that
variable is equal to zero thatmeans it's not in the data set.%then %goto invalid.And if we jump to
invalid, we're justwriting out a note
to the log statingthe variable is not valid.Next, position var--I'm using sysfunc
again, using varnumto look in this data set ID for
the position of that variable.Maybe it's an analysis
variable versus class.%if the position, the value
of that macro variable's zero.That means it does not exist.Once again, %then %goto
invalid where we write a noteto the log, the
variable is not valid.And lastly, %let var t for
type I'm implying, %sysfunc.I invoke the var type
function on the data setID for the variable position.That wasn't invalid, so we're
assuming it's valid position.And then %if that var
type is C for character,%then %do %invalid.branch will get put in the
log saying it's invalid,It's not valid.All right. %let data
set ID %sysfunc,close that data set ID, and
then terminate the macro programand return.And we'll see an example
of this code momentarily,but before we do, here's a
link to macro documentation.You can get online, and it's the
9.4 Macro Language Reference,5th Edition.So you can save this as a
favorite link and other Optionsyou can get quick access to it.What I'm about to do now is
move to the interactive sessionso we can try the programs for
the things we just looked at,and I'm going to start
with-- let's see--section 1 and lesson 2.We have a proc contents
where I'm submitting this,and we have 5 variables.Here are the variables
at the bottom.You can see some are
character, some are numeric.Now, I think many of you
recall that you can do a varnumand see the variables on
line from left to right.Well, in their order,
you see name and thenthe next variable.The first two are character
and last three are numeric.So keep that in mind.First two are character.Last three are numeric.So I have data1 macro program.The parameter's data set and
%if %sysfunc check to see if itexists.If it's one, it exists.Then I'm going to
do a proc print.Otherwise, write a note
saying it does not exist.So I will use my favorite
way of writing it out in red.So we will run this.And this data set does exist.Let's put a typo in
class, run it again,and we should get a
note in the log in red.Here we are.Not a valid data set.So that's working properly.That was an illustration
of the exist function.Let's scroll down to
the open function,ATTRN to get attribute,
numeric attribute information,and close function.So here's the parameter.%let dsid-- I'm fixing to use
a different name for the macrovariable--%sysfunc open that data set.Now we have an identifier,
an integer identifier,for that table.If it's not equal
to zero, %then %do.So if it's able to identify
and open up the table,it will have a value
non-zero, and thatmeans it was successful.I'll have %let numobs=%sysfunc,
the ATTRN function to goagainst that data set ID,
and look up the nlobs.That's a numeric value that's
returned and assigned here.And next we look up
another integer value,which will be the
number of variables--and I think you noticed that
there were five a moment agowhen we looked--and assign that to numvar.And then I close the data set.In my proc print
in my first title,I'm going to indicate
the number of variablesand the number of observations
that have been assignedto these macro variables
respectively with sysfuncand the ATTRN invoking
these functions.So let's run this.Notice-- and again,
I'm verifyingif we were able to open up the
table, we would do all of this.If not, we would write
a note to the log.So we will run this for the
sashelp.class table first.And that's successful.Look in the log.Not a problem.And secondly, we
purposely misspelledthe name, mistyped the name.And you get a note in the log.In this section for
the var type functionthat I showed you
in the slides--let's take a look at that.Once again, I checked for
the existence of the table.If it exists, do the
following, assign a data set IDto the open function
on that data set,and then I'll &let numvar.I'm using the attrn
on that data setID looking for the
number of variables,assign it the nvar, numvar.And I want to count the
number of character variables.So %let char=0--I could have used x or y--check it out. %do
i=1 %to &numvar.And you might recall
there were five variablesin the sashelp.class table.OK.Let's get back to this.There should be
five, one to five.Go inside the do block.%if this %sysfunc on the
variable type to get the typeof the variable based on that
data set ID in position oneinitiate.I is one.Remember that's automatically
a macro variable.If this equals to C, %then %do.Then accumulation, %let
char=%eval of this currentvalue plus 1.So zero plus 1 makes 1.%end closes that do block.This %end, i will increase to
2, still in range because I knowit's 5.%if the vartype on that data set
ID where the variable position2 is equal to C, then we
accumulate the value of charagain by 1.If it's not, it doesn't do
anything in that do block.It hits the end
in the increments.And we will keep doing
this until we exceedthat number, which I know is 5.When that's finished,
I close the data set,and I print the data set,
and I show in the first titlethe number of
character variables.And I added--I'm going to just run
it without this first.I added this line just to show
only the character variables.Let me just run this entire
macro definition in execution.So let's see-- name
and in the next column,those are character.The other three, you can
see are right aligned.They are numeric.And what does this say?You have two
character variables.If you want to see those
particular character variableson the var statement
for proc print,which controls what
you're printing,you can use _character_ to
see only the characters.You can also use _numeric_
just to see the numerics.Good way to validate.The next feature we
looked at is dopen.Instead of the open
for the data set,we're doing a dopen
for directory.So we can open up a
directory and explorethe members of that directory.So you will supply a
path to run programs,and I'm associating a file
reference, my directory,with the path.To open it up, I use %sysfunc
dopen on that directory path,and again, don't put an
ampersand here, but a dopen.Dirnum, I'm using
sysfunc to invokethe dnum on that directory ID.That will tell me how
many members I have.Then in a do loop,
I'm incrementallymoving from one to the next.%do i=1 %2 however members
I have in that directory.And I'm doing a %include.Some of you may have used
a %include, but basically,you point to a SAS program.souce2 will show you
what the code looks like.So I'm using sysfunc
to do a directory readon the directory ID for the
first number because i is one.And it will try to execute
that file then goingwith the ID of the SAS program.I hit the end. i changes to two,
and here comes the second pass.I will capture
the second member,and %include include will
submit it to be processedand executed.And we will keep doing this
until I reached to the end of--I've reached the last member.Then I close the directory.I clear that file reference, and
I am going with this program.Here's the path.It is different from yours,
and if you take a look,here's the path I
just referenced, SGF,and there's a folder in
there called Programs.Here's Programs.I have one called Data
Steps and one called Proxy.I have these two.Those are the only members
of that path and their SASprograms.All right.So we'll see how it works.Might see if we can find--this is old code.Need to see if we can find--we'll run this on this path.Here's run programs.I have MLOGIC turned on so that
shows me the beginning of it.And you can see the
path specified programs.I set up a file reference to it.And if you could see,
we would referencethe datasteps.sas program, so
that's the first one it finds.And it also finds the
procsteps.sas program.Here we go.And it will execute both.If you look at the plus
symbols, that's %include.So what I'm going to do is
turn off MPRINT so you can see[INAUDIBLE] more
of the informationrelevant to the source2 option.I'm also going to do no MLOGIC.Turn that off.So no MPRINT and
no MLOGIC, and Iwill rerun this
and check the log.And I still have SYMBOLGEN
turned on, but you can see now,%include that source2 shows the
code that was coming from thoseprograms.There's a proc
print, and it lookslike a proc contents,
proc print, and proc freq,and proc means in there.There's also a data
step called newpacks.That's creating a data set.We have one called shoe subset.So all the code in those
programs will be executed.So this will
conclude lesson two.Thank you so much
for joining us today.You have my contact information.If you have any questions
about what we covered todayin this course, feel
free to reach out to me.I teach the SAS Macro
course, level Essentials One.I also teach the
advanced course.I teach most of the
programming courses and SQL.So feel free to reach out to me.And we certainly again
appreciate your time,and have a good day."
65,"LAURENT MONTARON: Good
morning, or good afternoon.My name is Laurent Montaron.I'm with IBM's
hardware division.Specifically, I work in offering
management for Enterprise Linuxworkloads.And today I'm very pleased
to share some exciting newsregarding what we have to offer
for SAS solutions in general,and SAS Viya, in particular.So let's get into it.I'd like to share with
you some of our offerings.Before we get into
that, I'd liketo say I have 27
years of experience,and I spent 20 of those in IBM.And my specialty has always been
to work on the most demandingworkloads,
mission-critical workloadwith very high expectations of
performance and reliability.And that's why I
was really pleasedwhen IBM asked me
to focus 100% on SASand help bring our clients
to the new SAS Viya platform,because I think this is one of
the most interesting evolutionin the history of analytics.I'd like to start with a
few words about ModelOps.Hopefully, all of you, by now,
are familiar with ModelOps,inspired from DevOps,
and how SAS Viya enablesthe deployment of a very
successful and easy to useModelOps.What's absolutely demanding
on the infrastructureis the unpredictable nature of
having hundreds, potentiallymore, of models being
created, registered, deployed,retrained.And that tends to create
quite a lot of demandon the infrastructure,
the unpredictability.And a lot of data
gets moved around.Even though SAS Viya is
an in-memory platform,it often works with other
workloads like traditional SAS9.4 data sources.And we can move potentially
tens of gigabytes per secondon the same platform, while
all those models are beingdeployed and run under data.So the demands on
the infrastructureare actually increasing
with the introductionof the notion of ModelOps.And SAS can be
unforgiving if the sizingof your infrastructure
platform hasn't been performedsuccessfully, if you don't
have enough flexibilityand reliability.So I think the key here is the
infrastructure matters, mattersa lot.And with the introduction
of modern analyticsand the ModelOps, it
actually matters even more.And that's why we decided to
look at enterprise platformto deploy some of
those workloads.IBM and SAS partnered
to deploy Power Systems.And you can deploy
on IBM storage,or third-party storage.Obviously, deploying
everything on IBMwill continue to
reduce the risk.What we claim is
that that combinationis going to give our
clients a lot of agility,is going to reduce risk
by increasing reliability.And extremely important
for SAS workloads,we're going to help eliminate
all the bottlenecks thatare extremely common in the
lifecycle of modern analytics.So what does it look like?Agility-- the
number one concern,for ahead of analytics,
is often time to insight.And time to insight
requires a lot of thingsto work well together.You need to have highly
reliable systems.You need to have high
performance system.And it has to be an end to end.But it also requires
flexibility,because with ModelOps,
many different modelsdeploy at the same time, many
different jobs potentiallyrunning.And to better utilize
the infrastructure,you need to have
that flexibility.We in IBM would deliver
that flexibilitythrough advanced virtualization.Contrary to commodity
hardware basedon Intel processors, on
which you would typicallyneed to process third-party
virtualization like VMware,or install KVM
virtualization, we in IBMhave built-in virtualization
in our high-end serversthemselves.And it's entirely
implemented in hardware.That gives us very, very
fine-grained virtualizationthat is highly resilient.It is typical for a
client to be runningon the same system at 70%,
80%, 90% or more utilizationwith hundreds of partitions
sharing the same CPUs.And those populations
could be analytics--like SAS and SAS Viya--could be mission-critical
databases, could be ERP datasources, all of them sharing
the same hardware in harmony.Which, theoretically, you
could also do on VMwareon Intel systems, but not
with the same granularity,and not with the same
lack of overhead,which is a signature
of IBM Power Systems.In addition, still on the
topic of virtualization,our hypervisor, which is called
PowerVM and, again, is entirelyin hardware, has zero reported
security vulnerabilities,which is extremely
important, especiallyif you happen to be in
a regulated industrywhere security is a key topic.That flexibility
gives the IT teamthe ability to deploy faster.It simplifies the
management of resources.And it makes it absolutely
trivial to co-locate missioncritical workloads
on the same systemwithout having to worry
about the overheadthat third-party software
virtualization wouldbring to the platform.We reduce risk in several ways.First, reliability.If you want to have a good idea
of the reliability of a server,I highly recommend that you
look at a study by ITIC.Twice a year they call on
large clients for the Fortune500 and others, and they conduct
that global reliability survey.And it's a very
important data pointto look at the actual
uptime and the amountof unplanned
downtime that you canexpect from a particular
brand of server.We have been ranked
number one in reliabilityfor the past 11 years, and
that's for Power Systems.And our storage solution
provides 99.9999%availability, and 100%
if we double the storagewith our HyperSwap function.So this is a very
important factorbecause the unplanned downtime
has a significant cost,and it can be particularly
bad for analytics.When you have long-running jobs,
you cannot afford those jobsto crash and have
to restart them.Time to insight requires
predictability and eliminatingunplanned downtime
as much as possible.And finally, and equally
important for SAS Solutions,is the ability to
move data around.And the way it is
achieved on Power Systemsis through performance
enhancement end-to-end.We have better I/O bandwidth.We have better memory bandwidth.We have a better
CPU-GPU interconnect,if you're running SAS workloads,
that actually use GPUs.And we are able to move tens
of gigabytes per second.If you look at typical
SAS deployment,it is not unusual to have more
than 100 megabyte per secondper call in all
the SAS workloads,and we can do much
more than that.So this is a very
important factor.Before we get into some of the
line up and specific hardwareofferings that I think
are a good fit for SAS,I wanted to say a few
words about the partnershipbetween IBM and SAS.Obviously, this is
not a new partnership.We have been working together
for more than 40 years.And we have a very
special relationship.For instance, the
technology behind SAS Gridis actually an IBM product
called Spectrum LSF.Traditional SAS,
current release SAS 9.4,has been available
on Power Systemson Unix for quite a long time.And, of course, what's new
is that SAS and IBM, in 2019,decided to partner to make
SAS Viya available on Linux.So this is a Linux play.So why did we do this?And the answer could be in
this video from Ken Gahagan.He's a director on R&D at SAS.And you will find a link to
that video on this slide.It's on YouTube.At a very high level,
with the latest generationof power processors that
we have in our systems,SAS observed that we
have this high throughputcapability that other
processors simply do not have.I will encourage you
to listen to Ken.It's a two-minute video which
will explain more in detail whySAS decided to rekindle
the partnershipand take it to a
new level with Viya.We've been working
together since 1998,and that also contributes
to the risk reduction.We have significant SAS
expertise within IBM,and strong relationships
with many products.And that also helps accelerate
deployments and reduce the riskon deployments.Let's look at some of the
gear that we recommend for SASworkloads today in 2020.All the systems
that you see hereare running our latest
and greatest processor,which is the POWER9 processor.All of these systems
can run Linux.And, when it comes
to running SAS Viya,Red Hat Enterprise Linux is
our choice, obviously, Red Hatbeing an IBM company now.Those systems, many of them,
can run other operating systemsat the same time,
like Unix or IBM i.We have decided that SAS
Viya will be made availableon the entire lineup.And that means, on
the left-hand side,systems that are really designed
for GPUs and acceleratedworkloads.So, for instance, if
you deploy modulesfrom SAS Viya VDMML
that requires GPUs,that would be probably
the box you will choose.On the right-hand side,
we have virtualizedin hardware scaled out
and enterprise systems.Some models are
tailored to run Linux.Some models are tailored to run
Linux alongside other productsystems, which can be extremely
useful because, in many cases,our clients will mix
SAS 9.4 and SAS Viyain the same analytics
environment, and probablyother mission-critical
workloads.And you can run all of
that on a single system,or set of systems.On the storage side, we
want to keep it simple.When it comes to
SAS and SAS Viya,we systematically recommend
going forward flash technology.There is no longer a reason
to go to spinning disks,given the price points that
we observe on the latest flashtechnology.And we also recommend
NVMe as the protocolto bring the most
performance on Linux.We have integrated
storage offeringswith very attractive
price points,from entry-level FlashSystem
5000 to the highesthigh-end FlashSystem
9200, all of them runningthe same firmware, all of
them compatible with Red HatLinux, Unix, and IMB i.So I'd like to say a
few words about movingto Viya for a client who
is running SAS 9 today,and, potentially, SAS Grid.We have decided that
Viya, when running on IBM,it's going to be on
Linux, and specifically,Red Hat Enterprise Linux.And it will go to
Red Hat OpenShiftin the next release
of Viya to beable to use
Kubernetes containers.This is a modern approach,
a completely open stack,with open protocols, on
the latest processors,with Red Hat Enterprise Linux.But for clients running SAS 9.4
today, we help reduce the riskand provide flexibility
by letting youco-locate traditional
SAS 9.4 and SASViya on the same platform.And Viya will let you
run traditional SAS codein its own runtime.So if you have millions
of lines of codethat you have been
developing over 20 years, 30years, or more, using the
proprietary SAS programminglanguage, that code
will be able to runon Viya in Viya's pre-runtime.It will also be able
to be scanned and startusing automatically some of the
more advanced Viya features,if that code is compatible or
modified to run with the latestfeatures.And that really simplifies the
way forward for our clients,as many of our
clients do not wantto touch their infrastructure
because they had a stable SASenvironment and
lots of legacy code.But with this approach,
there is a clear way forwardwhich lets our clients
move code to the new Viyaenvironment at their own pace.And if something goes
wrong for any reason,you can still run SAS
9.4 on the same machine,so you don't have
to go back and forthbetween an old environment
and a new environment.There are a variety of
data access capabilitiesbuilt in both Viya
and traditional SAS.In fact, SAS Viya can use the
connectivity within SAS 9.4to get to external data sources.And you'll see here
on the right-hand sideat the bottom of the chart some
of the connectivity options.Viya can access your
data sources directly,using data connectors.The Viya pre-runtime can access
data sources directly usingSAS Access.And, of course, traditional
SAS can access data sourcesusing SAS Access, as well.So there's really
a complete harmonybetween the new Viya environment
and the traditional SAS 9.4environment.And if you're running
on IBM POWER9,you can co-locate all of this.And you can co-locate other
mission-critical workloadsat the same time.Finally I would like to
advertise this document, whichis updated every quarter.It's available on
the IBM website.It's called the
IBM Power Systemsfor SAS Viya Deployment Guide.It will give you
all the informationavailable to make
the right choices.Look at your deployment options.You can scale up.You can scale out.Many of our clients will
do a little mix of both.All the tuning recommendations,
the sizing consideration,installation tips,
best practices,and a list of pointers to many
resources available to youto reduce risk on
the deployment.And again, it is
updated every quarter.So we have a new update
published in April 2020,and we have an update
planned for this summer.In conclusion, I hope
that this presentationhelped you understand
why IBM and SAS decidedto work together
and make SAS Viyaavailable on our
Enterprise Linux platform.I'm looking forward to helping
you in your implementations.Thank you."
66,"BOB AUGUSTINE: Good morning,
good evening good afternoon.Thank you for attending
this virtual event.Today I'm going to talk to
you about SAS event streamprocessing at the edge and its
ability to reduce or eliminatethe need to transmit data to
the data center for analysis.I'm Bob Augustine.I've been working
at HP since 1988,and I spent the last 12
years working with SASas my primary or only partner.I just wanted to tell you a
little bit about our alliancewith SAS.HP and SAS have been
alliance partners since 1986.We're both in each
other's partner programs.And in addition,
my partner and Ialso have 30 years of
experience with SAS.I've been working
with SAS since 2008.In fact, my first
SAS Global Forumwas in 2008 in Washington, DC.And my partner,
Mark Barnum, who'snot on the phone with
me today, has beenworking on SAS for 18 years.So we have a combined
30 years of experience.HP and SAS have a
good relationshipwith SAS's R&D people.We have biweekly meetings where
we discuss issues, customerproblems, things like that.Why this is important
to you is we understandfamiliarity breeds ease of-- or
we understand our customer painpoints.We're able to take our
products and SAS products,and design things that will
work for our customers right outof the starting gate.And some of these
things are a little bitdifficult to configure and
make work in a robust manner.Our customers can
spend a great dealof time trying to determine how
to put these things together,or we can test with that
software on HP equipment,and we can share
with our customershow to put those
things together,greatly reducing the amount
of time between the timethe equipment arrives
on site and value.Let me give you a short example.A customer-- when we
do a test with SAS,we spend about four to six
weeks determining how bestto configure the equipment.If a customer were
to do that, theywould have to do
exactly the same thing.We're able to take the
information we garnerfrom doing this testing
with SAS softwareand provide that
customer so they'reable to go live probably six
or more weeks earlier than theywould if they'd had to do
their own type of testing.So I wanted to show
this slide to youas HPE and SAS's
edge-to-core scenario.Now, we're going to concentrate
on the left-hand side, the IoTEdge side.But the reason to
provide this slide to youis to show you that
SAS has productsand solutions for every stage
of the data center or the edge.And also HPE has equipment
that goes along with SASso we can tweak things
and lower your spend,because we don't have to kind
of push things into placeswhere they don't really belong.So we have peak systems
right at the edge,we have peak systems
in the data center,and we can bring that to
bear for a customer's site.Now let's talk about
some of the reasonsnot to transmit data
to the data center.Latency-- let me
give you an example.Autonomous driving--
if I'm driving downthe road and my car's driving,
if I have a person in frontof me start to
break, if I've gotto to transmit that
information up the data center,have it analyzed, and then sit
back down to release pressureon the accelerator and
placed pressure on the brake,if I'm doing that, if
I'm transmitting it, allof a sudden, I'm in the
back end of that the personin front of me's car.So latency can make the data
less impactful, or at worstcase, especially with autonomous
driving, can make it worthless.There'd be no way to do this
if we were not able to analyzethe data right at the edge.Bandwidth-- some of the data
that we fall from the edgewould take quite a
bit of bandwidth.Our customers today, there's
quite a number of themthat are doing research
on autonomous driving.And they go out for seven-hour
day, or an eight-hour day,excuse me.There'll be a person in the
passenger seat recordingthings on a clipboard.There'll be a driver that had
his hands near the steeringwheel, but not actually driving.And there's a
computer in the trunkit's collecting data,
all the parameters,things like cameras
on the bumpersor on the roofs of the
cars, things like that.That collects upwards of seven
terabytes of data per day.In order to transmit that data,
even in a 10-gigabit network,which is today an average
for a typical network,would take upwards
of two hours per car,and we consume the
entire bandwidthof that 10-gigabit network.HPE makes equipment that can
have the disk packs removedfrom the server and placed
into the data centersso we can transmit
that data or transferthat data to the data center
much quicker and much morecost effectively.Cost-- the larger the
pipe, the more cost.And it's not a
linear progression.Going from a 10-gigabit network
to a 100-gigabit networkis not twice or even
10 times as expensive.It's quite a bit more expense.Compliant-- there's some
industries, take health carefor instance, where it's
against the law to transmit dataoutside or away from the edge.HIPAA, for instance,
disallows things like that.Security-- whenever data is
transmitted, it can be hacked.Duplication-- and this
one's quite obvious,but if we have seven
terabytes of data at the edgeand we have to transmit or
move that the data center,we also have seven terabytes
of data the data center.That requires storage
in the data centeras well as at the
edge, and it alsorequires CPUs to process
that data at the data center.And reliability-- no matter
how good data transmissionprotocols get, sometimes they
introduce errors into the data.And so we would like not to
transmit data to data centerunless we absolutely have to.Now, let's take a look at
some of the edge use cases.Theft and crime prevention,
things like shoplifting, or--and we've all seen
this on the news--thieves placing facades in
front of ATMs to catch people'sPIN numbers and
their card swipes.We can see that with a camera.And if we see that happening,
we can do something about that.Customer insight-- this is
a particularly interestingscenario, at least for me.So if we have a
series of cameras,and we do already in retailers
for theft and crime prevention.When a customer
goes down a shelf,we can see through those cameras
what the customer happensto be wearing.So if they're in
an apparel store,for instance, we can see, oh,
well, they like, I don't know,this type of shoe.And we can say, oh, well,
we can read their facesand determine if they're happy
with what they're seeing.We can make
suggestions with whatthey might want to purchase,
and so on and so forth,and hopefully enhance their
customer shopping experience.Manufacturing, this is a this is
a particularly interesting onealso.We have a customer now that
manufactures disk drives.They have cameras on the
manufacturing line thatlook at the disk
drives, and theydetermine if there are pits or
problems with the disk rightbefore they go further in
the manufacturing process.So if I have a
particular platter that'shaving a problem, I can eject it
from the manufacturing processbefore it gets blended
into an entire disk driveand we find out that it's faulty
at the end of manufacturing,after spending everything.Smart City-- today we
can do gunshot location.We can tell you the
caliber of gun that'sbeing used in the gunshot.We can tell you the direction.So we can get police
officers out to sitesof gunshot activity
very quickly,rather than having to wait
for people to phone in 9-1-1.And traffic-- think about this.Traffic movements and traffic
lights is an activity.I live in a rather
large urban area,and sometimes the interstates
get rather cloggedgoing in and out of the city.If I'm able to read those
with sensors in the road,and maybe there's an
accident on the interstate,I'm able to deviate
traffic off the interstateto the secondary
surface streets,and then I'm able to
activate traffic lights sothat the traffic on
those surface streetsmoves much more quickly.And that helps public
transportation and tourists.So let's look at the
logical data flow.This use case came from
SAS Global Forum last year.SAS provided customers with
the ability to remote controla ball rolling
around on a floor.There was a camera
focused on the floor,and it tracked the ball as
it moved around on the floor.The feed came from
the video camerato the computer at
30 frames per second.We were able to,
during those 30 frames,or once every 30th of
a second, each timewe had to identify the
ball, identify where it was,compare where it was with where
it was during the last frame,and then generate vector,
speed, acceleration,and all that kind of
information on the fly.Here's the software
and versioningthat we were using
during our testing.Oh, yes, we use SAS Event
Stream Processing 6.1, Viya 3.3,and then we used NVIDIA
41.104 and CUDA version 10.0.I want to take a moment to
talk about the equipmentthat we use.We have a server chassis
called an EL4000.This takes one to four
computer cartridges.Each computer cartridge
is functioning entirelyindependently.They cannot be blended, but
they can talk to each other.The type of computer
cartridges that weare able to insert into
the EL4000 are m510s.Those come in two flavors--8-core 2 gigahertz or
16-core 1.7 gigahertz.We also have an m7 10x comes
in a 4-core flavor with 3.2gigahertz clock speed.And finally, we're able
to put the NVIDIA C4GPU into the server for use.Here are some of the
characteristics of the C4 GPU.Notice the massive
number of cores thatare available to processing.What SAS does is they just
aggregate the data feed.In this case, they
would just aggregate itinto 2,560 separate screens,
and each one of those screensget fed in parallel
to the C4 GPU.Now, think of each
one of those coreslike you would a
CPU on a system.The most cause we have
today, or which I'm awareis about 32 cores
per processor, but wehave this GPU, which
actually has 2,560 processor.Let's take a quick look
at the SAS data flow.We read the data in
from the video filethat SAS provided
us for this test.We push that data to W score.W score IDs the
ball on the floor.The we push that to tracking.This IDs where the
ball is currently.We filter the data, and we push
it to with outputTracks wherethe present location's
compared to the location whereit was during the
previous screen capture.Then we push that to speed
direction, which calculatesthe direction the
ball is travelingand its speed over the
past 1/30 of a second.The thing that we're
interested in here,and the thing that I
was interested in whenwe did this testing, is
we want to understandwhat difference fewer cores
with a higher clock speedmight make the throughput
versus more coresat a lower clock speed.And I'll provide you
with a little spoileralert ahead of time.There wasn't a whole
lot, if any, different.We want to know
the total frames wecan process on a
system-wide basisbecause that's going
to tell us kindof how many we can process, how
many streams you can process.And then we want to
know how many frameswe can process in a
per process basis.And we want to know that because
SAS speed fed the data to usat 30 frames per second, so we
kind of use that as a floor.Now, you might need in
your particular instance,you might need 90
frames per second.Or you might only need
20 frames per second,and that's a metric
which we want to track.And then what impact is
having a GPU in the systemhave to the overall
performance of that system?What difference
does the GPU make?Let's look at some of
the performance results.As you can see here, we
start out with one stream,and we go to nine streams.This is on the 8-core,
2-gigahertz system.When we start out with one
stream, we're at 90 eventsor 90 frames per second
across the entire system,and that goes up to 200 frames
per second with nine streams.On the next slide, you can see
what this does on a per processbasis.We're starting again at
90, as you would expect.We get the two streams are down
at about 75 frames per second.Three streams were at
about 55 frames per second,and we don't dip below
30 frames per secondtill we get to nine streams.So even with nine
streams, if you'reable to handle 20
frames per second,we're able to do that
with nine streams.You could have
nine separate videofeeds coming to each
cartridge in this system.Here's the same set of
slides or the same resultin the 16-core system.And again, we start at about
90 frames per second systemwide with one stream.We go up to about 200
frames per second systemwide with two streams.I want you also to notice--I didn't pull this out in the
previous couple of slides,but I also want you to notice
what the system is doingin the orange bars, in
the number of framesper second that did a
process without the GPU.Here is a slide that
has the 16-core system,the number of frames
per second per process.And again, you're going to see
just about the same resultswith the 16-core system
as with the 8-core system.The one thing to note is
that the orange framesor the orange bars
are a little bit lowerwith the 8-core system than they
were with the 16-core system.Now let's do a
performance comparison.This is the same data
that I showed you before.We put it all in one slide, the
8-core system with 2 gigahertz.So the gray line at
the top is the timeto complete the entire
test without a GPU,so just the system's
CPU processing the data.And the kind of teal
line at the bottomis it time to
finish or complete.As you can see, it starts
out with one streamat about 60 seconds,
believe it or not,to process all the frames with
the GPU in the SAS provided.With the CPU, we're
over 400 seconds.And that went up,
with nine streams,to 200 seconds with a GPU
and over 1,000 secondswithout the GPU.Here is the same graph,
the same graphic, whenyou get to the 16-core system.And while we didn't
get over 1,000,we came up with
950, 970 seconds.So that that's a little
bit higher impact.But we did, on the
bottom with the CPU,and we stayed at roughly
the same throughputor the same time to
complete those tests.Now, we're able,
with NVIDIA GPUs,to keep track of what
the GPUs are doing.NVIDIA, these GPUs are
not actively cooled,so NVIDIA needs to make
sure that we don't overheat,we don't melt the silicon.And as you can see, as we
start to ramp these things up,the peak temperature went
from about 70 degreesCelsius up to about
85 degrees Celsius.What happens with NVIDIA GPUs
is, as the temperature goes up,they start to lower
the clock speedto keep the temperature
from getting too highhaving us melt the GPU.You can see that also the
peak watts that we're used,and as we get to
the nine streams,you can see the dip in
the peak watts used.And that's a feature of
having to self-regulatethe GPU to keep it from
going well over temp.That's with the 8-core
2-gigahertz results,and here is the same
information with the 16-core,1.7-gigahertz.You can see the
same type of thinghappening, except the
knee of the curve,the amount of watts consumed,
going down much furtherwhen we had to self regulate
it up into the test.Here is where to get more
information about HPE and SASand what we've done with
reference architectures, howwe've configured things, what
the throughput analyses lookslike.At this time, I'd like to
thank you for your time.And I appreciate you coming
and viewing this video.I would like to call out
on this particular slidethe email addresses
for myself and oneof my partners, Sri Raghavan.You are welcome to contact us
if you have questions, comments,or concerns
regarding this video.Thank you very much,
and have a great day."
67,"BRENT LASTER: Hi everyone.Welcome to the SAS Global
Forum 2020 online presentation.In this presentation,
I'll be talkingabout an overview of
container technologywith a quick introduction
to a lot of different thingsabout containers out there.My name is Brent Laster.I was R&D Director in the R&D
DevOp Organization at SAS.And I manage multiple
teams workingwith deployment,
DevOps, and otherfocuses like security out there.As well, I do some
global trainingin open source in DevOp's
technologies and practices.And this presentation
is based on someof my previous
trainings out there.I hope you find it useful.In this presentation,
I'll be talkingabout a lot of
different open sourceand container related
technologies out there.We'll start off talking about
the basics of containers.What is a container?What makes them up?What is a container image?How does that differ
from container?How do we create these things?And how do we think about them?We'll talk about those
in the context of Dockeras well, with Docker
being the application thatwraps the formality or the way
they interface with and createcontainers into an
application itself out thereand simplifies using some of
the underlying Linux containertechnology to enable us to
work with containers easily outthere.We'll then move into
talking about Kubernetes--what Kubernetes is.As I say in presentation,
we can think of italmost like a data
center for containersout there in terms of managing
the individual containersand running in
things we call pods,and being responsible for
scaling, for cleaning,for making sure that they
have new ones spun off.If there's a problem with
it, then we scale up or down,and the kinds of things that
we might think about a datacenter managing for computers.We'll talk about the different
kinds of configurationand inputs we do for
Kubernetes, the waysthat we can tell it
what we need out there,the kinds of different objects
that we use with it the,things that it supplies for
us in terms of functionality.And we'll also get into
how within definitionswithin Kubernetes
we can do thingslike having health
checks and telling it,talking about the
health of the containersrunning in the
system and managingthe resources out there
available on the systemsrunning Kubernetes.We'll move in to
talking about Helm then.Helm is the orchestration
engine for Kubernetes,allowing us to help manage, how
do we reuse specifications outthere?How do we have them as templates
rather than hard coded thingsthat we can reuse
and that we cansupply appropriate customized
values in there for us as well?Then we'll talk about another
tool actually named Kustomizewith the K, out there, which
is the alternative to Helm.It's a newer tool,
but it's actuallybeing given quite
a bit of popularityin terms of being able to
help with orchestrationof Kubernetes as well.It takes a different
approach from Helm.Instead of parameterizing
things including templates,it's more about adding
additional layers on topof basic specifications
out thereand using those to build up
from the original Kubernetestemplates out there, and
add additional functionalitywithout having to modify
the basic templates thatare running at the base of it.We'll move it to
talking about Istio.Istio is a tool for helping
us achieve and implementwhat we call service meshes.Talking about
microservice meshes.How do we use Istio be able
to control communication,networking, encryption,
those kinds of thingsout there, across a wide series
of micro services runningin Kubernetes out there.And what is the value
that it brings to us?We'll help you understand
the different parts of itwith a lot of different
visualizations and suchas well.Then we'll move into talking
about Kubernetes' operators.Operators are a
way for us to havecustom processing for custom
objects or custom resourceswe could call them.out in Kubernetes-- things
that might not normallyrespond in the way that
Kubernetes would expect outthere in terms of things like
scaling or being restarted,those kinds of things.We'll discuss how operators
actually help that and helpmanage that for us.Then we'll talk about
to GitOps briefly.GitOps is the idea of being
able to take our specificationsfor things like Kubernetes
and manage them through a Gitprocess out there
to give us thingslike the review of
changes out there,being able to roll
back easier within Git,being able to track to see
if you make changes and such,as opposed to a human actually
working with a command lineto drive changes into
Kubernetes out there.And finally, we'll
talk about a few toolsthat are applicable
for monitoringwithin Kubernetes out there.We'll talk about Grafana and
Prometheus and the Kubernetesdashboard out there.Graphical interfaces,
it can give ussome real insight across the
board into how our clusters aredoing, what are containers
are doing out there,and that sort of thing.So we have a lot to cover.I hope you find it
useful and interesting.And we'll get started
next with talkingabout containers and Docker.Let's start out our
presentation daybut talking about
containers and Docker.So what are containers?Container is a standard
unit of softwarethat functions like a fully
provisioned machine installedwith all the software needed
to run an application.I sometimes tell people to
think about containers as almosta computer within a computer.That is in the same way
you might have a laptopor a desktop that has all
the software installed on itand provisioned and ready
to run applications,you can have a container
as a self-contained unitready to run
application as well,just virtually within
the context of somethinglike Docker.It's a way of
packaging software.So application dependencies have
a self-contained environmentto run in, are
insulated from the hostOS and other applications
and easily portedto other environments.Because we have this
container wrappingall of these pieces
in there and managingthe environment for
us, then we can easilyport that from one
system to anotherthat's able to run
containers out there,namely any system to have
something like Docker on itthat's available
to execute them.Any time you talk
about containers,you're likely to see this
obligatory picture of a cargoship out there.The cargo ship has a
set of containers on itto remind us that the containers
are these self-contained units.They can be a varying
types or sizes out there.And they all contain
what's needed in itto run a particular application
or a set of applicationsout there.So what might be in a container?If we think of the container
like a cargo container,if we were to look
inside, we mightsee that it has the application,
the runtime, the dependencies,the settings, the system
libraries, the system tools,and other pieces in there.Everything that we
need to run for to actlike its own environment, act
like a standalone computeror there.So we have the container there.And you'll see on the side
here this is the standard logofor Docker out there.Now a container is not a
VM, meaning a virtual imageout there.We'll talk in a few
moments about whatthe difference is here.But a container works by
leveraging several featuresthe Linux operating
system to carve outa self-contained
space to run in,as opposed to a virtual machine,
which works through a programcalled a hypervisor sitting on
top of the operating system,and working through that to run
the applications inside of it.The benefits of containers?Easy to create and deploy
once you define an image.Containers are based
off of an image.An image is simply
a provisioned setof software configured
in a certain way,so as to be able to support
the running of applications.And once we have
that image out there,we can create those containers
and are very easy to spin upor destroy, the image services
as a blueprint for that.The best practices of
continuous development,integration, deployment, allow
for frequent reliable buildsand deployments, meaning that
when we have containers outthere, we can use them in
our continuous deploymentintegration pieces out
there because we can easilyspin these up, scale
these up, get rid of them,those sorts of things, it helps
to facilitate these notionswe have around continuous
development, integration,and deployment out there.They're quick and easy rollbacks
due to image readability.What we mean by that is that
the image, the set of provisionssoftware that a
container is based on,is considered immutable.That means that it is
not changeable in there.We'll talk more about
that in a few minutes.But because the image itself
that we're basing the containeron is not changeable,
it's immutable,it's easy to create a new
container based off of thatif something goes wrong
in the one we have.They're created at
build or release timeinstead of at deployment time,
so applications are decoupledfrom the infrastructure.So basically, when we
have the container setup,we have the entire
system running.Think of it as if you have a
laptop out there for example,with all the software
provisioned on it.And when you start it up, that's
like starting up a container.The image of software is
always there on your laptop,even if it is turned off.But it's always there ready to
go as soon as you turn it on.So when these are created
at build release timeand they're turned on in
effect, out there, then wedon't have to worry
about the application,the infrastructures being
there or how we set that up.The infrastructure
is already thereby virtue of how we defined
it within the image that'sthe basis for the container.Containers provide observability
through application healthand signals, not just
from the operating system.So especially we start getting
into the world of Kubernetesand stuff, we'll talk more about
these sort of health signals,application health
checks in there,making sure things are
running out there are good.A container, because
of the way itis packaged up, because of
the things it can monitor,it can better detect the
application's health overalland to give us signals about
that and logs and such.Because the environment
is self-contained,it runs the same on
all infrastructure,you can run a container
again, anywherethat you have an infrastructure
like Docker out there.Could be a laptop, a desktop,
one of the public clouds.Portable across any OS or
cloud that supports containers.And you can manage
the applicationinstead of the infrastructure.Once you have your image
defined out there thatis supporting your
application, youhave the software
provisioned out there,then you can focus on
developing or tuningthe application there, rather
than the infrastructure thatsupports it.It allows applications
such as micro servicesto be loosely coupled,
distributed, managed,and deployed dynamically.We can create
different containersfor different
purposes out there.We can create a container for
example, for each microservice.And because of the way they
interact with each other,because of the way
the containers canbe the self-contained
environment for each one,it allows us to be more
loosely coupled there,and to get away from having
the monolithic software stacks.Has resource isolation
and effective utilizationof resources out there
within the container.And we have the
containers that arecreated from images out there.They are running
instances of images.So let's talk a little
bit more about whata container image is.We referenced that
a container imageis satellite having a
provisioned set of softwareout there to base a
running system on.Think of a container
image as beinglike a read-only template that
you use to create a container.It's like a snapshot of
a container out there.That is, the image is
like a reconfigured setupfor the container.And then when we
activate the container,when we start up
a container, it'slike turning our laptop on.The image is always out there
and represents a snapshot,a way that things are
configured to set upat that point in time.When we turn it on,
then we're in a placewhere we can actually use
it, modify things, and so on.Container images are stored in
a registry like a repositoryfor images out there.So such as a Docker registry.Images are generally
considered to be immutable.That is, once they're
created, the imageshould not be changed.If a change is needed, a
new image should be created.We'll talk more about
that in just a moment.But the idea here
is that the layersof software configuration and
the different provisioning thatis used for an image there is
intended not to be modified.That is, it is
specified and set up,and then serves as an image
status immutable out there.When we go to actually
create a container,we add in a read write
layer, or a placeto make changes in there.And we'll discuss more how
that works in a moment.But because the images are kept
immutable or changing again,it's very easy to modify or
create new containers offof those, delete
containers, but stillhave the underlying image
out there that is consistentno matter how many
containers we create from it.Note that immutability
does not implythat containers can't be
changed by configurationor environment, et cetera.The container that
is based on the imagedoes have this read
write layer outthere where things can
be modified and changed.Container images are built
up from these layers.And if you're using
Docker for example,the Docker build command is
used to create images from that.So what is an image layer?We've referenced
this idea of layersto build up an image here.So a Docker image is built
up from a series of layers.Again, think of the images
here as being the provisioningof the software that we
want to have in placeto run the application.We're just provisioning
it for this image thatforms the basis
of our container,much as we might
provision and image thatforms the set of
software that wewant to stage or to put on
different computers out there.So a Docker image is built
up from a series of layers.Each layer results from an
instruction modificationin the Docker file.So a Docker file
is a way in Dockerto specify the set of software,
the, kind of configurationthe set of local things that
we want to copy into our imageout there.It is a recipe for
how we provisionthis set of software that
makes up the image out there.For the majority of the commands
that we find in the Dockerfile, such as the ones that
you see on the right handside over there, we will
create an individual layerin our image that
corresponds to that.If you take a look at the
right hand side there,you'll see the image
file, which says startingwith the from command,
from Ubuntu 18.04,meaning that we're starting with
that as a base operating systemimage.This is a particular version
of the Ubuntu operating systemthat has been turned
into a Docker imageout there that
serves as the baseimage or the starting
image for us to then modifyto get to the point of where we
have the software provisionedthe way we want it
for our application.The second command is simply
copying in some local stuffinto this image here,
running a command for usto create some other
content, and thenhaving a command out there
to run this Python piece outthere.So each layer essentially,
in a Docker filewill end up creating a layer
in our image out there.Layers are stacked
on top of the onesbefore, kind of in the order
that we were looking at themin the Docker file there.Each layer consisting of
only a set of differencesfrom the last one.So if something is already
present in one layer,then it's only the differences
that are tied in the next layerthere.Think of the base layer
as being an OS image.If you think about
provisioning a computer again,that sort of analogy,
you might thinkof installing the basic
operating system on a computerout there and then layering on
other software as appropriate.Each layer in this
case, is goingto be read-only, except for
the last one the containerone, when we actually spin
up a container from it.The last or top layer is
that container layer, whichis a thin read write layer.So all changes made to
the running containergo in that layer.So if you look below
the image there,you can see that we have
a version of the softwareout there.We didn't have different
layers for the different thingsthat we have done
to this to add upto the set of provisioned
software out there.Again, the analogy
might be beingable to install software
onto a machine to build upto the image you want.And then when you
turn it on, youhave the read write
layer out there.You have the user
space for example,on a machine where the user
can create files and workwith them.But the user is generally
not working withor changing the underlying
software layers on the system.Creating and tracking
layers for images.So we'll look through
a little videohere you'll see on the
right hand side, whichis an example of actually
running a Docker build commandto get a set of content there
and to actually create an imagethere from a Docker file.You can see on the
right hand side thereit is executing the commands
that are noted in the Dockerfile on the left.On the left is the Docker
file, the set of instructionsfor building up an image.As you go to execute
them with the Dockerbuild command out
there, we have,you could see from
the steps out there,it's pulling down the
basic MySQL scale imagethat we're looking for.And you can see that it is
pulling down layers here.In other words, that
MySQL image is made upof a set of layers itself,
it's our base image.But at some point
the past, someonehas defined a set
of layers, a setof software provisioning that
went in to making of that MySQL image.And as its layers
have pulled downto get that
underlying base image,you can see the
process happeningin this stuff running on the
right hand side of the screenout there.Now you may notice
also the Dockerbuild command has that
dash T option on it there.That is actually a
tag option out thereto give it a human readable
identifier, to make it easierto work with the image.Then we see that as
it's gone through this,it's actually going
through and creatingthese intermediate
containers there,these hexadecimal strings
that you see on the screen.The hexadecimal strings are
our IDs for the various layersthat we're working with there.So you see that it actually
went through step one of four,pulled down the MySQL stuff.Step two of four
copied over some stufflocally that we
were using in there.Step three of four
set up an entry point.And step four of
four did a command.The idea here is that we started
out with a basic MySQL image.We then copied over some
script that when it gets runas part of the container
starting up there,it will actually create
some databases for usfrom MySQL to work with.This is the way the MySQL image
has been defined out thereto understand how to do
this when it starts up.Typically, when someone
creates a particular imagethat they expect to be
reused in a more general way,they will define some sort of
mechanism, some sort of scriptthat can be run at startup to
configure or populate or dosomething with the underlying
applications there.They could also define
other script thatrun at various times there,
or environment variablesfor information may be passed
in, that sort of thing.So in step two, we're copying
over some local contentthat we've created
and we've customizedto set up the database
the way we want it.Now in step three, here
we have an entry pointwhich says when I
start this up whatis the thing I want to run?I want to run the entry point
dot SA to shell script outhere.And then that will
go off and actuallyrun those pieces to go
through what we copied in,in this case, so the way that
this MySQL image is set upto work, start up that script
running, which will go through,execute the stuff
that we pulled inand copy it locally to
create the databases in therethat we want to have
presence in this container.Then step four is just a command
to start up the MySQL demonout there.But you can see that ultimately,
as we're going through this,we're getting into layers for
any base images that we need.We're adding on
additional layers,so hexidecimal ID,
which is differentsteps here in this
particular Docker file here.And then we end up with a
actual image as a result.The actual image ID at the
bottom there you can see.And we have a
successful tagging,which means that we have applied
a human friendly name to thisalong with a version
number out there.So how does this all
tie together then?Well if we tie it to-- if
we look at, for example,the basic functionality or the
basic output from the stepsthere as corresponds to the
Docker file and the historythat we have, using Docker
history command to seethe different layers that
have been used to build upthis image, then we can see that
when we do those step in bluehere, we pull down
the from MySQL,we had a corresponding
action with a Dockerbuild rand that pulled the
content down from there.Then we had a
corresponding layercreated for that, the ID
with the layer out therethat consists of that
MySQL base image.As we then went through, if
we look at the Docker history,we could go through and see all
of the different layers thatmade up that part.Now you'll notice probably the
missing identifier designationthere for these image layers.Missing simply means that we
do not have the original layerinformation out there,
the original layer ID thatwere used to construct this.Rather, we have the ultimate
ID of the underlying imageonce it was completed.But this was done
some three years ago.And it was not stored,
the informationwas not stored with it as far as
the ID freeze each unique layerout there.We also have then the
command to copy over,some local content
into our system.As we copy that
over out there, wecould see it that step was also
executed as part of the Dockerbuild command, which resulted in
yet another layer in our imageout there.Then if we do the, same
thing for the entrypoint the entry point
having the part--that thing to run when we
start up the container, again,resulting in another
layer in our image.And then finally, we have
them running the MySQL demon,adding another layer in there,
another action, which resultin another layer out there.And you can also
see that in history.So ultimately, what
we end up with thenin this sort of
blight green and whiteblock here is our
Docker image, whichis the set of
layers that resultedfrom running through
the Docker buildand provisioning software,
adding in things,configuring things as
we went along executingthe build from the Docker file.Now how do layers
and images intersectwith the operating system?So we have an image out here.Basically, if the layers
in the images are likefiles to the operating system.The image layers as we said, are
read-only, meaning immutable.They can't be changed out there.But when we actually
want to use theseas the basis for a container,
then what Docker would dois create a container
from an image.If we run the
Docker run command,we create a container
from an image.And that would
add another layer,a read write one on
there that gives usa place to make changes.Again, you can think of this
kind of from the analogy of Ihave a set of installed software
on my computer or my laptop,I generally are not making
changes against that software.It's generally
considered immutableas far as the operating
system files and stuffthat are installed upon it.But then I do have a user
space out there like a layerwhere I can actually
change my content.I can store things in documents.I can download things.That sort of pieces in there.So the container layer that
gets added on top of an imageto create a container comes
from the Docker run command.You can see here that when I
do a Docker run command here,I'm running some stuff.I want to run an instance
of the MySQL image.The MySQL image serves as
our provision software.The container serves as a
running instance of that.Again, think of it kind of
like turning on a computerthere he has an image
installed on it,and then being able
to make changesand work stuff in there.Now the dash E here, the
MySQL root password in there.It's passing in information
via environment variablesthrough the run command that
the image is looking for.Again, whoever set up
this MySQL image set itup so that it could actually be
looking for information about,for example, the user,
the password, and so on,passed in through
environment variable.That's not always of course,
the most secure way to do that.So when we get into
things like Kubernetes,we'll talk about other ways to
manage information like that.But this is the basic idea.So when we run this with
the Docker run command,then what happens is we
get another layer addedon top of our image.And that forms that
container the containerhas the read write
layer out there wherewe can work in there based on
top of the read-only layersfrom the images.At that point, if we
were to do a Docker PS--Docker PS syncs without looking
at the processes out there,we'll be able to see our
container in there, asdesignated by that ID that came
from the Docker run command,once we had the
generated the container.It gets its own
hexadecimal ID out there,which is the address or
the identifier for thatlabel out there, based on
the image we created earlier.So the layers here--we mentioned briefly
before that whenwe, have talking
about Docker and stuffin there that we have the
layers that make of the system,and that they are
immutable in there.And the difference that with
versus from a virtual machineversus a thing like a
container in there with Docker,is that the container,
it's implementedby leveraging some core
functionalities of Linuxout there.The Linux operating system has
been around for a long time,and has had containerization
support in itfor a long time as well.There is LXC, Linux
containers out there.And what Docker
does is it actuallyleverages its underlying
functionalities,but provides a nice
framework around them.The Docker file for
specifying commandsto use in there,
the APIs and stufffor talking to them,
and so forth in there.So Docker puts a nice framework
around these core Linuxtechnologies to provide the
simplified container and imagefunctionality that
we've come to knowor will come to
know through Docker.So layers can be viewed as a
single union of all the filesystems.The first piece of
the technologiesout here that we
rely on in Linuxis called the Union File System.And the Union File System,
since we have layers,allows us to look
through these layersto find content that we need.That is almost like a
path setting on a computerout there in an environment.We can look into one layer,
see if something is there.If not, look down
to another layer.If not, look on down to
the other layer and so on.So we have this thing called
the Union File System.The Union File System allows
us to have files or contentin different layers
there, but stillbe able to see them from the
container layer for example,by looking down through that.That is a Linux
functionality in there.To be able to search through
the layers to find the contentthat we want, basically
stacking the layers on topof each other in the
files and within that,but being able to this as a
unified file system out there,even though it is made
of multiple layersstacked on top of each other.One of the things then
that comes to mindis how do we actually
modify files?If we can't modify
files in the imagethat the layers
are immutable, howdo we actually go
about modifyingfiles that might be in those
underlying layers in the imagesthat we need to work with?The answer is one
of the other thingsthat is used in the Docker
container process here.And that is the Copy
On Write functionality.And the idea here is pretty
straightforward actually.It is that if I need to
make a change to a file thatis down deeper in
the layers here,I can simply get
that file, copy it upwhen I need to update it
into the container layer.The container layer
then has a copyof that file that
can be modified.And because we have that
Union File System out there,we still look through
to the other layers.So when we need to
actually make changesto things at lower layers
in there that are immutable,we can simply get
a copy of that.We have a Copy On
Write functionalitythat brings it up to
that top layer out therewhere we can make changes and
stores the changed copy there.There are other Linux facilities
like C groups for controlgroups and name spaces
that help us keepour containers separated out.The idea with the container
here is essentiallythat when we are
working with it,we are carving out
sets of resourceson the operating system to
be able to run in a protectedspace or in protected
resources therethat to the application
running within a container,it appears to have its own
system, its own provisionsystem or computer that
it's running on there.But really what Docker
is doing is carving outsome of those resources in the
operating system for purposesof running these containers.So the other nice
thing about the waythat we implement
containers hereis that because containers
are based off an image,and because that image is
immutable and non-changingand because a
container is reallyonly a thin read or write
layer on top of that image,we can actually have
multiple containersbased on the same image.Each container can have the
same underlying image out therebecause it is read-only.And then it can actually
add its own read write layeron top of it.And because the
Union File Systemthere, we can do this sort
of looking through the layersout there.So the idea would be
that we could actuallyhave multiple containers that
are using this same underlyingimage out there.They all have their
own individualread write layer out there.Think of it in the same way if
I had multiple users perhapsor multiple user on
the same computer.Each user can log on
and has their own spaceto make changes
in, their own spaceto do things and out there.But yet they're all utilizing
the same underlying softwarestack out there, the same
operating system and stuff.But each user has
their own spacethat then is on top of
the underlying provisionedsoftware.Same idea with multiple
containers out there.So the major difference again,
between a container and imageis that top drivable layer.All writes or updates go
into that top readable layerout there.And multiple containers
can share accessto the same underlying image
and yet have their own layerout there.So layer is also simplify or
rebuilding or updating images,meaning that only layers that
change need to be updated.If you think of
the set of layersas making up all the
software in there--if you update software for
example, on a PC or a laptopout there, you don't
redo everything.You update just that particular
piece, that layer in there.So it's the same idea.As an example here, if we
looked on that right hand side,you can see there is a command
there, Docker Pull Debian.What this means is that we're
using Docker to get the Debianimage down, the image that
would build the DebianOS out there, or have
a Debian OS in it.Notice that we pull down a
couple of different layersout there, down.And then we have
our image for that.Now notice then that we go
down next underneath that.And we pull Docker--a flavor of Debian
called Jesse out there.But notice that it has to, when
it goes to pull the layers,the actual layers that it
needs already exist locally.With Docker, the layers
themselves that make upthough-- who said you can
think of them almost like filesto the OS, they are stored
out there in var/lib/dockerin the common case.Then we would have the layers
based on their hexadecimal ID,store out on
var/lib/docker out thereand managed by storage
drivers that Dockeruses to banish the contents
layers and the writable layer.Out there so Docker has the
layers stored out there.So then if we need
a particular layerand we can verify that we
already have that layerlocally, we don't have to go
and pull it again out of there.So how do we think
about all this?We've talked about this
analogy already quite a bit.But the analogy of installing
software on a machineand doing a provisioning
systems for users.But let's look at what
this might look like sortof as a visualization.Suppose I have a new
computer that I'm setting up,either perhaps, as
something I needto do for my work or a
family member out there.Typically, what I would
do is I would go throughand I would set up this all
the operating system on it,maybe install some apps on it
such is the Microsoft Officeapps and maybe some virus
scanning software or somethingout there too--the different
layers of software.Those layers of software
installed and provisionthere on that disk
then form an image.That image can then be used
to instantiate or stand upother systems.I can take that set of
software provisioned out there,and I can actually then
copy it or install itonto another computer out there.In fact, then if I turn on
that computer out there,it's like turning on and
creating a container based offof the image.The image is always
present in thereeven when the machine
is turned off.When I turn it on, if I've
defined a user area out there,you can think of that user
area as being like a readand write layer.That is an area for the user to
make modifications and changesand for changes
to get pulled in.So we have that read write layer
kind corresponding to a userspace or a user profile area
on a computer out there.With that same image, I can
also provision another systemout there the same way.Gets the same set of software.The image is stored out there.When I turn it on and have that
user area out there in a readwrite manner, then I
have the equivalentof another container.So we can think
about this in termsof a image of set of
provision softwareto find out there configured can
be considered like the Dockerimage.Then a Docker container
would be each systemwhere that image is used
and where we have thatread write layer out there.We're running the
software out there.So just an analogy for
you to use for that.If it's helpful to you,
feel free to adopt it.If not, feel free to
find your own analogyor perhaps think in a way
that makes more sense for youout there.So what is Docker in reality?Docker, again, from a
marketing standpointis an open platform to
distribute applicationsfor developers in sysadmin.So basically, it's a way
to manage containers.More specifically,
though, Dockeris this framework
or this wrapperaround a platform for
using underlying Linuxtechnologies out there.We've mentioned these in passing
already with the union filesystem being able to look
through the different layersand treat those different
layers as the union of themall of the pieces together,
namespaces out therefor visibility, C groups,
or control groups.Again, the idea that
what is happening hereis when Docker is
installed on a system,it then is managing
these resources out thereto give the containers their
own set of resources carved outfrom the rest of the operating
system resources out there.Docker also provides
a restful interfacefor the service, a description
format for containers,the API for orchestrating
for running containers,building containers, those
kinds of things out there,and provides a way to create the
image by specifying the contentand operations at Dockerfile
that it reads then and buildsup the image from.How is Docker differ from a VM?We've alluded to this, but
if you think about a VMas being a virtual
machine, it requiresa hypervisor and a Guest
OS to create the isolation.For example, I can run a Unix
VM on a Windows system there,but I have to have the
hypervisor, somethinglike a VirtualBox,
for example, or VMwareout there running on that
system that then supportsme installing the virtual
machine on top of thatand running things
through there.In Docker, the
approach is different.In Docker, the
application Dockeris installed on
the system itself.There doesn't have to be this
hypervisor in the mix thereto run things on.Instead of relying on another
program to run things outthere, the hypervisor to
be the layer in between,what Docker does, again, is
to think of it as carving outresources for a
particular container areato run in there too, carving
out some of the systemresources using those Linux
technologies that we talkedabout to give each container
the appearance of havingits own area out
there to run with.Because Docker is
working that waywithout having to have that
hypervisor and the Guest OSand all those pieces in there
too, it's faster to start up.It's also more portable
in being able to runan image unchanged in
multiple environments.So what is a Docker file?Well, we've seen a couple of
examples of these already.It's a way to
created Docker image,a plan for how we're
going to constructan image that can only be
turned into a container.It's a text file that contains
in order all the instructionsto build an image.It has a specific format
and structure out there.Each instruction can create
a read only layer out there.Most of them can.Again, you see on the right hand
side here a couple of examples.And you can see that
on the upper one there,we're pulling down
a version of Tomcatout there to
support our web app.We have actually a web
app that I've built,and we are passing it in.We're going to pass it in.We're going to copy
that passed in webapp over to this directory
that runs in Tomcatand start up Tomcat
via the Catalina SH.So that starts up.That gets our web app that we
built locally, pulls it in,starts it up and running.The database one on
the bottom, we'veseen this one a
couple of times now.We start out with the basic
MySQL image, database imageout there that we can rely on
to have most of the MySQL piecesand stuff that we need.We copy in some
scripts that willhelp us to set up a database
when the shell script that isthe entry point is run there.And then we started the
MySQL daemon running.All of these things then, these
would be the Docker files,the recipes for how we provision
the image out there, howwe get that software stack
set up that we can thenbase containers off of.So Dockerfile to
image to container.We have a Dockerfile
on the left.We can do a Docker build, turn
the Dockerfile into the image.We saw this in the
animation I hadwhere we played through this and
saw it bringing down packagesthat it needed, bringing
down layers that it needed,creating the intermediate
containers along the way,giving those hexadecimal IDs,
and creating those layersand stacking them
up in there basedon the instructions
in the Dockerfile.We then go to run it.If we run from an image there,
we get that writable layeron top of that.Notice here that we had a set
of layers in our image herethat corresponded to some of
the different things in therethat we would be doing
maybe in a Dockerfile.Not this-- corresponding
to the actionsthat we had, building up the
layer for the image thereand then adding that read
writable layer on topto make it a container.If we were to do
a Doctor history,then, we would be able to
see all the pieces that wentinto this particular image.Docker has a lot of
commands out there.This is just a quick snippet
of some of them out there.We have the build command
to take the Dockerfileand turn it into an image.We have the images
command to lookat the images
available out there.RMI to remove an image.Docker inspect to give us some
metadata back from the itemout there.We can pull down code from
existing from a Dockerregistry with the pull command.We can give it a
tag name out there,log into a secure registry
out there, push things out,and so on out there.You can see we can look at the
ports, look at the history.We can also exec into
it and get logs out.We'll talk about
some more of theseas we go along throughout
the course here.So what does Docker
Run actually do for us?Basically, it pulls
down the needed image.It checks to see if the
image exists locally.In other words, you already have
this image available locally,so I don't have to go
out and get the piecesand build it again.If not, it's going to try to
download it from the registryout there.And if it already exists
on the local host,the Docker will just use
it for the new container.So remember that Run
here is trying to createa container from the image.So we have to have
the image availableeither locally already or by
pulling it down from a registryout there.Then we'll create a
new container on it.When Docker has
the image it needs,it creates a container by
just putting a file systemand mounting a read
write layer on it,creating a network interface,
setting up an IP address,running any kind of
application to processas specified in there, and
capturing providing the outputfrom the application.They're making that available
for us to look at as well.So what is a Docker registry?A Docker registry is
simply a repositoryto store Docker images in.We can push or
pull from it, muchas you might think
about pushing or pullingsomething like from a
remote repository in theget world out there.We can push images that we
create up to Docker registryto be stored in there.We can pull images
down from that registryto be stored out there.It can be public or
it can be private.The public one is out
there as Docker Hub.And then you can
have private onesinside of an enterprise,
inside of a company there,for example.It can also be
secure or insecure,meaning that it can require
authentication to get intoor it can be unauthenticated
and secure, open out there.The public Docker registry is
the default one, hub.docker.comout there, which has a lot
of the images out therethat have been around
for a long time, peoplecontributing to them
out there and so forth.We can also have these
private registry.We talked about
public and private.Private registries are typically
hosted at some path or portinside of your company or
even maybe just runningon your own machine, if you just
need a temporary one out there.But in order to work with
anything but the public Dockerregistry, you do need
to tag your imagesto add extra information
to your imageswhen they're created to
say the port and the paththat you were
targeting out thereto be able to get
to your registry.So for example, if
our federal registry,which runs at default
at localhost:5000,we had it running out
there and servicing throughlocalhost:5000 as far as URL.Then our image would need to
be tagged with localhost:5000slash the image name and
not just the image name.Being able to have
that information,the host path to get to
it and the port numberout there as part
of the image nametells Docker if you
are working with thisthat it should reference the
private registry as opposedto the public one.Without that kind of
information in there,if you do a Docker
push or pull, itwill simply try to go out to
the public Docker registryto get the image or to
put the image up there.In the screenshot on the
right hand side of the slide,we have a set of
repositories, a snapshotof the screen from
the public Dockerregistry out there,
hub.docker.com.And we have a set
of Jenkins CI imagesthat people have made
available and otherscan pull down and use.In this case, these are Jenkins
continuous integration imagesthat work in the Jenkins tool.Things that can act as a worker
node for the Jenkins toolout there.Variations of the various
options here on these.But the idea is if I get
one of these images downand then I spin it
up, do a Docker Run,start up as a container,
this containercan serve as a Jenkins worker
would serve out there too.It can serve in that
same capacity out there.It's just running in
a container instead ofon an actual physical machine
out there the same waywe would normally have stuff
installed and get it set up.Again, the image
would be provisionedwith all the pieces
of software that weneed to be successful here.It would be, for example, the
base operating system image.Java versions out
there, probablySSH or stuff depending on
the protocol to talk to it.And it would understand
once it spins up and startsrunning, we start running
it, how to talk backto the Jenkins master.That information could probably
be passed in and configuredvia some environment
variables, for example,that we might pass
in to the containerwhen starting this image up.A little bit of quick note
here about Docker tags.Doctor tags are user
friendly aliases for the hexIDs of the generated images.We talked about and
have seen that as wego through and create
these layers in images,each one has a
hexadecimal ID out there.It's the newest version
is a 256-bit digestthey call it, a checksum or
hash out there, for these IDs.And then if we don't want
to have to reference themby the image IDs.And by the way, you can
get away in most caseswith only using
maybe the first fouror five significant
characters of the image IDif you talk about the image IDs.We could use Docker tags to put
human friendly names on them.So these are human
friendly aliasesfor those hex IDs out there.They're a way of
referring to an image,similar to how you might
have a git tag referringto a particular commit instead
of having to reference itby the SHAH-1 value if
you're familiar with Git.Two general ways
to create a tag.We can do it at build
time with Docker Build.The dash t option out
there could give usa name to put on it as
well as the tag portion.Now, tag is an overloaded term
in the Docker tag vocabularyout there.Tag refers to the
action you can do.Tag refers to the version part
of it, the part after the colonand the name of the
image out there,or it can also
refer to the largerpart, which is the name and
the version piece out there.So typically, you could
do it via a Docker build.If I do a Docker build,
you'll notice the dash t here,the particular
name of the image,and then the tag or the
corresponding version number.I want to have the
reference to this.And then the dots.The dot is simply the context
where to get the things,where to look.In this case, the
current directory.So Docker build dash
t roar dash db v1.So in that case, you'll
take a Docker file.It would go through and create
the layers out there and createan image that was ready
to run this stuff that Ineeded for this roar db v1.Note that I'm tagging it
here with a name followedby a version information
number and then running itwithin that same context.The other way to do a Docker tag
is via the actual tag command.And again, same sort of idea.I can take an image ID
where an image ID mightbe that hexadecimal image ID.It also might be another
tag itself out there.And I can add in the
image name and a tag on itand give it a tag that way.Here's an example of that.Taking a binary image
out there, tagging itwith a name and a version
number to be usable out there.We can also note that we
can also do things like tagan image referencing it by
its existing tag, its existingname, and tag it with a
new one out there as well.Tagging for private registries,
private repositoriesrun on a registry host.We talked about, for example,
by default the one that you get,the one Docker registry is
just another Docker imagethat you can spin up and
run as a container thatimplements that registry when
it is running as a container.It acts like a Docker registry.For example, if it's
running on localhost:5000,out there then we would need
to tag the image with thatinformation.In other words, if we
didn't add it at build time,we would need to go
in with Docker tagand actually tag that image
with that additional informationabout the host
and port out therefor that particular registry.Again, that's a way
that tells Dockerwhen we're working
with it to get imagesin that this should not try
to go out to the public Dockerregistry out there but rather
to the one that representedby the information in the tag.For example, the one running on
our local host system on port5000 to act with
that registry there.Now, one other note here.In Docker there is a
notion of a latest tag,being meaning that portion
after the name there.That can be referenced
there instead of havinga specific version number.There is a latest tag.The best advice with the latest
tag is to simply avoid it,because it doesn't mean
what most people wouldthink it means.It does not mean that it is the
most recent image every timeyou reference one that
has the latest tag on it.Rather, it simply means that
the default version that isadded there if I don't
give a specific tag.Meaning that if I were to
not supply the v1 up there,for example, in roar db
when I was doing a tag,it would give me
roar db colon latest.At the point in time
I do that, it probablyis the latest version
of this image out there.So latest may make sense.But after that, if I do apply
one and give specific tags,it is no longer the latest.It will still have
latest on it as that tag,because the time
I tried to tag it,I did not give it that suffix
in there, that supportingthe suffix tag in there.But latest will not
be the latest anymore.So latest is just a
point in time default tagif I don't supply one.So the best thing to do is
probably avoid using latestand always try to use explicit
tags, explicit versions outthere when we actually
tag something out there,because latest can get
out of date very quickly.But the actual
latest tag will applyto a version, whatever version
it was on at the point in timethat that image was created.So the trick, again, is to
basically avoid it and usemeaningful tags instead.So then the question
becomes how do wework with multiple containers?So we have this idea now
of what a container is,that it's based off an image.It has that read write
layer out there for it,and it is based off of it.But most applications
are not goingto simply be a single
container out there.They're going to be
multiple containers.For example, the
demo applicationthat I'm using throughout
this presentationhas both a database piece
and a web app piece.So I have a container
for the web based piece.I have a container for
the database piece.When I start
talking about tryingto spin up multiple containers
from different imagesand start trying
to manage them, itcan be challenging to do
that and to get that thingmanaged easily out there.Fortunately, there are a
couple of different wayswe can actually
manage that here.One of them is the
Docker Compose tool.What Docker Compose
does for us is itallows us to start
containers together.It allows us to
reference multiple imagesand the information
about how theyshould be turned into containers
in a dockercompose.yml file.YML is YAML format, yet
another markup language.So we can specify in
this Docker Composefile that which
containers we needand also what kind of things
they need to pass in to work.What kind of
information about whichports should be opened
out there or exposed,what other ones it should
link to, and so forth.Notice here on the
right hand side,my web app is referencing to
my MySQL session out there.I'm on MySQL container.So we have multiple
counters in the mix.And we can specify
the relationships.We can specify the settings and
stuff in a Docker Compose fileand then just run this
Docker Compose up commandto actually execute that and
create the container from it.Note that Docker
Compose, two words there,sorry, hyphenated,
actually, when you run it,is not an actual Docker command.It is rather a tool out
there, docker-compose,that you run to get this
functionality out there.But certainly for just a
small set of containers,you could use Docker
Compose to managewhen you want to run them
and how they work togetherand those kinds of things.So when composing
containers together,essentially a
three step process.You create the Dockerfiles
for each part of the app.You define the services that
make up the app and a DockerCompose file.And then you run
Docker Compose upand it starts and runs your app.Another way to do it is
through the Docker Run.You could actually use a
single Docker Run commandto manage multiple
containers and link themtogether and so on.It works with the
dash dash link option,which is an extended
form of syntax out there.Note that they may be doing
away with that link optionat some point, and it gets
to be quite complicatedwhen I'm trying to on a single
command line specify thingsabout how multiple
containers work together.This is why it becomes useful
to have the Docker Compose fileout there with that
specification in itand use Docker Compose
tool to run that.There's also a Docker
Stack option out there.This is a fairly new one
they've incorporated in Docker.It's a command and the
Docker command lineinterface that lets you
manage your cluster via Swarm.Swarm was one of the competitors
to Kubernetes out there.And it also can leverage
Docker Compose YAML file.The final way or right
now the final waywe're gong to look at to
work with multiple containersis through Kubernetes
itself or through Kubernetesenvironments.This is part of the
design goal of Kubernetesis to be this orchestration
or this overseer for setsof containers running out there.And it is the preferred
way today as faras the best practices to
be able to orchestrateand run and manage large
sets of containers out there.The overall Docker flow lets
suppose our host system there.We have the OS, we
have Docker installed.We have an image out there.Docker images are
stored in a registry.We talked about the public
and private registries.A Docker file describes
how to create a new image.Just trying to give you kind
of a quick visual reminder hereof what we talked about.The Dockerfile then has
a set of commands in it.It can reach out and grab basic
images out there, pull stuffin from our local
systems out there,copy stuff in, run commands out
there, and ultimately createcontainers for us.We can have multiple containers
within our system here basedon the images we define.And then a Docker
Compose file thatdescribes how to create
containers and linkmultiple ones together.For example, how to take
a Compose YAML out therewith two images,
link them together,and start them up and running.One other point here as we're
finishing out our containerstopic here is to talk about
the open container initiative.So Docker has primarily been
associated with containersas the main application
that we've traditionallythought about as being able
to create, manage, and runcontainers out there.However, it is not the only one.In fact, in time, some
of Docker's, shallwe say, the market
share out there,there have been
other competitorsout there that can do
similar kinds of thingswhich are available to use.And so we have the open
container initiative, whichsimply is a way
that Docker has beenable to work with the open
container initiative partout there, the Linux
Foundation out there to createa specification and describe
a way of doing thingsso that if you do these things,
you follow these specificationsout there, you follow the
open container initiatives,then you too can be like Docker.That is, if you have
an application thatcan do the same kind
of container formatsout there or images or
so on out there too,by following this open
container initiativeand by being
consistent with it, youcan do the same kinds of
things Docker does out there.So that has led to other
players in the container space,and some of these have actually
been around for a long time.RKT or Rocket has been
around for a long time.Podman, Buildah are ways to
develop containers out there.Cri-O is about
container runtime,and they're the
same sort of thing.runC, runtime for OS
containers out there.So a number of
different contendersthat are available to
you other than Docker.In fact, I think in more
recent versions of the Red HatEnterprise Linux
pieces out there,you won't even find Docker
being included anymore.You will have a couple of
these kinds of applicationsthat are included and available
to you to work with containers.But the same ideas
and principlesapply that we've talked about.It's just that they are
implementing the open containerinitiative so that they can
act as you might expect Dockerwould in these situations.So next we'll talk
about Kubernetes.So what is Kubernetes?Kubernetes is a portable
extensible platformfor managing containerized
workloads and services.A little bit of
a mouthful there.But cluster
orchestration system.It's designed to help manage
all of the different containersout there that we have in
the Kubernetes environmentsout there.Frequently it's
abbreviated as k8s,K-8-S. It's derived from
the Greek for helmsman,thus the icon, this
ship's wheel out there.So k8s is the
Kubernetes abbreviation.Refers to the eight
characters thatare between the K and the
S, the first and last letterout there.This is becoming sort
of a stylistic wayto refer to these
applications out there.It was formerly known as Borg,
which is the internal Googleproject, open sourced
by Google in 2014,and groups containers
that make upan application into some
logical Unix for easy managementand discovery there.So it's about being able to
manage these containers as setsin groups that belong
together out thereand making sure that it is a
robust deployment out there,meaning that if something
happens to a container,it can stop it and spin
up another one out there.If it needs to scale up
containers to handle a load,you can allow Kubernetes
to do the scaling for youto have more
containers out thereand then just drop it back down.So especially if
you're having to payfor cloud resources for example,
you can save some money there.So Kubernetes
enables teams to workwith these containerized
workloads, multiple containersout there, in a much easier
manner than you would normallybe able to do
otherwise out there.Kubernetes has some features out
there, service discovery, loadbalancing out there
being able to sharethings between containers.You don't need to worry with
service discovery in mostcases.It's going to take
care of it for you.Kubernetes or K8s
here gives podstheir own IP addresses
in a single DNSname for a set of pods.What is a pod?A pod is a
constructed Kuberneteswraps around containers and
any attached volumes to them.In Kubernetes, the
pod is the lowest unitof granularity or things
that we work with there.Pods wrap around containers.Kubernetes then manages pods.Can do automated
rollouts and rollbacks.It can roll out updates to it.It can also monitor
progress of thingsand make sure that it can keep
the system going there, keepinstances of applications going
in the containers in the pods,and it can automatically
rollback changesfor you as well out there.Kubernetes also deals with
storage orchestration.You can automatically mount the
storage system of your choice.So if you're working on a
cloud, for example, in AWS,you can use S3 out there.The storage can be local on
a cloud or from the networkout there.So Kubernetes allows us to
manage where we store data.It also allows for
batch executionout there to make sure that
any containers that failare replaced out there.It is self healing, replacing
containers that die,replacing the
rescheduled containerswhen the node dies off.In other words, the
machine you're running on.And it can kill containers that
don't respond to health checks.So if a container goes
rogue and isn't respondingto a standard check
out there and threatensthe health of the overall
system, for example, or issue,then incubators can kill
that one off and spin upanother instance of the
container, again, based offof the image.It also supports secret and
configuration management.That is being able
to hide or obfuscatesensitive information out
there and what we call secrets.Configuration values like
environment variablesand stuff, we can store
those as Kubernetes objectsso that we don't have to rebuild
images or expose things outthere.Treat them as
separate objects thatcan be managed by
Kubernetes and defineeither the secret information
or configuration information.It also supports horizontal
scaling, automatically scalingup or down to have a certain
level of CPU usage out there.So if we have more CPU
usage going in there,we might add more
containers to actually--or lower them
depending on what weneed to do to get back to
the desired level of CPU.So it's just a way of
dynamically adjustingthe number of these pods,
pods wrap around containers,remember, to make sure
we can handle loadand to make sure we're
not costing too muchor running up
resource bills here.Automatic bin packing.Kubernetes can automatically
place containersbased on resource
requirements and constraintsout there, meaning
you can schedule themon the machines that are running
Kubernetes in the best wayto make as many fit on there as
it can to be efficient about itin there, to make sure it
can utilize the resources outthere.So if we have
multiple containersand they need a certain
amount of resources,if Kubernetes can when it
actually is working with these,it can schedule them
on the same systemthere to try kind of a
puzzle piece getting the bestfit for that machine out there
based on the number of thingsthe containers in
pods having to run.So what does K8s
really do for you?Well, Kubernetes provides
you with a frameworkto run distributed
systems resiliently.Takes care of
scaling requirements,failover deployment
patterns out there.It's sort of like your virtual
unattended deployment engineerout there, making sure that
your deployments are upand running, making
sure that theyare able to service
what you need,making sure if there's a problem
that it gets corrected outthere, that kind of thing.When we talk about how to think
about this, an analogy herewith this that I like to
use in terms of Kubernetesrelated containers, if we
think of containers as beinglike those individual
machines out there,we might think of a
data center as beinganalogous to Kubernetes.What do I mean by that?Then what is the main jobs
of a data center out there?If you think about a data
center you have at your workor perhaps is supporting your
on a cloud space somewhere,then the main jobs
of the data centerare to provide systems
to service certain needs,regardless of the applications.You want to be able to provide
the applications runningin containers out
there that servethe needs the
customer will have.Keep them up and running.If you think about a data center
when those computers out there,the ones that was supplied
for customers to use,want to keep them
up and running.If something goes down,
we get an alert or suchand we get information
then sent back to us.We have to go out there,
the data center personnel,see what's wrong, and make
sure that they update it.They go out and
they do what needsto be done it might even be
replacing a system that's diedwith a new system out there.Add more systems, remove
systems depending on the load.This is kind of like the
scaling that we talked about.If we are a retail
company, for example,we might add more computers
in our data centerto handle an increased load
during the holidays out there.And then afterwards,
we might remove themor scale back down, because
we don't need so many anymore.We don't have as many
customers coming to our site,then we don't want to pay
for the extra resources,the electricity, the
other things that arebeing used hardware for that.So we can actually--
a data center wouldbe responsible for helping
to scale up or scale downto meet load requirements.Deal with systems that
are having problems.We talked about that.If we get an alert that
a certain system is notfunctioning, we want to
make sure we address that.And more often these
days than not, itmight mean just pulling that
system out and replacing itwith a different system there.Deploying new
systems when needed.When we have a new
business application,for example, that
we want to work withand we need some compute
power to handle the websitetraffic we're going
to get from it,we would have deploying
some new systems outand data center perhaps
dedicated to that.Providing simple access
to pools of systems.If you think about a
data center operatoractually monitoring
sets of systemsout there through
a common terminal,that sort of thing,
making sure of the systemsare running regardless
of which systemthat things are connected
to out there havekind of this virtual interface
or overview of them out there.So in our sense there,
Kubernetes is everywhere.It's effectively won the war
versus the other competitorsout there.Other competitors include things
like Docker Swarm and Mesosyou may have heard about.But the cloud
providers all endorseit, provide ways to get
a Kubernetes cluster.There is the Amazon EKS,
Elastic Kubernetes Service.There is the Kubernetes
Google Kubernetes Engine, GKE.There is the Azure
Kubernetes Service, AKS.These are all ways that the
cloud providers have for youto get a Kubernetes cluster
to work with if you'reusing their cloud out there.As well Kubernetes exists
in certain implementationson enterprise platforms, things
like an OpenShift out there.OpenShift is the Red Hat
Enterprise implementationof Kubernetes.Has other functionality
layer on top of it.But at its core, it's
Kubernetes out there providedto run an enterprise with some
convenience features providedby Red Hat.So let's talk about a little
bit of terminology here.First off, what is a
Kubernetes cluster?So the cluster is a set of
machines, a set of typicallyof high availability computers
that run Kubernetes on it.For example, it could be
either virtual machines,actual physical
machines out there,whatever the case may be.So the abstractions
we have in Kubernetesallow us to deploy
containers in the clusterwithout tying them
to a specific coast.What we mean by that is if we
have multiple of these workernodes where we could run
things in these containers thatare wrapped in
pods, then we don'thave to care about which
system they end up on.That's Kubernetes' to
figure out how to run them.Just as it might be a
data center operator's jobto figure out where to put new
computers in the data centeror where to tie them in or
which rack to put them on and soforth in there.So Kubernetes automates
distribution schedulingcontainers across in
an efficient manner.And you'll see on
the right hand sidehere representation about
how things may be laid outin Kubernetes or how things are
laid out in Kubernetes in termsof the node.If we think of the whole
thing as a set of clusterand the set of machines out
there, each one being a node,then we can have a
couple of types of nodes.The one is the master node,
which is overseeing thingsfor the worker nodes,
helping with scheduling,those kinds of things out
there, interfaces to users,APIs out there as well.That's the master node.Then you can have
multiple worker nodesout there, which are
responsible for actually runningthe pods, which ultimately have
the containers inside of them.The nodes can be VMs or
physical machines, again,and the nodes will have a
thing called kubelet on them,the worker nodes, meaning that
this is the Kubernetes agentsoftware.We'll talk more about
this in a moment.Let's take a look at what
a cluster overview is hereto help you get a basic idea
of how a cluster is set up.So if we start out thinking
about the master node here,the master node is set up
to coordinate, get input,that sort of thing out there.And then we have
various worker nodesout there in our
cluster as well.So the master node has
an API server on it,which serves, as the
name says, to serve upthe API to provide API access to
things in Kubernetes out there.It will also be interacted
with through this command lineinterface called
kubectl, Kube CTL.And you'll hear it
called cube control.You may hear it called cube
cuttle, as in C-U-T-T-L-E,or cube cuddle as in
C-U-D-D-L-E out there.Any of those.You're talking about the
Kubernetes command lineinterface.The command line interface has a
config file associated with it.The config file
sets the environmentthat the command
line is running in.So that also ties into
and tells it which clusterit should be working with.So if you're using
the command line,it needs to know
which cluster itis supposed to send the things
to and work with out there.That's essentially what the
cube config or the config filedoes for us out there.So a person working
with this thenmight go through running
kubectl on the command lineto do some command
for Kubernetesout there to get sent
to the API server.The API server if it needs
to work with the workersout there, each worker has a
program on it called kubelet.Think of that like the
Kubernetes agent piece.The agent that runs on
the worker node thatallows it to talk back
to the master in the APIservers out there.The master also has
something called etcd on it.etcd is basically a
key value store keepingtrack of information out there.A controller manager.A controller in
Kubernetes is a processthat tries to ensure that the
specification that you haveprovided for Kubernetes
matches what you've actuallygot running in the cluster.It is trying to always
reconcile the information you'vegiven to Kubernetes with what's
actually running in the clusterout there, the controller loop
or the reconciliation loopout there.The scheduler is what
schedules work, schedules pods.The pods contain the
containers again.It schedules the pods
to run on the worker.The workers will also have
something like Docker on them.They might have another
one, but somethingthat implements that
OCI specificationthat we talked about.Because ultimately what we
are running within Kubernetesis containers.Now, containers are
created from an image.We have an image registry
out there we talked about.There's the public one,
the Docker Hub for example.Work you have the
private ones we know of,like the local ones internally.When we go and create an
image out there, get an imageand create a container,
those containersare housed in pods
within Kubernetes.The pods then can be used or
kept inside of the container.Sorry, kept inside of that pod.We can have multiple containers
and any of the sorts of storagethat they have
attached out there asrepresented by the green disk
here in this particular pod.We can have multiple pods
on the nodes out there.In fact, we expect to
have multiple pods.Each pod can have multiple
containers and their storageassociated with them.Additionally, on the nodes, the
worker nodes of the cluster,we would have the
kube-proxy out there.This is a proxy
that allow someoneto reach out and
work with the nodeitself or talk to
the worker nodesout there and to the
process is running on them.We'll have more to
say about how someof the ports and
openings and stufflike that later on in the class.But the kube-proxy allows
the worker to talk with it.The final piece
really is a mainstayhere of the overall cluster
is the Container NetworkInterface or CNI.This is the part that
allows things in the clusterto talk to each other,
the nodes and such,communicate through
various rules and such,and manage the access
to different things.But it's all about the
network communication there.These pieces are
typically plug-ins that wehave into Kubernetes.And they'll have fabric sort of
base names, things like calico,flannel, or weave out there.You can see sort of the
fabric theme in there.A networking interface
fabric out thereacross the cluster to allow
things to communicate.So that's our overview
of the cluster out there.You can see there's
a lot of parts.But none of them in this sort
of at least kind of a blockdiagram form too complex,
I think, to reallyunderstand or get
a handle on here,you get the idea, though,
that how it's structured.So some quick terminology here.We've talked about
some of this already.Pods are an object in Kubernetes
that contain and manageone or more containers and
attached storage volumesthey have.The service is an abstraction
that groups togetherpods based on identifiers
called labels.What we mean by
that is a service.This is not a microservice,
this is a Kubernetes service.When we are actually working
with an application runningin container, we need
to be able to access itthrough network information
or sending traffic to it,getting things from it.That goes through the pods.Each pod will have its
own unique IP address.We could connect to each pod
through its own unique IPaddress, but by
definition, pods aremeant to be able to
be deleted and spunup again if needed out there.So if something
happens in my podand I need to delete it out
there, I don't want to--when I get another pod
back, it will likelyhave a different IP address.So I don't want to tie my
connections to a specific podIP.So a service gives
me a virtual IPout there that I can connect to.And then on the
back end, it willmanage talking to one of
the pods in the group there.So the service
will figure out howto do the load balancing
connection and stuffon the back end.All I do is connect
to the service.The service takes care
of then connecting meto a pod running in
the background there.And so I don't have
to worry about that.I just connect up to the
service as an abstractionon the front end of a
set of pods out there.A deployment defines
a pod specificationbut also defines
something calleda replica set or replicas.Replicas out there say how many
of these pods should I have.This goes back to
the idea of beingable to deal with the load.I may just need
one pod to handlethe load of traffic coming in.I may need multiple ones.But the replica that's
part of deploymentallows us to specify how
many of these things we need.An ingress is simply
a resource that letscluster traffic be exposed--or cluster applications be
exposed to external traffic.And a namespace is
simply a logical groupingof these different
objects togetherso that we have a common
way of grouping themtogether and referring to them.So how are containers
organized on Kubernetes?Kubernetes clusters have nodes.We talked about if there
are already nodes out there.The nodes run the pods.The pods are wrapped around one
or more containers out there.The pods are front
ended by a serviceto be able to access them.And the pods are
scaled by deploymentsto see how many we need, how
many we need to have out there.And then namespaces within
Kubernetes cluster groupobjects like pod services
and deployments together.So the pod, again, is the
smallest deployable unitin Kubernetes.We talk in Kubernetes
terms, we'retalking about pods,
not containers.Pods wrap around the
containers there.Represent a group for
one or more containersand these shared resources,
which we said it can be volumesor unique cluster IP
addresses out there.Pods are scheduled on nodes
automatically by the master,but you can also specify
a particular nodeto run on with the selector.A selector is simply an
identifier, a label, a tagout there.We'll be talking more about
selectors a little bitlater on.Scheduling takes into
account the node resources.Meaning just the scheduler's
smart to figure outwill the thing I'm trying to
schedule fit onto this node?Are there enough
resources for itto be able to run
successfully on it?Containers should only
be scheduled togetherif they're tightly
coupled and theyneed to share resources
such as the disk.Two containers running
in the same podcan actually talk to each
other over local host.They don't have to
have any other way.They can talk to each other
over local host out there.So the pod will be scheduled
and then run on a node.Namespaces, again, simply
a unique working areawithin the cluster to kind
of group things together,separate them out
into logical unitsthat may consist of services,
deployments, and so on outthere.They provide a
logical partition,the cluster of resources.Kubernetes resources are
scoped to a namespace.A namespace becomes part of
the way we reference themby the namespace they're in.All resources in a namespace
must have unique namesout there.Kubernetes allows for
some quotas around thatif we need them.The namespace, the default
namespace in Kubernetesis, ironically enough,
called default.That's the name of
the namespace that weget by default out there.An namespace object
is abbreviated ns.When we talk about
objects in Kubernetes,they can all have ways of
referring to themselvesor referring to them.
ns is the namespace.An option is dash n
if we're supplying iton the command
line, for example,to reference a different one.We can set the context to
change the current one out thereif we wanted to to make
instead of default to saythe default one is
called foo for example,or some other namespace.What is kubectl?We've mentioned kubectl already.It's the command line
interface for out there.It has a kube config file
associated with it to tell itwhich cluster to
work, again, any kindof necessary configuration
information out there.The command describes
an operationto do on one or more resources,
such as a get, describe, log,create.These are all
Kubernetes commands.Most of them we'll talk
about throughout the course.It has a type that it can take.So we talk about the command
we want it to do or thing.The type of object we want
it to operate against.For example, we
want it to operateagainst the pod, a deployment,
a service out there.And then the name is the name
of that particular resource.For example, if we're
working with a pod,we give it the pod
name out there.If we're working
with deployment,we give it a deployment name.So we're running a
command and an operationagainst a particular
type of object designatedby one of those built in types,
pod deployment service, soon out there, and then the
actual name of the object,and then any optional
flags for the command.Some example kubectl
commands. kubectl get.Think of that as a list.That's what I tell people.Think of that as
a list out there.Get me a list of these things.Get basic information about
the set of objects out there.kubectl apply.We'll talk more about
this as we go along.But Kubernetes is based off
the idea for configuring itof being able to use these
YAML files that describehow many deployments we need
and what they look like,how many pods we need and
what they should look like,and that sort of thing.When we have these
specifications out there,to get them actually to happen
in the Kubernetes cluster,we use the apply command.The apply command says
take my specificationfrom the YAML file and make one
of these in the cluster for me.Make it happen in
the cluster for me.So apply is basically saying
take the specificationand execute the specification
to create the things Ineed in the cluster itself.Kubectl create.Similar idea.Typically, though, you're not
using a YAML file for that.You would be using a
created type of objectfor maybe a create
an ns, a namespace,called foo out there, kind of
a short term method of that.kubectl describe to get
detailed informationabout the current state of
an object or kubectl deleteto delete an object out there.So you could basically
get the idea hereof the kinds of things we
can do from the command line.Speaking of YAML, YAML is
a type of markup language.Defines Kubernetes
specs for resources.It's stored typically as
dot YAML or YML text filesout there.kubectl apply takes the such
a files input, updates clusterobjects based on the
specs, turns the YAML specsinto resources in the cluster.If you wanted, you could
have something runningin the cluster,
and you wouldn't beable to get the YAML that's used
for it to create it out there.You can dump that out with
kubectl get dash o YAML.Basically think of it
as list the resourcesout there in a YAML format.What is YAML?Again, you may already
be familiar with this.But it's a human readable
data serializationlanguage typically used
for configuration files.Big word for just
saying it's a wayto organize information or
data into a specification thatmakes sense for computers or
for things like Kubernetesand applications to use.Usually has an extension
of .yml or .yaml,YAML meaning Yet
Another Markup Language.It is space and
character sensitive.Used by a lot of
applications, particularlyin the Docker, Kubernetes,
Helm, those sorts of worldsout there.So on the right
hand side, you cansee one of these YAML
files here that definesome things for Kubernetes.In this case, we're
defining an objectcalled a persistent volume.A persistent volume refers
to a persistent storageout there, a volume
storage out there.In other words, storage,
it doesn't go awayeven if the container and
the pod go away out there.It's still persisted.There's an API version in there
for the version of the APIwe're using with this and
some metadata such as nameand labels we can put
in there, specificationsabout that particular
items we want.For example, we're talking
about storage here.So you can see we're talking
about a capacity access modes,the path to it, and so on.Then a persistent
volume claim isa way of saying let me get
access to that storage.So again, and from the same
sort of pattern API version,the kind of objects
and metadata out thereare names, labels,
and then informationabout what characteristics
we want this to have.So YAML file here.Note the indentation here.The way that we can identify
which things are associatedwith which other things at a
higher level by the indentationhere.As we go down through
it, that becomesvery important in there.So what do you need to
know about YAML really?Well, a couple of quick things.The conventional block
format, if you'redenoting a new item in a list,
uses the hyphen and a space,as you can see at the top on
the right hand side there.Keys are separated from
values by a colon and a space.Indented blocks use indentation
to separate key value pairs.So we have key and
value out there.We separate them indentation
and new lines out thereto do for the different items.Strings do not generally
require quotation marks.Data structure
hierarchy is maintainedby outline indentation.What that means is
the more you indent,the lower in the
hierarchy it is.So starting from the left
side, if it's high level,you would indent a little bit.You get the second layer,
indent a little bit moreto get the third layer,
and so on down the line.So let's talk about
understanding some Kubernetesobjects here.To work with
Kubernetes objects, youuse the Kubernetes API or you
can use the Kubernetes commandline.But basically, you're
using the API either way.The kubectl command line tool
makes calls to the API for you.You can also use
the Kubernetes APIdirectly through the
Kubernetes API client librariesif you didn't want to go
through the command line,if you wanted to write your
own programmatic interfaceto Kubernetes.Kubernetes objects are
persistent entitiesin the Kubernetes system, mean
that they are persisting outthere based on the
description, the way theyshould be in the YAML file.Kubernetes uses these
entities to representthe state of the cluster.They can describe, for example,
what application containers arerunning on which
nodes out there,what resources are
available to themand policies around how
these applications behave.A Kubernetes object, you think
of it as a record of intent.Meaning this is what I want
to have when it is in placein the Kubernetes system.It's declarative.Instead of telling
Kubernetes how to create it,you're telling it
as an imperative.You're declaring what you
want, and Kubernetes' job thenis to make that happen.After creation,
Kubernetes will workto ensure the objects exist.And creating an
object declares youwant the cluster what you
want it to look like its knownas to the desired state.Meaning that you're telling
Kubernetes what you want outthere instead of how to get it.It's a declarative model
versus the imperative model.So a little bit more
about this to kind ofget it fixed in your mind.Kubernetes is a
desired state system.The user supplies that
desired state be declaring itin manifest or YAML files.Kubernetes then works to
balance the current stateand the requested state.So it's trying to
take what you'veasked for in those YAML
files and make surethat it happens and is set up
and running in the Kubernetescluster out there.So desired state is what
you want your productionenvironment to be like.The current observed state
is the current statusof your environment.If you look at the
illustration here on the page,we have a user, have a
Kubernetes specification,the YAML files out here.The Kubernetes user could do
a kubectl apply for exampleand provide that
specification into Kubernetes.Kubernetes then, through
things like the controller,would run a reconciliation
loop to say you've specifiedyou want two of these.You want a service,
you want a deployment,whatever the case may
be, and get those upand running in the
cluster out there.So it would take
what you specify,get it running in the
cluster, and go from there.When we define a
Kubernetes object in here,we talked a little
bit about this,but let's look just a little
closer at what we have here.So when we define an
object in Kubernetes,we have to provide an
object specification to getthe described desired state.A basic info such as a name,
perhaps the tags, and soin there.The Kubernetes API
expects the infois JSON in the body
of the request,but usually you
simplified it with YAMLand the kubectl command line
converts it to JSON for you.JSON is JavaScript
Object Notation.A pretty typical way of being
able to specify informationto computers out
there to applications.So if we look at the
specification on the right handside, what we get, we
see the field there.There's an API version.Which version of
the Kubernetes APIis being used create the object?This helps us ensure
that we're referencinga version of Kubernetes that can
support what we need out there,especially if it's
something fairly new.You may need to specify a
newer API version that youcan run against or use.We have the kind, what
kind of object create.Is it a deployment?Is it just a pod definition?Is it a service
or so on in there?And then we have metadata
about the object in there.Typically this would be
things like the name we wantto assign it in the system.And the label,
perhaps, that we'reusing to select it or
search for it in the systemor look for it out there.It can even be though a
namespace that we have wewant to run it in.Remember that a
namespace in Kubernetesis simply a way to group objects
together with a common nameand a common scope there.The object spec
in there is goingto be different for
every Kubernetes object.We have a set of
required fields, thenwe have the object spec in
there with specificationsabout the type--for the type of object
we're wanting to work with.It can contain nested fields
specific to the object.And you can find more details on
the specs in the API reference.Let's take a moment
and take a lookat the specification on the
right hand side of this slide.We have the API
version at the top.We have the kind of
object out there.We have the metadata about it,
the name we're assigning it,the labels, the name, the
labels that we're providingas in a name equals value.The name equals roar-web.We could also have
bar equals foo.It's just a way of assigning
an identifier to it.We also have the namespace
we want to run it in.And notice that underneath
those first common areasof the required fields,
we have the spec area.The spec area would be where
I define the attributesor characteristics about
the specific objectthat I'm working with.In this case, we are
talking about the top right,a deployment.That is a deployment object.Remember that a deployment
is used to both define a podand also to define the number of
replicas or number of instanceswe want Kubernetes to
keep around for us.So here we have the
number replicas at one,meaning that we always want
to have one of these running.If something
happens to that one,then because we've set
up a deployment herethat says we have to
have one, Kuberneteswill go and spin up
another one there.Then notice that we have
some metadata about it,some more labels in there for
the replica piece in there.Then we have another spec.So again, the deployment
has both the numberof things we want and
then the pod specs itself.So the second spec there,
it's indented further,is about how do
we create the pod.The first one is how do
we create the replicasthat we need.The second one is how to
create that actual pod.So the container in there we
have for that pod, roar webis the name, an image.So this is the container image.This is the Docker image.This is how we
get that containerto go inside this pod.We tell it an image.Then what happens is Kubernetes
is pulling the image down,going and running and
creating a container,putting it in the pod,
and so on in there.We could also have
information that'sparticular to the
kind of applicationwe're trying to run
in the container.In this case, ports
out there that we'regoing to expose within
the container to make surewe get information in or out.Then notice that there's
a three dash is there.That is a separator.You can have multiple
specificationsin the same file separated by
these three dashes out there.We have, again, the
same required fieldsas we did for deployment.The API version, the kind
of object, the metadata.Metadata, again, having things
names or selectors as labelsout there and namespaces.And then note that the spec
looks a little bit different.We have a certain
type of service.We can tell what
type of service.We have the ports we
want to expose as well.So ways to specify the
Kubernetes objects in this YAMLfiles.Now, when we actually have
these YAML files, these objectswe put into Kubernetes, and
Kubernetes has taken thisand has actually spun them
up and is running them.Then we have
additional informationthat Kubernetes provides.Not just the spec but
also status information.We still have the
original spec out there,the way we defined it to
get it into Kubernetes.But then we also
have the status.Status information says
this is what Kubernetes,this is the state of the object.This is the status
of the object.You can see over there under the
status on the right hand sideall this information
about the last timeit did this or the last
transaction time, last probetime, those kinds of things.Information about the
object, informationabout the most
recent things it'sbeen doing or been done
to it out there, providingthe actual state.If you're wondering
how I got this,this is from a kubectl edit.In other words, a
Kubernetes commandline being able to edit
the object in place.You can actually edit it in
place and then modify things.You could also get
the information,dump it out with the dash
o YAML for that formatand get information there.The dash n at the
top of this commandis for the namespace
called roar2.We're asking for the pod object.Remember we talked about
the Kubernetes format?Typically has the command,
command is edit here.Any kind of flags.You can use the flags out
of order here a little bitand wherever they fit
in the command line dashn, the namespace.Then we have the type of object,
the pod, and the actual podname, the name of the
object in the system there.So updating resources in place.If we want to update something
in place, an object out there,ideally what we really do is
we would go back and modifythe YAML files and then put
a new one of these out there.However, since we
have already beentalking about it
a little bit, I'lltell you that there is a
kubectl edit command herethat can be used to edit
a running object outin the Kubernetes
system in the cluster.Basically, we're editing its
YAML definition or definitionon the fly here.So we can say kubectl edit,
the resource type, deployment,service, whatever,
and the resource name.And then basically,
this would give usa way to actually bring up that
object in an editor for us.We can configure
the editor out therewith the kube editor variable.Then we can go in,
edit the thing,make changes that
we need to, save it.When we save it, Kubernetes
will validate whether or notis syntactically correct.If it's not, it'll
throw you backinto the editor instead of
saving and exiting in there.Now, note that the change,
this is a on demandtype of change on the instance
of the running object.It is not persisted back to the
original configuration source.So if you were to go back
and apply the original YAMLs,you will still get the original
specification, not anythingthat you may change
as far as edit.You could also restart an object
out there if you needed to.Selectors and labels.Selectors and labels are
used throughout Kubernetes.This is the idea of human
readable identifiers and waysto identify things.Labels are mechanisms
that we used organizeKubernetes objects to basically
allow Kubernetes to tell whichsets of things it
should be workingwith for particular
situations but also to allowus to find sets of things
that we were interested in.A label in Kubernetes is
actually a key value pairwithout any predefined meaning.It's like a two part, the part
before the colon and the partafter.The part before is the key.The part after is the value.So we can have the label here.If you look on the
circle, the red circlehere on the right hand
side of the slide,you can see that we have a
label here, env development.Think of it as the environment
is equal to the developmentenvironment.Now, if we want to look at the
labels on different things,we can, again, do the
Kubernetes command line.Note that we have
the command followedby the type of object followed
by flags, in this case.We don't have a particular
object we're looking at,but we're telling it
to show the labels.If we did that, we would be
able to see the label out here,the environment equals
development label hereon the actual object out there.We can also add a label to
an object with kubectl labelcommand.Kubectl label pods out there
would give us the abilitythen to apply a particular label
to it for environment equalsor owner equals
Michael in this case.And so then if we
look at labels again,we would see both
the previous label,environment equals development,
and owner equals Michael.Selectors are labels.We talk about labels
as an identifier.They're also used
as selectors, waysto pick and choose particular
object within Kubernetes.So you can see here I can do
things from the command linelike get the pods where
the particular label isone of these set.This is kind of
set selection here.Set base in a particular set.So most Kubernetes
objects supportset based selectors
choosing which itemsbased on a set to select from.What's an example of this?In Kubernetes it might be
you have a service whichhas this virtual IP
you can connect to,and it needs to
know the set of podsthat it has a choice to
connect to on the back end.The way it identifies
those sets of podsis by a label that
is used in there.I'll show you an example
of this later on.But basically, the pods
would have a label on them,and then using that
label, the servicecould select which pods it could
actually work with out there.For the option to search
these or look at theseor to use the label,
dash l is the short form,dash selector is the long form.So in the first example there
on the left hand screenshot,you can see we have kubectl get
pod dash l env and productiondevelopment, which
simply means give methe list of pods in
whatever namespaceI'm in here at the
default namespace to thatare either in the production
environment or developmentenvironment, env in
production or development.Meaning that the env key can
have one of those two values.Other commands can also use
label kubectl delete pods.I can delete the pods
associated with a particular oneby using a dash l out there.You can see over on the
top right hand side,I can select them based on a
particular label if I want,get the pods.So the labels provide a way
to select specific objectwithin Kubernetes
or to get objectsmatching a common attribute,
those kinds of things.Another example I've used before
when I worked with Kubernetesis, for example, we had
nodes in the clusterthat we needed to
have one node thathad more resources,
significantlymore resources on
it, to run moreintense things on it there.So we added a node and we
gave it a lot more disk space.And I had a label that
said size equals biggest.So I know which one we
could then use and specifythat size biggest when we
needed to run somethingthat needed more resources.So labels can apply
to other objectssuch as nodes and services.You can have equality based
selections out there, equal,not equal to, or set based
ones, as we've seen here.The kubectl command line syntax.The takeaway from
this is the syntaxcan be in a couple
of different forms.We're talking about
the objects here.We've already talked
about this ideaof having the command, the
type of object, the flagsand so on in there.What we want to
point out here isif you look on the
right hand side,kubectl get service
dash n roar2.What that is saying is list
out the Kubernetes servicesthat are in this
namespace dash n roar2.And you see that we have the
MySQL and the roar web onesout there.We can also just
do the get servicesout there to get a list of all
of them in the second examplethere.But then notice
on the bottom partof the layer, the last
example, kubectl get dash nroar2 grep service.If I am just saying get the
list of services out thereor get the list of objects,
get all the objects out there,the all means get
all the objects.Then if I was only
interested in services,I would have to grep
those or look for them.Notice here too that when
I say get all the objects,the name of the object in there,
that service MySQL service roarDB, the type precedes
the object name.So service is the type.MySQL is the object name.Service is the type.Roar web is the object name.So when we can talk about
these things in Kubernetes thatis to identify an
object, we're typicallytalking about a type of object,
a name of an object in there,and perhaps a namespace.But the name and type can
be specified separatelyor together in this kind of form
like you see in the last linehere.So we could do
kubectl dash n getall to get all of
the things in thereor kubectl get all dash n.Notice that we
could swap the flagsaround in different locations
here on the command line.We can use different forms
of the resource names.We can use single,
plural, or abbreviations.Objects in Kubernetes will
have these three variations.They'll have a singular
form such as pod,a plural form such as pods,
and abbreviations such as poout there.And abbreviations exist
for most of the resources.po for pod, svc
for service, deployfor deployment, et cetera.So a deployment can be thought
of as a supervisor for pods.That is, it's actually putting
the pod in place, creating it.Once it's deployed, you
can see the deploymentis a replica set and the pods.So if I have a
deployment, let's sayI have a deployment
that specifiesa certain type of pod.It specifies two replicas.You see on the right hand side
the Kubernetes specification,the API version, the kind,
the metadata in therewith the name in there,
and then the spec.Remember that for
a deployment spec,we specify both the replicas,
how many of these thingswe want, and
information about that.And then we also
specify within that specthe actual things we want to run
in our pod and our containers.Again, things like the image
out there, the containerport that are exposed,
environment variables,that kind of thing.So deployment can be thought
of as a supervisor for pods.Once it's deployed, you can
see the deployment, the replicaset, and the pods.So if I were to
apply this manifest,this YAML file, then
I could go and sayKubernetes get or list out the
deployments you know about.You could say, OK, I've
got this deployment.How many I've got?We're up to date.I can say get the
replica set or replicas.It'll say I've got two of those.And then it'll say get the
pods associated with that.Now, one other thing
to notice, if youlook on that little screenshot
here on the left hand side,notice the naming here.Our top level item, the
deployment, has sise deploy.Our replica set here
has sise deploy and thensome random number on it.And then our pods
have sise deploywith the random number that
is the replica set and thenanother identifier, a random
identifier after that.Why do we have these
numbers and why do wehave these random identifiers?Because objects within a
namespace in the Kubernetescannot have the same name.So they have to have different
names associated with them.So they'll have different
names associated with themto separate them out
within the namespace.So the names of the pods are
derived from replica sets.The names of the replica sets
are derived in deployment.So what happens if we
make a significant change?We update a version
number or some piecein there, for example,
within the deployment specand then apply it.Then what happens is Kubernetes
is smart enough to understand,oh, I've gotten a new
specification for this.I should get rid
of the old one thatis based on the
old specification,and I should go out
and then update it.So it will terminate the
ones that are currentlyrunning based on the
old specification,spin up new ones
that are runningbased on the new
specification out here.So this is kind of cool.This is an easy way to
do updates out there.So you can have a new version
of your pod or deploymentout there, service,
changes in specification,apply it in there.And if the change
is significant,then Kubernetes will go off,
get rid of the old ones,and actually go back
and spin up new onesbased on the new thing out
there, kind of like an update.This also, though,
demonstrates the ideathat the objects you
have in Kubernetesare designed to be stateless.What do we mean by that?We mean that if the
object can be killed off,if it crashes or has
a problem with it,it should be able to be stopped
and then have a new one startedand pick up where
the old one left off.That is, the object
has to be stateless,not depending on the
existence of the objectto maintain a state.It might be writing out
state information to disk.It might be such
that it doesn't--it can matter.But think of it as like
a database, for example.If I have a database running in
a pod or access to a database,I don't want to store my
database data in the poditself.Because if something happens
that pod and it goes away,I lose my data.So I might write the data
out to an external disk,a persistent volume.Then if something happens to my
pod that's running my databaseengine, Kubernetes can kill
it off, spin up a new one,and the new one
can be conditionedor know how to go out and read
the data and pick up again.So when you're working
in Kubernetes and absentof something like an operator
to do custom processing,you want the applications, the
things you're working with,to be stateless,
to not be dependenton the existence of
a particular instanceto be able to succeed
or fail but just to bekilled off and started
again if needed.Deployments versus pods.Let's talk a little bit
about what that means.So deployments versus pods here.Suppose that I as a user am
working with a pod out there.We know the objects
run within a nodein a namespace, a Kubernetes
node and namespace.We have a user going through
the Kubernetes commandline, the kubectl, and we
have a specification here,a specification for a pod.We're just doing a pod,
not a deployment here.Again, the pod is
specifying the thingsthat we need for our containers,
the ports we need to show on,some metadata.But notice again that same sort
of outline of the same sortof structure in this file.The API version, the
kind, the metadata, name,labels, and so on.If we take that
specification, apply itthrough the kubectl
command line,put it out into our namespace,
then Kubernetes helpfullygives us a pod based on that.It will have a port
that's showing up,and we can connect
directly to that podif we know how to connect
up to that IP addressto that pod on that port.The user can then
do work out there.If the user is
doing work out thereand the pod goes away,
then what happens?Well, then we have to go and
manually start a new one.There's no deployment to handle
this for us, nothing to do.We have to go and manually start
a new pod out there ourselves.So the pod goes away.We have to go and do an apply
again, start up a new pod,and then we have
that pod out there.The chances are we will
have a different IPaddress on the new one.So we will not be
able to connect to it.If we have a
deployment, deploymenthas to include both a
replica set in a pod.The replica set or
replicas here meanthat a certain number of these
must be kept running and activeat all times.Let's see how that
would look differently.I have, again, my namespace
within my Kubernetes cluster.I take this specification.I apply it.Create a deployment.The deployment goes
out there and saysI have to have one
of these runningall the time, one replica.So it starts up a
pod running for methat I can connect
to there and access.What happens if
the pod goes away?It has a problem
in the containerout there, for example.If the pod goes away, then
my connection is broken.But the replica set
within the deploymentwill automatically
start up a new one,because it says you're
supposed to have one of theseall the time and now
I see you have 0.So let me make
sure I give you oneto be consistent with the
specification you put in there.So it gets us another
pod up and runningexposing the port out there,
and I can connect to it.Now note, again, that it
wouldn't be the same IPaddress in all
likelihood, so I stillwould have to figure that out.But I did not have to go through
the manual process of creatinga new pod myself.I was able to have the
deployment take care of thatbecause the
deployment understoodthat I always wanted at
least one of these runningat all times.Services, as we've
mentioned before,services Kubernetes provide a
level of instruction for pods.Pods can be volatile.We just talked about
the idea somethingmight cause a pod to go away.And by the way,
in Kubernetes, itdoesn't have to be that
a pod container crashes.It could be, for example, that
a container, an applicationrunning container, has a problem
where it starts chewing upall the memory on the system.And it starts
consuming-- it has a bug,and it starts consuming
all the memory.Kubernetes will
do whatever it canto try to protect
the system overalland to try to ensure
that as much stuff cankeep running as possible.So Kubernetes can
actually go outthere, detect that that system
is trying to use that pod,is trying to use too
much system memory,the system is in danger,
and it can kill off the pod.There's a certain
algorithm it usesto try to figure out
which wants to kill off.But your pod could be
killed off by that.So we don't want to depend
on pods always being around.Again, this idea that
applications should bestateless.They should be able to be
stopped, go away, start upanother instance, and
pick up where it left off.So service in
Kubernetes providesa level of abstractions for pods
because pods can be volatile.You can't rely on their
respective IP addressesto connect to.So the service provides
a virtual consistent IPto connect to for a set of pods.Pods can be selected by labels.So think again of a service as
being this kind of front endabstraction that provides this
not real IP but a virtual IPand then on the back end
decides which pod to talk to.That way if something goes
down, I'm not affected by it.I'm just still talking
to the service.It exists to forward
traffic to one or more pods.There's a kube-proxy
process runningon the nodes that
helps it understandwhich pods it can talk to,
the service can talk to.There are kube-proxy
queries in the API serverto learn about new services.So lets look at
an example of thatand how that might
look in our diagram.Suppose that we have our setup.We have a deployment
which has two pods in itbecause the replicas are two.We always have to
have two running.As a user, I can connect to
individual pods if I'm allowed.I could find out the
IP address of that,and I have permissions.I can go and connect
to one of thoseand talk to that one
over that IP address.I can do my work with it.If the deployment and
the pod goes away,we lose access to that pod.But we know the replica
set will detect thator the Kubernetes
will detect that.And because we've told
it we need to have two,it will start up another one
to ensure we have two running.Now again, that other one will
not have the same IP addressout there as the previous one.So if I go to try to connect to
that with the same IP addressas I was previously
using, I won't succeed.I'll have a problem with that.So I won't be able
to do my work.I can discover, go
out and investigate,and find the IP
address that I need.I can even connect
up to the other oneand still could
do work that way.But if a pod becomes
unusable, if itgets into a state where
it's not responding,for example, it may
not have crashed,but it may not be
responding eitheror it may be stuck in
some kind of a situationwhere it goes away
and is using up memorythose kinds of things out there.Then if it becomes unusable,
then the Kubernetescan kill it off to
protect the system.And so you can have other pods
that could handle the needs,but there's no way to
automatically discoveror connect.You still have to
manually do it.So let's say we have
the same situation,but we put a service
in front of it.And here's a specification
for the service.You'll note, again,
the same kindof structure the API version,
the kind, the metadata,the name, the labels on
there to allow us to selectit, the namespace, and so on.Then notice we have a node
port service here for the type.The node port in
there means that Ihave a port open on
the Kubernetes nodethat I can get to
things through.We'll talk in a moment about
the different kinds of servicesthat are available to use.But no port means they have
a service on there to use.I also have the port and
target port out there as well,which referring to the
port in the service I'mtalking to and the port
in the container that I'mtalking to as well.So notice also here that I have
a selector out there for web.So the service provides a
virtual address to connect to.And then it talks
on the back endto the particular [INAUDIBLE].You can choose which one
on the back end to talk to.The endpoint, we talk
about the idea of endpointsin Kubernetes, endpoints
simply mean the IP addressesof the set of things
I can connect to,the servers I can connect to.So in this case,
the endpoints, itfigures out which
services, I'm sorry,which pods it could connect
to based off of the selector.Note if you look closely
in the diagram here,both of the pods wrapping
around the containershave this label attached to them
that says name equals roar web.Then the selector or the
label that we're looking foris name equal roar web.And if we pull the IP
addresses or get the IP addressfor both of those, those become
our endpoint for the service.The service knows it can
talk to any of the endpointsthere that have that selector
out there that match up there.So for our point, we have
the IP address is out there.If we then want to
talk to the service,we have to get to the node
first through the node port.Then we have to
get to the servicesport and then the target port on
the actual application itself.You think about the idea of
pods running an application.The application of the container
may expose a port there.And the containers
running in these podscan expose the same port.You just pick the one you
want to talk to out there.That allows us to distribute
the load, for example,coming in to a set of pods.So if we're doing
a node IP, thereis an open port on
the node itself.Typically it's in
the range of I thinkit's around the 30,000,
31,000 in there.And basically when
we talk to that,you can see the green arrow is
here, we talked to the node IP.We can then go through that
based on the specificationto get to the 8089 port, which
is the port of the service,and then we can get through
that to talk to an open porton one of the pods, whichever
one the service routes usto behind the scenes.So the cool thing
about this now,if one pod goes down
and becomes unavailable,the service can then connect up
to the other pod on our behalf.Note that I'm not affected.I'm still talking to
the node out there.So I don't care that the
pod has actually gone down.I can still go through
and then the servicewill route me to another
pod on the back endeven while the replica
as part of deploymentsays you've got to have two.You're down to one.I'll spin up another one for
you, make it available for you,and then both of them
are available again.But note that I didn't have
to set up another pod myself.I didn't have to go
investigate whichIP address I could talk to now
of another service out there.I simply could go
through the service,and the service
was up all the timebecause it's just a
virtual IP address.And on the back
end, the service wasable to select a
pool of pods it couldtalk to based on the selector.And then I'll get back to two.The same condition if the
service becomes unavailable.I'm sorry, if the pod
becomes unavailable.Same situation.The service can switch on
the back end to another podand keep me up and running.Types of services.There are several types
of services we couldwork with within Kubernetes.There is called the
cluster IP service.This is basically
the default service.It's intended mostly for use
inside the cluster for appsto talk to each other
inside the cluster.There's no external
access reallyexcept via kubectl proxy.You can see there
inside the clusterwe have that internal traffic
that goes through the serviceport that then goes through
on the back end to the pods.There is a node port.We just talked about that one.The idea of having a port
open on the Kubernetesnode itself that we
can get in and talk to.Then traffic gets sent to
the actual service portand then to the
ports on the pods.This will use the node
IP address to connect to.And then the ports
will be in the range.The node port itself be in
the range 30,000 32,767.The load balancing
service is whatyou would typically
find if you'reusing a cloud environment.Puts a load balancer object in
front of that with a single IPthat forwards all
traffic to the service.Default method for directly
exposing a service out thereon a cloud environment or
such, no filtering, no routing.But there's a cost
factor associated with iton the clouds.Finally, there is also
an external name service.I didn't illustrate that here.But it maps the service to the
contents of an external namefield, like foo.bar.comm.Gives you a CNAME
record with that valueto work with for there.So going back to our data
center analogy for Kubernetes,we can think of the Docker
containers as being similarto the computers in a rack .Think of the containers
and the volumesout there as being similar--sorry, think of the
containers themselvesas being similar to
a computer in a rack.I think that's what I said.Think of a set of containers
out there in a pod,a pod being similar
to a rack of machinesout there having
multiple machines in itand then replicas as being
kind of like multiple racksand a service as being a central
control for a log in serviceto get to the back end
to a set of things.And then on the namespace,
similar to a server room,containing all of these
different types of objects.Let's talk a moment about some
other Kubernetes objects here.We've finished up the basics.Config maps and secrets.So we have a couple
of approaches.Any object that we
work with is goingto need some kind
of configuration.It might be simply environment
variables or some optionor whatever.We need some kind
of configuration.And it's an alternative for
hard coding data or usingenvironment variables.We can within the pod spec
set environment variables,set hard coded values in there.We could do that.We could also inside of
a Dockerfile or insideof the Kubernetes YAML file.But the bad part
of that, then, isthat that's going
to be hard codedtied to a particular
container or deployment.And if we need to
change them, we'dhave to go back and
modify the specificationfor that container and
redo the deployment.So the data also has to be
duplicated across objects.So a better way to
manage this, then,is to separate out those
settings, those configurationvalues, into what's
called a config map.A config map is a
Kubernetes object, then,that contains a configuration
value and setting.There are multiple of them.And then you separate that out
from the pods from deploymentsand so on.And then you make that
available for them to use.So they can read information
out of that config map.Multiple objects can use that
and get that information.You don't have to
duplicate it across objectsin terms of specifying
the specification.You don't have to
redo the deploymentto actually update it.You can just change
the config map itself.A secret in Kubernetes
is a lot like that.It's just a matter
of an object that'sdesigned to obfuscate or
hide the information that'ssensitive information like
passwords or API keys or so on.Unfortunately, with the
secrets, the defaultis really just a
base 64 encoding.So it doesn't do that much.It hides it from plain
text, but it's very easyto crack or reverse engineer.If you really want to be
secure with secrets and stuffin Kubernetes, you
would really wantto use something
like Vault. Vault isan application of
HashiCorp that is actuallyuseful for storing this
kind of informationand managing it for you.So secrets and config
maps are similar,except that secrets have this
base 64 encoding by defaultto obfuscate or hide the
information so it's notin plain text.Let's take a look if we
want to actually createexamples of these.Suppose we had a spec
on the right hand side.We have a container
called environment test.And note that it has a API
key in it in a language.If I wanted to go the config
map route and the secret routeon this, I would break
that information out.Instead of having it
hard coded in here,I would actually create
a secret, for example,with the API key.And you can see we're using the
kubectl create command thereto create this object that's
of secret type, the typeof object, an API key, and from
literal, the information wewant to store in there.And then we could
also specify itin a config map
for the language.The same idea.Create a config map that
has this information storedin there.So in this way, we can specify
it through config maps.Once we actually update
these or create these,then we would need to update
the spec to use those.So we changed it from being
hard coded in our spec,but now we need to make sure
we change the thing thatwas using these or
needed these valuesto reference the
object we created.So note that from the top to
the bottom there on the bottom,we changed it to
have an environmentvariable, the environment
variable for languageto get the value from
a config map key ref.In other words, the
config map we created outthere, the name of
that config map,and the value we're looking
for in it or the key.Likewise, the API
key, we changedthat to get the value from this
secret out there called API keyand to look for the value
in it called API key.So we take the hard coded
values, the config maps,the secrets, those
kinds of things,break them out into their
own object in Kubernetes,and then change the objects
to other objects that needthem to use the new objects.That's the basic workflow.Persistence, volumes and claims.Persistence has to
do with storing dataabout information on disk.The trick here is
that if we were justto store things in the
container itself, thenthat's considered ephemeral.Meaning if the container goes
away, we lose the information.Think about it
from the standpointif you had a container
that was a laptopor a standalone computer.If I destroy that
computer and that'sthe only place I
have the data, Ihave lost the data
associated with that.Same idea with the container.We've already talked a lot about
containers within Kubernetesbeing thought of as stateless
and being they can go away.They can be killed off.Then the new one started up.So we have that container and
we don't want to store themwith the container.When we have the
container going away,then we lose the information.So another approach here is to
store it as a separate volume.That is a volume defined
within the port level storagethere, which is great.If the container
goes away, we stillhave the information
stored within the pod.But if the pod
goes away, then welose the information in there.So if a deletion
of the pod, it'sgoing to delete the volume.So we really want to store the
information somewhere outsideof the pod, outside
of the container.And in Kubernetes,
we have an objectfor that called a
persistent volume.It means that it has
persistent storage independentof the lifecycle of any pod.And to get that storage,
then you actuallyissue a persistent volume claim.A persistent volume
claim is a wayin Kubernetes of
requesting a set of storageor a kind of storage out
there getting a claim to it.It defines the specific
amount of storage requested.It defines specific
access modes.And what Kubernetes
does is it looksto find a persistent
matching persistent volumeand match it to the claim.So over on the right
hand side, if youlook at the print outs
or the listings there,you can see that we have a
persistent volume claim whichsays that I want to have read
access, read write access,read write once access.I need something that
is a storage of 128meg, a certain storage
class out there that I need,and I need the name of it to
match up with this redis stuffout there.If you look at the-- we
have a deployment thenwe're specifying, note that
down further in the deployment,over on the right hand side
we have in our containerspec volume mounts.How do we get to
the data volumesa place to look for the data
and a persistent volume claimout there that's
read as PC, readas PVC, which says this is the
persistent volume claim objectI want you to look for
and give me storagethat matches what it asked for.So this is the way that
objects in Kubernetescan ask for different
kinds of storage.You have a persistent
volume, whichcan be a particular type
of storage available to youon whatever environment you
have an object for that and thena persistent volume claim,
which means that an object canput in a claim to request that
sort of storage out there.PVCs have a different
access mode.Read write once, one bound pod
with read write access, manybound pods with read only
access, read only many,read write many, or many
pods with read write access.So again, persistent volumes
are resources in the cluster.They are the physical
volumes of the host machinethat store your persistent
data, the actual storage you'reusing.The PVCs are requests for these
or they act like claim checksto get to that, the
resource you want.It's a way of
telling the platform,telling Kubernetes what
kind of storage you wantand what kind you need as
defined in persistent volumes.Persistent volumes get
attached to your podvia the persistent
volume claims.So you can think
of the chain hereas being that a pod has a
request, a PVC, PersistentVolume Claim, to get a certain
type of persistent volumes.Persistent volumes, then, are
defined by storage classesout there and exist on
some host system out there.Storage classes help us
define the type of PVCsthat a user can request.So when we talk about
storage classes,then storage classes will
have a required provisionervia the volume plug-ins or via
volume plug-ins in the system.You can see here the
kinds of volume plug-inswe're talking about.And you can tell from the names
and such the kinds of storagethat they're tied to.For example, Azure files for
Azure, stuff in the Azurecloud, AWS as for the
Amazon Web Services,out there, elastic
pieces out there.You can also see the iSCSI, some
of the more common ones there.NFS, local, and so on there.StorageClasses provide
a way for adminsto describe classes of storage.They can provide and also
describe the characteristicsfor those different
kinds of storage.Things like quality of service
level, backup policies,other custom policies.The StorageClass is also
going to contain informationabout the provisioner, any
kind of parameters associatedwith it, the reclaim policy,
those kinds of things.So this will all be used in
a persistent volume and classneeds to be dynamically
provisioned.Admins can specify
default storage classes sothat if you request
a persistent claimthat you would get a
certain default out thereif you don't override that.You can see on the
right hand side thereit's an example of
the StorageClassand different
parameters that it has.Let's next talk about
resource requests and limits.Request and limits
are mechanismsKubernetes uses to control
resources like memory and CPU.I usually tell people to
think of request as minimumand limits as the maximums.Requests are what the
container is guaranteed to get.It's what we are
saying we have to haveto be able to run successfully
within the system.Kubernetes is going to be
smart and only schedulecontainers on a node that
it can meet the request.Actually scheduling
pods we should say.So think of it this way.If we are saying we need a
certain amount of storageor a certain amount of--sorry, a certain amount of
memory or certain amountof CPU, then for all of
the things that Kubernetesis looking at
trying to schedule,the sum of all of
those requests stillneeds to be less than or
equal to the resourcesavailable on the node where
it's trying to schedule.Limits defined
upper bounds for us.Again, why we say think
about limits as a max.Container is only allowed
to go up to the limit.If it hits the limits, it's
going to be dealt with.And we'll talk about what
that means in a moment, howthe system reacts to
actually things thatare going over the
limit or threatento consume all the
system resources.Specifying the resource request
and limits is a best practice,because it allows the
Kubernetes schedule to makebetter decisions about
where to put pods.So you can have memory limits
in there measured in bytes.These can be expressed as either
bytes, fixed point integers,or this mebibytes, which is
a power of two equivalent.CPU limits can be
measured as units.Usually it's a CPU core.And we can talk about
them in terms of thingslike millicores
can be fractional.As in a pod with a CPU limit 2.5
is allowed two times the powerof one unlimited 0.25.In our example, we
have 100 millicores.So 100 out of 1,000 there.So 1/10 of a CPU core
out there to work with.So again, a mebibyte
is just a multipleof byte expressed
as a power of two.So how pods with resource
requests are scheduled.When you create a pod,
the Kubernetes scheduleis going to select a
node for it to run on.Remember, the
scheduler had that partof the Kubernetes of
the cluster master.We talked about having
a schedule on itto decide how to schedule
things on the nodes.Each node has a maximum capacity
for resources, memory, and CPU.And the scheduler is
responsible for ensuringthat for each resource type,
the sum total of all the thingsit's trying to put on there
and the pods, containersbeing scheduled is less
than capacity of the node.So it's making sure,
again, that everythingfits, that it does this
intelligent scheduling whereit doesn't overburden
or overrun the resourcesavailable on the node.The scheduler
sticks the requestsand won't schedule if
requested more than the node.So even if the actual
resource usage is low,if the request
has been specifiedacross all the things you're
trying to be scheduled on thereis more than the node
can tolerate or morethan the node has, things
won't get scheduled.So you can sometimes
end up in a casewhere there's not enough
resources available on any nodeto actually schedule the
particular item out there.That's why it's important to try
to be as clear and as accurateas you can with the request
when you're putting theminto the pods for the systems.The request being, again, the
minimum you need to work with.So what happens when we
do pass resource limits?We have actually started
to overrun these resourcesthat we have available
on a particular node.So if a container pod
with a container in itexceeds its memory limit,
it may be terminated.If that container
is restartable,Kubernetes will
restart it for you.This is, again,
alluding to that ideathat anything that you're
running within the Kubernetessystem should ideally
be restartable.It will pick up
where we left off,even if it gets killed off.This is another example.Think of Kubernetes,
again, as that data centermodel that we talked about.One of the jobs of
the data center modelis to make sure that
things can get--that we have enough
system resources out thereto run our stuff.And if something
gets out of hand,starts to eat too
much resources,we may remove that system
in terms of the data centerand replace it with
another one or even justkick it and restart
it and reboot it.So if a container exceeds
its memory requested,the pod will likely
be evicted whenthe node runs out of memory.So the badly behaved
pod is punished.So that means that if it's
exceeding the memory requested,it can be evicted by
Kubernetes, kicked outso that it's no longer
running in the system.Container may or may not be
allowed to exceed its CPU usagefor extended periods of time.The system is usually less
strict about if a containeror a pod with containers
is exceeding the CPUusage out there, because it
understands that sometimes thatdoes spike up or not.So containers don't get killed
off for excessive CPU usage.The kubelet, the
primary node agentthat we talked
about, that kubeletthat runs as an agent
on each of the workernodes out there handles
these kind of issueswith error conditions
since it's on the node.The kubelet's main job,
or one of its main jobs,is to preserve stability
when the node resources getlow to make sure that it is
keeping the node healthy,to make sure that
the node doesn'tget overloaded with
different thingsconsuming too much resources.So the kubelet the ability to
evict one or more of the podsto reclaim system
resources if things startgetting too intense in terms of
consuming the memory, the CPU,those sorts of things.When the pod is
failed by the kubelet,the terminate terminates
all the containersrunning inside the pod.And kubelet makes the eviction
decisions, though, based offof signals it gets about the
resource and certain evictionthresholds.So node conditions indicate
the state of the resources.You can get these conditions
reported sometimes and reportedin the output about certain
pods or in there about pods.If they're exceeding
disks, they mayhave if the node is
running low on disk,you may get the
disk pressure alert.If it's exceeding the
memory requirements,you may have the memory
pressure warning.So then alerts on these can be
triggered by eviction signals.If pressure conditions
are triggered,it tries to discourage the
scheduler from schedulingmore pods on the node.Basically, the idea
is if it's alreadydetecting that
it's overloaded, itdoesn't want to schedule
additional things runningon the node to
load it down more.The OOM killer, this
is an interesting termyou may hear at some point
working with Kubernetes.OOM is O-O-M, Out Of Memory.It's a special
handler to help dealwith the system
out of memory eventthat happens before the
kubelet can reclaim memory.Basically, what
happens here is kubeletsets an OOM score
for each containerbased on the quality
of service for the pod.If the OOM gets
called into action,it calculates its own score
based on the pod's scoreand how much memory
the pod is using.The container with the highest
score then as calculatedthrough this process
gets killed off.The intent here is that
containers with the lowestquality of service
using the highest memoryshould be killed first
to reclaim the memory.What that means is
basically if it is nota very important
container, if it doesn'thave to be up all the
time, if it is notimportant or significant
in terms of staying upall the time, having a
high quality of service,it starts using
the highest memory.The idea is the
system will kill offthe lowest important containers
that are chewing up the mostmemory in an attempt to provide
more memory for the ones thatdo need to be a higher
quality of service and do needor are more important.Health checks.In addition to the
health of the system,Kubernetes is concerned
with the health of the pods.So ideally, pods should have
health checks built into themas a best practice.We have both a
readiness probe, whichallows a kubelet to assess if
a service is ready to startaccepting traffic.In order for a pod to
be considered ready,all of its pods are ready.So can this stuff accept
traffic coming into it?Is it ready to respond
to it and receive it?The liveness probe allows
kubelet to monitor and assesswhen to restart a container.Think of it almost
like we might callit a heartbeat or something.It's a liveness probe that we
can at least be checking nodethat our pod is
responding to things,responding to these signals
we're sending to it,and that is still alive.And if we don't
get that livenessin probe, that liveness probe
isn't getting out there,then the kubelet will know
that it probably needsto go out and restart this.Three kinds of health
checks can be includedin either of these probes.HTTP health checks.The kubelet will
call a web hook.If it returns between 200 and
399, it's considered a success,otherwise it's a failure.Container exec.The kubelet will execute a
command inside your container.If it exits with status 0,
it's considered a success.And there's also TCP sockets.The kubelet will attempt to
open a socket to your container.If you can establish
a connection,the container is
considered healthy.If it can't, it's
considered a failure.And you can see over
there on the right handside for this pod, we're
specifying a liveness probethere for the container,
which is the TCP socketone on the port on there.An initial delay to give
it time to actually spin upand then a time out to say if
it doesn't respond this time,we need to try again.All right, let's
talk next about Helm.So what is Helm?Helm is a orchestration
tool for Kubernetes.That's one way to
think about it.Kind of like a yum or apt but
for Kubernetes package managersort of thing.Bundles related manifests, such
as deployment YAMLs, serviceYAMLs, et cetera into a chart.These kinds of things that we've
talked about for Kubernetesin the course.That is, deployment services,
all those different kindsof Kubernetes objects
that we can put togetherinto a single chart where
they're grouped togetherto apply as a unit.When installing the chart,
Helm creates a release.So we take a chart with
all these different kindsof YAML files manifest in there.We deploy it out
there and then createsa release running in Helm.It has lifecycle
management that itcan do as well to
create, install, upgrade,rollback, delete, give
status, do versioning,those kinds of things in there.The benefits it also provides
are templating, repeatability,reliability,
multiple environment,ease of collaboration.So we'll talk about
some of these thingsas we go along in the course.But the basic idea of why
we need something like this.If you think about what
we've looked at so far,we have the ideas that
we have different filesto specify the services
we want to have,different files to specify the
deployments, different filesto specify whatever it might
be, ingresses or something,secrets, config maps.All of those YAML files, those
can be difficult to edit,lots of duplication in them,
and can be hard to overwrite.And so especially as we get
more and more pods out therewithin our system,
more and more services,more and more
deployments, it canbe tricky to manage all
of those and be ableto keep up with all the changes
and what you need to putin the specific YAML files.So instead, what you can
use, you can use Helm.Helm helps to simplify this
kind of management for us.The Helm components,
and I shouldmention here we're talking
about what was active in Helm 2.Helm 3 is out as of now.Overall, the functionality
is still basically the same,but the implementation
is different.And we'll talk about there's
no longer any Tiller.We'll talk about why there's
not a Tiller here in a moment.But the Helm client is
a command line client.And in version two of
that, it interactedwith a tiller component
or server thatwas inside the
cluster and allowsus to do local chart
development work with that.The Tiller server
was in the cluster,listened to the Helm client,
interacted with the KubernetesAPI, and managed the lifecycle.So it basically took the place
of if you had the command lineclient from Kubernetes
talking to things in there.Tiller is the server
piece for the Helm client.Tiller sits inside
of the clusterto act against
the Kubernetes APIto make the changes
specified in the chart there.Helm charts are a
collection of Kubernetesresource definitions files
inside the directory.At the top level it's
just the directory,but you can see on the
right hand side hereit has several different
components in here.Can deploy simple and
complex applications.Most files in a chart
can be templated.What we mean by that is
that it has a placeholderto substitute in actual values.It's a template.You can actually substitute
in different values.That allows us to do
things like reuse.Reuse one of these templated
deployment files or servicefiles and fill in
the values that wewant for that and to have
everything else to be the same.It uses a variation
of the Go template.If you know Go, the programming
language or know of it,there is a thing called Sprig.But it uses a variation of
the Go templates in Helm.The values to instantiate
templates come from this filecalled value YAMLs.You'll see values YAMLs
over on the right hand sideas being one of the
top level files.And then underneath there,
we have a set of templates.Those things should
look more familiar,things like deployment.yaml,
ingress.yaml, service.yaml,and so on.So those are the
actual specificationsfor the Kubernetes these
objects that we want to create,but they'll be templated
with placeholders where wewant to put in actual values.And the actual values
with the help of Helmcan come from the
values.yaml fileor from the Helm
command line override.There's also a
helpers.tpl file, whichis a helper template file that
can define different functionsbased on actually kind of
programming that we can do,the sort of programming we
can do for different functionsto return values to be
put into the YAML files.And you can have a
directories of chartto directories out there.So a chart is a package, a
bundle of Kubernetes resources.A release is a chart
instance loaded into Helm.Same chart could be
installed several times.Each chart will have
its own release.And a repository is a
repository of published charts.Template is simply a
Kubernetes configuration fileset up as a template.Helm operations.A set of operations we can do.Helm init, which simply goes
to install Tiller as a runningcluster and sets up any
necessary local configuration.Helm create allows us
to create a new chart.Helm delete is basically
given a release name.Member, the release
is a running instanceof something deployed
in the cluster by Helm.Delete the release
from Kubernetes.There's a dash dash
purge option to removethe release completely.There is a Helm install
to install a chart.Helm lint to make sure to
check a chart for issues.Helm list lists a set of
releases of the chartsthat are installed in a cluster.Helm rollback to roll back
a released previous version.Helm status, checking
on the status.Helm template to
render the template.In other words,
plug in the valuesand dump out what that
template looks like with valuesplugged in there but don't
actually apply it to cluster.And Helm upgrade,
upgrade a release.So all these kind of
different options and commandsaround dealing with
charts, making sure thingslook good, Ahh releases
in the cluster.Again, once you have a chart,
specifies all the piecesyou need for the full set of
objects for something runningon Kubernetes.And then when you're
running it on the chartthere, when you put the
chart into the cluster,then you have a release.And then at that point,
you can list the releases,roll back the releases,
do upgrades, and so on.But again, you're able to manage
that set of things runningin the cluster as a unit,
as a release using Helm,just as you're able to manage
the set of manifest filesfor all different
things using a chart.Helm charts are based
off of that they'reallowed to have
templates in themor templating based off
to Go templating language,as we've mentioned.Template directives look
for values to insert them.The template directives look
like the opening and closingbraces there.A couple of them, two
on each side there.The values passed in and can be
thought of as a namespace wherea dot represents each element.Take a look at the right
hand side of this chartto get a better idea for it.So here we have a values file.We'll pretend that it has
a YAML format the value,a key value pair.Favorite drink is coffee.Then we have our
template file there.And if you look closely
at our template file,it's a config map.You'll see things that
look very similar, justlike a regular
Kubernetes object.We have the API version,
the v1, the kind config map,some metadata about it, some
data values to go into it.But note that we also have these
areas, these sections enclosedin this opening closing, these
pairs of opening and closingbrackets.And that we have
these dot releasedot name dot values
dot favorite drink.So the opening closing
pairs of bracketsthere simply mean this is a
place where values can getplugged in.This is the template here.This template file has places
to substitute in actual values.Some values will be computed
for you automatically.For example, the
release name willbe defined based on what
you have in the chart.So, go .Release.Name.Think of the dot as separating
it from a section point of viewor a level or a hierarchy.So the top dot there,
starting at the highest,we go into the release
section, release level.And then we get the
name value out of that.When we talk about the drink one
there in the second screenshotthere on that page,
you see drink.You see dot values
dot favorite drink.So the dot values
here representscoming from the values
file or getting passed in.And then the dot
favorite drink is the keywe're looking for in there.So if I do this thing of
Helm install down belowon the third
screenshot here, Helminstall dash dash dry run
dash dash debug my chart.The dry run and debug are
basically just saying,don't actually apply
this in the cluster.Just show me what
it'll look like.You can see that towards
the bottom there,it actually took the template
file, plugged in the valuesthat we had there,
plugged in the releasename, which again, was generated
for us based on the chart.And then for the data value
there, the drink in therewas substituted in with the
value from the values file.That dot values
favorite drink meansgo out to the values file,
find the favorite drink keyand substitute an
associated value with it.In this case, coffee.So the leading dots
and the dots in betweenindicate a hierarchy as well.Again, certain
items are built in.You can do dry runs to
see rendered versionsof the templates, what they
look like with values pluggedinto them.So Helm charts.Again, one of the
key values hereor one of the key
things it provideis being able to
override values easily.So it's not only being able to
plug in values from a valuesYAML file but also being
able to override iton the command line.For example, if you need it to
run through multiple charts,install multiple things,
you could differentiate themjust by supplying different
overrides on the commandline or different
value YAML fileand then you deal
with multiple onesmore easily without having
to duplicate or hardcode all the things in there
as you would without Helm.So overrides can come from
child charts, additional valuesfiles, or command line values.If you use the dash
dash set optionon the command
line, that's goingto have a higher precedence
than the values YAML file.So that's how you can overwrite
things in the command line,because it takes a
precedent over what wehave in the values YAML file.The values can also be
structured in there.It means the dot notation
indicate the hierarchy.Lower scoping requires
more levels in there.As you get down further
and further and further,you add more dots to
it to separate it.So we talk about Helm charts
on the right hand side there.Note that we're installing,
have a dry run, debug.Again, this is just the
try it without actuallyapplying the cluster.But note the one there
that says dash dashset favorite drink equals slurm.And then we actually then plug--when we run that, instead of
getting the thing in our valuesfile, we will actually get
the substitution of whatwe supplied on the command line
with that dash dash set option.Could also use functions and
pipelines in our templatesin Helm, meaning that we can
define the values or the valuesfile.But we also have ways
with the functionsto transform data in a way to
make them more useful to dothings like repeating
them multiple times,putting them in quotes,
even adding numbersto initial starts
and other numbersthere, that sort of thing.Lots of different functions
that we can actually leveragefrom the Sprig libraries.So functions
transform data valuesin a way to make
them more useful.For example, if we
have a quote function,note that we could quote
the thing that we'llbe substituting in there.We call the quote function
and pass an argument.Helm has over 60
built in functionsfrom the Go template language
or the Spring templates.And pipelines, in
this case, we'renot talking about delivery
pipelines or deploymentpipelines, any of
those kinds of things.We're talking about
chaining functions together.So if you look on the example
on the right hand side,we have our values file
with the favorite in therewith the keys favorite
section with the key of drink.The favorite drink's coffee.The favorite food is pizza.Our templates file
says that we aregoing to plug in for the key
drink, the value we get outof our values file.So we have it as
values favorite drink.Again, looking at that
hierarchy in the values file.Values on the front dot values
means going to the the valuesdefinition.Top favorite from the
favorite section thereand in getting the value
associated with the key drink.Then notice after
that, we pipe thatthrough function to
repeat five timesand then to quote it for us.On the food part, we actually
do something similar,except we pipe it through
an uppercasing function,and then we quote it after that.So we actually
rendered this, then,if we actually ran through
this, did a dry runor actually ran it
through and applied itwith the installing chart there.The values file would
get or the template filewould get rendered as
my value hello world,the thing we already had up
there literal, plus the drink.Notice coffee, coffee,
coffee, coffee, coffeerepeating it five times.And then food is pizza
all in uppercase.You can also do nice things
like you can actuallyhave functions that would add
an offset to a starting value.I've used that in
the past when Iwas trying to work with multiple
items in containers in there.And in particular, they
wanted to deploy Kubernetes.But I wanted them to
each have different portsthey started with or different
ports they were talking to.And I wanted the
initial starting portto be relative so
that I can installmultiple instance
of these thingsand I'll just had to
specify a starting port.In each of the
items inside, we'llfigure out a starting port based
on their particular offset.So I was able to pass in a
starting port via the commandline and have a Helm function
in there that would actuallyadd a preset offset for each
type of object in there.That way I could actually
install multiple chartsfrom the command
line and specifya different starting
port for each oneand be assured that
then within that,they would add the
appropriate offset,and I wouldn't
have port conflictsif we were using the same port.I didn't have to go through
and hard code anything either,because I could use
Helm to actually computethat for me for each one.Now, one of the things about
Helm 2, one of the reasonsprimarily why Helm
3 exists these daysor is as important as it
is is because of Tilleris considered evil.What I mean by that is the
Tiller piece of the Helmcommand line versus
in Tiller there,Tiller the server piece that
lives inside the Kubernetescluster.When it was first put out
there, when they firststarted using Helm, Kubernetes
was not secured in the waysthat it is now.And we'll talk about the
RBAC, Role Based AccessControl permissions, a
little later on in class.So Tiller is out there listening
to the Helm client interactwith Kubernetes API.But the problem is that the
way Tiller has been set up,it has too many permissions.Too much access is required.Too much access is
a security risk.It's hard to secure because it
was done before this whole RBACthing.So it's been removed
in Helm 3 because it'scaused quite a bit of angst
over being in the clusterand having that kind of access.In Helm 3, they removed it.However, in Helm 2, there's
a simple work around.You can simply do a
Helm template commandto render the different template
files filled in with the valuesthat you supply
in the values YAMLor supply from the command line.And then just pipe that
through a kubectl apply.So basically, you're just
dumping out the completedYAML files with the
substituted values inand then running it
through a kubectl apply.In that way, you don't
need Tiller in there.Now, if you don't use Tiller,
though, in that sense,with Helm 2, then you
do lose the abilityto do things like the upgrades,
like the rollbacks, allthose kinds of things,
because you're not reallydealing with releases
in the same waythat you would be if Tiller
were in the mix there.So Helm charts and sub-charts.The most basic way
of using Helm isto have a single chart
for a single application.This single chart
contains all the resourcesneeded for an application.You can also have a
chart with dependencieson other charts,
an umbrella chart,as you can see on the
right hand slide there.Other charts can be external
and could leverage requirementsYAML file.You can also get away
with a simple hierarchyif it's all local,
meaning if you justhave one servers,
one deployment,those kinds of simple cases.You can just put them
in a directory structurethat will be understood
and just use it that way.Helm versioning.Helm charts have
their own versionwhich is separate from
the application version.So this is contained in the Helm
metadata, the kind of top levelarea where we define charts.Has an app version, a
version of the application.The chart has a version
of the chart itself.So in Helm, we talk
about the versionsof the Helm objects, the
Helm charts, and thenthe versions of the
things they're deployingwith the Kubernetes pieces.So the simplest
versioning scheme,we were trying to figure out
a versioning scheme there,is just to keep the
chart version in syncwith the application.That is, every time you make a
new version of the applicationschart, you update the chart
version out there as welland just don't use
the app version.The advanced
versioning scheme isto use if change is happening
in charts all the time,you want to track several
within the application,you can change the
different pieces of it,do independent Helm versioning,
as shown in the right handside.Helm packages and repository.Helm charts can be worked with
in two forms, either a packageform, a version chart archive as
a TGZ file, or a tree of files.If you just have
a tree of files,you can check them into a
Git repository, for example.You don't have to use
a Helm repository.You can use it if you want.It's geared towards Helm to
store package repositories.Works with the Helm repo
or Helm repository command.And you get an index
file, the metadata filefor the chart package that's
generated through this Helmrepo index.Helm repo adds a store
of the chart in it.Helm fetch to get
the chart down.And there's also an open source
one called chart museum thatcan help you host Helm charts.So this is all
about the commandsto get things stored
in a repository.Think of it like a project
for a get, for example.You take the commit for
Git and put it out there.And you do a get fetch or get
pull or those kinds of thingsto get code down.Same kind of ideas.You're managing your Helm charts
within a repository there.So quick couple of examples of
what the different items here.Helm initialize cluster
installs Tiller, sets upa necessary configuration.You can see by the other
picture we're doing a Helm init.Still in the pod.Installing a chart.Helm install a chart, which
puts out there and thendeploys the various pieces
out into the cluster.Upgrading a chart, Helm upgrade.The release that you want to go.So the charts when
you get installthem have a release in there.So you upgrade the release.You can upgrade pieces of it.For example, in this
diagram, we go and lookat Tiller to talk
to the API serverand update the deployment.Next we'll talk about Kustomize.So when we talk about working
with all the YAML filesand such that we use
to configure Kubernetesand that we apply to it to
get these kinds of objectswe want in our cluster,
those Kubernetes YAML filesare often reused.There are several approaches
for reuse out there.We can modify the original
one to suit our needs.We can make multiple
copies of themseparately for different needs.So we can take the one
that worked for us,go in and tweak it to
have the values in that wewant to set up on our cluster.We can make additional
copies over thereand keep multiple copies around
in there to keep and to use.But the challenges with this
is that we have a divergencefrom the original source files.The original ones
aren't necessarily thereanymore in their same form.They may have a copy
somewhere, but wemodified them now for that.And having multiple
disparate copiesmeans it's more challenging
to leverage a common base set.If I'm having separate
copies out there,I can't necessarily
get some kind of reuseout of a core set of stuff.I have my own copies I
have to keep up to date.You have to make
consistent changesacross multiple independent
copies for new versions.And so yet another approach
is parameterizing templates,the kinds of things we
saw with Helm pluggingin those kinds of information.The challenges with Helm
are that templates and valuefiles are not usable
Kubernetes specifications,meaning that those files with
the placeholders, the kindsof things we have
in a Helm chart,we can't just take
that and apply it.They have the
placeholders and thenthey have the
function calls in it.They're more like a new kind
of programming that wrapsaround the Kubernetes spec.Parameterized templates change
the original specification.So they modify the
original stuff we have.And many things
become parametersfor many different users.So you start building up this
kind of overhead with Helmthat you have of trying to
have a sort of programmingaround the spec, trying
to have the placeholders.It can get very
complex very quickly,and it can be challenging
then to do thingseven like making sense
out of a simple YAML file,trying to read through it, and
trying to keep it all in therewith too many parameters.So this counters a
typical goal of reuse,which is keep differences
between specs small and simple.Although it can be simple
depending on if you'rejust changing a few
things out there.It can get very complex as well.So another option
that's come on the sceneis Kustomize with a
K. Kustomize with a Kcreates modified Kubernetes
manifest by overlayingthe specifications on
top of existing ones.It allows you to define
things that are overlay.Kind of like the
union file systemthat we saw early on
talking about containerswhere we talked about
having the layers in there.Layering on these type of
things on top of other files.So changes to make
an overlay arestored in a separate file
called kustomization.yaml.Again, with a K. Kustomize reads
the kustomization.yaml fileand the Kubernetes
spec files and dumpsout completed resources
standard out with the changesfrom the
kustomization.yaml appliedon top of the standard
Kubernetes files.Again, think of it
almost like thatlayering that we described
in a container imagebefore, where we have the
different pieces layeredon top of each other.And the command,
the kustomize buildcommand causes files
to be processedin the output created.The output, in this case, would
be the result, the resultingYAML that we would
feed into kubectl applyfor the cluster, which would
be starting with our baseimages or our base
specificationsand then the overlays on top of.ThoseSimple example here.Let's suppose we
have a directory,a couple of directories
of a little application Iuse, a deploy and a service
and a kustomization.yaml.So the web one has a
set of things in it,deployment for our app.The service is a simple
service definition.The kustomization.yaml has some
common things to put on it.So Kustomize has some
keywords, some termsthat uses common labels.As you might guess,
a common labelto apply across a set of things
and a list of resource filesthat is going to read modify
and emit as a YAML string.So it's going to
read those filesthat we have with the
common label criteria there.Note that when we
actually do this,do a Kustomize build, what
we get out the other sidehere is a set of
YAML, just as wemight expect, to feed
into Kubernetes, whichwould have things on the--
there notice that each serviceand deployment now both
have this label here leveltest, which was a
common label appliedfrom the kustomization.yaml.That's a very simple case,
but you get the idea.We can describe within the
kustomization.yaml what wewant to have end up in there.But note that we
didn't have to modifyeither of the original files.We simply added
something on top of themor later on top of them.Common changes you can
do through Kustomize.CommonLabel adds a common label.Name value to each
Kubernetes resource.CommonAnnotations
adds an annotationto all Kubernetes resources.NamePrefix adds a common prefix
to all the resource names.Here you can see
commonAnnotations.If I do a kustomized
build, then each Iget the build latest
and branch master.Notice I get those annotations
added in to my Kubernetesobject there.Likewise, if I
had a name prefix.Note that I have the prefix
name attached in thereonto my different objects.Variants is a common use case
is needing multiple variationsor variants of a set
of common resources.Dev, stage, prod, for example.To do this, Kustomize has the
concept of overlays and bases.Both represented via
kustomization.yaml file,including resources
customization,resources being the things
that are in the original filesout there.The customizations
are things that I'mdoing to them to update them.So the base declares things
the variants have in common.The common starting point .The common set of files
we want to start with.The overlays declare
the differences.Variants can also apply patches.Patching is a way to update the
stuff, so to add something newor to modify it
slightly in their.A bit more than just
adding something brand new,but actually modifying
something existing.Partial deployment
spec to updatesome existing spec as opposed
to adding new information.It adds a new change to
that, but it's actuallyworking against an
existing object in there.So let's look at an example.Suppose that if I go out
to my directory here,I have a set of
things for the web appand a database and
application I'm working with.I have deployment
service specs for both.I have a kustomization.yaml
in each oneand then I have a
kustomization.yamlat the level and a namespace.So I have the resources
for the database.Deployment simply looks like
a deployment with informationabout the things I need
to run by my SQL database.The service represents
the service.Things need to run for that.The kustomization.yaml
then represents the changesthat I want to make.So notice here it pulls in
my underlying YAML files.Changes then uses a
patch operation thento change the name from
this roar db to MySQL.So it'll work as
expected, assumingthat we need the service
to be named MySQL insteadof the original item here.And notice the
patch feature here.It's pretty easy to understand
starting at line 11 there.We have the operation, the
kind of patch we're doing.We're replacing things.We're looking for
a particular pathin the file in value
in there in our targetin our roar-db service file.I'll show you how that
works more in just a moment.And we have the
resources to include.The patch operation in
there to find the patchto make to the existing text.The type of patch, as we said.The location in the hierarchy
and the replacement value.How to find it.And the item to search for.Then we have our
service definition.So if we were to actually do the
Kustomize build against this,think of it as going out for
the patch functionality herelooking for this particular
object, the serviceobject, the service definition.Finding the name
of the object thatmatches what we
had in our patch.We're identifying the
one we want to modify.And then we have the path.So we find the
path, the hierarchywithin the specification
there, within the manifest,the metadata name.And we want to find
the name object,then we want to replace
it with that value.So then the YAML we get
out from the other endhas that actual, if we actually
do the Kustomized build,has an actual item in there
replaced with that name.So the Kustomize base is the
original Kustomize resourcesplus the kustomization here.We have the original
resources in here.Looking at this the web
deployment, the web service,the list of
resources files we'regoing to read in, the
resources to include.And then if we do a Kustomize
build, we get the set of thingsout the other side.The namespace in there
is the namespace.It's just an item to allow us
to create a namespace fairlyeasily.Kustomization.yaml
at the top levelthen sets some common labels.Also includes the resources
that we have, the lower leveldirectories in there.So add a common label
in the metadata.And we have the
resources that weinclude relative and
absolute path to them.Do a Kustomize build.We'll get the expected
things from just runningthrough those directories,
pulling in the resources,applying any kind
of customizationsthat we have in
kustomization.yaml.Now let's talk a
little bit about howsome special functionality
or ways that Kustomizehelps us with config
maps and secrets.So if we have the YAML
defined on the left hand side,we can define things traditional
config map and secrets.We talked about this
in the Kubernetesportion of the class.We can have it generate
a basic config map.And remember, a
config map is simplya map being sort of a key
value pair to something.So in this case, we're
creating a config mapobject for Kubernetes.We have the metadata is a name.Then we have the data
that we want to use.We can include this
via the resources spec.So if we have it
down there, we cansee the resources we pull into.We can pull in the
config map objectthat we're creating out there.And this is great.It's used by deployment,
but the challenge with thisstill is if a config
map is changed,the deployment has
no knowledge of it.So if we go out and
edit the config map,there's nothing to tell
the deployment that itneeds to pick up a
new value, it needsto pick up a new config map.So if we generate a
config map like this onewe have, then if I go ahead
and if I were to apply it,so in other words, if I
edit the config map I had,then I were to go off and just
apply that into the cluster,then I would see something
output that says, hey,your config map is configured,
but note that the deploymentstuff out there says unchanged.So even though we've
changed this valuethat our deployment
should be depending upon,nothing has triggered
the deploymentto actually pick
that up that change.Similar sort of process
if we were using secrets.So option two that we have with
Kustomize is a generator model.That is here we can
define somethingas a config map generator
or a secret generatorin kustomization.yaml.So we can define that
using those terms there.Config map generator
you see down below that.And what this does for us, when
the Kustomize build is run,it adds a hash onto the
generated item reference.So that you see up the top there
when we have our config map,we have a name in there, but we
also have that hash added on.And then in the
deployment itself,when we're referencing
the config map,it adds that same hash.So then this means
if the configure mapgenerator data is changed
in the kustomization.yamland the Kustomize
build is done again,the referencing pieces, in
this case the deployment,are changed too.So if I were to go in and,
let's say, change this databaseto be prod from
whatever it was before,then I get a new hash added onto
the config map in my config mapitem.But it also then notices
that for the deployment,we have to change
that one as well.So it's config map
generator data is changedand Kustomize build has done,
again, the referencing piecesare changed too.They're tied together
by that hash.So kubectl applies the
resulting config map,and dependent items get the
new hash for the updated configmap.So in this case,
if I did the apply,you would see both
the config mapand the deployment that
references that it alsoconfigured.All right, next let's
talk about Istio.So Istio, before we start
talking about that completely,helps to have the idea
of this init containers.We'll talk about sidecar
containers with Istio.But Istio, in addition
for our init containershere, they're not
limited to Istio.The init containers can be
used anywhere within Kubernetesthere.But the idea is in addition
to having multiple containers,pods can have 0 to many
init containers, whichrun as startup before the
other containers, run serially,and they all have
to exit cleanlyin order for the regular
containers to begin execution.So we can have, over
on the right hand side,you can see the init
containers out there.We have just a simple
little BusyBox,a volume of things in
there, and a simple commandthat it runs to echo out
some information there.So Istio is a service mesh.Let's talk about what
a service mesh is.Lets you connect,
secure, control services.We're talking about
microservices here.We're not talking about the
Kubernetes type of service.We're talking about
microservices here.But an example of a
service mesh wouldbe a network of microservices
interaction between them.So in this case, a
typical service meshdoes things like discovery, load
balancing, failure recovery,all the things you
see on the slide here,metrics monitoring, A/B
testing, canary rollouts,so on and so forth, without
really requiring any changesto the microservice code.So that's a lot of what
Istio buys you hereis that you can use Istio
to get a lot of this sortof functionality for
free in terms of youjust have Istio
installed runningalongside your other containers
out there in the pod.So Istio allows these kinds
of functions out there.And it does it by actually
installing a sidecar proxy,kind of like we talked about
init container a moment ago.It's a sidecar container,
another containerrunning alongside of the other
containers with a proxy builtinto it.So the traffic intercepts
all the network traffic thereand then can do all
these different thingsbased on being able
to control or beingable to filter or intercept
the network traffic that'scontingent for the microservice.How does Istio work?Istio deploys an Istio proxy
called an Istio sidecarnext to each service.We're, again, talking about
the microservices here.So then all the traffic
meant for that microserviceis directed to the proxy.So Istio puts itself out there,
installs a sidecar proxy,allows it then to
intercept the traffic.The proxies then can
be governed by policiesthat are managed through
Istio kind of as a global sortof policy management out there.Each proxy then uses the
policy to decide how, when,or if that traffic should be
deployed to the microservice.So Istio sits out there
kind of a traffic cop havingthe sidecar inside the pod.And then based on how it
intercepts the traffic,it can decide what to do
with it based on policiesthat are set in there.This also enables
sophisticated techniquessuch as canary deployments,
fault injections, circuitbreakers.These are various
patterns out there.Canary deployment being
things like traffic shiftingwhen you're trying to roll
out a new application,a new version of
your application,being able to shift part of the
traffic over to it for a while,then shift a little bit
more over to it and so on.And as long as it doesn't fall
over, then you're good to go.You can move all the
traffic over to it.Fault injections being
able to, if you'retesting an application
especially,periodically inject
faults in there.Things like a 500 error
that you might get backfrom a HTTP process or a
delay in a certain numberof milliseconds in there,
those kinds of things in there.So Istio runs a Linux container
in the Istio Kubernetes podsusing Istio sidecar, meaning
just another containerthat is inserted
into the pods to runalongside the other pods.And when required, it injects
and extracts the functionalitybased on the
configuration needed.So you can set up
Istio to automaticallyinject these sidecars
into the podsso that it can then serve
as that proxy for it.The data plane in
Istio architecture.We talk about some
terminology here.Data plane is composed of
sort of intelligent proxy,these Envoy things, that are put
in sidecar containers in there.The proxies mediate and control
all the network communicationalong with using this
thing called Mixer.This is one of the primary
components of Istiowhich works with the
different sidecars.So the idea of the
service mesh isthis mesh is made up of
a series of these proxiesto these sidecar containers that
are put into each of the podsout there, but they're all
controlled back through Istio,back through things like Mixer.And Istio this data plane.This is what the data plane,
the sidecar proxies and Mixer.Then the control plane, which
the other part of Istio's setupmanages to configure the
proxy to route traffic.It also configures Mixers
to enforce policy collecttelemetry So again,
this idea that Istiois kind of managing all
of these proxies for usand controlling all the
things that they need to do.Taking a look here at a visual
representation of Istio.Talking about the service
mesh and the data planes.If I have a cluster out
here, say I have a cluster,and I'm running
multiple microservices.Three microservices.Then how is the service
mesh implemented?Again, the microservices
exist in the pods.Traffic is ultimately
routed to them.So without Istio in the
picture, the cluster trafficgets generated to the
pods and between the pods.There's nothing in
there to filter it,nothing to intercept it.When we talk about
using Istio, we'retalking about injecting
this additional containerinto each one of these
pods to the Envoy sidecar.Envoy here is the
name of the proxy.It is written-- the
original version of Envoywas written by Lyft, the
ridshare company too.We have a modified version
of it running in here.But we put these proxies in
beside each one of these podshere.Then all the network traffic
goes through the sidecar proxyfirst.Istio routes all
the network traffic,makes sure it goes
through the proxy here,and then we get
the communication.Instead of going through
the microservice directly,it goes through these Envoy
sidecar proxy pods first.Microservice instance is only
aware of its local proxy, nota larger network.So we put this installation,
this traffic cop in therein front of the microservice.So the microservice only
gets what's sent to itor what's let through by the
Envoy sidecar proxy there.The distributed network
then is abstracted awayfrom the microservice
programmer.You don't have to worry
about as much of that.So Envoy, again, is a
high performance proxy.This is developed to manage
all inbound services comingin there.And most basic
Istio functionalitycomes from Envoy itself.Load balancing, fault
injection, et cetera.The nice thing about Istio is
I can do, with these proxiesin there, I can
have them do thingslike the encryption or
traffic shifting and all that.And I don't have to change
any code in my microservices.That's the idea.The proxies intercept
the traffic,do what they need
to with it, and Idon't have to change any of
the code in my microservices.I don't have to deal with
all these different thingswithin my microservices
if I rely on Istioand have it installed out there.The data plane with Istio.Sidecar proxies make
up the data with Mixer.Mixer is the Istio
component that'sresponsible for
policy enforcement.So basically it's
managing the policyfor the different sidecars.And so we have the
adapter we can use,and data plane then
provides service discovery,authorization, authentication,
routing, load balancing,observability.So overall, the data plane looks
at, translates, and forwardseach network packet that
goes in or out or goes intoor comes from the
corresponding microservicesthat the proxies
are in front of.That's the data plane.The set of proxies
in Mixer there.So besides the data plane,
the other main part of Istiois considered the control plane.What is the control plane?The control plane takes a group
of individual stateless sidecarproxies, creates a
distributed system from them,meaning that it helps to manage
the sets of sidecar containersthat we put into
the pods as a unit.Some of the features of this
are automatic load balancing,fine grained traffic control,
pluggable policy layer,configuration API.In other words, being able
to configure this and controlhow they handle the network
traffic that comes into them.Remember, again, that
the sidecar containersits in front of
the microserviceand handles the traffic for it.Also the control plane is
responsible for gatheringmetrics, logs,
traces, and ensuringthat the service to
service communication,the microservice to microservice
communication, is secure.The different components
of the control planeare a pilot, which is
configured as all the Envoyproxies in the mesh.Again, think of these
sets of sidecar proxiesworking together as being the
mesh out there considering itprovides service
discovery for the sidecarsand is a core component
for traffic management.So it helps configure
all the proxiesto work the way we want.Galley, which is distribution
and centralized configurationmanagement.Insulates rest of
Istio componentsfrom Kubernetes and validates
config from the users.Then there's Citadel, which
is the security piece of it,providing strong end
user authenticationand authentication between
services using mutual TLS.Has built in credential and
identity management as well.So let's talk about a few of the
Istio Kubernetes optics here.The first one we'll talk
about is the gateway.So gateway describes
a load balancerat the edge of the surface
mesh, taking informationin from outside.So receiving incoming
or outgoing HTTP or TCPconnections.There's a spec for
it which looks justlike any other
Kubernetes object.As you'll see on the right,
there's the API version,there's the kind of
gateway, and there'smetadata and my gateway,
a name and namespaces,the same sorts of things.And then a selector as
well to select with it.And then you see the other
information there too.So think of this as just
yet another kind of objectthat Kubernetes understands
when you're running Istio.The spec self-describes a set
of ports that should be exposed,that type of protocol
to use, configurationfor the load
balancer, et cetera,and sets up a proxy
exit load balancerexposing port 80 for HTTP
traffic and 443 for HTTPSand so on.It could be applied
to the proxy runningon a pod with a label such as
the app my gateway controller.You see the selector.Another kind of Istio Kubernetes
object is the VirtualService.The VirtualService defines a
set of traffic routing rules.When a host is addressed,
each routing ruledefines the kind of matching
criteria for trafficparticular protocol.The VirtualService
is what allowsus to kind of split up and
steer the incoming trafficto different services or
different within Kubernetes.So the traffic source can also
be matched in the routing rulesto put out to be customized
for client context,meaning the prefixes, the
URLs, those kinds of things.If you look on the right there,
you'll see an HTTP section.You'll see what matches it.The URIs different prefixes,
those sorts of things.And you can see
how it gets routedbased on what it matches there.So for example, all traffic
http by traffic defaultgoes to pods the review
service with label version v1.HTTP requests the past starting
with wp catalog consumercatalog here get rewritten
to the new catalogand sent to pods with
label version v2.So we can redirect traffic,
we can split up traffic,we can filter traffic as we need
it using the VirtualService.The VirtualService relies on
having destination rules whichwork with the VirtualService
and declare a subset or versionof route destination.So it helps us to identify that
a certain particular servicehas subsets.In other words, if we select
a service version by a label,for example, the v1 version
of the service, thengets traffic routed to
it on the v1 name there.The one that's v2
has that label on it.That selector gets stuff
routed to it from v2.This ties back in with
the VirtualService before.Now, note that when we talk
about services here in termsof the host, you'll see
on the right hand sidethis sort of syntax
reviews dot prod dotservice dot cluster dot local.We are talking about
a Kubernetes service.So the Kubernetes service
actually then is in--when we talking about
VirtualServices destinationrules, it's a
service we're talkingabout versus microservices.I know it's a bit confusing,
but we're talking--but when you see things written
out here in this sort of syntaxin the host syntax
error, reviewsdot prod dot service
dot cluster dot local,it's talking about a service
running on the local cluster.So this is talking about a
Kubernetes service, whichis the host when we're
talking about routing traffichere and sending things here.Some VirtualService
variations allowus to do some of the
advanced functionality wecan get with Istio.One is the fault injection
abort setting here.It's a specification to allow us
to prematurely abort a request.So why would we want to do that?Well, think about it from
a testing standpoint.If you wanted to say
every so often throwa 400 or 500 or something
error out there, HTTP error,back from your application while
you're testing it or trying itout, this would be
one way to do it.You can see here, again, it's a
VirtualService type of object.You can see the same kind
of Kubernetes definitionas before.You see the host there.Again, think of that
as the local Kubernetesservice, that cluster.local
on the end of it there.And then you see
we have a route.And periodically every one
out of every 1,000 requestwe're going to send back
an HTTP status code of 400.Another variation is the
injection fault delay.This is similar to the idea
of sending back a return codeor how often or how we do that.But in this case, we're
going to send backa fault one out of
every 1,000 requests.So that'd be one service.And again, it's all
within that specthat you have for the
route, as you see down thereat the bottom of the slide.So the big picture
of Istio here.Let's try to tie
this all together.We have the control
plane, the data plane.But at the core, Istio is
extending the functionalityof Kubernetes.It still requires a
deployment and a servicethat we have up and running.We have a deployment for my
current app, for example.And we have metadata
associated with that.We have services
in front of there.So it doesn't take the place
of these Kubernetes objects,but it augments things to
extend the functionality.In addition to control plane and
data plane portions of Istio,as we've talked about, then
we have additional piecesfor routing functionality.We have the gateway, which
has the load balancerout at the edge of the mesh.Configures our
proxy load balanceHTTP and other kinds
of traffic in there.Doesn't do any traffic
routing but doeshave VirtualServices then
that configure to havethe list of routing rules.How do we want to split up?How do we want to
redirect traffic in therebased to different versions
of services, for example.Controls how proxies route
requests for servicesin the mesh.Then we have the
destination rulesthat actually define those
subsets or those configurationsthat we want Istio
to apply to request.After we get the routing,
we'll send a VirtualService.So the gateway allows us to
get information or traffic in.The VirtualService
then allows usto split it up or filter it
based on some sort of criteriain there.And then the
destination rules helpus define the subsets,
different sets of servicesthat will handle
which kind of trafficwhich tie back into
the VirtualService.So if we're looking
at this, the gatewayis bound to the VirtualService
to specify routing.So think of those yellow and
blue ones as kind of being--little yellow and blue
funnel looking thingsas being filters or funnelling
the traffic in different ways.So the Istio components do
run in a separate namespacewithin Kubernetes, one
called Istio systemwhen you install it.So that is where the app
namespace, the pieces of Istioexists there in the
application namespaceor whatever namespace you have
for your product out there.And then there's
Istio system namespacewhere the different
pieces of Istio,the main core parts of
it, live and run there.So let's take a look at
some kind of example visualsto help us think about
some of these items.For example, routing example.We have traffic represented
by the green arrow cominginto the gateway there
going through our filters.And if we're doing this sort
of traffic shaping or trafficshifting functionality
there, we might, for example,take a large portion
of our trafficand redirect it to the
current version of our app.You see on the top of the slide
on the right the high level,the app current
deployment, whichwould have the current
version of our service.Notice that it has a
version called current.Has a label in it.And then we would take the
smaller portion of our trafficand redirect it to
our newer version.So we have a
smaller portion thenthat can go and talk
to the newer versionto make sure that it's
going to support the trafficand going to work as expected.When we actually
then have confidencethat it can support
that level of traffic,we might increase a little bit.Take a little bit more off
of the current version,put a little bit more
onto the new version.This is the kind of canary
testing we can think about.So then we have a larger
portion we route overto the newer version
of our app, lesserto a current version of our
app, until eventually wecan get to the point where we
feel confident that the newerversion of our microservice,
our application,can handle all of
the traffic loadand we can shift it over
entirely to the new versionand deprecate the current
version of our app.One of the other examples
here, fault injection.Let's suppose that
with fault injection,we set it up so
that we send trafficto our current microservice.Perhaps we're testing
it out here, want to seehow it responds.But every so often, as
indicated by the red arrow,we do send a fault over to it.We have Istio
generate a fault so wecan make sure to respond
to that correctly.So again, the idea of sending--
we send out these traffic.Most of time it's fine, but
occasionally we inject a faultin there just to see
how it works and seehow it handles it in there.Likewise, delay injection.Same idea.We will have a set of
traffic generated in there.It typically goes
at a regular pace.However, every once
in a while, we'llinject a very slow
set of trafficthere to simulate things
like network latency,slow web, a slow network.Make sure your application
handles that well.All right, now we'll move into
talking about the operators,Kubernetes operators.And we'll talk a little
bit about some thingsthat we need to
understand up frontto have a little bit of
prerequisite knowledgefor that.In particular, we'll talk
about scaling resourcesin Kubernetes, replica
set versus deployments,and role based access controls.So kind of permissions
we need around Kubernetesand how we manage that.So scaling here.Let's talk about scaling first.So scaling allows you
to scale resourcesup down via the command lines.So for example, I can say
kubectl scale replicas equalsthree and the scale
replica set named foo.Note the RS abbreviation
in front of foo.Or I can say scale
or resource specifiedin a particular YAML file to
a certain number of replicas.Or I can say if the deployment
names, in the third example,if the employment name
mySQL's current size is two,scale it to three up there.So then it goes ahead and checks
to see if the current value iscertain one.If so, it scales it up there.Scale multiple
replication controllers.We can scale objects within
Kubernetes pretty easily.This is a manual
scaling out therewhere we actually run the scale
command from the command line.Now, what do you think happens
if you scale a resource to 0?Well, if you scale
a resource to 0,that's effectively leaving
it running but turned offor leaving it out there
but leaving it turnedor having it turned
off out there.Scaling resource to 0
can be effective wayto leave it in the cluster.So it's easy just
to turn back on asopposed to totally getting rid
of it and leaving it out there.So scaling to 0 can
be an effective wayof scaling a resource down to
turn it off effectively, nothave it use any
resources at the moment,but still have it
present in the cluster.And then you can scale
it back up when you'reready to turn it back on.Kubernetes scales a number
replicas within a given rangeto maintain a preset
level of resourceif you use the
autoscale function.This is a kubectl autoscale.The other slide was using
the manual scaling command.But for example, in this one
we have kubectl autoscaledeployment.CPU percent equal
50, min one, max 10.It says defines an HPA
that maintains between oneand 10 replicas of pods.An HPA in this case is a
Horizontal Pod Autoscaler,meaning that it could
automatically scale upthe number of pods
there based on the--it's based on one
of your replicasto maintain an average of
50% CPU usage in this case.So if we want to
have them maintaina certain level
of resource usage,CPU usage here, then we can
tell Kubernetes to automaticallymake that the case.So we have the
number of pods thatwill give us that desired
CPU usage between the minimumand maximum that we have.So we can have it
autoscale for us again.Again, think of that
thing we discussed earlierwith the idea of Kubernetes
acting like a data centerto scale the load
up based on needor to scale it down based
on saving resources.So this is one way to do it,
to have it autoscale for us.You can certainly scale
it mainly as well,but this lets Kubernetes
just take care of it for us.Again, that idea
of Kubernetes kindof managing the containers
running the pods like a datacenter would manage the systems.Replication controller
is we're talkingabout replication
controllers here versus whatwe call replica sets
and stuff these days.Replication controllers
copies of a pod,manages a set of pods,
ensuring the cluster'sin a desired state.So this is the idea behind
the way we would formerlymanage replicas in there.Has all of these
kinds of things.So when we talk about that,
replication controllersare criticized for
being too imperative.Meaning, again,
imperative the ideathat we tell them how to do
things as opposed to declaringwhat we ultimately want and
having the system figured out.And they weren't
considered flexible.So they've been replaced then
by replica sets and deploymentsas we've been talking
about those itemsthroughout the course here.So the kind if you see on
the right hand side there,again, just a representation
of the deployment,remember again the
deployments havereplica sets in them or replicas
in this case we call them.And then we have
the specs in therefor actual containers that go
into the pods, the actual podspec.So replica controllers,
if you hear about those,they're replaced by replica
sets and deployments.Replica sets are responsible
for managing pods, scalinginstances.Deployments have
additional functionalitylike rolling updates and such.But deployments encapsulate
replica sets and pods.So deployments are
the basic name.Replica sets contains
the deployment nameplus the unique identifier.Pods contained a replica set
name plus unique identifier.The point of this is
simply pointing outthe sort of relationship here,
the hierarchy between them.At the higher level, we have a
deployment with a specific namethat we give it.Then the replica set being
a part of the deploymentgets its name based
off the deploymentplus a unique identifier.And then when you
have pods, podscontains a replica set name
plus the unique identifier,some hash.The reason for
this is because wecan't have-- the reason
for the hash partis because we can't have
things with the same namein a namespace
within Kubernetes.So we can't just use a pod
name for multiple instancesin multiple pods.So each pod has to have this
unique hash or identifier on itto designate it,
have a separate namerather from the previous pod.So next let's talk about
Role Based Access Controls.How do we secure Kubernetes.So why do we need
Role Based AccessControls in the first place?Because in a real world
Kubernetes environment,we may need to have
things like multiple userswith different
properties, establishingproper authentication
mechanism, have full controlover which operations each user
or group of users can execute,have full control over which
operations each process insidea pod can execute.So realize here that RBAC is
not just about human users.It's also about the processes
inside of Kubernetesand permissions they can have.And we may need to limit the
visibility of certain resourcesof namespaces.There's three elements involved
when we talk about RBAC.The set of users,
the subjects, whichare the set of
users and processesthat want to access
to Kubernetes API.Again, the idea of there's
both the human users as well asthe processes.Then there's resources.The set of Kubernetes
API objectsare available in the cluster.These are all the things
we've been talkingabout throughout the course.Pods, deployment services,
nodes, persistent volumes,those kinds of things.And then verbs.The set of operations could be
executed on resources above.So if we think about
what a human user mightdo to a particular object in
Kubernetes, the kinds of thingswe'd run from the
kubectl command line,we're talking about things
like get, watch, create,delete, et cetera.Ultimately all of them are this
CRUD, Create, Read, Update,or Delete kinds of operations.We think about the set of
things that we're tryingto connect together here.We have subjects which, again,
are the people or the personsor the processes out there.Could be an operating system
process or process in a pod.The API resources,
the kind of objectswe talked about
within Kubernetesitself, and then the operations.The operations that are
available as verbs or thingsto list.So we want to connect the
subject's API resourcingoperation.In other words,
you want to specifygiven a user which
operations can be executedover a set of resources.So one of the ways we do
that is a Kubernetes objectcalled a role.So the role connects
API resources and verbs.For example, if I want
to be able to list podsor to create a namespace
as sort of thingsout there connecting
of the actionswith the objects
in Kubernetes therecan be used for
different subjectsif this is bound to a namespace.So we also have
K8S cluster roll.So in RBAC, when
you see somethingwithout the word
cluster in it, itmeans it's a namespace scope.It's intended for use in
a particular namespace.When we see the cluster
as part of the name,that means it can be used
across the cluster there.So the role binding then, so
a cluster roll, in this case,would be that across a
cluster, we would connect upone of the API resources, one of
the object types in Kubernetes,with the different
kinds of verbsor with the verb we
could use on there.The role binding then takes the
role and binds it to a subject,meaning it binds it to
a user or a process.Given a role that combines
the API objects and the verbsdefines which
subjects can use it.The cluster role binding,
as you might expect,then takes this-- connect
subject to cluster rolesout there.Again, that sort
of cluster scopeas opposed to namespace
scope in Kubernetes.Here's some simple
examples here.We have a role binding.We have a role
here that allows usto do the list get
and create, thoseverbs against the pod resource.And then we want to actually
have a particular user whocan do a role binding with
that to actually connect upto the role that we have.You can see, again,
this is simplya-- these are simply
Kubernetes objects.Kubernetes objects that are
connecting things together.The role defines
the actions that wecan do against the objects.And then the role binds
that to some subject.So that means that in
this case, this usercould do these kinds of things.They could get pods.That's allowed
from the role thatsays we are able to get pods.You can describe the
pod, create a namespace,those kinds of things
that are described.But they wouldn't
be able to do thingslike get the pods from
a different namespaceor get the pods with the dash
w, which would require the watchaction or the watch verb to be
included in the permissions.In Kubernetes RBAC, users
are global, meant for--user things are meant
for global or meantfor humans or processes
living outside the cluster.The service accounts
are for our namespace,meant for inter-cluster
communications or processesrunning inside the pods.Both will authenticate
against the APIto get access to resources,
but there is no true Kubernetesuser object.So you can't do a
kubectl create service.You can do a kubectl create
service account name but nota kubectl create user.The bottom line is
cluster doesn't storeany information about users.So that has to be managed
outside the cluster wherecertificates or tokens are
some of the usual mechanisms.So RBAC providing
access, if we wantto take a look at
example providing accessto an application
and deployment,here we can see sort
of a Helm template,in this case, that has the
service account for RabbitMQ.If we have the RBAC
enabled there at the Helm,this is another feature
of Helm, for example, Icould go through and say
if something is enabled,then we can define
a role allowingget verb to an endpoint.And then the role binding
connects the service accountand the role out there together.So that then we have
that service accountfor RabbitMQ able to do
the get on the endpoints.All right.Let's move into talking about
actual Kubernetes operatorshere.So Kubernetes operators.What is it?Well, an operator is a method
of packaging, deploying,and managing a
Kubernetes application.It's basically a way to extend
the functionality of Kubernetesand to customize it and to
provide a way for things thatare not typically runnable
or usable in Kubernetes to beable to use them in there.What I mean by that is simply
that if I have somethingthat is a stateful application,
meaning that it needs to--it normally tries to maintain
a state with the object.It's not stateless such
that if it normallywas killed off by
Kubernetes, you couldn't juststart up a new one.But you had special
handling you neededto have for a type of object
you wanted to run in Kubernetes.Operators allow you to do that.So Kubernetes
application, in this case,is the application both
deployed in Kubernetesand managed using the Kubernetes
API and kubectl tooling.Kubernetes by
definition or by defaultis going to want to
be able to manageyour application as a
stateless sort of application.That means that it can
kill it off and restart it.If it needs to, it can
easily scale it out there.Your application
is ready to respondto these kinds of
requests for Kubernetesand behave as Kubernetes would
expect it to or needs it to.If that's not the
case, then an operatorcan actually be used to have,
as we said in the slide here,purpose built running a
Kubernetes applicationoperational knowledge baked in.Being smart enough to understand
and to handle from Kubernetes.If I need to run
things differentlyor if I need to do things in a
way to respond to Kubernetes,I can have an operator that will
take those Kubernetes actionsthat Kubernetes tries to do and
interpret them from my objectand then have my object still
be running but still behaveor respond to
Kubernetes in a waythat Kubernetes would
expect right there.Cloud like capabilities
encoded to operator codecan provide an advanced
user experience.Automate features like
updates, backups, and scaling.You can have additional
functionalitybuilt into your object.If you have a particular
type of objectthat has a certain way
it needs to be backed upthat doesn't really correspond
to the way Kubernetes would bedoing things or updates or those
sorts of things or scaling,you can build that logic
into the operators there.So all of this is accomplished
using standard Kubernetestools, the CLI and the API.So one of the pieces
that's out thereto be available to use
now designed by CoreOSbut now part of Red Hat is
called the Operator Framework.The Operator Framework
has several pieces.The main one is
the Operator SDK,which is designed to make
it easy to code up operatorswithout having to know
Kubernetes API details.And then it has a
lifecycle managerto oversee the install,
installation updates,management lifecycle
of operators,and also operator metering
in there, which is intendedto be able to support usage or
able to support usage reportingfor operators that provide
specialized services.Operators can be coded up
in Go or Helm or Ansible.Go is the main language that
you want to really probably useif you're going to
have an operator that'sintended to do
anything more than justdo sort of an initial deployment
of items in the cluster.It's trickier to do any
things like Helm and Ansibleto do sophisticated
operators with those, simplybecause those are more
kind of one time deploythings versus the Go code,
which allows you to code up moresophisticated operators
that respond in the waythat Kubernetes would expect
over the lifecycle of objectsin the cluster.How do operators work?Kubernetes is essentially
a desired state manager.We've talked about.We provide desired
state via a specand makes it happen and
tries to keep it that wayeven if something goes wrong.We talked about
this controller ideato control plane on
the master there.It includes controllers
that try to reconcilethe state by monitoring the
set of the state of existingobjects, comparing your object
state to Kubernetes YAMLspecs, the specifications
we give it,in the YAML files
for what we needin terms of services,
deployments, pods,those kinds of things.And if there's a difference,
try to correct it.In other words, we have
things running out there,we have something go down, it
doesn't match the spec anymore,the number of things
we've declared we want,so Kubernetes tries
then to reconcilethat by creating
enough to make surewe have the number we want
based on the spec that we have.Native Kubernetes objects,
deployment services etceterarespond well to this model
because, again, they'reconsidered stateless.This is the idea.They can be deleted,
destroyed, and then recreatedand automatically pick
up where they left off.But the model has issues more
complex stateful applicationssuch as a database
running on multiple nodes.If the nodes go down, we need
to reload the data to get backto a previous good state.So the typical cloud
friendly operationslike scaling, upgrading,
disaster recovery,for these kinds of stateful
app don't fit out of the box.So to handle this, we write
operators in Kubernetesto allow defining a custom
controller to watch your appand do custom processing.The applications to be
watched by the operatoris defined as a new object
type in Kubernetes a customresource.So we hear the terms custom
resource and custom resourcedefinition, which are
essentially your custom objectsin Kubernetes.They're not a deployment,
not a service, not oneof the standard
types of objects.They're custom
objects that you haveyou want to run in Kubernetes.And you have a custom
resource definition thatdefines the spec for those.And then the custom
resource thatruns in Kubernetes
and the operator thatruns around the custom
resource to make surethat it responds in the way
that Kubernetes expects.Typical kinds of operations.So the overall behavior of an
operator's controller controlwhat you write as
part of an operatoris similar to how the
native Kubernetes handlesreconciling those objects.Operators give you the chance
to do custom processingwhen you're reconciling
things there.On this slide, you'll see an
example of a custom resourcedefinition.Again, it'll look familiar at
the top kind of API versionagain, a kind, metadata, and a
spec that's associated with itthere.So the Kubernetes API server
creates a new restful resourcepath for each version you
supply, the REST interface outthere.The CRD can be scoped
either for the namespaceor the cluster similar to how
we talked about the RBAC stuff.And it has a scope
field in it as well.And then you'll see
things in there as wellin terms of the
names we can use,the plural, the singular,
the kinds of things.We can manipulate
just as other objects.We have this spec
and we apply it.And here's the
restful API endpointyou can see for this one,
the Chrontab in there.You can look at
and see an example.The endpoint URL can then be
used to create managed customobjects.And the kinds of these
objects will be the kindthat you defined in
your specification.For example, crontab
in this case.It then makes these
custom objectsand use them with
operations like kubectl get.So once you define an object
that is out there, a customresource, you can use it just
like any other Kubernetesobject with the
standard list or theget to create, delete
those kinds of things.So OperatorHub.io.This is the operator registry
for Kubernetes operators.This is supported by Red Hat.And it is a way to surface your
operator to the wider communityto let people know about
it, know what it can do.And so if we want to have an
operator that manages thingsfor us, we can just
have that availableand have people be more aware
of it and be able to use it.Let's next talk about GitOps.What is GitOps?GitOps is a way to do
continuous delivery.It works by using
Git as a sourceof the truth for declarative
infrastructure and workloads.Per Kubernetes, this means using
a Git push instead of kubectlcreate apply, or
Helm install upgrade.What are we saying here?We're talking about
being able insteadof having to do things to get a
kubectl command line out therebetween just relying on users to
do things via the command lineand getting that
right and things.Instead we manage our YAML
specifications in Git.That is, we put the
files up to Git,and then there is
something, maybeit's an operator or a
specialized process,that identifies that there's
been a change in the Gitproject, much like
sort of a CI thing,if you know what
Continuous Integration is.It identifies there's been a
change in the source repositoryfor our YAML, our
specifications for Kubernetes,and actually identifies
the change and goes offand applies those to
the cluster for us.So instead of having to
interact with the clusterthrough the Kubernetes
command line,we simply make the
change within the specand then put it
in Git in and thenGit applies it to the cluster.So why would we do this?Well, for a couple reasons.One is if we can manage things
as code, meaning we managethings in a Git
repository here, thenwe have that way to actually
do things with the file in codeas we would for any
other source code.We can do code reviews on it.We can track changes over time.We can do diffs on it.We can even roll back
easier, because we could justgo back to our previous version
of the files stored in Gitand be able to use those.So if you think about
the standpoint of a useractually running a
kubectl command lineand making a change to
the cluster that way,if that user messes
something up or if weneed to know what they did,
they've gone on vacationor whatever the
case may be, then wedon't have that information.We only have some record
in there of something,but we don't have as easy of a
way either to roll things backor to see what the
differences were or do that.However, if we manage our
manifest for the clusterthrough Git, we're
able to actually takeadvantage of all
the things that Gitcan do in terms of differences
looking at the code,so on about that,
and be able to dothat with our Kubernetes
manifest instead of havingto manage it by person.So in traditional CI/CD
pipelines, CD per CI processcreates build artifacts and
promotes them into productions.Continuous delivery versus-- or
per the continuous integration.In a GitOps pipeline model,
any change to productionmust be committed
in source controlprior to being applied
to the cluster.It should ideally be
done through a pullrequest or a merge request,
those mechanisms in the sourcemanagement system where we can
merge things and create them.The advantages of
this is code reviewfor changes, tracking who
did what, again, rollbackis via Git.Can simply go back and
get a previous versionof things out of there.The whole
infrastructure can alsobe recreated from
source control.That's another key
point here is that if Ihave all of my specifications
for my cluster in Git,then if something bad
happens to my cluster,I can actually just
run this process again.All the specifications
will be in there,and it will go out and
create those for meand recreate the cluster based
on the specification storedin Git.GitOps with Kubernetes.So what do we need here?We need a Git repository
with our workload definitionsin YAML format, helm chart,
any other Kubernetes customresource that defines our
cluster's desired state.This is the config repository,
the set of Kubernetes objects.I'm sorry, the
set of YAML files.A container registry where
our continuous integrationsystem pushes immutable images.What do I mean by that?Basically think about
this a couple of ways.Even though we are always
telling Kubernetes what to doand what to have via the
manifest, at the lower levels,what Kubernetes needs
to do is still spin upcontainers, containers
that run within those pods.And the way it spins up
containers is from the imagesthat we've talked
about all along,the Docker images
whatever processused to create the images.And so we still need to
have a place to get images.So if I am updating
some resource in therefor a pod spec, for
example, to a newerversion of a
microservice, I needto have a newer version of an
image that that's based on.So I need to be
cognizant of havingboth a repository
for my specificationsbut also one for the images
that I need to use in there.So what we would need also
in addition to those tworepositories, one for images,
one for our YAML files,is an operator that
runs in a clusterand does a two way
synchronization.So it would watch the
registry, the image registryfor new image releases.Based on deployment policies,
update the workload definitionsin there and committing the
changes to config repositorybased on those.Then we would go out
and update the cluster.Or it would go and update
the clusters for us.So watches for changes
to configure repositoryand applies them to our cluster.This is just a quick
example diagramhere looking at the idea of
having the users pushing Dockercontainers or images,
rather, into a repositoryor registry committing
things into a Git repositoryfor YAML specifications.There are different
products out there,like that Weave Flux
one, for example,which would then go
out there and can applyvia Helm or some other
orchestration thing the stuffinto our actual deployment
into our actual cluster.Let's talk about
monitoring here.So there are some basic
tools for monitoringthat come in handy when you're
working with Kubernetes.One is the Kubernetes
dashboard, which hasa visual interface to cluster.It's a general purpose
web based UI for clusters.You can get overall
status about objectsout there in the cluster.You can evoke it
from the commandline via if you're running
a mini kube, for example.A mini kube dashboard or
kubectl apply from locationand a kubectl proxy.You can also create or modify
objects via create link.So you can do operations
in it as well asgetting information about it.But it's basically a way to
interact with Kubernetes sortof graphical interface there.You can upload a specification
file to it, but you can't--and you can type in
JSON or YAML as well.So you could create objects.You can do specifications
in it as well.It's probably most useful for
being able to look at clusters,get information from that.There's also a tool
called Prometheus you usefor event monitoring alerting.It's an open source application
for those types of functions.What it does is it records real
time metrics in a time seriesdatabase.Think of it as recording
information about your clusterover time in a database, adding
a query language on top of it,and then it has a monitoring
platform which typicallyhas different kinds of exporters
that will run on the hostto export metrics.Things that are run
within your clusterto send out information that
can be stored in this over timein this database there.Prometheus then to centralize
and store the metrics.Alert manager to trigger alerts.You can have alerts
triggered if you'rerunning short of resources
or having issues in there.And then you can also
use a tool calledGrafana to produce dashboards.Grafana is a metrics and
analytics visualization suite,basically meaning that it's
a way to build cool graphs.You can have information,
for example, that you pullfrom things like Prometheus.You see others down there.Elasticsearch, InfluxDB, MySQL,
Postgres, Cloudwatch, all that,different kinds
of data coming in.And it allows you via
a visual interfaceto have different panels,
different kinds of graphsto add data in there and get
different kinds of information,like what your system is doing
over time those kinds of thingsout there.That concludes
this presentation.Thank you, and I hope
it's been useful to you.If you have any questions,
feel free to reach outto me at the email
address on the screen.Thanks, and have a great day."
68,"Thank you.I'm Dave Nelson from
Eli Lilly and Company,and I am presenting on behalf of
myself and Siew Wong-Jacobson.And we thank Fast Global
Forum for the opportunityto present our SURVEY COV macro
for complex survey correlationsand multivariate analysis
and data modeling.My opening story is that
I love new SAS PROCs.When there is new SAS
PROCs, I get so excited.And not only do I want to
learn about the new PROCs,I want to use them right away.My extreme example is when SAS
introduced the PROC INBREED.You may not even know
that SAS/STAT hassomething called PROC INBREED.But when I first saw
that, I got so excitedthat I thought to myself, I have
health and family tree data.I can calculate the
inbreeding coefficientsin this population.So I had the same reaction
when I saw the survey PROCs.Went SAS SURVEY PROCs
like SURVEYMEANSfirst were
introduced, I was justas interested in
these new PROCs,and so I started
reading about them.And about three paragraphs
in, I thought, never mind.I'm never going to
use those PROCs.Well, fast forward until
today, and about 95% of my workis the analysis using
the SAS survey PROCs.So you never know where your
career is going to take you.So that helped
motivate our desireto have a macro that helps
extend complex surveydata to more analytic
opportunitiesthan it is currently
in the SAS system.So we will introduce a bit about
complex survey data we willdiscuss how the %SURVEYCORRCOV
macro works and increase outputsimilar to PROCs CORR, but it
uses also the ability to outputdata matrices for both
progression and multivariatemethods.Complex survey data
is an important sourceof real world evidence.It's often designed to be
nationally representative,and it's processed with
embedded data quality checks.Because it's often
free of charge,it's popular among students
for research projects, theses,and dissertations.In fact, I've helped
four of my colleagueswith their
dissertations and thesesfor epidemiology
graduate degrees.I like data collected through
simple random sampling, whichis what we do most of the time.Analysis of complex
survey data musttake into account
the sampling design,and also the details of
this kind of approachare well described elsewhere.Several textbooks will describe
the methods to deal with that.I won't do that here.But too often people
use complex survey data,and they do not properly use
the correct analytic techniquesfor complex survey data.So we recommend three
checklist steps the foundationof analysis for
complex survey datathat create both
unbiased estimatorsand proper standard errors.The step 1 is always use the
survey procedures in SAS,for instance, SURVEYMEANS
and SURVEYFREQ.Always use the cluster strata
and appropriate weightsfor your data.For example, in NHANES, there
are several different weights.So if you're using only
the questionnaires,use the interview weights.And if you're using
laboratory values,to use the exam weights.So that is the basics
just make sure you'reusing the appropriate weights
for the analysis you'reperforming.Also do not delete observations.Typically, and most
times if we wantto analyze only the diabetes
patients out of a dataset,we only remove those
patients and create a cohort.But in the complex survey
data, do not use BY or WHEREstatements to subset your data.Create an analytical
subset for useas a domain, which is
analyzing the subgroup alone.For instance, if you only looked
at a certain disease state,you may have empty
strata, and that wouldaffect the standard errors.So why did we create this macro?Well, SAS does provide complex
survey univariate analyseslike SURVEYMEANS and SURVEYFREQ,
and multivariable analyses--SURVEYREG, SURVEYLOGISTIC,
and SURVEYPHREG,and then also
imputations, SURVEYIMPUTE,and a really powerful
tool, SURVEYSELECT,which can be used for a
wide variety of tools.But within SAS/STAT,
multivariate analysisand multivariable
model selectionare not part of the SAS complex
survey analyst's toolbox.So our motivation is twofold--first, to construct a macro
that mimics the key featuresof the PROC CORR procedure, such
as creating a pairwise matrixof correlations and
p-values, and we alsoprovide an option to use
ranks similar to a Spearmancorrelation.Secondly, I think
our main motivationis that we can take those
correlations and covariancematrices from this
procedure, from the macro,and then input
them into other SASprocedures, such as PRINCOMP,
FACTOR, VARCLUS, CANCORR,and REG, which will expand the
tool box for complex surveyanalytics.So these are the
special datasets in SASthat can be input into
procedures and the proceduresthat will take them.So I highlighted the CORR,
COV, and some squaresand cross-product matrices.Those can be input
into each theminto the procedures that
are highlighted here.For instance, principal
components analysis, typically,you put in your dataset.But really, the
analysis starts whenyou're out the correlation
matrix or covariance matrixstage anyway.That's really when
principal components begin.So if we can take survey
data and then generatethe covariance and
correlation matrices,we can go straight into
some of these SAS PROCsand be able to
use complex surveydata with their capabilities.This is kind of under the
hood of how the macro works.The key thing is that
it uses SURVEYREG,but I'll go through
some of the steps.The main thing that
we're outputtingare correlation matrices,
covariance matrices,and sum of squares and
cross-product matrices.So the idea is that if you want
to use a rank-based analyses,it will use PROC RANKs to
generate within domain ranks.But the key thing is it inputs
the data into SURVEYREGsand generates a matrix
that's an option in SURVEYREGcalled the XPX matrix.That is a weighted sum of
squares and cross-productsmatrix.So if that is the
output, then thatcan be my final
endpoint using the macroto generate that matrix.But I also will then put
it into PROC PRINCOMPand generate a covariance
matrix or a correlation matrixon your data.We have another option I'll
describe a little bit morelater called OUTP4REG
as a dataset selection,and that adjusts the sample
size to use in PROC REG,and I'll describe
that a little later.The main thing from the
ability to create a PROC CORRtype of output is pairwise
univariate PROC REGsof each pairwise set of
variables that you specifyand the p-value from that.And that is extending--Jessica Hampton had done a
previous SAS paper calledPROC SURVEYCORR,
and we're extendingthat to generate
matrices and the abilityto extract the matrix
for other purposes.So this is an example macro call
from using the %SURVEYCORRCOVmatrix.So you'll see that
there is various optionsto include your dataset,
your typical surveycharacteristics such as
the strata, the cluster,and the weight, the
domain statement, and alsothe variable names.All of the examples we're
going to show today and alsoin our proceedings paper are
using the Medical ExpenditurePanel Survey, the
MEPS data, whichis a publicly available complex
survey dataset availableregarding US health care,
utilization, and cost.So this is the PROC
CORR-type output,based on using SURVEYREG
R squared values,reducing them to
what we typicallysee in PROC CORR output.And there's two output options.You can either do output
them to your listing fileor in Excel there is an option
to save all of your datasetsin Excel.And so basically,
this is the outputthat you can see that
similar to PROC CORR.These are correlations
of three MEPS variables.But the key thing that
we find interestingis the ability to use those
matrices for other purposes.And so this one shows
the steps for usingfor regression analyses.OUTP4REG I had
mentioned previously,that creates a
correlation matrix thatis sample size that's adjusted
for using in PROC REG.So we used the analysis for
the three MEPS variablesas independent
variables, and just showacross the different
results to showwhat the macro datasets can do.The SURVEYREG
output is what you'dexpect from the
typical PROC SURVEYREGanalysis that adjusts for
all of the complex surveycharacteristics.So you can see we have
three independent variablesand their parameter estimates
of standard error, the p-values,and also account for
the design effect.So if I used directly
from the macro,the correlation matrix, what I
call OUTP from the macro, thatwill generate-- you'll see
the same parameter estimatesare available from
SURVEYREG and usingPROC REG with that
correlation matrix.But you'll see that the
T values are much higher.And so the reason that is
is that if you use the OUTPmatrix, it will use a sample
size based on the weighted sumof the population.So for instance, if you're
using the entire US,your sample size was estimated
at being 300 million,which is not the correct
assumptions for generatingthe standard errors, so
providing the rate parameterestimates, but not the
correct standard errors.So what we did was
we have an optioncalled OUTP4REG, which is
basically an output correlationmatrix for regression
for PROC REG.And basically, it
divides the sample sizeby the design effects that are
shown in the SURVEYREG output.So what the impact
of that is that now Ihave an adjusted sample size.So I think go into PROC
REG and run the analysis,and you'll see that the
parameter estimates,again, still the same,
but the standard errorsare much smaller than if I had
just used the PROC REG resultsfor the correlation matrix.So you can see the they're
not exactly the same as PROCSURVEYREG, but there are
what I would considera conservative result so that
all of the standard errorsare at least as small
or slightly largerthan that from SURVEYREG.So that ability to be able
to use complex survey data,generate a correlation matrix
that can go into PROC REGcan yield some
important information.What we have in more detail
in our SAS Global ForumProceedings paper is
how to use PROC REG.We only have third
PROC SURVEYREG.Why would we want to include
information from PROC REG?Well, there are lots of things
that PROC REG can do that PROCSURVEYREG can't.For instance, if you
have a lot of variablesthat are correlated,
the calculationof a variance inflation factors
to a evaluate multicollinearityis the one thing you can do in
PROC REG with the output data.Also, there's an example
in the proceedings paperof how we used ridge regression
within an output correlationmatrix from the macro.And also, we have an example of
how to use variable selection.Again, that's not available
in any of the SAS SURVEY PROCsfor complex variables.But we do have an example
with selection equals CPto show how the
PROC REG can be usedto provide informative
information for usfor analyses with
complex survey data.Again, the final analyses should
always be using SURVEYREG.These are only
pieces of informationthat can help you as you
build your final model.In addition, we have an
example of a different approachto building a multivariable
model using variable clusteringto identify the
independent variablesor multivariable
regression, and that's alsoin the proceedings paper.Next, we also have
so many opportunitiesto use these output correlation
and covariance matricesfrom the macro to do
multivariate analysis,and one of those is principal
components analysis,which is a really
valuable tool for whatis called unsupervised
learning, wherethere is no dependent variable.Principal components
analysis createsan correlated linear combination
of variables startingfrom principal component 1,
which accounts for the mostvariability, and
then sequentiallythrough to principal
components that eventuallyequal the number of
original variables.So there are options in-- when
you do principal componentsanalysis, the default is
the correlation matrix.So the correlation matrix
is often very helpfulwhen you have variables
of different scales,of different types of measures.It really reduces all variables
to their standardized formof mean 0 standard
deviation of 1.But we're looking at
medical expense tapesfrom the MEPS dataset, and
all of them are in dollars.So the available information
is that we could,instead of having
standardized variables,we could use the covariance
matrix to analyze the dataand provide information from--so the covariance
matrix is accountsfor the actual variability
of the data pointsso that it uses the
variability of the actual datarather than the
standardized variables.You can see in this
example, inpatient staysdominates the
principal component 1because inpatient stays or so
is the such a variable measure.Most people have $0
for inpatient staysmany people have over
$100,000 for inpatient stays.That variability then dominates
the covariance of analysisof using the covariance matrix.So probably the best solution
is using the covariance matrixfor using a log transform
first because then thatuses the covariance
matrix, but ituses the relative
variability and notthe variability of the
entire sample, but justthe relative variability.So you see, in that
case, principal component1 is much more
balanced, rather thanthe results from just purely
the covariance matrix.And one of the
things that I thinkis the best way to view
principal componentsis the way that PROC SURVEYREG
can provide a mosaic-type plotof the points.Since we're looking
at survey data,I think the best way to
look at distributionsis using this kind of
mosaic presentation, whichwill basically show that
the areas of high redare the highest values, and the
most people are in that area.So then the PROC
SURVEYREG can createthese principal components
and the graphicsthat provide that information.There are other examples
in the PROC SURVEY.In the preceding
paper, there areexamples where we use PROC
FACTOR to do exploratory factoranalysis.We also use PROC CANCORR to do
canonical correlation analyses,maximizing the correlation
of a linear combinationof one set of variables
with a linear combinationof a second set of variables.And so we have more details
in the proceedings paper.So in conclusion,
we want to wantedto replicate the
survey-based analysis in PROCCORR and as a conduit
to other procedures.And that's why we created
the %SURVEYCORRCOV macro.We hope this macro helps run
the types of analyses performedwith complex survey data.We encourage any
comments, questions,and problems and
experiences with the macro.Particularly, we don't have much
experience using PROC CALIS,but if someone wants to
use this with PROC CALISand give us some feedback
and how the analyses usingthe correlation matrix
from PROC CALIS works out,we'd love to hear about it.Updates in the current version
of the %SURVEYCORRCOV macro aremaintained at that GitHub site.This is my contact information.Like I said, please contact
me if you have any questionsor comments about the macro.Thank you again for SAS
Global Forum for allowingus to present at this time."
69,"Welcome.I'm Richann Watson, And I'm
with DataRich consulting.I have been programming
in SAS for over 20 years,and I am currently
working on a book dealingwith clinical graphics.Today, I'm going to talk
to you about controllingthe appearance of your
graph with colors.So as long as I can remember,
I've always loved to color.And even as an adult, I
have lots of coloring books.And I have map colors and
crayons and colored pencils.So I thought, can I take this
to SAS, my love for coloring?So I'm going to talk today
about how to do colorswithin your SAS outputs.There are different approaches.These are the four
that I typically use.I'm going to be concentrating
today on discrete attribute mapand style template.The other two options are
discussed in the paper.So SAS has over 50 styles
available that you can use.And you can see that they vary
vastly based on color, basedon font, based on size.There are a number of
attributes that vary.And depending on
what your needs are,you can pick from one
of these 50 styles.But sometimes, those styles
don't always meet our needs.So we're going to talk about
how to adjust these colors usingvarious techniques.The first one is using
an attribute map.The colors are
based on a rotation.So if they're missing data or
data is not sorted correctly,your colors can end
up being incorrect,based on how you
want to visualize it.Or they may not be
consistent from each run.How do we control that so
that the colors are alwaysconsistent?You can control that by
using an attribute map.So for this
presentation, I'm goingto focus on building an
attribute map using a SAS dataset and referencing that
attribute map on the PROCSGRENDER.The paper also talks about how
you can build your attributemap within the PROC template
and also referencing itwithin PROC template.But for the presentation,
we'll focus on the dataset and PROC SGRENDER.When you create a data set,
to have the attribute map,you only need top include
the necessary attributes.You can include all of the
attributes that you want,but you only need the
ones that are requiredfor that particular output
that you're trying to produceand for the attributes that
you're trying to control.Here are some of the
types of fields thatare needed in your data set.You have to have
the ID variable,but you have other attributes
such as fill color,line color, marker color.You can include things like
symbol, size, and line pattern.And when you create
your data set,you can have multiple attribute
maps within the same data set.So in this example, I
have two attribute maps.One is called TRTCOLOR,
which is my colors basedon various colors of pink
or various shades of pink.And then I have TRTCOLOR2,
which is the various shadesof turquoise.And so now I have two
different attribute maps.I'm using the ID variable
to uniquely identifythose attribute maps.Now that we've created
this attribute map,how do we get it into
our actual templateso that we can reference
it with our plots?So now that you have
it, you're goingto use your discrete
attrvar within PROC TEMPLATEto now associate the attribute
with the correspondingvariable.So within the discrete
attrvar, thereare three options that need
to be used in order to makethat association.Your first one is
you have the attrvar.And this is actually
the name that you'reproviding for that association.This attrvar is going to be used
in other portions of your PROCtemplate so that you can
actually reference this map.The next is your variable.And you have to now
associate the variable that'sgoing to be used within
the attribute map.And you're going to use
the var statement for that.In our particular
example, our variablethat is going to be
using the attribute mapis our trtan variable,
our treatment variable.And then the attr map specifies
the name of the attribute mapthat is used.So if you recall in
the previous slide,we had two attribute maps
specified in the data set.We had to TRTCOLOR
and TRTCOLOR2.For this particular template
that we're creating,we're going to use the TRTCOLOR.Now that we've specified our
discrete attrvar statement,we can now use that
associated name, the attrvar,and reference that on
our plot statement.So you'll notice
in the bar chartstatement for the grouping,
we are using colorvaras our group variable
versus just using trtan.If we would not have
the attribute map,we would typically use
our treatment variableas our grouping.But because our
treatment variable nowhas this attribute
tied to it, wewant to use that attribute
map associated name, whichis the colorvar association.And now this is going
to tie those colorswith that variable-- that
treatment group variable--so that we get the colors
we want on our outputwhen we go and render it.So now we've created
the template.We've made our
template so that we arereferencing a discrete attrvar.But now we've got to
make sure we actuallyuse that attribute map when
we render that template.So when we go to
call PROC SGRENDER,we're going to
specify on the attrmapoption which
attribute map we aregoing to actually reference.And so we're saying we
want the attribute map thatis stored in the
ADAM libname thathas a data set called attrmap.So it's now going to point
to that particular data setand grab the attribute map
that's stored in there.But what happens if
later on, you decideyou want to change the color?So now my favorite color is no
longer pink, it's turquoise.And I want to change all of
my output to now be turquoise.And so now instead of
going in, I can go inand I can go change my
discrete attrvar statement.So where my attrmap
used to be TRTCOLOR,I can change it to TRTCOLOR2.And now all of my outputs
would have that new attributemap associated with it.And you can do this
if you know that it'sgoing to be a one-time change.But what happens if
you know that youhave to switch back and forth
between two different typesof attribute maps, or
you know that you'regoing to have to be referencing
multiple templates to usethe same attribute map?So we have multiple
templates thatwant to change to TRTCOLOR2.I know in my industry,
we have to doublethe programming for everything.So that means if I
change the template,it has to be fully revalidated.I don't want to have to go in
and touch that template overand over just to change
my attribute map.So there is an easier
way to do this.You can actually do this
on the rendering statement.So when you go to
render that template,you would just now specify
DATTRVAR and specifythe variable and what attribute
map you want to associate with.So you'll see I have
two statements here.We have the first one
referencing the TRTCOLOR.The second one is
referencing TRTCOLOR2.So now I can go back and
forth between the two colorswithout ever having to
change that PROC template.So when I was using
my pink colors,I had my DATTRVAR
equal to TRTCOLOR.I get my nice,
pretty pink colors.And then now somebody
says, I would ratherhave it in turquoise.And now we'll just
change that one statementon the PROC SGRENDER to
DATTRVAR equal to TRTCOLOR2.And so now I have my turquoise.There are other approaches.The next one of them we're
going to be talking aboutis a style template.Typically, when you're dealing
with the style template,you're going to start
with an existing onerather than try to
create one from scratch.This is because the style
template is so complex.There are many styles
that SAS offers.As I mentioned earlier,
there are over 50.And you can determine
which one youlike the best, the one that's
closest to what you need,and start with that as
your starting point,and then modify
only the elementsthat you need to modify.There are different ways
to modify these elements.You can use a modification macro
that is built in within SAS.You can use the STYLE
statement or the CLASS.Statement I am only
focusing right nowon the STYLE statement
and the CLASS statement.There is an explanation on
how to use a modificationmacro within the paper.But before we get
into that, I justwant to go and give
you a little sidebarof why using these different
approaches is beneficial.If you were to, say, out
of curiosity, want to go inand see what's sitting behind
the Styles.Daisy template,you can run proc template
source STYLES.DAISY,and it will actually write all
of the code that is sittingbehind that style to the log.When I ran the code, I saw
850 lines written to the log.And my first reaction was
yikes, that's a lot of codethat I have to shift through
just to try and figure outwhat I want to change.But one of the
things I noticed wasthat it inherited from HTMLBLUE,
which was another style.So then I had to run the
same proc template sourcecode to see what code was
sitting behind HTMLBLUE.When I did that, I noticed
that HTMLBLUE inherited itfrom default. So then I had
to run the same code againfor proc template for default.
And when I did it for default,there was over
1,000 lines of code.Now granted, some
of the lines of codewould be overwritten
for each styledepending on what
you are inheriting.But in short, you're basically
having to change most of thator create 1,000 lines of
code just for one file.Just for graph
colors alone, therewas over 150 lines of code.I personally do not want to
go and try to modify that.I want an easier way.So there has to
be an easier way.So I'm going to talk about some
easier ways to actually changethe style without having to go
through 1,000 lines of code.When you're creating
a customized style,you want to give it a name.You have to give it
some type of name.In this particular example,
I'm calling it mystyle2.And by default,
the style is goingto be saved to the SAS
user.template area.You can actually
save your templateto a permanent
location, especiallyif you're going to have
others within your companyuse the same styling.But for right now, I'm just
using the default, whichis the SAS user.template area.For our data, we
had three groups.And we want each group
to be a different color.We are going to use the
style statement specifiedin this example, just style
GraphData n with our colorequal.When we use this style
as illustrated here,it's basically telling
us that we onlywant to change the color.All the other attributes
associated with thatwill have a non-group
data attribute.This means that
all the other areaswill have the same color, same
marker, same line pattern.We're only changing
the main colorfor that GraphData1,
GraphData2, and GraphData3.Now, right now, that
doesn't make a lot of sense.It will become
clearer as we startexplaining some of
the other styles thatuse different features
or different options.And then when we actually show
the illustration of the graph,it will actually finally click.At least for me,
that's what happened.It was like, oh, eureka,
I figured it out.So this statement here
looks almost the sameas the one we previously did.The difference is is now you
see that we're using styleGraphData from GraphData1.So basically, that's
saying inheritsall of the options or all of
the features from GraphData1--from itself-- but I only
want to change the color.So everything else is going to
be inherited from the parentSTYLES.DAISY, but only
changing the color.So again, this probably
isn't very clear,when you're just hearing
in terms of words.As you say, a picture is
worth a thousand words.And we'll actually show
you a picture in a minute.But let's talk about
the class statement.So now people start seeing
the use of the word class.Class statement looks
exactly the sameas the previous one, which
is just a plain style.But we just replaced a word.So what's the difference
between style, stylewith a from clause, and class?Well, style with a
from clause and classare basically the same.Class has an implicit from self.So basically, class is saying
class GraphData from GraphData.And I realize that GraphData
was typed in twice on the classstatement.It should have only
been typed in once.But essentially, it
has an implicit self.And so it's saying
inherit everythingfrom the parent for GraphData1.Just change the colors.So if you want to inherit
everything for the groupingaspect, you can either use
style with a from or a class.If you only want to change the
color or change the one elementand use a non-grouping, then
you would just use style.Again, this probably
is not very clearuntil you actually
see a picture.So this is style
without the from.So this is just style
GraphData/ color equal.And you'll notice that
we have our pale pink,our pale turquoise,
and our plum,but the lines around our
bar charts are all blue.That's because we only
said to change the color.Everything else, all
the other attributes,are going to be using
non-grouped data.So that's basically saying
take the first colorin our STYLES.DAISY and use that
for all of our unique values.And so the first color
in that rotation is blue.So that's why it is considered
non-group data, because it'sjust going to make everything
look the same in regardsto the contrast, the
lines, and the symbols.But if we use a style with the
from or the class statement,we now see that the bars--the line around the bar charts
are now different colors.Because now, it's
taking and inheritingthe actual attributes from
GraphData1, Graph Data2,and GraphData3 from
the data style.It's only changing
that fill area color.Taking this a bit
further, there'ssome graph options that
use a color responseor a color model.And you may want to use
that so that you don't haveto actually specify a list.You can also use, on the colors
response, a color gradient.And you can actually
change the color gradientthat is the default for
the particular style.When you are going to
change the color gradient,it's a two-color ramp
or a three-color ramp.There are four predefined color
ramps-- the two-color ramp,the two-color alt
ramp, three color ramp,or the three-color alt ramp.If you're using the
two colors, then youhave a start color
and an end color.And you'll see in this example,
I have deep pink and darkturquoise.If you're using a
three-color ramp,you have to include the
option of neutral color, whichis your middle color.So your start color
is your lowest color,and your end color is
your largest color.And so basically, SAS is
going to determine a gradingfrom the start to the end.And if you have a
neutral, that neutralis going to be
your middle color.The color ramps
with alt in the nameare used for
markers, plot lines,while the colors without the
alt are used for the fill areaa.In order to use a color ramp
that you've just defined,you need to actually specify
that on your ODS stylestatement.And so you'll see that we have
this color ramp that we definedin our style called mygradient.I now have to reference the
mygradient on the ODS PDF styleequal.And this is now going
to associate that stylewith that particular output
that I'm going to be producing.I can then now reference
the color model,which was the color ramp alt
ramp, which is basically sayingI want to now do my fill areas.I mean my markers.I'm sorry, my markers.So by default, this is
going to be our DAISY.It's based off of gray as a
starting color and dark blueas the end color.And you'll see that
SAS automaticallycreated a gradient.So it'll show that going
from gray to dark blue,you have a blending
of the two colors.What happens if we have two
vastly different colors, saythe the deep pink and
the dark turquoise?SAS will still do a
blending of the two colors.And me personally, I like
this coloring better.I love bright colors,
and pink happensto be my favorite color,
followed by turquoise.And so this grading,
to me, is much nicerthan the gray to blue.So in conclusion,
you're not limitedto using standard colors
in a predefined style.And there are different ways
in which you can actuallymodify the colors.You have the option on
the BEGINGRAPH statementand the individual
plot statements, whichare defined and outlined in the
paper, the discrete attributemap, and you can
use PROC TEMPLATE.So happy coloring, and I hope
you enjoyed this presentation.Thank you."
70,"GUOHUI WU: Welcome to this
tutorial for SAS Global Forum2020.I'm Guohui Wu at SAS.In this tutorial, we will talk
about the spatial econometricmodeling.I would like to start this
tutorial with two questions.The first question is,
why spatial data analysis?First, we need to consider
spatial data analysis,because spatial data is
abundant in many applications.For spatial data,
we can associateobservations in the data with
different points or regionsin space.When data are collected from
different points or regionsin space, nearby observations
tend to be correlated,as described by the first
law of geography, which says,""Everything is related
to everything else,but near things are more
related than distant things.""From the modeling point of
view, the first law of geographysuggests the need to account
for spatial dependencewhen analyzing spatial data.While classical regression
methods are widelyused in many data analyses,
they rely on the assumptionthat observations are
independent from each other.This assumption of
independence, however,is often unrealistic
for spatial data.Moreover, ignoring spatial
dependence in the datacan lead to biased parameter
estimates and flawed inference.Spatial data analysis,
on the other hand,aims to account for spatial
dependence in the data,to ensure that the resulting
parameter estimatesand inference are correct.My second question
is about big data.In general, the definition for
big data differs from one areato another.For example, big data could
mean the number of observationsin the data is large.It could also mean the number of
variables in the data is large.As far as results are
concerned, our goalis to realize big
values from the data,so that we can turn data
into actionable insights.To uncover hidden
values from big data,we need software packages
with desirable functionalityand scalability to empower us.In this tutorial, I will
discuss some challenges posedby big spatial data and
introduce two SAS proceduresfor spatial
econometric modeling.This tutorial is
organized as follows.We will start with
a brief introductionto the classical
linear regression modeland explain why it fails
to model spatial data.Building on this,
we will introducespatial econometric modeling,
covering key points,including spatial weights
matrix, autocorrelation tests,and a unified
modeling framework.Then we will introduce two
SAS procedures developedfor spatial econometric model,
PROC CSPATIALREG and PROCSPATIALREG.Examples will be
presented to demonstratethe syntax and the usage
of these two procedures.Lastly, we will conclude
this tutorial with a summary.Let's start with the classical
linear regression model.In a multiple linear
regression model,we assume a linear
relationship betweenthe dependent variable y and
some independent variables.This linear relationship can
be described by the equation yiequals xi beta plus
epsilon i, wherebeta is the vector of
regression coefficients.The random error
term epsilon i areassumed to be uncorrelated,
and have a mean zeroand constant variance
sigma squared.Another two key assumptions
are independence and normality.Importantly, the
normality assumptionis only needed for inference.For interpretation,
the regressioncoefficient the beta q
measures the direct impactof an explanatory variable xq
on the dependent variable y,because changes in an
xq for the j observationdo not affect yi if
i is not equal to j.And explanatory
variable xq does nothave an indirect impact on y.Although the classical linear
regression model is often used,it is not suitable for
modeling spatial data.First, the key assumptions
of uncorrelated errorsand independent observations
for linear regression modelscan be violated by spatial
data due to spatial dependence.In addition, spatial data often
exhibits spillover effects,allowing an explanatory variable
to have indirect impactson the dependent variable.When the assumptions break down,
the estimates and inferencefrom the classical
linear regression modelcan be biased and misleading.On the one hand, ignoring
spatial dependenceor endogenous
interaction effect leadsto biased and
inconsistent estimates.As a result, the inference
based on these estimateswill be incorrect.On the other hand, ignoring SAS
error dependence or correlatederrors leads to unbiased
yet inefficient estimators.In this case,
since the estimatesfor the standard errors
associated with the parametersare biased, any
resulting inferencessuch as t-tests and
f-tests will be biased.Spatial econometric
models accountfor various forms of spatial
dependence in the data,to ensure that the resulting
parameter estimatesand inference are correct.The key message here is that
ignoring spatial dependencecomes with substantial
risks, whichis why we introduce spatial
econometric modelingin this tutorial.Before we talk about spatial
econometric modeling,we will first introduce
three types of spatial data.They are aerial data,
point-pattern pattern data,and point-referenced data.For aerial data, the
target variable ysiis an aggregated value over
some finite and fixed area unitswith well-defined boundaries.For point-pattern data,
our target rebel ysioften refers to the
occurrence of certain eventat some random location si.Different from aerial data, the
locations at which we observedata are random and not fixed.For point-pattern data
analysis, the focusis on whether certain
event of interestis equally distributed
over space or not.In other words, we are
interested in the distributionof locations.For point-referenced data,
our target variable ysiis a random realization
of a spatial processat some location si.This locations si
vary continuouslyover some spatial domain.However, the focus
is on inferenceabout the properties
of a spatial process,for example, stationarity,
and on predictionsfor unobserved locations.In this tutorial, we will focus
on spatial econometric modelingfor aerial data.Next, some background
information about spatialeconometrics.Spatial econometrics
is a sub-fieldof econometrics that deals
with spatial interactionand heterogeneity in regression
models for cross-sectionaland panel data.Spatial econometric
models extendstandard econometric models by
including spatial componentsinto the models to account
for spatial interactionand heterogeneity.Two key components in
spatial econometric modelingare spatial weights matrix
and model specification.According to the first
law of geography,data collected across
different units in spaceare often correlated, and the
strength of such dependenceis determined by the proximity
of two spatial units.In spatial econometric
modeling, weuse a spatial weights matrix,
often known as W matrix,to describe the proximity
of two spatial units.In SAS, there are
two procedures forspatial econometric modeling,
the CSPATIALREG procedureand the SPATIALREG procedure.Spatial weights matrix W
plays an important rolein spatial econometric modeling.On the one hand,
we'll use W matrixto describe neighbor
relationships among observationunits.In many practical
applications, Wis an n by n matrix with
non-negative entries, wheren is the number of
spatial units in the data.Wij, or the ij-th
entry of W is positiveif units i and j are neighbors.By conversion, the
diagonal elements of Ware all zeros, because a
spatial unit is not considereda neighbor of itself.On the other hand,
we use W matrixto parameterize
spatial dependencein spatial econometric modeling.In practice, W matrix is
often row-standardized.Next, we'll discuss how to test
for spatial autocorrelationor spatial dependence using both
Moran's I test and the Geary'sC test.Moran's I statistic is
defined by the first equationon this slide.In this equation,
n is the numberof spatial units in the data.W is the spatial
weights matrix, and yis the variable of interest.Moreover, y bar
is the mean of y.As we can see, the numerator
in Moran's statisticinvolves the cross product
between the deviation of eachof observation yi from the
mean, and the deviationof neighboring
yj's from the mean.Under the null hypothesis of
no spatial autocorrelation,the expected value of i is
negative 1 over n minus 1,which goes to 0 for large n.The test score Zi is given
by the second equation here.Asymptotically, Zi follows the
standard normal distribution.To compute Zi, we
need to computethe variance of
i, which is oftendetermined under the assumption
of normality and randomization.Under the normality
assumption, weassume that the
variable y followsthe normal distribution,
whereas for the randomizationassumption, we assume
that the values of yare equally distributed
over the spatial domain.To interpret Moran's i, we
compute a Z-score and p-value.When p-value is significant,
a positive Z-scoreindicates positive
autocorrelation,whereas a negative
Z-score indicatesnegative autocorrelation.For Geary's C test,
the test statisticis defined by the first
equation given here.According to this equation,
Geary's C test statisticuses squared difference
of neighboring valuesand is always non-negative.Same as for Moran's I test, the
test score for Geary's C testalso follows the standard normal
distribution asymptotically.Under the null hypothesis, the
expected value of C is one.Similar to Moran's
I, the variance of Ccan be computed
under the assumptionof normality or randomization.To interpret Geary's C,
a C-value significanceless than one indicates
positive autocorrelation,whereas a C-value
significance greater than oneindicates negative correlation.This is contrary
to Moran's I test,where a significant and positive
Moran's I index indicatespositive autocorrelation,
and a significantand negative Moran's
I index indicatesa negative correlation.As a result, Geary's C is often
said to be inversely relatedto Moran's I.Moran's I and Geary's C
tests show some similarities.First, neither of
these two testsis a model specification
diagnostic.What this means is that when
we reject the null hypothesisof spatial randomness, neither
Moran's I or Geary's C testtells us which models
should be used to addressspatial dependence in the data.Second, both Moran's
I and Geary's C testsdepend on the spatial weights
matrix that we choose.The test results can be
sensitive to different choicesof spatial weights matrix.Although test
scores in both testsfollow the standard normal
distribution asymptotically,results are not robust when
the sample size is small.Despite these
similarities, thereare some major differences
between these two tests.On the one hand, Moran's I
computes the cross productbetween the deviation of
neighboring observationsfrom the mean, whereas Geary's C
computes the squared differenceof neighboring observations.Since mean is sensitive
to extreme values,Moran's I test is
sensitive to outliers.On the other hand,
Moran's I testfocuses on global
autocorrelation,whereas Geary's C test focuses
on local autocorrelation.Neither Moran's I
or Geary's C testis a model specification
diagnostic.Therefore, a natural
question to askis, which model should
we pick to accountfor spatial dependence
in the data?To answer this question, we
now discuss a unified frameworkfor spatial
econometric modeling.In spatial econometric
modeling, spatial dependencecan arise from three different
sources, exogenous interactioneffect, endogenous interaction
effect, and the interactionamong the errors.For exogenous
interaction effect,the value of the dependent
variable y for unit idepends on the value of
an explanatory available xin another unit j.For endogenous
interaction effect,the value of y for unit
i depends on the valueof y in another unit j.Lastly, the interaction
among errorsmeans that errors in different
units are correlated.The understanding of different
forms of spatial dependenceis crucial for spatial
econometric modeling.This unified modeling
framework canhelp us understand many
spatial econometric modelsthat we will discuss later.To prepare data for spatial
econometric modeling,we need to obtain
geographic informationfor spatial units in the data.For example,
geographic coordinatessuch as longitude
and latitude values.To manage and
manipulate spatial data,we can use the geographic
information systemor some procedures in SAS.As an example, the
GEOCODE procedure in SASenables us to add geographic
coordinates to an address.With some geographic
information,we can determine how
observation units in the dataare spatially related.For example, longitude
and latitude valuescan be used to compute
the distance between twounits, which can then be used
to determine their neighborrelationships.In spatial econometric
modeling, weuse a spatial weights matrix to
describe how observation unitsare spatially related.After preparing the
data, we can proceedwith model specification
and model fitting.Now, we demonstrate how
to convert an addressto longitude and latitude values
using PROC GEOCODE in SAS.As an example, we
consider a dataset named Customers, which
includes four variables,address, zip, city, and state.In this data set,
the variable addressrefers to street address,
and the variable ziprefers to zip code.The other two variables,
city and state,refers to the name of the city
and the state respectively.Each observation
in the data refersto a location that
can be identifiedby the street address, zip code,
and both the city and the statenames.The lower right panel
shows you the SAS codeusing PROC GEOCODE.The method=street option
specifies the street geocodingmethod.The lookupstreet= option
specifies street matching dataset.With street geocoding
method, the GEOCODE procedurefirst attempts to match the
street name and zip code.If it fails, the
GEOCODE proceduremakes the second attempt, to
match the street name, cityname, and the state name.The data= option specifies the
input data set whose addressobservation that
you want to geocode.The longitude and the
latitude variablesfor the geocoded street
location of the matchwill be added to the
specified output data set.Next, we discuss how to create
a spatial weights matrixW for spatial
econometric modeling.As we have already mentioned,
the spatial weightsmatrix W describes neighbor
relationships among observationunits in the data.In its simplest form, you can
think of a spatial weightsmatrix as a binary
matrix, with oneto represent a neighbor
relationship between two units,and zero otherwise.To determine if two units
are neighbors or not,we need some neighbor criteria.In practice,
neighbor criteria canbe based on contiguity,
distance, and many morefactors.In the next few
slides, some exampleswill be given to show how
to create a spatial weightsmatrix based on the contiguity
and distance criteria.First, we talk about k-order
contiguity matrix basedon the contiguity criteria.For the contiguity criteria
being discussed here,two units are neighbors if
they share a common border.For k-order binary
contiguity matrix C,the ij-th entry of
C, or Cij, is oneif units i and j are k-order
neighbors, and zero otherwise.For k equals one, we have
first order neighbors.In this case, Cij is
one if units i and jshare a common border,
and zero otherwise.For the general case where
k is greater than one,we say unit j is a
k-order neighbor of unit iif unit j is a neighbor
of k minus one orderneighbors of unit i.As an example, consider the
graph on the left panel.In this graph,
five spatial unitsare labeled from L1 to L5.For unit L1, it has two first
order neighbors, L2 and L4,because both L2 and L4 share
a common border with L1.As a result, for the
spatial contiguity matrixC shown on the bottom, both
the second and fourth elementin the fourth row of the matrix
C is one, and the rest is zero.For unit L2, it has two first
order neighbors, L1 and L 5,because both L1 and L 5 share
a common border with L2.Accordingly, the first
and the fifth elementin the second row of the matrix
C is one, and the rest is zero.Similarly, we can figure
out the remaining elementsin the matrix C. Based on
this contiguity matrix C,we can get a row-standardized
spatial weights matrix W,by dividing elements in
each row of the matrix Cby the sum of that row.The key to creating
first order spatialcontiguity matrix is to
identify spatial units thatshare a common border.The example presented
on this slideshows how to create a
list of North Carolinacounties that share a
common border using SAS.We start by creating a map data
set for all counties in NorthCarolina.In step one, we create
a map data set and namedmap_nc for all counties
in North Carolina,using a SAS-supplied map
data set named counties.This SAS-supplied map data set
contains geographic informationfor all counties in the US.In the where statement, we
use the fixed state code of 37to select only North
Carolina counties.After step one is
completed, the dataset map_nc has the
mapping informationfor all counties
in North Carolina,including longitude
and latitude values.In step two, we
create a variable x_yas a stream representation
of longitude-latitude paircorresponding to
each observationin the data set map_nc.Next, we identify two
neighboring countiesthat share a common border
by using the variable x_y.The idea is that for
two counties sharinga common border, the
variable x_y correspondingto points on a common boundary
will have duplicate values.Building on this
idea, we first sawthe data set created in step
two by the variable x_y usingPROC SORT.And the resulting
sorted data is writtento a new data set named temp.With the nodupkey and dupout,
any duplicate observationsthat have the same values
for the x_y variablewill be written to a new
data set named border_lonlat,but eliminated from
the data set temp.By matching values
of the variable x_yin the two data sets named
border_lonlat and temp, usingPROC SQL, we create a new
table named boderinga,which contains a
list of countiesthat share a common border.The remaining PROC
SQL code is to ensurethat neighbor
relationship definedby the contiguity
criterion is symmetric.Next, we discuss
k-nearest-neighbor spatialweights matrix based on
some distance criteria.In this case, we need to
define some distance measureto compute the distance
between two spatial units.For example, a
distance measure couldbe Euclidean distance
or geodetic distance,computed using the
longitude and latitudecoordinates of two units.The right panel
shows how to createthe k-nearest-neighbor
weight matrix C step by step.In step one, we
begin by computinghow wide distance dij
between a spatial uniti and any other unit j.In step two, we sort
dij's in ascending order.Then we get a set of
k-nearest-neighborsfor unit i in step three.In step four, we set the ij-th
entry in the weight matrix Cijto be one, if j is one of
the k closest units to i,and zero otherwise.Unlike contiguity-based
neighbor relationshipsthat are symmetric,
neighbor relationshipsbased on a
k-nearest-neighbor criterionare not guaranteed
to be symmetric.This is because the
neighbor relationshipbased on the k-nearest-neighbor
criterion is directional.There are a few SAS
procedures that youcan use to create a
k-nearest-neighbor spatialweights matrix.This slide shows two examples.The first example
shows how to obtaina list of k-nearest-neighbors
using PROC MODECLUS.The MODECLUS procedure clusters
observation in an input dataset using some non-parametric
density estimation-basedalgorithms.The second example shows how
to use PROC DISTANCE to computethe distance
between observationsin a data set based
on proximity measuressuch as Euclidean distance,
shape distance, and many more.You can refer to the user
documentation of these twoprocedures for more information.If you want to create
a spatial weightsmatrix based on some
customized neighbor criterion,SAS/IML can be used.Now, we discuss two
challenges posedby big data for spatial
econometric modeling.By big data, we mean the number
of observations n is large.Memory-wise, it could
take a lot of memoryto store a full W matrix,
especially if n is large.For example, it takes about
30 gigabytes of memoryto store a full W matrix
for 65,000 US census tracts.Computation-wise, the
maximum likelihood estimationfor spatial econometric
models requiresthe inverse and the log
determinant of some nby n matrix.This computation becomes
prohibitive for large n.To reduce storage cost, we
can use compact representationof the W matrix.This compact representation
of the W matrixstores only nonzero
weights in the W matrix.The use of compact
representationis advantageous
when the W matrixis sparse, which
is often the casein many practical applications.To resolve computational
challenges for big data,two approximation methods,
Taylor approximationand Chebyshev
approximation, can beused to speed up computation.The key to the compact
representation of the W matrixis to store only its
nonzero weights togetherwith their indices.You can think of the compact
representation of the W matrixas a matrix with
the number of rowsbeing the number of nonzero
weights in the W matrix,and the number of
columns being three.One of these three columns
stores those nonzero weights,whereas the other two columns
store the row and columnindices corresponding
to the nonzero weights.As an example, we consider
the row-standardized spatialweights matrix W
discussed before.For the first row of the
W matrix correspondingto unit L1, there are two
nonzero weights, both of whichare 0.5.These two nonzero weights
corresponding to weightscarried by units L2
and L4 on unit L1.As a result, the first two rows
in the compact representationof W matrix reads L1,
L2, 0.5, and L1, L4, 0.5.Comparing the full W matrix
with its compact representationto the right, we see that
the compact representationavoids storing many zero
elements in the W matrix.As the W matrix gets sparse,
using compact representationcan significantly
reduce the storage cost.In addition, using
compact representationalso reduces computation time
by eliminating operationson zero elements.The maximum
likelihood estimationfor spatial econometric models
discussed in this tutorialrequires the log determinant
of a matrix of the formI minus rho times W, where
I is an identity matrix,rho is a scalar parameter, and
W is the spatial weights matrix.To resolve computational
challenges posed by big data,we briefly introduce two
methods for approximatingthe log determinant of
I minus rho times W.First, for any square matrix A
satisfying certain conditions,the log determinant
of A equals the traceof the logarithm of
A. Based on this,Taylor approximation for
the required log determinantinvolves only computing the
traces of powers of the Wmatrix, whereas Chebyshev
approximation for the requiredlog determinant
involves computingthe traces of Chebyshev
polynomials of the W matrix.Between these two
approximation methods,Taylor approximation has
a good local performance,whereas the Chebyshev
approximationhas a good uniform performance.This two approximation can
speed up the computationsignificantly, by reducing the
problem of computing the logdeterminant of a large matrix
to the problem of computingtraces of the powers
of the W matrixor Chebyshev polynomials
of the W matrix.Now, let's talk about
two SAS proceduresfor spatial
econometric modeling.The table presented
here includesall spatial econometric models
supported by PROC CSPATIALREGand PROC SPATIALREG.To relate these models to the
unified framework discussedbefore, endogenous
interaction effectsare accounted for by
including spatiallylagged dependent
variable Wy in the model.Moreover, exogenous
interaction effectsare accounted for by
including spatiallylagged explanatory
variables Wx in the model.In addition, interaction
in the error termsare modeled using either
an autoregressive or movingaverage model.All but one models presented
in the table accountfor either one form
of spatial dependenceor a combination
of the three forms.For example, spatial
lag of X model accountsfor only exogenous
interaction effects,but including spatially
lagged explanatory variablesin the model.Spatial Autoregressive,
or SAR model,accounts for endogenous
interaction effects butincluding Wy in the model.To account for correlated
errors, both spatial movingaverage model and spatial
error model can be used.Among these models, both
SDAC and SDARMA modelscan be used to
account for all threeforms of spatial dependence.On rho, interaction
in the error termscan be modeled using
either a spatial movingaverage structure or spatial
autoregressive structure.These two different
structures have very differentimplications.As can be seen from these
two error structures,the spatial moving
average structureemphasizes low order
neighbors, whereasthe spatial autoregressive
structure emphasizes high orderneighbors by including higher
powers of the W matrix.Spatial dependence
in the data not onlyviolates the assumptions for
the classical linear regressionmodel, but also leads to
different interpretationfor parameters.In spatial econometric
models, the direct, indirect,and the total impacts
are used to measurethe impact of
explanatory variableson the dependent variable.For the direct the impact
of explanatory variable xq,it is defined by the derivative
of yi with respect to xiq.In spatial econometric
models, changes in xiqmay impact the yi through a
combination of within regionand the neighborhood influence.The neighborhood
influence arisesfrom impacts passing
through neighboring unitsand back to the unit itself,
while the indirect impactof an explanatory
variable xq, describedby the derivative of
yi with respect to xjq,measures the impact of a change
in xjq from unit j or yi.In spatial econometric
models, indirect impactscan arise from
spillover effects.The total impact of an
explanatory variable xqis the sum of the direct
and indirect impacts of xq.The direct, indirect,
and the total impacts,vary with the spatial units.As a result, the average of
direct, indirect, and totalimpact over all
spatial units is usedas a summary measure of the
varying direct, indirect, andtotal impact respectively.Now, consider spatial
Durbin model as an example.For this model, the derivative
of yi with respect to xiqequals to the i-th diagonal
element of the matrix SqW,which depends on
the parameter row,regression coefficients
beta q and gamma q,and the spatial weights
matrix W. For rho equals zero,spatial Durbin model reduces
to a spatial lag of x model.And the direct impact of
an explanatory variable xqequals beta q.However, if rho is not equal to
zero, the direct impact of xqvaries with spatial units,
and it is not equal to beta q.As a result, for spatial
econometric models that includespatial lag dependence,
beta q cannot be interpretedas the direct impact of
an explanatory variable xqon the dependent variable y.Similarly, for the
indirect impactof an explanatory variable
xq, the derivativeof yi with respect to xjq
equals to the ij-th entryof the matrix SqW.For rho equals zero,
the indirect impactof an explanatory variable
xq equals gamma q.However, if rho is
not equal to zero,the indirect impact of an
explanatory variable xqis not gamma q.The take home message
from this slideis that for spatial econometric
models include spatial lagdependence, parameters
in the modelhave a very different
interpretationfrom those in the classical
linear regression model.We need a summary measures of
the direct, indirect, and totalimpacts, to quantify the
varying impacts of changesin explanatory variable.The average direct impact
measures the impactof changes in explanatory
variable xq in a given region,on the dependent variable
y in the same region.It is calculated as an average
of the diagonal elementsof the matrix SqW.The average total impact
measures the total impactof changes in explanatory
variable xq in all regions,on the dependent variable
y of a typical region.It is calculated as
an average of the sumsof each row in the matrix SqW.The average indirect
impact of xqis the difference between the
average total impact of xqand the average
direct impact of xq.It measures the
impact on y of a givenregion arising from changes
in xq in all other regions.Now, let's talk
about two proceduresfor spatial econometric
modeling in SAS.First, let's look at the
syntax for PROC CSPATIALREG.The CSPATIALREG procedure
takes two input data tables.The data= option names
the primary data table.This primary input data table
contains dependent variables,explanatory
variables, and so on.The WMAT= option names the
secondary spatial weights datatable, which contains
spatial weights information.We can provide either
full spatial weightsmatrix or the compact
representationof a spatial weights matrix.The APPROXIMATION= option
controls options that arerelated to Taylor and Chebyshev
approximation methods.We can use either the keyword
Chebyshev or Taylor to specifyan approximation method.By default, Chebyshev
approximation is used.The IMPACT= option specifies
options related to impactestimation.The model statement specifies
the dependent variableand independent covariates
for the regression model.The TYPE= option in the model
statement specifies the typeof model.In addition, the
SPATIALEFFECTS statementenables you to specify
covariates for which spatiallylagged variables
will be created,to account for exogenous
interaction effects.Lastly, the SPATIALID statement
specifies an ID variablethat identifies a spatial unit.The syntax for
PROC SPATIALREG isvery similar to that
for PROC CSPATIALREG.Unlike PROC CSPATIALREG,
that supports only one modelstatement, we can have more
than one model statementsin PROC SPATIALREG.This is very useful
because it allowsus to fit many different spatial
econometric models by callingPROC SPATIALREG only once.Although the SPATIALREG
procedure and the CSPATIALREGprocedure have very
similar syntax,there are some major differences
between these two procedures.First, PROC CSPATIALREG
operates on multiple threadson multiple machines,
whereas PROC SPATIALREGoperates on a single machine.In terms of functionality,
impact estimationis supported in PROC CSPATIALREG
but not in PROC SPATIALREG.In addition, multiple
model statementsare supported in
PROC SPATIALREG,but not in PROC CSPATIALREG.Let's look at two examples.In the first example, we
consider a simulated data setthat contains car sales of
some dealership in each of 100counties in North Carolina.There are three variables
in this simulated data set.The dependent variable
y refers to revenueof each car dealership.Two explanatory variables
are x1 and x2, both of whichare important predictors
for car sales revenue.For example, x1 and x2 might be
standardized median householdincome and unemployment
rate respectively.For data generation,
we're forcedto create a spatial
weights matrixW for counties in
North Carolina,based on a contiguity criterion,
used in this W matrix.We then simulate data
from an SDAC model,with two parameter values
specified for the model.The values of x1 and
x2 for each countyare simulated from the
standard normal distribution.For random error
attempts epsilon i,they are simulated from a
normal distribution with mean 0and variance 0.2.There are three
main goals that wewant to achieve by analyzing
the car sales data set.First, we want to know whether
car sales revenue of a cardealership in one
county is affectedby car sales revenue
of car dealershipsin neighboring counties or not.Second, we want to
identify a modelthat best describes
the car sales data setusing certain criterion.Third, we need to understand how
the two explanatory variablesx1 and x2 affect
car sales revenue.Loosely speaking, the
first and second goalis related to model
specification and modelselection respectively.The third goal, however,
is about inferenceor the interpretation of
parameters in the model.We will demonstrate how
to address these threeanalytical goals through spatial
econometric modeling usingPROC CSPATIALREG
and PROC SPATIALREG.Before we fit a model to
the car sales data set,we visualize the data by
plotting car sales revenuefor North Carolina
counties on a map.From this plot, we can
see that car sales revenuefor neighboring counties
seem to be similar,which is an indication of a
positive correlation in carsales revenue.To formally test spatial
autocorrelation for car salesrevenue in the car
sales data set,we consider both Moran's
I and Geary's C tests.The table on the
top shows the resultfrom Moran's I test,
and both the normalityand the randomization
assumptions.Based on the p-values
under the two assumptions,we reject the null hypothesis
of zero spatial autocorrelationin car sales revenue at
the 5% significance level.Furthermore, the
positive Z-scoresindicate a positive
autocorrelation.The second table shows the
result from Geary's C testand both the normality and
the randomization assumptions.Based on the p-values
and the two assumptions,we reject the null hypothesis
of zero spatial autocorrelationin car sales revenue at
the 5% significance level.In addition, Geary's C
index is less than oneon both assumptions,
which indicatesa positive autocorrelation.To summarize, both the
Moran's I and Geary's C testssuggest a positive
spatial autocorrelationin car sales revenue across
100 North Carolina counties.Now, we fit two models to
the car sales data set.The first model we fit is
a linear regression model.This model includes two
explanatory variables, x1and x2.The other model we
fit is the true model,which is an SDAC model.The table on the left shows
the parameter estimatesfrom the linear
regression model,whereas the table on the
right shows the parameterestimates from the true model.From the parameter estimates
table for the true model,we see that the estimate
for parameter rhois significantly
different from zeroat the 5% significance level.As a result, parameter estimates
from the linear regressionmodel on the left are
biased and inconsistent.The bias in the
parameter estimatesfrom the linear
regression model canbe checked by comparing
this parameterestimates with the true values.For example, the estimate
for the interceptfrom the linear regression
model is about 7.74,which is about twice as large
as the true value of 3.0.Moreover, the estimate of
the regression coefficientcorresponding to x1 and x2 in
the linear regression modelis about 1.94 and negative
0.95 respectively,which is larger than the
respective true value of 1.5and negative 0.6
in absolute value.Typically, non-spatial
models tendto attribute variation
in the dependent variableto the explanatory variables,
leading to large parameterestimates in absolute value.To assess the impact of the
two explanatory variablesx1 and x2, we present the
average direct, indirect, andtotal impact estimates
from PROC CSPATIALREG.The two tables shown here are
the parameter estimates tableand impact estimates summary
table from the true model.The comparison between
these two tablesshow that the estimate of the
regression coefficient for x1is different from any of the
three impact measures for x1.From the impact
estimate summary table,we can see that both the direct
and indirect impacts of x1are positive and significant
at the 5% significance level.This suggests that we would
see increased car salesrevenue in counties
experiencing an increasing x1.The indirect impact from
x1 in neighboring countiesis about twice as large
at the direct impact,suggesting a large
spillover impact from x1.The total impact of x1 is
positive and significantat the 5% significance
level, with about 2/3of the total impact contributed
by the spillover effectfrom x1 in neighboring counties.Similarly, we can see that the
direct and indirect impactsof x2 are all negative
and significantat the 5% significance level.This suggests that we will see
decreased car sales revenuein counties experiencing
an increase in x2.The indirect impact from
x2 in neighboring countiesis about three times as
large as the direct impact.This suggests that there is a
large spillover impact from x2.the total impact of x2 is
negative and significantat the 5% significance
level, with about 3/4of the total impact contributed
by the spillover effectsfrom x2 in neighboring counties.According to the impact
estimate summary table,we can conclude that
increase in x1 by one unitfor all counties leads to a
total increase in car salesrevenue by 5.6 units.Moreover, increase in x2
by one unit in all countiesleads to a total decrease in
car sales revenue by 3.2 units.This slide shows
the SAS code thatwas used to fit the true model
for the car sales data set,and to request the
impact estimate summarytable using PROC CSPATIALREG.The use of type=SAC option
in the model statement,together with the
spatialeffects statement,specifies an SDAC model.The spatialid statement
specifies county nameat the spatialid variable.The test statement specifies
a joint hypothesis,rho equals zero and lambda
equals zero, to be tested.In addition, the type=wald lr
option in the test statementrequest the joint hypothesis
to be tested using both Waldand likelihood ratio tests.In the output statement, the
out= option names the outputdata table.The resid=, pred=, and
copyvar= option request threeobservation y statistics,
residual, fitted value,and the spatial ID variable to
be included in the output datatable respectively.To select a model that best
described the car salesdata set, we consider model
selection using the AkaikeInformation Criteria, or AIC.We fit all 12 models supported
in PROC CSPATIALREG and PROCSPATIALREG, with both
x1 and x2 includedas explanatory variables.The table on the left
shows the AIC valuesfor all 12 models
being considered.Based on these AIC
values, we concludethat the true model is the
best model, since its AICvalue is the smallest.Among these 12
models, SDARMA modelhas the second smallest
AIC value of 143.8.The main difference between
SDAC and SDARMA modelis that SDAC model uses
a spatial autoregressivemodel for disturbance,
whereas as SDARMA model usesa spatial moving average
model for disturbance.Two tables on the right
show the test resultsfor the joint
hypothesis, rho equalszero and lambda equals zero,
from SDAC and SDARMA models,using both Wald and
likelihood ratio tests.Based on these test results,
we reject the null hypothesisat the 5% level.This leads to the
conclusion that thereis some spatial
dependence in the data.On this slide, we compare
three summary measuresof direct, indirect, and
total impacts for twoexplanatory
variables, x1 and x2,in both SDAC and SDARMA model.The first impact estimate
summary table correspondsto SDARMA model, whereas the
second impact estimates tablecorresponds to SDAC model.Despite the difference between
SDAC and SDARMA models,the comparison
between the two impactestimates summary table
shows that the summarymeasures of direct, indirect,
and total impacts of twoexplanatory variables, x1
and x2, from the two modelsare very similar.The key message to
be conveyed hereis that two different spatial
econometric models can havea very similar interpretation.Now, we summarize some
analytical insightsthat we have gained by analyzing
the car sales data set.First, we conclude that there
is a positive spatial dependencein car sales revenue, because
the estimate for the parameterrho in the true model is
positive and significantat the 5% significance level.In other words,
car sales revenuefor dealership in one county
affects car sales revenuein its neighboring counties.Second, we identified the
true model as the modelthat best describes the
data among all 12 modelsthat we have considered,
using the AIC.Third, we find that x1 has
positive and significantdirect, indirect, and
total impacts on car salesrevenue, whereas x2 has
negative and significant direct,indirect, and total impacts
on car sales revenue.Moreover, an increase of x1
by one unit in all countiesleads to a total increase in
car sales revenue by 5.6 units.In contrast, an increase of
x2 by one unit in all countiesleads to a total decrease in
car sales revenue by 3.2 units.To demonstrate that the
CSPATIALREG and SPATIALREGprocedures can
scale to big data,we consider a simulated data
set in our second example.In this second example, we
create a spatial weightsmatrix W for 64,999 census
tracts in a 2000 US censusbased on the three nearest
neighbor criterion.Using this W, we then
simulate the datafrom the true model, which is
a spatial autoregressive or SARmodel, using the two
parameters specified here.For this simulated data set,
storing the full W matrixfor 64,999 census tracts
requires about 30 gigabytesof memory.Moreover, fitting the true
model to this simulated dataset using exact computation
is almost infeasible.With the compact representation
and two approximation methodsimplemented in
both procedures, wecan feed the true model
to this simulated dataset using either procedure
in about a minute.As an example, we present
the parameter estimates tablefrom the true model using
PROC CSPATIALREG, whichis given on the right.The comparison between the
resulting parameter estimatesand the true values shows that
these parameter estimates arevery close to the true values.To interpret the
parameter estimates,we present the summary
measures of direct, indirect,and total impacts for
all explanatory variablesfrom the true model.According to the impact
estimate summary table,we can see that x1 has negative
and a significant directand indirect impact on
the dependent variable y.This suggests that we
would see a decrease in yin census tracts, experiencing
an increasing in x1.The indirect effect of increase
in x1 in neighboring censustracts is negative
and significantat the 5% significance level.This suggests that the
value of y in a census tractis negatively impacted
by increase in x1in neighboring census tracts.In addition, the
total impact of x1is negative and significant
at the 5% significance level,with about 1/3 of this comprised
of the spillover effectsfrom x1 in neighboring
census tracts.Similarly, we can interpret
the summary measuresof direct, indirect,
and total impactsfor the rest of
explanatory variables.On this slide, we
compare the valuesof the dependent variable y
in the simulated data set,with the in sample predicted
values from the estimated SARmodel.The figure on the left
plots the values of yin a simulated data set, whereas
the figure on the right plotsthe in sample predicted values
from the estimated SAR model.The comparison between
these two figuresshows that the in
sample predictedvalues capture the overall
pattern in the simulated dataset very well.This is expected because
the parameter estimatesfor the true model
from PROC CSPATIALREGare very close to
the true values.To conclude, we introduced
spatial econometric modelingfor cross-sectional
data in this tutorial.Spatial econometric models
address different formsof spatial dependence
in the databy including spatially
lagged dependent variable,spatially lagged
explanatory variables,and correlated errors
in regression models.These spatial econometric
models are parameterizedby using spatial
weights matrices thatdescribe neighbor relationships
among spatial unitsin the data.To fit spatial
econometric models,both CSPATIALREG and
SPATIALREG procedures in SASwere discussed.These two procedures
allows us to considerparameter estimation, hypothesis
testing, and impact estimationfor commonly used spatial
econometric models.To resolve the challenges
posed by big data,both CSPATIALREG and
SPATIALREG proceduressupport the compact
representationof spatial weights matrices,
and two computational efficientapproximation method.This slide includes
most references thatwere cited in this tutorial.You can refer to
these referencesif you want to know more
about the topics introducedin this tutorial.Thank you for watching
this tutorial.If you have any questions,
you can reach me by emailat guohui.wu@sas.com."
71,"Hi.I'm Olivia Wang from SAS China.I'm very glad to be here to
demonstrate our SAS VisualDefect Detection System with
SAS deep learning and ESP.My team members Jackie
Hu and Tianlun Gucontributed a lot in
building this system.Really appreciate their
efforts and creativity.In steel manufacturing,
the downstream costdue to defects in the steel
sheet can be expensive.Steel manufacturers are
seeking new AI-based solutionto visually inspect
the steel sheetand improve on today's steel
sheet defect classificationmodels.Using Viya image
analytics and ESP,the system demonstrated
how this can be done.The idea of our system
is coming from PoCfor one of the largest
steel company in the world.The major three business
challenges they are facingare first, improve the
efficiency of the inspector'smanual work by automating
the process, and second,improve the steel sheet defect
classification accuracy rateby computer vision.And most importantly,
how to achievecontinuous optimization, how to
embed experts' domain knowledgeinto the model and apply it
into production line seamlessly.We solve all these
problems in mind.We designed this visual
defect detection systemto connect the people,
technology, and process.Our system consists
of two parts--real-time monitor and
continuous optimization.This is our real-time monitor.It's designed to help
quality inspectorsmonitoring their production
line in a real-time manner.Quality inspectors can
choose the product linethey want to monitor.To simulate a real
situation, we made a videoby pictures taken from
the production lineto demonstrate how it works.As you'll see, immediately,
the monitoring summaryshows total number of
defects and defect typesand total labor hours
released becauseof this adorable, smart,
automatic real-time system.The numbers keep changing
with the running videoto demonstrate the system is
doing real-time deep learningscoring and getting results.The pie charts can help
the inspector monitoringwhether there are huge
amounts of certain defectsare increasing abnormally.For example, if the red
iron defect is continuouslydetected, it will raise the
attention of the inspectors.In this video, each
image shown hererepresents a defect
that's detected and scoredby the system.On the right is the image
ID and defect label.SAS model predicts
the ruler couldhelp the inspectors
to get the locationand size of the defects.The lower part is
the defect repositoryto show the most recent
scoring results of each typeand keep updating.The sentence beneath
the images canhelp the junior ones to learn
the number of observed defectcharacteristics.So this system can not
only help inspectorsto monitor their product
line in a real-time manner,but also can be an online
enablement platformfor their junior staff.So this is the
real-time monitor part.Then let's go to the
continuous optimization part.It's designed to help the
customer to embed domainknowledge into the system to
make it smarter and smarter.Quality analyst can set
the selection criteria hereto find defect pictures
from the repositoryand scroll up and down to get
the one they plan to work with.Then the defective
pictures comes herein the middle to show
him or her the labeland the model scores,
probability of each type.Because some of the
defects are so similar,the analyst tends
to have the intereston the defects which the top
two probability are quite close.So I input 0.01 in
the tolerance of fieldjust now to search for those
pictures model most likelymade mistakes.For example, this.The defect label
is black stripe.But by analyzing the image, the
analyst think it's black line,actually.So he can click on black
line then click change.Then the label of this
defect is modified.We can embed all these domain
knowledge into the modelby just click between.It may take us some time.After the model we
trained finished,this learning curve shows the
training and validation lossduring the training
of the model.A learning curve that can help
analysts to find out the modelis underfit, or
overfit or good fit.We can easily judge whether this
is a good or bad model basedon the curve shape.Now you can
understand how easy weconnect the people, technology,
and process with SAS Viya.By providing this
visual, interactive,and real-time application,
inspectors and analystscan work in one application to
better understand each other.And also by providing model
result in a simple way,we can help customers to
know how AI model works.And it really can help our
business user to adopt AI.Label modification and
model retrain functioncan make the solution
achieve rapid iteration,learn from the
expert, and benefitingquickly and continuously.This is our system architecture.We use SAS Viya 3.5 demo
image from easy demo centerto build a system.The user interface is
designed for business userwith JavaScript.As you see in the demo,
when video comes in,JavaScript collect the
pictures from videoand sent it to ESP
server through ESPJS.Then the image stream will be
loaded into ESP source node.Then calculate node
will do image resizingand send to score node,
where the pictures willbe scored with our ASTORE file.This ASTORE file is loaded
from model reader node.In this demo, we only show
four types of defects,but our capability is
much more beyond that.After scoring, the result
will be sent to the UI.When training
requires being scored,Viya VDMML will
retrain the model.Then the ASTORE file is
updated for future scoring.After retrain finished--
it will take a while--the feed history diagram will
be shown on the UI for analyststo evaluate the
model performance.You can see it's
a great framework.Can apply to not only steel
manufacturing, but alsoalmost all industry.We can help the taxi company
monitoring driving behavior.We can help the insurance
company make a better and quickdecision on car damage claims.And we can also help on security
monitoring, natural disaster,first aid action
taking, et cetera.By doing so, we can not only
help our customers' business,but also make a better world.We can also package our
ESP project from serverinto XML file and deploy
it on Edge device.Then the customer can
monitor their production linein real time with Edge device,
like Nvidia TX2 or others.So we can not only support you
deploy the system on Cloud,but also, we can easily
deploy it on Edge devicewithout extra development work.And we can also embed our
continuous optimizationinto SAS Viya VA and
support quality analyststo modify the label
and retrain the modelthrough a mobile device.This demonstrates our AILT
capability of develop once,deploy anywhere.Special thanks to Xiangqian
Hu and Wenyu who help us a lotin the PoC's And also
thanks to Wildtrack project,which inspired my team to
make this real-time visualapplication.With this system,
more value can beprovided to the business user.So thank you very much.If you have any questions,
feel free to contact us."
72,"Hi, my name's Kate Parker.And I lead the Customer
Experience Transformationteam within SAS's Global
Customer Intelligence practice.Today, our topic is customer
experience transformation--connecting planning, execution,
performance, and analytics.The customer is the center
of a marketer's universe.And marketers spend time
getting to know their customers,growing relationships
with them, retaining them,driving loyalty, and at
times, winning them back.The experience
that organizationscreate for its
customers is oftentheir only differentiator.In fact, an
organization's successrelies on the
experience that theycreate for their customers.When it comes to
engaging with customers,marketers focus on the journeys
that they want to enable.The decisions marketers
make about each customerat every stage of the
journey ultimatelydictates the quality
of their experience.While there are many drivers
behind effective customerexperience and
engagement, today we'regoing to focus on how leveraging
SAS's technology createsa differentiating
customer experienceat every step of the journey.Today, we're going
to focus on the valuein connecting, how to
strategically plan and managemarketing efforts across
the organization moreeffectively and efficiently,
how to determinewhich customers to engage
with, when, and how,how to leverage analytics
to optimize marketing spend,and finally, how to
measure your successand make improvements
moving forward.We're going to review how
connecting planning, execution,performance, and analytics
improves and then transformscustomer experience.As we open Customer Intelligence
360, I can see a menu of thingsthat I can do as a marketer.I can establish my
marketing plans.I can manage my overall
strategy, content.I can determine
the segments that Iwant to market to, et cetera.First, we're going to
focus on planning-- again,how to strategically plan
and manage our marketingefforts across the
organization moreefficiently and effectively.From here, I'm going to go
ahead and collapse my menuand click into my plans.For today's demo, we're
going to focus on our 2020retail marketing plan.As I click into
my marketing plan,I can now view sample
metadata that'sbeen configured to
capture strategic planninginformation in support of my
overall 2020 marketing efforts.When I click on the
Relationships tab,this immediately shows me
all of the programs, seasons,activities, and
campaigns that I'mrunning in support of my plan.Perhaps a better view is to
view this via a calendar.And today, we're going to
focus on our spring season.With a quick expansion
here, I'm able to viewall of the campaigns and
activities that are supportingmy spring season.And again, specifically
today, we'regoing to be focusing on
our women's apparel launch.Here you can see that we have
a lot of omnichannel customerjourneys that generate-- that
are going to help us generatedemand for our new spring line.Again, focusing on
our spring season,I'm going to go ahead
and click into my objectand navigate to my financials.So here you can see the
budget that I've definedin support of my spring season.Throughout the
planning process, Ineed to manage this
overall budget.And once the program
executes, I thenneed to reconcile that budget
with the actual expensesthat we end up spending.Managing and optimizing
budgets have historicallybeen a very onerous process
for marketers in gettinga true marketing ROI.Embedded financials
also will help meas a marketer optimize
my marketing spendusing techniques like
attribution and optimization,which we'll touch on a little
bit later in the presentation.As we move through
the planning process,there are always a
series of handoffsthat need to occur as
part of any process.Managing these handoffs is
typically a very manual processbetween various groups
within the organization,such as your campaign designers,
brand managers, and of courseyour agency resources.So as you can see here, we have
configured a sample workflowwhich helps manage those manage
the back and forth that needsto occur between the various
teams, helping gather inputs,capture reviews and approvals
throughout the process as well.Again, managing these
handoffs, reviews,and approvals
within the solutiondoes help our customers improve
their operational efficiencies,as traditionally this is
a very manual process.Managing these handoffs,
reviews, and approvalswithin the solution
helps improve customersoperational efficiencies,
as traditionally thisis a very manual process.So as you can see here,
as we just showed you,SAS has embedded our
comprehensive marketingresource management
capabilities nativelywithin our solution
to help streamlinethe sharing of information
between the various groupswithin the organization
and to removethe need for disparate
and niche solutionsthat are traditionally used
to manage planning efforts.Once our plans are
established, we'reready to now manage the
execution process wherewe determine who we
want to engage with,how we want to engage with
them, and of course, when.First, we're going
to focus on whowe want to engage with in
support of our women's apparellaunch by viewing a segment
that we've previously created.I'm going to go ahead and click
up here and into my dressessegmentation.So as you can see here, we've
created four core segmentsthat we're focusing on on
tailoring our marketingmessages to as part of
our overall spring launch.And as a marketer,
I need to havea robust set of
segmentation capabilitiesto help me build
these segments out.So as you can see here,
we have a common listof selection tools to help
our marketers build outthese elaborate segments.One key thing that SAS provides
in regards to data accessis flexibility, which is
supported by our hybrid cloudarchitecture.So with our hybrid
cloud architecture,the data sources
for our segmentationcan be in the cloud, on
premises, or private cloud,or both.This enables organizations
to keep important datawithin their firewall and
still have the abilityto do their job.We're the only marketing
cloud that providesthis unique hybrid approach.We want to give our
customers the dataflexibility they require.With our segments in hand,
let's personalize our customerjourney.So we have a number of customer
journeys for each segmentthat I created to support
our spring season launch.And if I go to my open items,
I can click on my spring launchcustomer journey
that was previouslysupporting my overall launch
activities within my plan.As a marketer, I
want to create demandby creating
personalized catalogsand individualized email blast
to support this overall springlaunch.Within this journey,
I've created an emailthat I want to use to help
showcase a red dress in supportof our overall
launch activities.So let's go ahead and
click into this email.Now, I've already created
this particular one.But let's go ahead and see
how we can preview the contentthat we've laid out here and
how it's going to potentiallyrender on different devices.So here I have the
flexibility to view themin responsive
previews where I canview this on a mobile
device, a tablet,or how it would appear on
someone's computer for example.Let's go ahead and
close out of thisand return to our
particular journey.As a marketer, I also
want to create demandby responding in real time to
the behaviors of my customersas they're dynamically
engaging with usacross our digital properties.So for example, we can
personalize the web siteor a mobile app in real
time based on behaviorsthat our customers exhibit
and based on AI models thatpredict behavior.And if this customer
leaves the web site,and a retailer wants to remarket
to them on other websitesor social media
platforms, SAS canhelp the marketer get more
granular and analyticalin the audience purchases
they make with advertiserslike Google and Facebook.Now, of course,
Facebook and Googlehave some basic
rules that marketerscan create to buy audiences.But using SAS, marketers
can get much more complexin the rules and insights they
use to buy these remarketingaudiences.So let's go ahead and click
into this particular task.As you can see here,
I've set up this taskto track if a customer clicks
on our new dresses threetimes in a 30 minute
window and that theyhave a high propensity to
buy based on an AI modelthat we've previously built.All of these inputs
are the eventthat will ultimately
trigger whensomeone qualifies for this
Google remarketing audience.SAS is helping the marketer
maximize each dollar spentthrough our AI-driven
decisions and drivingreal-time personalization to
customers across all channels.OK, so let's go ahead
and close out of this.Next, we've touched on analytics
throughout this demonstration.But let's focus
on how we can useanalytics to further optimize
our retailer's marketing spend.We have a number of embedded
AI and optimization techniquesbuilt into the system.But for this
demonstration, we'regoing to highlight a new
and modernized optimizationcapability.So in the customer
journey we just discussed,you'll recall that
I was planningto send emails, catalogs, and
potentially other direct emailsto customers from an
outbound perspective.And there's typically multiple
versions of each of these.We can leverage optimization
to determine the best versionfor each customer while adhering
to constraints that we'veput in place like keeping
the spend within the budgetand adhering to other
business constraints.So let's go ahead and
take a look at a scenariothat I already have open.I'm going to go up
here and click downinto my optimization scenario.Within this scenario,
let's view the limitswe've defined to enforce our
overall constraints like we'dassess like keeping
the spend within budgetas you can see
here or our abilityto establish contact
policies as well.Additionally, the marketer is
able to do sensitivity or whatif analysis to see how changing
some of the constraintswould impact the
overall revenue.What if I was able to increase
the amount of catalogs we sent?Or what if I was able
to increase the budgetand lessen our constraints?Would it increase the amount
of revenue that we generated?Now, this is just
one quick exampleof how analytics can empower
marketers to better engagewith their customers.Finally, we have
planned, executed,and started to analyze as we
move throughout our executionefforts or our
marketing efforts.The key to ensuring that
we're meeting all of our goalsis to measure our success and
gauge our overall performance.Let's go ahead and return home.In addition to providing
out-of-the-box insights withinthe solution as you can see
here in our Insights menu,insights are also embedded
throughout the solutionto provide marketers with
a real-time view in howtheir campaigns are performing.So let's go ahead
and open a sample webtask, web AB task that I
have already open up here.Here I can see the
red polka dresswas our winning variant with
the highest conversion rate.Additionally, I can also see
that the system also discovereda segment as you can see here.This is just one
example of how marketersare able to view how things
are performing to quicklypivot and adjust
course as neededto ensure maximum results.To quickly recap,
we focused on howyou can transform the customer
experience by leveragingSAS's Customer Intelligence
360 solution to connectyour planning,
execution, performance,and analytic efforts.Thank you for attending.Please feel free to
reach out if thereare any additional
questions or comments."
73,"Hi.I'm Vince DelGobbo, and
I'm a developer at SAS.I started using SAS
in the early 1980sand have been working at
SAS since the early 1990s.Some of you might know me from
my work over the past 20 yearsor so with integrating SAS
output with Microsoft Officeproducts.But you might be
surprised to find outthat during that
same period of time,I've also been involved in
developing technologies thatenable you to web
enable your SAS code.In SAS 9, the technologies
are SAS Stored Processesand the SAS/IntrNet
Application Dispatcher.And in SAS Viya, the technology
is SAS Compute Server Jobs.Today we're going to talk
about running your SAScode from the web using SAS
Viya Compute Server Jobs.The goal of this
presentation is to show youhow to execute your
existing or new SAScode from a non-SAS client.By non-SAS client, I
mean a client applicationthat you do not get from
SAS and has no SAS softwarerequirement, such
as a web browser.In contrast, examples
of SAS clientsare applications that
you get from SAS,like SAS Enterprise Guide
or the SAS Windowing System,sometimes referred to
as SAS Display Manager.More specifically, you can
use any client applicationor programming language
to execute your SAScode as long as that client
can open an HTTP connection.This is because the code is
executed by accessing a URL.As you'll see
shortly, the processis similar to calling
a web-based service.You can specify the URL in the
address bar of a web browseror in any other
application listed here.The only SAS
software requirementis that SAS Viya must be
licensed on the server machine.The technology
used in SAS Viya issimilar to SAS9 Stored Processes
and SAS Internet ApplicationDispatcher.And much of what is covered
in this presentationalso applies to those
SAS technologies.We'll start with
existing SAS codeand then go through
the steps neededto execute it from the web.Two macro variables are defined
and then used in the proc sqlcode to select specific records
from the sashelp.cars data set.The subset of the data is stored
in the work.cars data set.The macro variables are
also used in the titlethat precedes the output.The sgplot and
means procedures usethe subset of data that was
stored in the work.cars dataset.Here are the result of
running the existing SAScode from SAS client such as
Enterprise Guide or the SASWindowing System.Note that the proc means output
is to the right of the chartbecause I moved
it there to enableyou to see the entire
output in a single slide.The means procedure output
appears below the chartwhen you run the SAS code.Here's an overview
of the steps neededto web-enable any SAS code.We use the SAS Job
Execution web applicationto create a Compute
Server job and thenadd existing or new
SAS code to the job.After saving the job,
view the propertiesto get the URL
for the execution,and then use the execution
URL in a web or other clientto execute the job on
the Compute Server.Our existing code uses %LET
statements to specify hardcodedprogram input.You can instead use an HTML
user interface to collect inputfrom the user at runtime and
pass that input to your programas global macro variables.This eliminates the need for
hard coding the input datausing %LET statements.We won't have time to
show the steps to do this,but they are covered in
detail in the documentation.You begin by accessing
the SAS Job Execution webapplication using
your web browseravailable at the
address shown here.Navigate to a folder
location to store the job.Then select New File
and specify a nameand optional description.We specify Automotive Compute
Server Job for the name.Accept the default values for
the other fields in the dialogand then press OK.Edit the file and then
specify the exact SAScode shown earlier.No changes to the
code are needed.Select Save and then
exit the Editor tab.Select Properties, then Details,
and locate the execution URLin the Job Submit section
of the Properties dialog.This is the URL that you
use to execute the job.The fully qualified URL
contains the server addressand the program--underscore program URL
parameter specifiesthe URL encoded location
and the name of your job.Note that space characters in
the job name have been encodedas %20.Copy the URL.Now you are ready to execute
your SAS code using onlya web browser client.Paste the URL into the address
bar of your web browser.The code executes,
and the resultsare the same as what we saw
earlier when the code wasexecuted using the SAS client.Again, the proc
means output appearsto the right of the chart
because I moved it thereso you can see it
all on one slide.Here are some other ways
to use the executionURL in your applications.We saw that you can paste
the URL into the web browseraddress bar to
execute your code.You can also paste the URL into
the body of an email message,and the recipient can select
the URL to execute your code.You can add a link to a web
page that executes the job whenthe link is selected.If you have a dashboard or
third-party application thatsupports the iframe
tag, then youcan use the URL to
incorporate fast resultsin that application.You can use the
URL in an image tagif your SAS program
generates only image output.Refer to the return gchart
image output and the returnsgplot image output samples
in the documentationfor more detail.Many JavaScript applications
accept CSV or JSON data format.You can write a SAS program
that creates and returnsdata in the appropriate
format and thenuse the program
as a data providerto the JavaScript application.You can also use
the execution URLto include your program
results in SAS Visual AnalyticsReports.Here we're looking at SAS
Visual Analytics report thatcontains a list table object
on the left side sourcedwith filtered data from
the sashelp.cars data set.On the right side is a SAS
Visual Analytics web contentobject.We specified the job execution
URL and the propertiesfor this object so that
the code was executedand the results displayed.These are just a
few ways that youcan use the Job Execution
web application to web-enableyour SAS code.Here's a summary of
what we just learned.You can use the SAS
Job Execution WebApplication included
with SAS Viya to authorand execute your SAS code
from non-SAS clients.A common non-SAS client is
an ordinary web browser.We didn't have
time to cover this,but mention that you can
create a user interfaceto collect input from
the user at runtimeand pass the input to your SAS
code as global macro variables.Refer to the
documentation for detailsand for samples of doing this.You can use the execution
URL in HTML tagsand also with
JavaScript libraries,allowing you to build
robust web applications.And we saw that you can
include real-time resultsfrom executing your SAS code
in SAS Visual Analytics Reportsby using the web
content container.Here's a link to the most
current documentationfor the SAS Job
Execution Web Applicationand also my contact information.Thank you so much for
watching this presentation,and thank you for using SAS."
74,"Hello, everyone.I'd like to welcome
you to my tutorialon ""PROC SQL Programming
Techniques for SAS Usersand Programmers.""I'm Kirk Paul Lafler.As an entrepreneur, consultant,
programmer, educator,and author, I have been a
SAS software user since 1979.We're going to look at a variety
of topics in this tutorial.We'll begin by exploring
unique PROC SQL programmingtechniques.Then we'll turn our attention
to join algorithms and executionplans.Then we'll look at fuzzy
matching programming techniquesand how we can use logic
scenarios with caseexpressions.We'll then take a look
at the rules, strategies,and usage of indexes.Then we'll look at the
PROC SQL macro interface.And finally, we'll look at
the data-driven programmingtechniques that are available
in the SAS software.As with any tutorial, examples
are very important to shareideas, to share techniques.In this tutorial, we're going
to look at the movies table.The movies table consists
of 22 rows, or observations,and six columns, or variables.The second table
we'll look at isthe actors table with 13 rows,
or observations, and threecolumns, or variables.In this first section, we
will explore unique PROC SQLprogramming techniques.Let's begin by looking at
the PROC SQL select statementand its clauses.The select statement's purpose
is to retrieve or read datafrom one or more
underlying tables or views.Although the select statement
supports multiple clauses,only one clause is
required, the from clause.All remaining clauses are
optional and only specifiedas needed.Some of you might wonder what
the order of the select clausesare.I came up with a little jog
that will help you rememberthe order the select clauses.If you recite ""SQL Is Fun
When Geeks Help Others,""this should help.Let me show you how.The first letter in each word
corresponds to the clause.Here we see the order of
the SQL select clausesis Select, followed by Into,
then From, on or Where,then Group by, then Having,
and finally, Order by.So if you can remember, ""SQL
Is Fun When Geeks Help Others,""this should help you
syntactically specifythe clauses in
their correct order.Now, what is the difference
between these clauses?This execution order is
different than the waywe specify the clauses.The FROM clause is the
only required clausewith the select statement.Its purpose is basically
to describe and collectthe data and information, the
variable names, variable type,number of rows, and other
important informationwhen we're dealing with
tables and/or views.So it's the first clause that's
executed in a select query.The second clause that's
executed if specifiedis the INTO clause.The INTO clause
is used to createone or more macro variables,
where the values canbe used to manipulate data.We'll explore this later
on in this tutorial.If specified for
subsetting purposes,the ON clause is the next
clause that's executed.It's used to subset
rows of data basedon the condition specified.And rows that aren't satisfied
by the condition are discarded.If specified, the WHERE
clause is the next clausethat's executed.Like the ON clause,
its purpose isto subset rows of data based
upon the condition specified.And rows that aren't satisfied
by the condition are discarded.If specified, the
GROUP BY clauseis the next clause
that's executed.The GROUP BY clause
takes the rowsthat where subset
with the WHERE clauseand grouped based upon common
values in the column specifiedin the GROUP BY clause.If specified, the HAVING
clause is then executed.The HAVING clause
applies the conditionsto the grouped rows specified
in the GROUP BY clause.And any grouped rows that aren't
satisfied by the conditionare discarded.Now the select statement
is processed and executed.Expressions specified in the
select statement are processed.And finally, if specified, the
ORDER BY clause is executed.Its purpose is to
sort the rows of datain either ascending,
which is the defaultorder, or descending order.Now let's turn our attention
to the various join types.To illustrate the techniques
of each join type,we're going to
use Venn diagrams.The first diagram you see here
is the inner join, sometimesreferred to as the intersect.If we have two tables
that we're trying to join,table A and table
B, the intersectis the highlighted
area in cyan or teal.The next join type
is the left join.Here we see table A and
table B. The intersectis highlighted, but
also is the left table,which happens to
be A. Table A couldbe classified as the dominant
table, the controlling table.So we're going to collect all
the intersect plus preserve allthe rows from the left table.The next joint type is
classified as a right join.Viewing the Venn diagram,
we see table A and table B.And the highlighted area is the
intersect, the common elementsbetween both tables.And then in this
case, the B tableis the dominant or
controlling table.So we're going to also preserve
the contents of the unmatchedrows from the B table,
or the right table.And finally, the last join
type that I'll discussis called a full join.A full join should
not be confusedwith a Cartesian product,
or in some cases knownas a cross join.Yes, it does contain
more data, oftentimes,than any of the other join
types that we discussed.But typically, if we're
looking at table A and tableB, the intersect, the common
elements between both tables,plus the preservation of
all the unmatched rowsfrom the left table,
table A, plus allthe unmatched rows from
the right table, table B,are preserved and provided
in the results set.Now let's take a look at how
we can construct an inner join.An inner join, as
I mentioned, isgoing to create an
intersect, highlighted herein teal or cyan.If we have the movies
table and the actors table,the common rows, the common data
between both tables and the keyare going to be preserved.In this case, we have
PROC SQL NOPRINT.Our purpose is to
create a table.The table name is going
to be called Inner_Join.Since it's a single name, it is
going to be a temporary table,so it's going to be
called Work.Inner_Join.And it's going to consist
of the SELECT query--Title from Movies,
Rating from Movies,Actor_Leading from
the Actors table,and Actor_Supporting
from the Actors table.Now, this construct
that I'm showing herein the FROM clause, the
FROM clause says Movies.I'm going to assign a
table alias called mfor movies, comma--this is one way to write
the inner join construct--and then the second table,
Actors, with the tablealias a for Actors.Optionally, we have
a WHERE clause.And this is where we relate the
title, the key in the Moviestable to the Title variable
or column in the actors table.When they are the same,
the intersect is formed.In the second inner join
construct, very similar,just a different way to
express the inner joinor the intersect--in this case, again, we're
going to create a table calledwork.Inner_Join.We're going to select the same
columns or variables, Title,Rating, Actor_Leading,
Actor_Supporting,from both tables,
Movies and Actors,but instead of the
comma in the FROM clauseseparating movies
from actors, we'reactually going to specifically
specify the Inner_Join keyword.Sometimes I like to use the
Inner_Join keyword opposedto the comma because
it succinctlyspecifies the type of join.So here we're going
to say FROM Movies,the table alias m, INNER JOIN
with Actors table alias a.The WHERE clause is
replaced with the ON clause.The ON clause here
says movies.Titleequals actors.Title.The result is going to be the
same as the previous example,construct number one,
just a different wayof expressing and specifying
the inner join or the intersect.Now let's see how we can
construct a left join.In this case, keeping in mind
the Venn diagram, the visualitself, our purpose is we
want to be able to createa left join type of table.So we'll construct a table
called work.Left_Join,selecting the same columns
or variables we did before,Title and Rating from
the Movies table,Actor_ Leading an Actor_
Supporting from the Actorstable.FROM Movies-- instead
of inner join,we're going to specify
LEFT JOIN as our key wordto indicate the type of
join we want to create,followed by Actors.In this case, the
LEFT JOIN keywordsays the Movies table is our
dominant table or controllingtable.And then the ON clause
specifies m or Movies.Titleis equal to Actors.Title.The end result
from the left joinconstruct is we're going
to create our intersectplus preserve all the unmatched
rows from the left table,the movies table.Now let's turn our attention
to the right join construct.Many think, well, if you
have a left join construct,do you really need a
right join construct?Well, I really think that it's
useful, because sometimes youwant to make sure that
you treat the second tableas the controlling table
or the dominant table.In this case, we don't-- we
could make this join the sameas a left join, but I'm
purposely not going to do that.I'm going to specify this
right join construct,where the Actors
table now is goingto be the dominant
or controlling table.Take a look at the FROM clause.The FROM clause
specifies Movies.Well, that's the left table.In this case, since
we're specifyinga right join keyword or
right join construct,the Movies table is
the subordinate table.Now the Actors
table is going to betreated as the controlling
or dominant table.And just like before,
the ON clause keywordspecifies that the
Movies.Title titleis equal to the Actors.Title.So the result set of
Work.Right_Join table is goingto contain all the matched rows
representing the intersect plusall the unmatched rows from
the right table that have beenpreserved.Finally let's take a
look at how to constructa full join construct.In this example, we're going
to create a table calledWork.Full_Join, select the
same columns we did earlier,Title and Rating from
the Movies table,Actor_Leading and
Actor_Supporting fromthe Actors table.Our FROM clause is going to
specify the Movies table.And the key word here to
indicate the join constructis going to be a Full_Join.This type of table construct,
along with the left and rightjoins, are sometimes
classified as outer joins,because they preserved
the unmatched rowsfrom either the left table, the
right table, or both tables.Now I'd like to bring
to your attentiona process of emulating one
of my favorite techniquesthat I've used in the data
step called FIRST., LAST.,and Between.This type of processing is
extremely popular in the datastep.I worked with SAS software
in the late '70s when we onlyhad the data step, and into
the '80s, before PROC SQL wasimplemented.And this was the only way we
could collect information,or data, or observations that
are treated as first, last,or between.Now one of the things
they want to show youhere is, by definition,
it's very difficultto emulate First., Last., And
between processing with SQL,but there is some techniques
that are available.I came up with a
technique a few years agothat would allow me
to use SQL to emulatethe stalwart approach that
we've all, the ones, usersthat have used the data step
to do this, fell in love with.So unfortunately, there is
no out-of-the-box PROC SQLsolution for handling
this type of processing,but we can emulate the
techniques with subqueries.So after several years of
research and experimentation,I developed a technique
using SQL subqueriesthat emulates the popular
data step technique.I'll just show you
this technique.Whether you decide to
use it, that's up to you.But sometimes some organizations
are very SQL-oriented.They either don't have the
skills of the data stepor their user base
is not trainedin data step techniques,
but they are in SQL.So I wanted to give
SQL users, because SQLis such a popular
language and itworks with so many different
relational database managementsystem software, that I
wanted to be able to come upwith some way of
solving this veryimportant problem of identifying
first rows, last rows,in groups by groups, as
well as the between rows.In this first
example, I illustratehow to process by groups
that are representedas first rows in each by group.Now what I want to do is I want
to show you that I'm creatinga table work.first_bygroup_rows.I'm going to select the columns
of interest, in this case,Rating, a categorical variable,
as well as the movie Title.And I'm going to dynamically
create a variable.And I'm going to assign
it a value of FirstRow.Now, this dynamically
created variableis going to be called ByGroup.I'm going to read
the Movies tableand assign the
table alias of M1.Then in my where clause,
I'm going to say,where title equals--and this is the
reason why this works,is I'm going to
select using the minfunction, the min
statistical function, title.From Movies-- so I'm going
to read Movies a second timeand assign it a
table alias of M2,where M1.rating is
equal to M2.rating.Essentially what is enclosed
in parentheses there,the subquery select is going
to process that informationand then parlay it out
to the outer query.And then the results
are going to be orderedin ascending order by the movie
rating followed by the movietitle.And this is what
we're going to get.We're going to create by group
processing that representsthe first row in each group.So we have four distinct rating
groups, movie ratings, G, PG,PG-13, and R. This represents
based upon the sort orderthe min value for
each of the groups.And notice the ByGroup column,
that dynamically createdvariable.I don't really need that,
but it's helpful later on.And you'll see why.It's helpful to create,
because it gives youa better understanding
of what was classifiedas either a first row
by group, a last rowby group, or between by group.So this, using the min
statistical function or minfunction, we can collect the
first row in each by group.And the by group, again,
is the movie rating.Now let's take a look at how we
can emulate the last by group.In this case, you might
already be guessingor you might already know.Maybe you've even
used this technique.Instead of the min
function, we'regoing to use the max function.Almost everything's
the same in this codefrom the first by group code
emulation, with the exception,I'm going to create a table
called not first_bygroup_rows,but last_bygroup_rows.And my dynamically created
variable for ByGroupis going to contain
the value LastRow.And in the previous example,
we used the min functionfor title.Now I'm going to use the
max function for title.Otherwise, everything
is the same.And when we run this
code, we're goingto see that we have four
distinct groups, the last rowin each group.Now you might say, oh, does
Wizard of Oz appear twice?Because Wizard of Oz also
occurred for their firstby group.And now it's showing in
the last by group as well.And that's perfectly fine,
because by definition,it is the first and
last in each group.And then we see the
other distinct groupsfor PG, PG-13, and R.The last code emulation,
because this is important--it's often very important to
know what the between rows are.This gives us the
ability to know when,what the first row
in each by groupis, what the last row
in each by group is.But we also might
want to know whatthe rows are that are not
first or not last within eachby group.And this code gives us the
ability to create a table.And I called it
between_bygroup_rows.We're going to select
the columns of interestfrom the Movies
table, rating, title.I'm going to create
dynamically the Min_Title usingthe min function and the
max function for Max_Title.And I'm also going to
dynamically create the ByGroupvariable with the
value BetweenRow.I don't necessarily have
to have the Min_Titleand the Max_Title.But to make this work and
show the results correctly,it is important.Then we're going to group by
the categorical variable rating.Using the HAVING
clause, because now wewant to use this HAVING
clause against the group by,the grouped variable rating.Having CALCULATED-- now,
what is this CALCULATEDkeyword all about?Well, the CALCULATED keyword
allows us-- and think backto the order of execution
of the query clauses,the select query clauses.The select clauses-- and
we're defining the Min_Title,the Max_Title, and the by group
dynamically on the select.Now, the select statement
is the first statementthat's specified
in a select query,but it's the second to
last statement or clausethat's executed.Consequently, we want to be
able to access the contentsof these dynamically created
variables, Min_Title,Max_Title, and by group.Consequently, we must use the
CALCULATED keyword to do so.So having calculated
Min_Title is notequal to calculated Max_Title.And calculated Min_Title
is not equal to Title.And calculated Max_Title
is not equal to Title.It's going to select
just the between rows,not the first row
in each by group,not the last row and each by
group, just the between rows.And then we're going to
order in ascending orderby the by group variable
rating followed by title.And this is what we're
going to get from this code.We see that we have--and if you look at the
last column, by groupall says between row, the
Min_Title, the Max_Title.So by looking at this, I
see that, ah, Wizard of Ozdidn't appear here,
because Wizard of Ozis classified as a
first and a last row,but not a between row.It only appears once in the
Movies data set or table.But these movie titles for
each of these movie ratingsare not classified as a
first row or a last row.Consequently, they're
identified as between rows.Now, it might be useful to be
able to concatenate or appendthe identified first rows,
between rows, and last rowstogether.You don't necessarily
have to do this,but I thought, ah, this
would be kind of neat to showeach one of these three distinct
groups concatenated togetherrow by row.So in this example,
I'm going to usethe SELECT statement with the
variables of interest, rating,title, and by group,
from each distinct group,the first by group rows,
the between by group rows,and then the last by group rows.And I'm going to concatenate
them with the UNION ALL setoperator.Now, there are other ways
to do this, obviously.But since this is an
PROC SQL tutorial,I wanted to show you a way
to concatenate using SQL.And the UNION ALL
set operator isgoing to collect all the
rows including duplicates.And this is what
we're going to get.We're going to see all the
first rows appear first,all the first rose in
the by group category,then all the between rows,
followed by all the last rows.Now, we did start with 22 rows.And if you look at the
row number on the SQL--and I purposely
left it in there--you see that there is 23.That's because the Wizard
of Oz appears not onlyas a first row, but a last row.Now, we can get
rid of that, but Iwanted to show you the pluses
and minuses with this emulationapproach of using PROC SQL
to identify First., Between,and Last.Now I'd like to show you how
we can emulate nearest neighborprocessing.The problem is some
of the techniquesthat we may use in the
data step are not alwaysavailable in PROC SQL.Also, PROC SQL performs
operations row by row.Nearest neighbor
processing is usedto identify content that
is close to another value.The data step is often used
with the lag and lead functionsto perform nearest
neighbor processing.The data step is a
powerful language.But once again, you may
work in an environmentwhere there is little or no
experience, or understanding,or knowledge about how to
use data step techniquesbut there is an abundance
of knowledge usingPROC SQL or some form of SQL.These techniques can be
used within the SAS systemto be able to emulate these data
steps stalwarts very nicely.But let's take a
look at something.In the data step, we can
use lag and lead functions,but PROC SQL does
not support the useof the lag in the
lead functions,and consequently produces
error messages on the SAS log,so it fails.So if you need to be able
to process nearest neighbortype of activity,
perhaps we needto come up with an approach.So this approach I
just am showing younow what happens when I try
to use the lead and the lagfunctions within PROC SQL.Here is code here.I'm going to dynamically create
a lag title and a lead title.When I run this, I'm
going to get an error.Notice the two errors.The first one says,
the lag functionis not supported in PROC SQL.It is only valid
within the data step.The second error says, function
lead could not be located.What is one to do?Do I just stop or
quit and say, well,I guess I have to
learn the data step,or I guess I have to write data
step code in an SQL-centricorganization?Nothing wrong with learning--I love learning, as
all of you do too.But sometimes being
able to come upwith an emulated approach in
the desired language workingwith data from an
RDBMS is the way to go.So how can we
address this problem?This is definitely a problem.It didn't run.The code gave me errors.No results were produced
because of the errors.What can we do?Well, here is a way.I don't know if
it's the only way,but this is a way that I've
used for a number of years.I love Output Delivery System.ODS gives us the ability
to output the results.Now, this approach is
going to purposely assignrow numbers to our table rows.And I know from an SQL purist,
a database purist point of view,that's not always desirable.I've been working with databases
since the mid '70s, a varietyof databases that,
in many cases,don't even exist any longer.And I've worked all the
way up to the current timeworking with
relational databases,including the leading
ones on the marketplace.So I'm very well aware
that it's not alwaysadvisable to create rows of
data that have row numbers.But if you want to be able to
emulate using this approach,I have to do it.I have to create row numbers.So I'm in a specify ODS
output SQL results equalsthe table, a user-defined
table, Movies_with_Row_Numbers.And then I'm going to say,
PROC SQL NUMBER NOPRINT.In other words, I'm
going to purposely turnon the row numbers
and write the resultsto that output, that
ODS output statement,Movies_with_Row_Numbers.I'm going to select title and
rating from MyData.Movies.Then I'm going to
turn the number off.In the second PROC SQL, I'm
saying PROC SQL NONUMBER.And then I'm going to say
create table Nearest_Neighbor.So I'm going to create
a new table calledwork.Nearest_Neighbor.I don't necessarily
have to drop the row,but I'll do that just
to clean things up, asselect all columns, select title
from Movies_with_Row_Numberswhere row equals m.row minus 1.In other words, we want
to emulate the lag featurefor the previous title.Whatever movie
we're looking at, wewant to see what the
previous title or the rowwith the previous title is.This emulates the lag
feature or the lag function.Then we say, I want
to select the movietitle for movies
with row numbers,where row equals m.Row plus 1.This is going to
do a look ahead.And it's going to be able
to perform or emulatethe lead function.Now, as you might guess, I'm
using minus 1 and plus 1.Those are values
that can be changed.If you want to go
back more than oneor you want to go
ahead more than one,you can change the 1, minus
1 and plus 1 to somethingdifferent.And I've hardcoded
these values, but theycould be specified as
parameters into a macroor whatever the case is.And I'm going to then select all
columns from nearest neighbor,the table that I created in
the create table statement.This is what we get,
the results from this.And if we look at the
rows in the table,the Movie table, the Movies
table title, Braveheart,Casablanca, Christmas Vacation,
Coming to America, et cetera.And if you go over
previous title,for Braveheart, there wasn't
one, so it's blank or missing.The next title says, Casablanca.If you go down to the second
row, you see Casablanca.Now let's take a look
the row Casablanca.The previous title
says, Braveheart.If you go to the first
row, you see Braveheart.If you go to the next title,
it says, Christmas Vacation.Go to row three.There is Christmas Vacation.So the emulated approach to
the nearest neighbor within SQLseems to be working.Yes, I had to read the table
not once, but multiple times.But sometimes
extra processing isnecessary to be able to
achieve the result that oneneeds to achieve.And in this case, I needed to
be able to use SQL to emulatenearest neighbor processing
that I've been usingfor decades in the data step.But the client told me, we
don't want data step code.We are an SQL shop.And we want SQL solutions only.So I had a choice.I can either turn the
project down or come upwith an approach
that will satisfytheir desire of having an SQL
approach to producing nearestneighbor results.So what are the summary of
takeaways from this section?The select statement
retrieves or reads datafrom one or more
underlying tables or views.The FROM clause is the
only required clausein an SQL SELECT statement.All the remaining
clauses are optional.To help remember the order
of the SELECT clauses,simply recite, ""SQL is Fun
When Geeks Help Others.""And remember, the order that
we specify the select clausesis different from the execution
order of those clauses.The FROM clause
is the only clauserequired with the SELECT.Consequently, it
is the first clausethat's executed in a query.Again, it collects information
about the table itself,the environment, the variable
names, the variable types,the number of rows, and
other important information.If specified, the
INTO clause is usedto create one or more
macro variables, whichwe'll see in a later
section in this tutorial.If specified, the
ON clause is thenexecuted to subset
rows of data basedupon the conditions specified.And any rows that aren't
satisfied by the conditionare discarded.Should a WHERE clause be
specified, it is then executed.The WHERE clause is used
to subset rows of databased on the
conditions specified.And rows, once again,
just like the ON clause,that are not satisfied by
the condition are discarded.If specified, the GROUP BY
clause is then executed.The GROUP BY clause
takes the rowsthat were subset with the
WHERE clause and group basedupon some common values in the
columns specified in the GROUPBY clause.If specified, the having
clause is then executed.The HAVING clause
applies the conditionsto the grouped rows specified
in the GROUP BY clause.And any group rows that aren't
satisfied by the conditionare discarded.Then the SELECT
statement is executed.It's the first thing we specify,
but next to last statementthat gets executed.Essentially, the
SELECT statementspecifies what columns we want
to select or use in our queryor what columns we want
to dynamically createin our query.And if specified, the last
clause that's executedis the ORDER BY clause.Its purpose is to
sort the rows of datain either ascending,
which is the default,or descending order
for viewing on outputor for processing on output.So this table shows
the execution orderof all the SELECT clauses.Now, why is this important?Because if you understand
the order of the clauses,not just the order you
specify the SELECT clauses,but the execution
order of the clauses,I believe that you will write
better and more efficientqueries.We then turned our
attention to the join typesusing a Venn diagram, the inner
join or intersect highlightedwith the teal or cyan.We then looked at the left
join, where the intersect iscollected plus to preservation
of all the unmatched rowsfrom the left table.Then the right table,
the intersect datais collected plus all
the unmatched rowsfrom the right table, and
finally, the full joinconstruct, where they intersect
information is collectedplus the preservation of
all the unmatched rowsfrom the left table and
the preservation of allthe unmatched rows
from the right table.We then looked at how to
emulate using SQL First., Last.,and Between processing.The first observation
in each by groupis selected with
the min function.It uses subquery techniques.The last observation
in each by groupis selected with
the max function.The between observations
in each by groupis selected with the
HAVING clause logic.For example, the min
function with the titleare grouped by variable.The last by group processing
uses the max function,ordering it by the
categorical variable rating.The between by group
processing uses the group by,and the having clauses
to be able to collectto between by group rows.The nearest neighbor
processing, since PROC SQLperforms operations row by row,
nearest neighbor processingidentifies content that
is close to one another.The data step is
a marvelous toolto be able to achieve nearest
neighbor processing using lag,lead functions, et cetera,
to be able to performnearest neighbor processing.But if you're an SQL
user or an SQL shop,PROC SQL does not support
the use of the lag and leadfunctions, and consequently
produces error messages.So I wanted to show you a way--you decide if this is
worth exploring further--but a way to emulate nearest
neighbor processing usingOutput Delivery System, ODS,
output statement to numbera data set or a table
with row numbersand then be able to process
that through select logic,going backward one or more
rows to be able to collectlagged rows or
nearest neighbor data,or go forward one or more
rows to look at the leads.Let's now learn about PROC SQL
join algorithms and executionplans.Users supply the
names of the tablesalong with the join conditions.The SQL optimizer will determine
which join algorithm to use.The available join
algorithms with PROC SQLinclude nested loop, sort
merge, index, and hash.We're going to look at each
one of these individually.A nested loop join algorithm
may be selected by the SQLoptimizer when processing
small tables of data,one table is considerably
smaller than the other table,the join condition does not
contain an equality condition,and a sort merge index and
hash join have been eliminatedfrom consideration.A short merge join algorithm
may be selected by the SQLoptimizer when processing small-
to medium-size tables of data,a step loop, index,
and hash joinhave been eliminated
from consideration.An index join algorithm may be
selected by the SQL optimizerwhen it is determined
that using an existingsimple or composite
index will improvethe performance of the
join relation, a step loop,sort merge, and hash
join algorithm have beeneliminated from consideration.A hash join algorithm may be
selected by the SQL optimizerwhen sufficient or real memory
is available to the processto store at least the smaller
of the table into memory.The MEMSIZE equal and/or
BUFFERSIZE equal optionscan also be set optimally,
a step loop, sort merge,and index join
algorithm have beeneliminated from consideration.How do you go about
overwriting the SQL optimizer?Well, there is MAGIC options.MAGIC equals 101 instructs
the SQL optimizer to selectthe nested loop join algorithm.There is also a
MAGIC equals 102.MAGIC equals 102 instructs
the SQL optimizer to selectthe sort merge join algorithm.And there is also
a MAGIC equals 103.When specified, it
instructs the SQL optimizerto select the hash
join algorithm.Let's take a look at
the magic equals 101.We're asking SAS
optimizer to choosethe sequential loop join.So we're going to
specify MAGIC equals 101on the PROC SQL statement.And then whatever join
construct that we'vespecified in our example
here, we have an intersect.The results from
specifying the MAGICequals 101 option will
appear on the SAS log.The log will show a note.In this case, the
PROC SQL plannerchooses sequential loop join.Taking a look at the
MAGIC equals 102 optionfor merge join, we can specify
on the PROC SQL statementMAGIC equals 102, essentially
telling the optimizer that thisis the join algorithm
we want to be chosen.Once the code is executed,
we can look at the log.And we should see, by
specifying MAGIC equals 102,that the PROC SQL planner
chooses the merge.The third MAGIC option
is 103 for hash join.When we specify MAGIC equals
103 on the PROC SQL statement,essentially what we're
doing is, is we'retelling the SAS optimizer
to perform in memorythe join algorithm, at least
with the smaller of the tables.Once the code is executed,
we can look at the SAS logand see what the
results might be.In this case, when
specifying MAGIC equals 103,we're forcing the optimizer
to choose a hash join.But the PROC SQL
planner, by default,would have chose the merge join.But because we forced the issue
by specifying MAGIC equals 103,a merge join was
transformed to a hash join.Now let's take a look at how can
we look under the hood of SQL,in other words, see how
our query or queriesare processing.We can display the execution
plan with the _Method option.The _Method option basically is
going to display the hierarchyof query processing
or an execution plan.The results will be
automatically displayedon the log.For example, by specifying
_METHOD on the PROC SQLstatement, we can
specify our query,our select query in this case.And then we can examine
how it performedby looking at the log.Now, the _Method is going
to display one or more codeson the log.It will not display
the descriptionsthat I am showing here, but
it will display one or moreof the codes, for
example, SQXCRTAfor creating a
table as a select,SQXSLCT when a select statement
or clause has been specified.Or it might display SQXJSL
when a step loop joinor Cartesian type of product
was specified or selectedby the optimizer,
or it might chooseSQXJM indicating that
emerge joint operationwas selected by the optimizer.So these options,
these codes itselfgives us the ability
to inspect howour query is performing behind
the scenes or under the hood.SQXJNDX indicates that an index
joint operation was performed.SQXJHSH indicates that a hash
join operation was selectedand performed by the optimizer.SQXSORT indicates that a
sort operation was selectedand performed by the optimizer.SQXSRC indicates the
source rows from the tablethat were selected.SQXFIL indicates that there
was row filtration occurringin the query itself.SQXSUMG indicates that
an aggregate or summarystats were executed because
of a GROUP BY clause.And SQXSUMN indicates
that summary statisticswere performed or executed
when no group by clausewas specified.These are all codes that
could appear on your log.So if you'd like to see
what your query is doing,how it's performing, what
type of resources it's using,specify _Method on the
PROC SQL statement.And then whatever
query that you have--in our example here, we
are specifying an intersector an inner join construct.And then when you execute this
code with the _Method option,your log will show one
or more of these codes.And the codes will give you
an indication and a betterunderstanding of
what your query did,and what the optimizer did,
and how the optimizer performedusing your query.It's a very helpful way
to better understandwhat's happening with queries.Now, these codes are a
bit difficult to remember,so that's why I provided the
brief description to help yourecall what each code means.Now, there is another
very useful thingthat I like to have all
SAS users be aware of.It's called OPTIONS
MSGLEVEL equalI. It's a SAS system
option that allowshelpful notes about index
usage, sort utilities,and merge processing to be
displayed on the SAS log.It has nothing to
do with just SQL.It can be used or specified
anywhere in the SAS system.But OPTIONS MSGLEVEL
equal I is goingto give you informative messages
that you would not ordinarilysee on the SAS log.It's extremely helpful in
better understanding whatyour queries, what
your programs aredoing with the available
resources that you have.The log will show
in an info message,for example, any index that
was chosen by the optimizerto speed up processing.So in this case, in lieu
of sequential processing,by turning on OPTIONS
MSGLEVEL equal I,we see that the
optimizer found and usedan index called rating.So what I'd like
to do is give youa summary of takeaways
for this chapter,for this section
that we covered.Again, the available
join algorithmsare nested loop, sort merge,
index, and hash with PROC SQL.So the optimizer has choices.We can override
the SQL optimizerby forcing it to
use, or specify,or execute a sequential loop
join with MAGIC equals 101,or MAGIC equals 102
for a merge join,or MAGIC equals
103 for hash join.Now, don't put this
override in production code.Let the optimizer
make the choice.But during testing,
it can be very useful.We also looked at how we
can explore and betterunderstand what's happening
under the hood of our queriesso we can better explain,
better understandwhat type of resources
are being used,what kind of join algorithms
are being executedby the optimizer, et cetera.And it's done with
an _Method option.The _Method option gives us one
or more codes that are goingto be displayed on the SAS log.These can be extremely
helpful in helping you betterunderstand what your query
and the SQL optimizerhas done with the query
that you've executed.And again I list these codes
with a brief descriptionof what each code means.Now, you may see one, two,
or more of these codes.So become familiar with the
_Method option and it will bevery helpful.Then we finally looked at
MSGLEVEL equal I SAS systemoption, giving us the ability
to display helpful notesabout index usage, sort
utilities, and merge processingon the SAS log.We're now going to learn about
fuzzy matching techniques usingPROC SQL.We have an additional
two tables that I'mgoing to look at and show you
in the examples in this section.The movies with
messy data consistsof 31 rows or observations
and six columns or variables.And it does have some
issues with the data.I purposely added issues
with the data itself.And you'll see by
looking at the datathat there is misspellings.There is incorrect rating types.There is some invalid movie
titles, some duplicates,and things like this.Then we have another table,
Actors_with_Messy_Data.This table contains 15
rows or observationsand three columns or variables.It too has been purposely
added some bad datato this table, misspellings,
incorrect movietypes, missing
values, duplicates,things of this nature.Matching with common keys--today's data exists
in many forms,including data sets, RDBMs,
tables, spreadsheets, CSVs, etcetera, and involves matching
two or more data sourcesto create a combined file.Using a common and
reliable identifier or key,two or more data sets can be
matched, merged, or joined.With our Movies table
and Actors table,we have a key, title
in both tables.In order to match, or merge, or
join these two tables together,we're going to use the key.If the key is
reliable, consistent,we should have very little
problems in joining or mergingthese tables.But what happens when a shared
and reliable key between datasources is non-existent,
inexact, or unreliable?This can make the entire
matching process morecomplicated and problematic.Consequently, Steve Sloan and
I have put together six stepsto successful fuzzy matching.Step one, remove the
extraneous characterssuch as quotes, single quotes,
or double quotes, ampersands,dashes, question marks, things
of this nature, anythingthat really doesn't
serve a purposeor a real important purpose.Step two, put all the characters
in your search argumentsin uppercase notation and
remove leading blanks.Now, these steps that we're
showing here are useful.You can opt to avoid these
steps by cleaning the data,performing data cleaning.Or you can do-- use
some of these techniqueswith the fuzzy matching to
improve the process of mergingor joining tables of data when
the keys are not that reliable.Step three, remove extraneous
words that might not appearin different databases when
matching company names suchas Inc, Ltd, LLC, CO, Corp,
.com, Company, Corporation,Division, et cetera.Anything that will cause a
matching process to fail,try to remove the differences
between the different datasources.Step four, handle numeric
and character zip codes.Differentiate between
US and Canadianzip codes, because they
are different, and includeleading zeros when matching.Step five, choose a standard
for addresses, such as streetor St., avenue or Ave, road or
Rd, court or Ct, lane or Ln,et cetera.Consistency helps to join or
merge our matching processimmensely.Step six, match the names,
addresses, company names,and other fields using Soundex,
SPEDIS, COMPLEV, and COMPGEDfunctions.If you specify
them in this order,processing speeds
can be improved.A side-by-side comparison
of each one of these fuzzymatching techniques--Soundex essentially
is an algorithmthat matches words
that sound alike,hence the name phonetic match.It ignores case, embedded
blanks, and punctuations.It works best with
English-sounding names.It assigns essentially a code
to each letter in a word.The SPEDIS function
translates a wordinto a smallest distance value.It returns a non-negative value.An exact match when it occurs
returns a value of zero.The matching process
can be controlledusing the SPEDIS function.The third technique
is using the COMPLEV.It's a function in
the base SAS system.And it computes what's
known as Levenshtein EditDistance, or LED
score, returninga number of operations
that were performedto make one key's value
match another key's value.An exact match
returns a value of 0.In it too, the matching
process can be controlled.The last fuzzy matching
technique is COMPGED.It's also a function.It computes what's known as
a Generalized Edit Distance,or GED score.The lower the GED score, the
better the match, typically.An exact match
returns a value of 0.And once again, the matching
process can be controlled.So let's start by looking at
the Soundex phonetic matching.The Soundex algorithm
matches strings or dataon words that sound alike.Soundex is nothing new.It was invented and patented
by Margaret O'Dell and RobertRussell in 1918 and
1922, respectively,to help match surnames
that sound alike.Now that's over 100 years.It adheres to the
following rules.It ignores case, in other
words, case insensitive.It ignores embedded
blanks in punctuations.The biggest problem or
deficiency in using the Soundexis it's best in finding
English-sounding names only.So how does the
Soundex algorithm work?SAS determines whether a
name or a variable's contentssound alike by converting
each word to a code.The value assigned
to the code consistsof the first letter in the word
followed by one or more digits.Vowels such as a, e, i, o, and
u, along with the letters h, w,y, and non-alphabetical
charactersdo not receive a coded value,
and are consequently ignored.Words with double letters
such as double T's areassigned a single
value for both letters.For example, deriving the
Soundex algorithm codes,letters b, p, f, and v
receive a value of 1.The letter c, s, g, j, k, q,
x, and z receive a value of 2.The letters d and t
receive a value of 3.The letter l when encountered
receives a value of 4.The letters m and
n when encounteredreceive a value of 5.And the letter r
when encounteredreceives a value of 6.Let's examine how the movie
title Rocky is assignedthe coded value of R22.R has a value of 6, but it is
retained as R. O is ignored.C is assigned a value of 2.K is assigned a value 2.And y is ignored.The general syntax is variable
equals Soundex some value.Looking at this
example, we see that wehave an SQL query with
a WHERE clause thatsays Title equals asterisks--that represents sound like--""Michael,"" in quotes,
a character string.It's going to derive the
code behind the scenesand evaluate other titles
that sound like Michael.The result is, it's going
to extract or displaythe movies with the title
Michael and Micheal.Because the ""ae"" in Michael or
the ""ea"" in Micheal are vowels,they are ignored,
so they sound alike.Now let's turn our attention to
the SPEDIS or spelling distancefunction.It contains two arguments.It evaluates possible
matching scenariosby translating a keyword into
a query containing the smallestdistance value.The SPEDIS function returns
a non-negative value.A SPEDIS value of 0 is returned
when the query and argumentsmatch exactly.Users are also able to
control the matching processby specifying spelling
distance values, for example,in increments of 10,
10, 20, 30, et cetera.The general syntax for the
SPEDIS is function is SPEDIS,and since it's a
function, argument one,argument two, query, keyword.Now let's look at an example
of how the SPEDIS function canbe used in an SQL query.We're going to use
the SPEDIS function.Argument one's going to
be the variable title.Argument two is going to be
the value we are looking for,Michael.We're going to assign it to
a value called SPEDIS value.And then in our WHERE
clause, we're going to say,WHERE Spedis_Value is
greater than or equal to 0.I won't code this,
but I want youto see the different values
itself in this output.We see that there are all sorts.In the last column, the
SPEDIS value column,we see that there is
all sorts of numbers.The larger the number,
the less likely the match,the smaller the value
for the SPEDIS value,there is a better chance
that it's a match.So for example, if you look at
seven for Michael or Micheal,that looks like it's a match.So it's consistent with the
Soundex algorithm as well.Let's take a look
at another example.In this example, we're going
to look at SPEDIS titlefor Michael, assign
it to SPEDIS valuewhere SPEDIS is value is
less than or equal to 20.So now, instead of looking
at all these large values,we're going to constrain
the value to 20 or less.The hope is that we're
going to find a greaterlikelihood of matches here.And we see in the results
that Michael as well asMicheal, Michael receives zero.Michael receives
seven, falls within 20.Now you decide whether
or not these are matches.We can use these functions and
these fuzzy matching techniquesto help us in our
matching process.The next function we're going
to look at in the fuzzy matchingarea is COMPLEV.COMPLEV stands for
Levenshtein Edit Distance.The COMPLEV function
provides an indicationon how close two strings
are with one exception.The COMPLEV function
returns the numberof operations that have
been performed to makestring one match string two.The more operations that are
performed, the less likelythat there is a match.Consequently, the
lower the numberof operations, the
better the match.For example, zero equals best
match, one, next best match,et cetera.The general syntax for
the COMPLEV functionis COMPLEV, string one and
string two, which are required,and an optional cutoff
value and modifier.Again, the required
arguments are string onein string two, which essentially
specify a character variable,constant, or expression.Now we're going to look at what
the optional arguments are.The cutoff value specifies a
numeric variable, constant,or expression.The modifier specifies
a value, when specified,that alters the action
of the COMPLEV function.Valid modifier values
include I, L, N, and colon,I for ignore case, L for
remove leading blanks,N, ignoring quotation marks
around string one or stringtwo, and colon, truncate
the longer valuea string one or string two
to the length of the shortervalue.In this first
example of COMPLEV,we're going to select the
movie title for the Moviestable, rating, length,
and category, creatinga new variable or derived
variable called COMPLEV number.And at that point,
what we're going to dois we're going to say,
order by, in ascendingorder, movie title, where the
title is not equal to missing.So what we want to
do is we want to lookat the category variable
looking for the value drama.The last column,
the COMPLEV number,is going to indicate
how many operationswere performed in order
to make the stringone equal to string two.And once again,
Michael or Michealhas a very low
value here, 0 or 6.In this next COMPLEV
example, we'regoing to look at, once
again, using the COMPLEV,the variable category
for the keyword""Drama,"" assign the
number of operationsto the COMPLEV number.And now we're going to say
COMPLEV number is less thanor equal to 1 and then order
the results by the movie titlein ascending order.Now that we have a better
understanding of howthe COMPLEV function works
and the types of valuesit provides, we can control
it with our WHERE clauseor our ON clause depending
upon whether or not wehave a join operation going on.So it gives us the
ability to controlwhat type of output,
what type of matcheswe want or don't want.In this case, since we specified
in COMPLEV number less thanor equal to 1, we see the
Casablanca with drama,Forest Gump with drama,
Michael with drama.Notice there is a spelling
error on the second Forest Gump.It has two m's.It has a value of 1, meaning
that one operation wasperformed to make it match
the single value, drama.So this gives you a better
idea and understanding what'shappening behind the scenes.The next function and the
last fuzzy match matchingfunction we'll look at
is the COMPGED function.It's a technique that
works by computing what'sknown as a Generalized
Edit Distance,or GED score, when
comparing two strings.The GED score acts as a
measure of dissimilaritybetween two strings.The higher the GED
score, the less likelythe two strings match.Consequently, users should
seek the lowest derived GEDscore for the greatest
likelihood of a match.The general syntax for
the COMPGED functionis very similar to the COMPLEV
function, the two requiredarguments, string one
and string two, and thentwo optional arguments,
cutoff value and modifier.Again, the required argument,
string one and string two,specifies a character variable
constant or expression.And the optional arguments
for cutoff value and modifieradhere to the same syntax and
the same values as the COMPLEV.The cutoff value specifies a
numeric variable, constant,or expression.When specified, the
modifier specifiesa value that alters the action
of the COMPGED function.Valid modifier values
are I for ignore case,L for remove leading blanks,
N for ignore quotation marks,or colon, truncate the
longer value string oneor string two to
the shorter value.Just so we see the
full complementof all the different fuzzy
matching techniques with SQL,this first example is going
to show how the COMPGEDfunction is used
in a join process,in this case, an inner join
process or an intersect.What we're doing here
is we're actuallygoing to retain the movie
title from the Moviestable, the title of the
movie from the Actors table,the movie rating, category,
the leading actor.And then we're going to
evaluate for the Movies tablethe title against the
Actors table in the titleand store that value
as a COMPGED score.And then we're going to order
the results in ascending orderby the movie title.And what we see here, since
we didn't constrain or limitthe number or the size of the
values in the COMPGED score,we're seeing the
whole litany of valueshere from 0 to 400 and
everything in between.Now you might wonder, ""Ghost""
in mixed case versus ""GHOST""in upper case, why does
it have a value of 400?Well, we'll see how we can
fix that a little later.COMPGED function example two--again, I want to show you how
you can perform a match mergeor a join process with SQL.In this case, we're
going to retain the valuefor the title in Movies,
the title in Actors,rating category, leading actor.And then we want to
derive the COMPGED scorefrom both tables,
Movies_with_Messy_Dataand Actors_with_Messy_Data,
where the title is notequal to missing, and
the calculated COMPGEDscore is less than 100,
less than or equal to 100.What is CALCULATED?Where did that come from?Well, CALCULATED
is a key word thatallows us to take advantage
of derived variablesthat we're creating
on the select.If you recall in
lesson one, I talkedabout the order of execution.If we don't specify
the CALCULATED keywordprior to the COMPGED
score, the valuewill not be available
in our WHERE.So the calculated
allows us to takeadvantage of derived values.And in this case, less
than or equal to 100,we now have a more likely
matches here between the Movietitles and the Actors titles.In our third COMPGED
example, we'regoing to continue with
the same code basicallythat does a match merge
or a join process,but we're going to take a look
at a modifier, the I modifier.And basically what we
want to be able to dois we want to be
able to ignore case.That's what the I modifier does.If specified, it allows
us to ignore the case.So case is no longer
important, whether it'suppercase, or lowercase,
or mixed case,as long as it's
spelled the same.And then we want
to display subsetwhere the calculated
COMPGED score is less thanor equal to 100 and then display
the results in ascending orderby Movie title.And now we see ghost.And I highlighted that, Ghost
in mixed case versus GHOSTin the actors table,
all uppercase,is now being
treated as the same.Notice its COMPGED score is 0.Prior to this, it was 400.Now it's 0.This gives us the ability to use
one or more of those modifiersto process our data so that
we don't get mismatcheswhen it comes to perfectly
spelled data, but just caseproblems or case issues.In our last, fourth and last
COMPGED function example,I'm going to show you same code,
except for now the modifierhas, in quotes, 'INL.'What this is going
to do is it's goingto remove or ignore the case.It's going to essentially
remove the leading blanks.And it's going to process where
the quotes around the datavalues themselves, it's
going to remove those.And we want to basically
display the COMPGED scoreswhere it's less
than or equal to 100in ascending order
by movie title.And now what we have here
is we have less than 100.Now we have a list
of those movie titlesthat probably,
can't say certainly,but probably are a match.So you can play with the values.Maybe 100 is still too large.But you can reduce that to 20,
or 50, or whatever you want.So what are the summary of
takeaways in this lesson?The six steps to
successful fuzzy matching--step one, remove
extraneous charactersthat are not necessary
for performing the match,or to look up, or to join.Step two, put all characters
in uppercase notationand remove leading blanks
or use the modifier thatignores the different cases.Step three, remove
extraneous wordsthat might not appear
in different databaseswhen matching that could
be causing problemswith the matching process.Step four, handle numeric
and character zip codes.Differentiate between
US and Canadianzip codes, because
they are different.And include leading
zeros when matching.Step five, choose a standard for
addresses, cities, states, etcetera.And step six, now that
you have been introducedto some of the fuzzy
matching techniques,perhaps use the Soundex,
the SPEDIS, the COMPLEV,and/or the COMPGED to help
you in the matching, lookup,or join process.And once again, the Soundex, it
matches words that sound alike,and ignores case, embedded
blanks, and punctuation.It works best, though, with
English-sounding names only.It essentially assigns a
code to each letter or word.SPEDIS function
translates a wordinto a smallest distance value,
returns non-negative values.An exact match returns
a value of zero.But the beauty is the matching
process can be controlled.The COMPLEV function, if
you decide to use this,computes what's known as a
Levenshtein Edit Distancescore, returning the
number of operationsthat were performed to
allow string one to beequal to a string two.An exact match
returns a value of 0.And once again, the matching
process can be controlled.The COMPGED function computes
a Generalized Edit Distance,or GED score.The lower the GED score,
the better the match.An exact match
returns a value 0.And just like the previous
SPEDIS and COMPLEV,the matching process
can be controlled.Let's now learn how to construct
logic scenarios with caseexpressions in PROC SQL.Two case expression constructs
exist, a simple formknown as simple case
expression whichsupports one or more
WHEN-THEN-ELSE conditions,compares an expression
against multiple values,specifies in a
quality check only,and does not support AND or OR
between boolean expressions.And a more sophisticated
robust formof the case expression known as
the searched case expression,it supports one or more
WHEN-THEN-ELSE conditions,permits the use of
comparison operators,and supports the use
of AND, OR, and NOTbetween boolean expressions.Case expressions serve
as an embedded constructto reclassify or regroup
data into separate and uniquecategories or groups.They are very handy.Case expressions look
something like this,the keyword CASE with
an optional columnname with one or more
WHEN-THEN conditions,and an optional ELSE.Every case must end
with an END keyword.And you have the ability to
assign a user-defined columnname from the results.Specifying the column name
in a simple case expressionreduces the number of keystrokes
required to be entered.For example, in
our SELECT query,we're selecting
title and categoryand then have an embedded
case expression on rating,WHEN G, then assign a value
of general audience, WHENPG, assign a value of
greater than 13 audience,ELSE assign greater
than or equal to 17audience to the derive
column, the new columncalled My_Rating.So the end result of this is
going to be TITLE, CATEGORY,and My_Rating.A search case
expression providesmore robustness by supporting
one or more WHEN-THENcondition and comparison
and logical operators.For example, we can
specify in our SELECT queryTITLE and CATEGORY and then
our embedded case expression.WHEN RATING equals G, then
assign a value a GeneralAudience to My_Rating.WHEN RATING equals PG, then
assign greater than 13 audienceto My_Rating.ELSE, assign greater than
or equal to 17 audienceto My_Rating.Now, this gives us the ability
to use not only equalityscenarios, but other
comparison operators,less than, greater than,
not equal, et cetera.A search case expression
permits two or more columnsto be combined with
boolean operators,or in this case,
logical operators.In our SELECT query, we're
specifying TITLE, CATEGORY,along with our case
expression, WHEN RATING equalsG AND CATEGORY equals Adventure,
then assign General AudienceAdventure to My_Rating.ELSE assign Other.So it gives us the
ability to createcompound conditions using
logical operators as wellas comparison operators.So search case expressions are
more robust, more powerful,and give us all the
ingredients to beable to construct our logic
scenarios intelligently.Now what are some
summary of takeaways?In this section, we looked at
logic and case expressions.There is two forms of case
expression constructs,a simple case expression which
is limited, but it's simple.And it reduces the
amount of keystrokes.The problem is that
it does not supportanything but equality checks.And it does not support AND or
OR between boolean expressions,and a search case
expression, whichis more robust and powerful
and gives the users the abilityto support one or more WHEN-THEN
conditions, WHEN-THEN-ELSEconditions, permits the use
of the comparison operators,along with supporting the
use of AND, OR, and NOTbetween boolean expressions.Let's now look at the
rules, strategies, and usageof indexes in the SAS system.What are the
benefits of an index?The way an index
is designed, it maybe able to improve
performance by providingfaster and efficient
access to data subsets,by returning rows in
ascending value of orderfor by group processing without
using the sort procedure.There is two types of indexes in
the SAS system, a simple indexconsisting of a single column
where the index name isthe same name as the column.The second type of index
is a composite index,representing two
or more columns.The index itself is assigned
a unique, user-defined name.An index file contains entries
organized hierarchically.Entries are connected
by pointers.The entries themselves
are organizedin ascending value order.Entries contain the
following information--a unique value and one or
more unique observations.This is referred to as the
Record Identifier, or RID.Something that the SAS
system does for us is itbasically reclaims unused
or used space from an index.One index file exists per
SAS data file or table.It possesses the same name
as the data file itself.And it's stored in the same
SAS library as its data file.An index represents
a separate entityand has a unique
member type of INDEX.A binary search is performed
on the index file itself,positioning the index to the
first qualified value thatmatches the WHERE clause
expression using pointers.Visually, entries
representing the index filelook something like the slide.This table shows unique
values for the movie rating G,PG, PG-13, and R, and
the Record Identifier,or RID, representing
the observation itself,so for example, on observation
21, the movie rating G,for observations 2,
9, 14, 15, 18, and 19,the movie ratings for PG.Indexes are not free.They do cost resources,
because an index must bemaintained by the SAS system.An index is updated
each time a table ismodified, for example, a new
row, deleted row, or updatedrow.The SQL optimizer
ultimately determineswhether an index will be used.Index usage-- an index is
triggered with WHERE clauseprocessing.Keep in mind though,
sequential accessmay be just as efficient
when a subset is large.Avoid defining more
indexes than are needed.Avoid constructing
indexes when the datafile page count from
the contents procedureis less than three.And the optimizer may
use a defined index,whether it's simple or
composite, when the subset is15% or less of the population.For example, the page
count from proc contentswhen you specify
the details option,you'll see the number
of data set page size.So this is very useful
piece of information.So become familiar
with the resultsfrom either proc contents
or proc data sets.This is very useful
metadata information.Now how do you go about
selecting columns?That's going to be
used in an index.Select column that are used in
queries containing WHERE clauseprocessing.The columns used
in an index shouldselect the fewest observations.When creating a
composite index wherethere is two or more
columns, the first columnshould be the most
discriminating.The second column should be
the next most discriminating,and so on and so forth.One way of understanding
the distribution of data,let's take, for example, the
variable category, the moviecategory, is to use proc freq.So if we delve into the details
of the data with proc freqand produce a
tables category, wesee that we have an
alphabetical listof the category values,
their frequency, the percentof that frequency,
cumulative frequency,and cumulative percent.Category by itself with
the exception of one valuefalls below the 15% rule.How does the movie
rating variable hold?If we see the distribution
of data for the ratingvariable in the movies table, we
see that we have G, PG, PG-13,and R rated movies.We look at the frequency
and then the percent.Three out of the four
values exceed the 15% rule.Consequently, the
optimizer may beless inclined to use the
movie rating variable or indexwhen processing a data set.Now how about the
distribution on two columns?What would that look like?Well, we can continue looking
at and using proc freqon the movies data set
by specifying tablesand the most discriminating
variable, category,followed by the next most
discriminating variable,let's say rating,
in a two-way table.We're going to suppress the
row and the column information.And we see by looking at this
table that we are beneathor below the 15% threshold,
which is a good thing.So how do you go about
creating an index,in particular, a simple index?An index references
one or more columnsin a table such as the
movie category or rating.The name given to a simple
index is the same as the columnthat the index is
being created for.Take for example, to
create an index with SQL,we use the create
index statement.So if we want to create an
index on the variable category,we can say CREATE INDEX
CATEGORY ON MOVIES.Now, this does require that
we have exclusive accessand use of the data set itself
in order to create an index.How about creating
a composite index?The name given to
a composite indexmust be different than the
individual column names.Following the syntax
from our simple index,we can use SQL to
create an index.In this case, we want
to create an indexon the movies table or
data set for CATEGORYas our primary variable, and
our secondary variable, RATING.So we're going to have to create
a user assigned index name.And the name we're going
to assign it is CATRATE.How about creating a
unique composite indexwhere the duplicate
values in indexare prevented from occurring by
specifying the unique keyword?Using the code we created to
create our composite index,we can use that same code and
just add create unique index.And let's say we want to create
a different composite indexfor TITLE and RATING, TITLE
being the primary variableand RATING being a
secondary variable.The index we're going to
create is called TITLERATE.As we mentioned in section
one, turning on messages,by specifying the
MSGLEVEL equal I option,helpful notes describing
index usage, sort utilities,and merge processing are
displayed on the SAS log.This is very handy, because
this is the only waythat I'm aware that we can see
and determine what index hasbeen selected by the optimizer.If we don't specify
options MSGLEVEL equal I--and this goes with
anywhere in the SAS system,whether we're using SQL, or
a DATA step, or anything.If we specify OPTIONS
MSGLEVEL equal I,this is the only way we can
determine and have it displayedon the SAS log which index, if
any, the optimizer has chosenfor us.And we see, by specifying
OPTIONS MSGLEVEL equal I,that we have an info
message appearingon the SAS log it says,
index RATING selectedfor WHERE clause optimization.What this means is, in lieu
of sequential processing,we're going to
dynamically accessthe contents of the data
based upon the index key.How can we further test
this to evaluate the index?We have something known
as an IDXWHERE equalsdata set option.This IDXWHERE data set option
ignores sequential processingaltogether, thereby
letting the optimizerto select which index is best.So if you're unsure
which index to use,perhaps choosing IDXWHERE equals
yes, it will automaticallyturn off sequential processing.The optimizer will
have no choicebut to choose one of the best
indexes for the query itself.And to provide
detailed informationas to which index was chosen,
we specify the OPTIONS MSGLEVELequal I system option.We have another way to
specify and evaluate indexes.By specifying the IDXWHERE
equals data set option,SAS can be made to ignore
sequential processing.So what we're going
to do is we're alsogoing to request timing
statistics, CPO, I/O,the amount of memory, by
specifying on our OPTIONstatement, MSGLEVEL equal I--that will give us
information about whichindex was chosen
by the optimizer,assuming one or more exist--and FULLSTIMER, which gives us a
complete listing of performancestatistics, resource
statistics, and thingslike this that will be
displayed on the SAS log.And we're going to specify
IDXWHERE equals yes, meaninglet the optimizer choose
which index is best to satisfythe processing of the query.We also have another
data set option.It's called IDXNAME equals
to evaluate indexes.Just like the
IDXWHERE equals, SAScan be made to ignore sequential
processing, thereby letting usto select which index to use.Now, both the IDXWHERE
equals and the IDXNAMEequals data set options are well
and good for testing purposes.I would refrain from specifying
them in production code.I'm a fan of letting the
optimizer choose whichis the best for
everything, whether it'sa join algorithm,
whether it's an indexor sequential processing.I don't like to
force the SAS systemto use one method over
another unless I'mtesting and evaluating.So just keep that in mind.And some general
index housekeeping--when a column
containing an indexis dropped from a
table or data set,then the index is also
automatically dropped.This housekeeping
operation is automaticallyperformed by the SAS system.There are times when an index
is no longer wanted or needed.In those cases, we may
want to remove an index.We can do this easily
with the SAS system,in this case, PROC SQL.We can specify drop index.And then assuming we know the
name of the index we want--if you're unsure
of the index name,you can always do a proc
contents or a proc data.Sets and it will
tell you not onlythe data set names, but
the index names, the viewnames, and other things.But we can display DROP
INDEX CATRATE FROM MOVIES.Again, this requires
exclusive use and accessof the data set in
the library itself.So what are the summary of
takeaways from this section?First, there is two
types of indexes--a simple index consisting
of a single column.The index name is the
same name as the column.The second type of index
is a composite indexrepresenting two or more
columns or variablesin the data set or table.The index itself is assigned
a unique user-defined name.Anatomy of an index--one index file exists per
SAS data file or table.It possesses the same name as
the SAS data file or table.It's stored in the same SAS
library as its data file,whether it's work, SAS user,
or some user-assigned libref.An index also represents
a separate entityand has a member type, a
distinct member type of index.And a binary search is performed
against the index file,positioning the index to
the first qualified valuethat matches the WHERE clause
expression using pointers.What are some rules
for index usage?Indexes are triggered
with a WHERE clause.Keep in mind,
sequential access maybe just as efficient when
processing or subsettinglarge data sets.Avoid defining more
indexes than are needed.Avoid constructing
indexes when the datafile page count from
the contents procedureis less than 3.And the optimizer
may use an indexwhen the subset is 15% or
less of the population.How do we go about
selecting columns?Select columns that are used in
queries containing WHERE clauseprocessing.The columns used
in an index shouldselect the fewest observations.And finally, when creating
a composite index,the first column
or variable shouldbe the most discriminating.The second column should be
the next most discriminating,and so on and so forth.Let's now explore the
PROC SQL macro interface.There is two types of macro
variables, a single valueor aggregate and a value
list, or essentially,an array of values.Single-value or
aggregate macro variablescan be created in PROC SQL by
specifying the INTO clause.In this example, we're selecting
the movie title and ratingfrom the Movies
table or data set.And we're creating
two macro variableswith the INTO clause, which
is an optional clause, usingthe colon.Colon, and then we're going
to name the macro variable.I'll call it mtitle
and then mrating.A WHERE clause is
specified to controlwhich row is selected
in the assignmentof macro variable values.Now, this is important, because
in the absence of a WHEREclause, the first row or
rows will automaticallypopulate the macro variables.The WHERE clause
gives some controlover what row, or
observation, or observationsare going to be used to
populate your macro variables.So in this case,
we're carrying onwith the same example
from the previous example,selecting title, rating,
creating two macro variables,mtitle and mrating, WHERE UPCASE
rating is either PG or PG-13.Multiple macro
variables can be createdby specifying a range of macro
variables in the INTO clause.In this case, we're using
or we see the INTO clausecolon mtitle1 dash--that's not a minus sign--colon mtitle3, comma,
mrating1 dash colon mrating3.In this case, without
a WHERE clause,we're going to be taking
the first three observationsand populating the contents
of the movie title and movierating for each one of
those macro variablesthat we're creating.And the percent put
statements just list outon the log what the values are.Macro variables can be created
from system variables usingthe INTO clause for reporting
purposes and whatnot.So for example, if we
wanted to select the systemdate and the system time,
and assign a format of date11and time5, and store the results
into two macro variables,the macro execution date
and macro execution time,we can do this by just
selecting one observation,the first observation.It's going to give us this
valuable information thatcomes from the system
date and system time.It wouldn't be important
to select all observations.We only need one
observation to do this.And then we can use that content
in our reporting, et cetera.We also have the
ability to createwhat's known as a value
list macro variable.We're going to construct a
tilde-delimited value macrovariable containing
values of a single columninto one macro variable.So in this case, we're going to
continue using the into clause.We're going to read in the
data for the movie title,storing the results out for
PG-rated movie titles separatedby tilde.The separated by keyword
with the into clauseallows us to create
an array of values.Each distinct value is
separated with a tilde.The reason why a
tilde was specifiedis a tilde is found
very infrequentlyin the world of data, so
it's a good separator.Continuing with this theme,
we want to select valuesfor searching purposes
with a %scan function.And we're going to accomplish
this with the WHERE clause.So for example, using that
same example we just did,select title into colon
mtitle separated by tilde,where rating equals PG.We're going to use the
%scan and a %let statement.And we're going to assign
the value for the third wordfor the macro variable
we created, &mtitle,looking for the third
word, in other words,the second after
the second tilde.And we're going to store
the results in scanvar.It's a new macro
variable that's goingto contain the name of the
movie of the third wordin this list of values.Then we're going to
use in our WHERE clausewhere title equals
amper scanvar.So amper scanvar
is going to resolveto whatever the third word is.So what are some of the summary
of takeaways in this section?There is two types of macro
variables, a single value,or aggregate, and a
value list representingan array of values.The macro variables that are
created with the PROC SQL INTOclause is magnificent.It gives us power to be able to
use these content in the macrovariables anywhere
in the SAS system.In this last
section, we're goingto look into
data-driven techniques.Programming languages
are often classifiedby their basic features into one
of the programming paradigms.Three popular programming
paradigms are in use today.Procedural programming--
procedural programmingis represented by blocks of
code being organized logicallyby function such as data
input, data processingor manipulation,
and data/results.Object-oriented
programming is representedby a combination of
functionality or behaviorsand data attributes hidden
inside an object whichcan then be arranged into
what's known as classes.The third data-driven
programmingis represented by data
essentially controllinga flow of execution
in a program.So what is data-driven
programming?Unlike procedural
programming languagesand object-oriented programming,
data-driven programminginvolves decisions
and processes that arecontrolled by the data itself.Why design data-driven programs?Data controls the flow
of a data-driven program.It has the ability to
write other programs.It also allows us to minimize
hard-coding approaches, whichtranscends into being
easier to maintain.And the hope is to reduce
programming efforts alongwith costs, providing
reusability and cost savings.Let's take a look at a
data-driven program example.A data-driven approach
will be illustratedto create multiple Excel files.Excel is the world's
number-one software product.And we're going to
see how we can do thiswith the SQL procedure
and the macro language,triggering by calling a
macro-containing macrocode, PROC SQL, the ODS Excel
destination, and proc freqto send output and
results to Excel.Step one, we're going to use
PROC SQL with the INTO clause.PROC SQL with the
into CLAUSE is usedto dynamically create
single-value and valuelessmacro variables.In this example, this first
part, step one of this exampleshows that we are
essentially creating a macro,calling it multi
Excel files, selectingthe count of distinct ratings.So we have four unique ratings
in the movies data set, G, PG,PG-13, and R. And
then we're goingto store that into a macro
variable called mrating_cnt.We're then going to select the
distinct rating, the valuesthemselves, storing the results
into a macro called mrating_lstseparated by tilde.We're going to order the results
in ascending order by rating.Step two, both macro variables
are processed with a proc freqand an iterative macro %do
statement, a %scan function,and where equals
data set option.Essentially, we're going
to iterate through this,however many unique values there
are for the rating variable.In our case, there is four.So we're going to
iterate through thisdo loop four times.This macro do loop is
going to structure through.And for each movie rating, it's
going to create an Excel file.And it's going to
name the Excel file.For example, the
first pass through,it's going to name
it G_list_Rpt.xlsx.It's going to
generate a proc freq.It's going to do this four times
for each distinct movie rating.The results are four
separate Excel files,one for G-rated movies,
another for PG-rated movies,a third for PG-13 movies, and
a fourth for R-rated movies.What we've taken
advantage of is,using the ODS Excel destination
along with proc freqand the capabilities
of the macro languageto create a reusable
tool with SQLto create our macro
variables to createa very handy way to
dynamically create Excel files.So what are the
summary of takeawaysI'd like for you to
know from this section?What is data-driven programming?Unlike procedural
programming languagesand object-oriented programming,
data-driven programminginvolves decisions and processes
that are controlled by data.Why design data-driven
programming programs?Data controls the flow
of a data-driven program,so it behooves us to have
clean data or reliable data.It also has the ability
to write other programsand to minimize
hard-coding approaches,resulting in something
that's easier to maintain,also reducing programming
efforts along with costs,providing reusability and a
cost savings to the organizationitself.In conclusion, we explored many
powerful PROC SQL programmingtechniques, techniques like
First., Last., Between,and nearest neighbor processing.We then looked at join
algorithms and execution plans,join algorithms like the
step loop, sort merge, index,and hash join algorithms,
and execution plans _Methodand MSGLEVEL equals I.We looked at fuzzy matching
programming techniquesincluding Soundex, SPEDIS,
COMPLEV, and COMPGED.We then looked at
logic scenarioswith case expressions.And there are two types of
case expressions, simple caseexpressions and search
case expressions.We then turned our attention
to index rules, strategies,and usage, creating
two types of indexes,simple and composite indexes.We looked at the PROC
SQL macro interfaceto create two types of macro
variables using the SQL INTOclause, the single value
and the value list.Finally, we looked at
data-driven programmingtechniques using PROC SQL,
the macro language, proc freq,and ODS Excel.A little self-promotion--I've written PROC
SQL, Beyond the BasicsUsing SAS, the Third
Edition for SAS Press.And it contains intermediate
and advanced PROC SQL contentincluding fuzzy matching
and data-driven programmingtechniques.I'd like to think each
and every one of youfor viewing this video tutorial.And I'd also like to think Lisa
Mendez, the SAS Global Forum2020 Chair, her EC, SAS
Institute, and all the userseverywhere.Thank you."
75,"Hi, this is Ari Zitin,
and this workshopis deploying and managing
models using SAS Viya.We'll be learning how to use
some of the tools in SAS Viyato make it easier and more
efficient to keep trackof the models that you build
and where and how you'regoing to deploy them.When we build models,
we like to startby looking at the analytical
lifecycle to motivate our work.We start with data.We've collected some
sort of operational datafrom our business, or
we've collected datathrough some sort of
data collection program.We need to prepare
this data for modeling,put it into rows and columns,
figure out useful inputsfor building predictive models.In the discovery
stage, we're actuallygoing to learn things
about this data.So we might look at
some of the inputs,understand the distributions,
look at relationshipsbetween inputs and targets.Once we've learned a
little bit about the data,then we'll go ahead
and build the model.That's really still part
of the discovery phase.This is building predictive
models for business value.It could be
something like tryingto predict who's going
to default on a loanso we can decide who to
prioritize assigning loans to.After we've done some
discovery and built our models,we go to deployment.This is actually
using the modelsto generate business value.So I mentioned an example would
be trying to figure out who'sgoing to default on the loan.Deploying the model would
be, actually, offering loansto the people we think are
unlikely to default on loans,or rejecting people
for loans whowe think are likely to
default. The deployment stageis actually using the model
to drive your business value.In this workshop, we'll
be mostly focusingon the deployment phase of
the analytical lifecycle.But it is a cycle.So once we deploy our
models, we collect more dataabout how that deployment went.When we've decided who
to give a loan to or not,we can afterwards--9 months later or a year later--determine whether or not people
are paying back their loanon time to then build another
machine learning modelbased on previous data from
the previously deployed machinelearning model.So the idea of the cycle
is that every time wedeploy our models, we're
going to generate new datafor further models
in the future.And we'll see in
Model Manager, whichis the software
we'll be using today,there are tools that
are going to makeit easier to keep track of
how your models are eitherdeveloping or potentially
degrading over time.In this demonstration, we'll
be registering some SASand open-source models from
pipelines and from some code.So we'll see some demonstrations
using Model Studio,and then moving into
a Model Manager.We'll start by going to SAS
drive, which is the SAS Viyalanding home page, basically.I'll sign in with my
username and password.We just have a
student test account.I am not an administrator,
so I will notsign in as an administrator.We're just going to
use this as a user.Basically, someone--
an analyst who'sbuilding predictive models.We land on this SAS
Drive home page.We're actually going
to go to Build Models.So in the top left, I'll click
the Show Applications menu,and I'll select build models
to take me to Model Studio.So this is the SAS Viya
server/Model Studio.Model Studio is
organized in projects,so we will start by
creating a new project.We'll name our
project Model ManagerWorkshop because this
workshop is on Model Manager.We'll start with
a blank template,and we'll do a data mining
and machine learning project.We'll browse for
some data, and we'llupload a local data
set up to the server.So I'm going to click Import,
choose Data Menu, Local Files.I'll select a local file.I've got a little data
folder in a course folder.I select this HMEQ.CSV.This is a comma
separated value file.I'm going to upload this
into the public library.And I'll just click import item.Oh, the table already exists.I forgot to click Replace file.Someone had already
imported this.I'll go ahead and just re-import
it just to show the steps.That someone was probably
me when I was testing outthis workshop earlier.Now that the data
has been imported,I'll click the Available tab.And we'll just look
at it real quick.If I click the
HMEQ data table, wesee after it loads
for a second, itgives us some information
about the columns in the data.There's a column BAD, which
stands for binary applicantdefault. This is
banking data, soinformation about
banking customers,historical information.Binary applicant default
is whether or notthese customers defaulted
on a home equity loan.The HMEQ stands for
home equity data.And of course, the rest of
the inputs then loan more duevalue.These are all inputs about
the account information.So the value of the
loan, the mortgage valueon the house where
they're taking equityout of, the reason
for their loan,their job, their years
on job, various inputsthat will be used to try
and predict whether or notthey default on this loan.I'll click OK.Now click Save.The very first thing we do
and we've created a projectis set some metadata for our
predictive modeling task.So I mentioned that we
have this Target BAD,binary applicant default.
The very first thingwe'll need to do is tell
the software that we wantto use this as a target for
all of our predictive modelingin the subsequent modeling
that we'll do in Model Studio.So I'll select that.And of course, it
immediately warns usthat we do have to assign a
target before we do anything.I'll change BAD from
input to target.And I won't make anymore
metadata changes.We'll use the rest of the
columns in this data tableas inputs for our predictive
modeling algorithms.I'll click Pipelines to
go to the Pipelines tabto build a machine
learning pipeline.We start with data.Some of our values
in our data aremissing, which I happen
to just know because I'mfamiliar with the data set.We might have wanted to explore
it to double check that.I'm going to right
click on data,add child node, data mining
preprocessing, imputation.I want to impute missing
values in the original data setso that when we build
our predictive models,we don't have any missing
values that we're feeding them.By default, we're
replacing class inputs.That's categorical columns
with the count method,which is basically the mode.So whichever level
is most common.And we're replacing
missing valuesfrom interval columns
and numeric columnswith the mean of the column--the mean of the
non-missing values.I'll add a predictive
model here.I'll right click on
Imputation, add childnode, supervised learning,
gradient boosting.And I'll leave the gradient
boosting on the defaultsettings, but I'll go ahead
and also add a neural networkmodel.So I'll right click
on Imputation,and child node, supervised
learning, neural network.We've got two
predictive models builtby SAS, a gradient boosting
model and a neural networkmodel.We'll also added in
an open-source model.So I'll right click on
Imputation, add child node,this time it's
miscellaneous, and we'lladd an open-source code node.Before I do anything,
I'm just goingto right click on the
open-source code nodeand rename it just
so I know what it is.I'm going to be building
a Python forest.So I'll name it Python Forest.So I'm going to build a forest
model using scikit-learnin Python in this node.From the node, I could
right click and select Open,or I'll just click over here
and click Open Code Editor.There's some notes
about, essentially,building Python code in here.But I'm actually just going
to copy and paste some codethat I have in a file.So I go to my desktop, I'll open
my pythonforestcode.textfile.This is just some code
that I prepared in advance.I'll copy all of the code.I'll go in here
and paste it over.It actually wants me to
use control V to paste.I'll paste using
control V. Real quick,just before we run this code,
let's look over it real quick.We're going to
import the ensemblemodel from scikit-learn.DM input is a variable that
SAS automatically renders,which is the input variables
set in metadata that we alreadychose.So this is a Python
variable SAS defines for us.And we're going to add the
partition to the inputsand call all of these
inputs full X, which we'regoing to feed to scikit-learn.We need to dummy encode
our categorical variable.SAS does this automatically.Python requires we
do it manually here.We want to split our
data into training data.That's essentially looking
at that partition variableand selecting out the
training and validation data.For now, we just need
the training data.We also need the targets
for the training data.In SAS, we just have
the entire data tabletraining inputs and targets.Scikit-learn expects
them separate.We have some parameters that
we set for the random forestmodel.And then, we build
the model naming itsomething DM model, which
is actually a variable thatsaved in the SAS.We use the FIT
method on this model.This is scikit-learn
code to actually fitusing our inputs and our
target for the training data.When we do this, it renders
variable importance tablesin the form of data frames
that we can consume.So we get variable name
and variable importancefrom scikit-learn.We're going to write this out as
a comma separated value, a CSVfile.And we're going to put it in
this location DM_nodir, whichis a directory created by
Model Studio, essentially.So this is code that's going
to be run on the server.It's run on a single
machine in Python,but the results are
going to be savedto the server in this
particular location.And that means that
Model Studio willbe able to consume these results
and display them to us later.At the very end of the
code, we score the data.And we output the
scored data as thisobject DM scored DF, which is
a data frame of scored datawhich will be consumed by SAS
for generating assessments.So here we're actually
scoring the validation data,or actually the full data set.Not just the training data.The training and
the validation data.And the scored
columns are PBAD 0for the probability of binary
applicant default being 0.And PBAD 1 for the probability
of binary applicant defaultbeing 1.We'll save the code.We'll close our results.Not our results, we'll
close the code editor here.I'm going to right click
on the Python forest.This is a predictive model.So I'll select move to
supervised learning.When we moved it to
supervised learning,we notice it goes from orange
or yellow to purple telling usthat this is, in fact,
a predictive model.We have the option to
generate a sample of the data.Our data set is only
5,950, I believe,rows, which means that if we
sample 10,000 observations,we'll get the full data set.So I'm going to,
in fact, just goingto turn sampling method to none.Normally, we might want to use a
data sample because SAS Viya isa distributed
computing environment,and Python runs on
a single machine.So we'd have to kick back all
of the data to a single machine.This data set just
isn't very large.We want it to run quickly and
easily for this demonstration.So we don't really need
to do a sample here.I'm also going to uncheck
include SAS formats because wedo not want to include
the SAS formats when wesend the code over to Python.It doesn't need them and
might confuse it a little bit.All right, now that we have
our full pipeline created,we can run the pipeline.Now that our pipeline
is finished running,we can right click
the Python ForestModel and select Results.These results include
some results from Python.So we see this Python
output down hereon the bottom left, which is
when we printed the model,it's a random forest classifier.And it lists all the
settings for the model.We also see the raw Python code.I just like to point this out
because I think this is neat.We see here, below
at the bottom,this is the code that I
pasted into the code editorbecause this is the Python code
I wrote to create the model.At the top, this is
Python code that SASwrote to create these
variables that I talked about.So things like the
node directory,which is the location where
the files and the nodemight live for them
to be surfaced.We'll see one of those
files in just a sec.Some of the categorical
and interval inputs,all of the inputs, targets.It also has import--importing the PANDAS module,
as well as loading the datainto a PANDAS data frame.So various things that SAS
does for you automaticallyin the Python code node.This RPT_var_CSV is the
variable importance tablethat we wrote out as a
CSV file from Python.When we saved it into that node
dir location, that directoryfor this Python node,
this Model Studio node,it allows us to surface this
table here in the Results tab.So we see variable
importance for someof our imputed variables.Notice that some of these
are categorical variables,they have many levels.I'll go over to Assessment.These assessments are
created automaticallyfrom the scored data frame
that Python gave to SAS.So we get cumulative lift
plots for training, validation,and test data.We also get ROC charts for
training, validation, and testdata.So these ROC plots were
created from the scored data.This is done by SAS,
although the scoredata was generated from the
Python model that we created.I'll close the results here.And I'll go to the
model comparison node.Right click, select Results.We see the SAS gradient
boosting model ends upbeing the champion model
based on the KSU statistic,although it also has the
lowest misclassification rate.We can see more FIT
statistics here.When I go to full screen, I can
look at average squared error.The SAS gradient
boosting model definitelydoes the best there as well.Evidently, this SAS
gradient boosting modelwill be our champion model.If I go over to
assessment, I cancompare all of these models.So I could look at ROC for,
let's say, 1 partition.I'll choose the
validation partition.I could compare the three
models on validation data.Looks like the Python forest
and the neural networkdo fairly similar, and the
gradient boosting modelis much stronger.I'll close the results of
the model comparison nodeand go over to
pipeline comparison.We really only have one
pipeline in our Model Studioproject here.But pipeline
comparison is the waythat we're going to get
access to the deploymenttools in Model Studio.So we see our gradient
boosting modelis selected as our champion
model for pipeline one.If we had other pipelines, we
might see other models here.Here we just see the results
of the gradient boosting model.We'll go ahead and register
this model as our champion modelinto Model Manager.So I'll click these three dots,
the snowman menu on the right.And I will select
Register Models.Once the model is
registered successfully,we can go into Model Manager to
look at the model in the modelproperties.So I'll go up to the left,
select the Show Applicationsmenu, I'll choose Manage Models.This takes us to server--
the SAS Viya server/SAS ModelManager.Model Manager is
the application willbe using to manage our models.Here we see a list of models.Here's the gradient boosting
model that I just registered.Here's a couple of other models.They were registered
by the student account,but they were registered by
other instructors associatedwith other courses.You'll notice some of these
are text analytics models thathave been registered.I'll click over
here on the left.I'll click this Projects option.I would never have
guessed this wasprojects just from the picture.So I usually like to click
these arrows at the bottomto get a list of options here.So I see projects.I click on Projects.And I'll select the Model
Manager Workshop Project.Here, we have the
gradient boosting model.If I look at variables here,
we have variables registeredinto this model project.Here's the input variables.All of the inputs needed.The output variable
is the probability.EM event probability is
the probability of defaultthat is generated by this
gradient boosting model.We'll go ahead and set some
project properties here.We could add a
useful descriptionto tell people what's going on.It's automatically identified
that this is a classificationproject.But we should give it the target
variable, binary applicantdefault, or BAD.The target event value is 1.The target values are 1 for the
event and 0 for the non-event.The target level is binary.So it's a binary target.And the output event
probability variable--and this is important-- this
is EM event probability.The reason I mentioned this
is important is later on,we'll see if we don't
have SAS models,this might be named
something different.So this is the default
name for SAS models,but other models might have
a different output eventprobability variable.I'll select EM
event probability.And I'll click Save to
save the model properties.Now that we have the
model properties,we could do things like
score tests for our model.But before we do that, let's
register in one more model.We're going to register in an
open-source model from R code.Let's look at some
R code real quick.So I'm going to go
into my course folder.I'll open up this
Rlogisticregressioncode.text.I'll go through
this fairly quickly.Let me make this a
little bit bigger.We start in R by
imputing the data.We load our CSV file in.So here's our raw data file.We're going to find missing
values for our intervaland nominal variables.And then, we're going to
impute our interval and nominalvariables.We're going to
partition the data in R.We're going to build a
logistic regression model.So this is all data
preparation up above.Here, we're actually
building the model,a general linear model.So it's just a logistic
regression model.The target is BAD,
and we're goingto use all of our input
variables to predict it.And finally, the
important step at the endhere is we're going to
load the PMML libraryand save our model
as a PMML model.That's predictive
modeling markup language.It's basically just a
standard for model formats.We'll save this XML
file in that format,and we will load that
into Model Manager.In addition to saving the
XML file, which reallyis all we need to load our
model into Model Manager,we'll also generate some FIT
statistics and some informationthat we will find
useful in Model Managerin the form of JSON files.So here, we're just
writing some JSON files.We load this JSON library.We're writing this DM CAS
FIT stat SAS JSON file.We're essentially modifying it.This was a template,
and we're filling outthis template with the
FIT statistics valuesfrom this logistic
regression model.We fill it out with things
like average squared error, Cstatistic, we do test
data, validation data,and training data.We also create a JSON
file for the ROC curve.So again, we're just
writing informationthat the model generated
out into a JSONso Model Manager can consume it.We're going to generate
predictions for our model,then generate the
ROC information.We have to do this manually
in R because R is the--essentially, R is the
source of the model.That's how we're going to
actually generate the scores.So we generate sensitivity
and specificity,use that to construct
the ROC curve,and export it as a JSON file.I will exit this.We've already run this code.So we have-- and I'll
just point this out--we have the XML file
that was generatedas well as some of
the JSON files--that FIT statistic JSON files.Here's the two that
we just created.I will go back to Model Manager
and I will click Import.Lost my mouse.I'll import a model.Select one file at a time
to import model contents.I'll select the XML
file that I created.So we'll look at
Rlogisticregressionmodel.xml.Click Import.I'll just name it R
Logistic Regression Model.That's a fairly
descriptive name.I'll click the box
next to the model,and I'll register this
as a challenger model.So this model was
the gradient boostingmodel defaulted to being
the champion model,and that's that
role right there.So we see that's the
champion model role.We'll register this
as a challenger model.I set as challenger.When I set the model
as a challenger model,it tells me we expect this
output EM event probability.That's what the other
models were generating.What's the output
from this model?Well, this is the R model,
and it outputs the target--the prediction for the target
as predicted_targetname.So in this case,
it's predicted_BAD.I'll click Save.This registers this model
as a challenger model.I'll go over to scoring.And I want to do a test score
of this model to make surethat it works correctly.So I'll create a new test.I'll just name it Test 1.When I choose the
model, I'll choosethe R logistic regression model.For my input table, I'll
just score the HMEQ table.If I had some sort of score
data, I could test it on that.I'll just use the same data set.I'm really testing the
mechanics of scoringjust to make sure
that the system hasloaded the model correctly.Click OK.I can click Save
to save the model,but I can also just click Run
to save and run the model.By default, it'll
output the score datato the public CAS library.I'll click run to
run the score test.The score test is
completed successfully.We can click the Results
and look at the results.I just real quick want to
see that there is actuallya column of score data.So I'll click Output.This allows me to select the
columns I want to look at.Predicted BAD is the column
that I'm interested in.It is visible.Let me just scroll down.Grab this window, scroll
all the way to the right.Here we get generation predicted
BAD for some of our values.I guess we were
missing predictionson some of the columns
that are missing values.And that's because when we
actually scored the data,it needs the input.So we didn't impute
any missing valuesbefore we generated the scores.So this is a lesson for
us that when we actuallyscore our production
data, we wantto make sure to score it
by imputing the missingvalues first so that it
knows how to score it.I'll close these results.I'll go back to models.I'll click on the R logistic
regression model to open it up.When I've opened the
model, we see a listof files associated with it.Here's the score code,
the XML file we uploaded,and an XML of
inputs and outputs.These are just simple XMLs
listing inputs and metadata.We're missing some FIT
statistics and the ROC curvethat we created in JSON files.So I'll click Add.I'll select a few files to add.I will add dmcasfits.json.And then, hold Control and
make sure to add dmcas_roc.jsonas well.I'll click OK I'll click Add.And now, we've got more files
loaded into Model Managerto give us information
about our model performancewhen we build the model.Adding these also allows us
to do some model comparisonsbetween models in our project.So I'll close this R
logistic regression modelto go back to the project.It actually takes me
back to the Models tab.I'm going to go
back to the project.Here's our Model
Manager workshop.I'll click Gradient Boosting
and R Logistic Regression Model.When I've selected
them both, thiscompare option is available.So I'll click Compare.This generates a
side-by-side comparisonof some model information.So we see they have
the same inputs.I could look at it
a little longer,but they definitely
have the same inputs.The output variables are
going to be differentbecause the SAS model generates
all these imputed variables.Whereas, the R Logistic
Regression Model just generatesthis predicted BAD.It does the actual
imputation in placeis basically the way
I think about it.So SAS generates new
variables, whereas Rimputes over the
existing variableswhen we actually
train to the model.We also see some FIT statistics.We have FIT statistics for
training data, validation data,and test data.The values from the SAS
gradient boosting modelget loaded automatically
from Model Studio.These data values came from the
JSON file that I just uploaded.And we see these plots.Lift, captured lift, captured
response, and then ROCcumulative captured response.And then, down here we see ROC.I wanted to highlight we
see an ROC plot for the Rlogistic regression
model because weloaded that ROC JSON file.So we loaded that information
from R into Model Manager.We don't see these
lift charts because wechose not to load that data.We could have easily done
that, just would havebeen a little bit more work.I can click Show Differences
to see the differencesbetween the model.Of course, most of the FIT
Statistics and the plotswill be different.So I think that's more
useful when you lookat input and output variables.I'll close these results.These are two models.If we have built our
models and we're satisfied,we can choose one as a champion
and one as a challenger.But if we find our models
aren't doing very well,or want to change them,
we have some optionsfor retraining our models.These retraining
options, essentially,are for building a new model
using all of the same settingsif you've gotten new data.So in our next
demonstration, we'llbe looking at monitoring
model performance over timeto try and see if your
model's been degrading.In this demonstration, we'll
look at some performance datasets.So different data sets
over time that we'vecollected to evaluate how our
model performance might degradeas we move past the time
when the model was created.So I'll go back to the top left,
select the Show applicationsmenu.I'll click Manage Data to
upload some new data sets.I'm going to upload my
performance data sets.This takes us to
SAS data explorer.This just allows me to
upload some new data.I'll go to Import,
Local Files, Local File,and I'll select
HMEQ_1 through HMEQ_4.I will click open.I'll click Replace file because
I might have imported thema little bit earlier.And I'll click Import All.Now that these
files are imported,I'll go back to Model Manager.So I'm going to navigate
back to Manage Models.This takes me right back to
my Model Manager workshop.I'll click on Performance
to create a new performancedefinition.These are performance
reports whichevaluate model performance
on different performance datasets, which are data sets that
you've collected over time.So of course, it's your
responsibility as the modelerto have the right data.You feed it in here, and it will
tell you how the model changes.When we create a new
performance definition,we will use a
library that containstables with a specified prefix.I loaded these tables into
the public CAS library.I may have forgotten to mention
that, but that is the default.So I'll scroll down to Public.Select OK.And the prefix is HMEQ.So all of the data sets
were HMEQ_1, HMEQ_2.That ordering, I guess,
the numbers there matter.And the software is
going to read themout and build performance
reports based on these numbersdata.We have some information about
what those numberings are.Basically, it has to have a
sequential number at first,and then a time ID.So you'll see the
data set was somethinglike HMEQ_1_Q1, HMEQ_2_Q2,
et cetera, et cetera.I'll name this performance
Test Performance Report.I will not include
a description.It's often a best practice.We've selected the
tables that we want.We're going to use
the reference model.So we'll just use
this champion model,the gradient boosting model.We want to evaluate it
on the champion model.It defaults to the right
input and output variablesas well as the right properties.So we'll just go ahead
and click Save herein our performance report.Now that I've created the
definition, I have to run it.So I'll click Run to run the
performance report definition.Now that the performance
report has finished running,we can look at how
some of the variablesmight be changing over time.So I'll look at the variable
distribution for Years On Job.And we see that as--
so these bins are justbins for different numeric
values of this Years On Job.And we're looking at
the different quarters.So we see as our years
are changing, basically,as our quarter is changing
throughout the year,our data is changing
dramatically.So we see the distribution
of this Years On Job variableis dramatically different
in these different datasets, which indicates,
potentially,that we might need to
retrain the model to scoreon newer data.Some of these other variables
don't exhibit the same pattern.So if we look at
things like CL age.This is pretty much unchanged.You only see one because all of
the others are underneath it.So this variable hasn't
been changed very much.So that's how I
read these plots.As we're looking for
differences across these binsof different--excuse me, differences
across these different colorsfor these different variables.We also have a
deviation index, whichis illustrating the same thing.But we see most of our variables
don't have very much deviationas we go from quarter
one, quarter two, quarterthree, quarter four.But our variable
Years On Job, YOJ,deviates significantly as we
go from quarter to quarter.I like to look at
this lift chart.We see at quarter one
we have the best lift.Quarter two, our lift is
going down a little bit more.Quarter three, we
have lower lift.Quarter four, our
lift is even lower.Basically, the
way I read this isas we move further and
further in time from wherewe trained our model, the
time when we had the data thatactually trained our model, we
see that the lift is degrading.Because we're capturing
less and less signal,there's new signals
in the data that we'rescoring that are not
present in the training datathat we used to build the model
because things have changedover these quarters.We can see the same information
in the form of the ROC curve.Quarter one has
the best ROC curve.Quarter two is next.Quarter three and quarter
four are a little bit weaker.We can look at model
evaluation statisticslike the Gini index
or the KS statisticto see that both of these
model performance statistics--bigger is better for these
assessment statistics.They are degrading as we go from
quarter one to quarter four.This tells us that we probably
want to retrain the model.So I'll go up here
to this button,click Actions, click Retrain.I'll retrain the model
on the newest data.So rather than I'm setting--
so I could just set the projectto say, retraining is
needed and let someone elsebe responsible for it.Since I'm trying to
demonstrate this for everyone,I'll be the one who takes
responsibility for retraining.So I'll retrain now
with a new data table.And I'm going to go ahead and
browse and actually importthe newest data table I have.So I'll go to Import,
Local Files, Local File.We have this HMEQ_5_Q5.This is the newest data we
just collected, perhaps,let's say, last quarter.So we want to retrain our
model on this newest datathat we have.I'll click Replace
File, Import Item.Once it's been
imported, I'll click OK.And I'll send this
to retrain the model.And we've triggered the
model for retraining.I'll go back to the models.I'll go ahead and refresh here.Sorry, I had to
refresh it real quick.It got a little bit
confused because Iasked for a little bit too much
at once before refreshing it.We want to look at all versions.It's still doing the retrain.So we're waiting for
version-- oh, here we go.So it just finished the retrain.Version two of the
gradient boosting modelhas now shown up, and that was
the model when I retrained iton the newer data.This is the newest model.So we want to actually
deploy this model.So I'll select this model.There's an option here.I'll select Publish to
publish to a scoring location.Currently, on this
virtual machine,we only have the ability to
publish to MAS, Micro AnalyticServices.But there are other publishing
destinations that you can have.And in our newest
version of Model Manager,there's actually the option to
publish to Docker containers,which is very exciting.So I'll click Publish.And when I publish,
it will send thisto this microanalytic
services locationso I can call scoring
basically via an API call.To call data any sort of data
that I have against this model,and SAS Viya will generate
the scores for that.I'll close these results, and
we'll look at the history.I'll go ahead and refresh this.So we can see what we've done.We created the project,
we've done some editing,we've set a couple of champion
and challenger models.And at the very
end of the day, wepublished a SAS
microanalytic services,this location MAS local.And incidentally, I just
like to point this out,if I back to models and
I click on this modelthat I published, this
gradient boosting model.It has this A store file.It's this incomprehensible file.I always like to click
here to show that it'san analytic store file.It's basically a binary file.And this is what is actually
going to be doing the scoring.So when you generate scoring,
you'll score against this file.Excellent.So those are our steps
for deploying and managingmodels using SAS Model Manager.This gives you the ability
to manage version controland history of
your models as wellas keeping track of
everything that's been doneand deploying them all
in some central location.Thanks.To wrap up, I wanted to point
out a few other deploymentoptions.We registered our models
in the SAS Model Manager.We published it
as an A store fileinto SAS Micro
Analytic Services.We could have also
registered our modeland published it as
DATA step score code.And we have a couple of
different publishing locations.SAS Cloud Analytic Services is
basically just publishing itinto the SAS Viya server to
score on the SAS Viya serverwhere we trained the model.SAS Micro Analytic Services is
where we published our model.This is a more portable format
where we can deploy it anywherewhere SAS code can run.SAS Event Stream
Processing allowsus to publish our models
directly into SAS Event StreamProcessing and
Engine for reading inand scoring streaming
data as it comes in.So the Micro Analytic Services
and Cloud Analytic Servicesare used for scoring
data at rest.And SAS Event Stream
Processing is usedfor scoring data on the move.We also have some
other options if you'reusing other SAS tools.So I just wanted to point out
another deployment pathway thatis a little bit more
complicated, so we don'tillustrated in this workshop.But I think it's
interesting to think about.We generate a SAS model using
DLPy, which is deep learningin Python using SAS Viya.So we're using Python code
to build deep learning modelsin SAS Viya.Once we've built these
deep learning models,for example, perhaps an
image classification model,we convert them into
the ONNX model format.So that's open neural
network exchange.So we've converted this
deep learning neural networkinto a standard format that's--I think of it similar
to that PMML XML filethat we generated for our R
logistic regression model.And then, we take
that ONNX modeland we can deploy it to
devices, for example.Like people's
iPhones or Android.Now, depending upon the device
you want to deploy it to,you might have to convert
the model a little bit.So to show a little bit more
complicated this pathway,we deploy that DLPy model.We can automatically create this
open neural network exchangeXML file, basically.From there, we have to convert
that XML file into somethingthat-- this is for
an iOS application.Something that the iOS
engine can actually read.So Core ML is tools
for machine learningon iOS developed by Apple.So we have to convert
our open neural networkmodel into something
that our iOS can read.Deploy it as an application.We have to write that
application on iOS,and then send that
over to a device.And SAS has some
demonstrations there.I can't really show it in
the context of this workshop.But SAS has created some
of these demonstrationsand customers can
do this as well.This is, of course,
much more complicated,and it's going to
require a lot more workthan deploying a simple
business application usingall of the SAS tools.But there's a lot
more flexibilityif you want to do
Edge devices or deploythings to people's phones.Excellent, thanks."
76,"Hello, everyone.Welcome to my SAS
Global Forum 2020 talk.My name's Chris Barefoot.And today we'll be talking about
natural language understanding,and more specifically,
annotating training datafor natural language
understanding.Let's just jump right in.So I figured we'd set a
base level for what are wereally talking about when we
talk about natural languageunderstanding.It's a pretty wide topic
in computer science.And so we're really, given
the short nature of this talk,going to focus down on
a specific piece of it.And so when we talk about NLU,
what we're really talking aboutis understanding
free-form text that wemight receive from users.That text is obviously
a natural languagethat the user's
speaking or typing.And we want to understand
something about it.And so today more
specifically, we'rereally talking about intent
classification and slotfilling.And so what I by
this is that whenwe receive a piece
of text, we wantto classify it into a
known set of categoriesor classifications of intents.So what does the user intend
when they provide the textthat they're providing?And then more specifically
within that piece of text,are there certain
entities or key termsthat we need to determine?And do they fill certain
slots in a broader intent?So that's setting a base level
for what we're talking about.So let's talk a little bit
about how it's actuallytrained in this case.So we are talking about
supervised learning.So all the data that
we're talking aboutis annotated and that it fits
into a known set of categoriesor classifications for
the machine learningto determine and then further
score or classify things into.There are three key
terms that we'lltalk about as we talk
about this training data.The first are intents.So these are the broad
categories that thingswill be classified into.So for example, we might
have one called order pizzawhere we're going to think
about all the different waysthat a user might say that
they want to order a pizza.Within an intent,
you have utterances.So these utterances are the
actual training data itemswith an intent, the
various ways we expectpeople to say these things.And this is really
what we're goingto hone in on today, how to
better flesh these out soyou can get good accuracy
with your NLU domain.And lastly, we're going
to talk about entities.So within these utterances,
within these piecesof training data, you'll want
to identify certain termswithin them for a
variety of reasons.One reason you might
want identify those termsis just to say that these terms
are special within my sentencethat I want to key in on that
these things are occurringand highlight their importance
within the sentence.Another reason you might
want to highlight themis to add variety, so maybe
then in this case, order.Maybe there's different
ways you can say orders.You can say, purchase or buy.And they all mean the same
thing within your domain--and furthermore, things
like toppings, pepperoniand sausage, another way you
could vary the items that youexpect to hear from users
without having to specify themall out by hand.And furthermore,
these entity valuescould be captured more than just
a hardcoded list of, let's say,certain types of toppings.You could have some sort of
named entity recognition goingon in the background.You could have regex matchers
matching on pieces of the text.So that's another way
entities can be a little morepowerful than just, oh, here's
an enumerated set of terms.So as I talk about
it today, I'm goingto draw parallels to
image classification,mainly for a number of reasons.I myself am a visual learner.So I think it's going to
help as we guide alongto think about how an
image classification mightwork in parallel with
text classification.And second, I'm a car guy.So any chance I get to show
cars on screens is a good thing.So let's think about
image classificationand specifically about the
variety of training datawe might provide.And I want to talk about
specifically qualityover quantity.So unlike maybe
image training data,text training data
can be hard to come byand requires a lot of hand
annotation ahead of time.For that reason, we're often
dealing with smaller datasets rather than larger ones.So if you had, let's
say, this trainings dataset for image
classification on the left,do we think it would
be able to classifya car like we see on the right?Probably not.It's got a brighter color.We're talking about
the back of the car.All we have is front
end images over here.So let's think about
this in the same waywhere we might think about text.So the first thing we'll talk
about in terms of varietyis phrasal variety.So think about
the different wayspeople might phrase
things to meantthe same thing within a domain.So in this case, we're still
talking about ordering a pizza.It's just as likely
that somebodymight say, how much does a
one-topping pizza cost as,what will you charge
for a one-topping pizza.Or for a one-topping pizza,
what's the current price.So you can imagine there's
lots of different wayspeople might phrase these.And you want to--just as you would want
to take different imagesfrom different angles of a
car, want to approach this as,what are the
different ways peoplemight say the same
thing, all of whichis supplying better training
data to the machine learningunderneath the covers.The next thing you want to think
about is verb-specific variety.So for those action
words in your sentences,how can you vary those
within your domain suchthat they still make
sense, so in this case,please delete the
file named input.json.You can imagine
replacing and havingan entity list
with delete of alsomaybe remove, trash, destroy.But the thing I want to
caution here is that, oh, well,you might say,
well, shouldn't itknow that trash and
delete are the same thing?Well, every domain is different.And every domain uses
terms in specific ways.And so in general, synonyms
are a pretty slippery slope.And I've included some
words at the bottomthat if you were to
just pick up a thesaurusare said to be similar or
closely related to the word""delete,"" but in our
specific domain instance heremake no sense.So just keep that in mind
as you're building outspecific entity values.The next thing I want to talk
about is less is often more.So if I were to say,
hey, here's an imagethat we can add to our
car classification dataset, you'd say, huh?I mean, yes, there are two
cars off in the distance.But there's so much
noise here in the image.And if we were to look
at the second image,OK, that's slightly better.I can tell there's
cars in the image.But there's still a
lot of background datahere that some machine
learning algorithmis going to try to make
sense of before it startsto identify that there's cars.And lastly, this last image
is still better still.But you can tell there's
clearly pictures of cars.But there's still
a lot going on.So I want to parallel
that to text.We need to think
about the same thing.So if I were to say on my shared
drive, if no one has it logged,delete the file
named input.json.This is a perfectly
valid utteranceyou might receive from
one of your users.And in it, inside
your domain, youneed to determine, what
are the things thatare important for this intent,
for this action, actuallybeing carried out by
the underlying system?So if it is important
that it's a shared driveand that no one
has a file lockedand you need the file name,
maybe this all is important.But realize when you
supply this utterance,you're telling the underlying
algorithm that all of itis important.So furthermore, if
we start to peel backthe layers of the onion,
you could say, oh,on my shared drive, delete
the file named input.json.So this is a little
more generic.Right now we're just
talking about, well,deleting a file name named
this may be on a shared drive.And as we peel it back
more, delete the filenamed input.json.And then just delete the file.As you're building
a domain, justthink about each
layer of this as,how generic do I want my
intent classification to be?If I'm only concerned
with deleting files,maybe the last one is perfect.If I need to know the
file name, maybe Ihave some of those as well.And don't be afraid to mix
and match them as well.This isn't an all or nothing.You could have, let's say,
the last two utterancesas a piece of
training data, meaningyou're going to capture
maybe more generic statementsabout deleting the file.But you're also going
to capture ones thatrequire the name be specified.And just know that at
runtime, you'll have to handleboth cases, one where the
file name might be presentand one where it's not.But just know that
the more restrictive--the more language you put
in a training utterance,the more restrictive you're
making the algorithm.And one word of caution--so as we peeled back
those layers of the onion,you can see where,
oh, well, I could justget down to one word.And I say that that is the
way to classify my intent.And I just want
to caution peopleagainst single word utterances.So in some cases,
they work really well,things like yes and no where
you want to just classify, hey,is somebody saying an
affirmative or negative?But as soon as you start having
utterances in training datawith just one important
term by themselves,like download or
error, just realizeany time a user's utterance
that they're supplyingmight have one of
those words in it,you're going to classify
it as that general intent.And so sometimes that's OK.But often you're going to end up
misclassifying and overfittingyour data.So just keep that in mind.Lastly, I just want to
talk about always consideryour domain when you're
creating these and reallythe specific language
that your users might use.So got pictures
of race cars here.These are cars.But more specifically, it's
a certain subset of cars.And so similarly,
when you're supplyingthese utterances for
training data, think about,what are the types of things
you expect users to say?And then, what
are the boundariesbetween especially
closely related intentswithin a system?And how much ambiguity
maybe are youcreating with
blurring those lines?So the clearer you
can separate thingsinto separate intents, which
have very concrete differences,the better off
you're going to be.And so lastly, I just want
to plug that all of thisis applicable in SAS's new
product, SAS Natural LanguageStudio.This is a chatbot
framework, one pieceof which is natural
language understanding.And inside of here, we've
built a really nice GUIto basically help you train all
of this, this NLU domain data,using natural language examples,
using just WYSIWYG clickand highlighting of entity
values, and a coupledifferent matchers to allow
you to specify whether you wantentity values to be matched
very generically or veryspecifically.So go ahead and check
that out, and let usknow if you have any questions.And thank you very
much for your time."
77,"MAGGIE DU: Hello, everyone.Today, my partner
Juthika and I willpresent a demo of real
time image analyticson edge devices.My name is Maggie Du.I'm a machine learning developer
on the SAS deep learning team.I focus on the development of
computer vision algorithms.I joined SAS in 2018, after
I got my PhD in statisticsfrom NC State University.JUTHIKA KHARGARIA: Hi, my
name Juthika Khargharia.I am a principal
solutions advisor.I work in the Internet
of Things divisionat SAS, specializing in
business applications of machinelearning and deep learning.I have a PhD in astrophysics
and planetary sciencesfrom University of
Colorado at Boulder.MAGGIE DU: OK.In a typical computer
vision project,a user may have some
images or videosin which they want to find
something, such as identifyingfaces in the image.No matter what insights
a user wants to find,a computer vision
project usuallycontains three basic steps.First, acquiring
images for analysis.Very often, images are in high
volume and even in real time.Second, processing images
through our analysis pipeline,which typically involves deep
learning models nowadays.Third, understanding new
images using the analysis modelfrom the previous step
by deploying the modelto the place where
images are generated,such as an edge server
connected to a camera.In the next slide, I'm going
to use the photo of a dogand a cat of mine to
illustrate the differencesamong some computer
vision tasks.The left one, image
classificationis mainly giving a
label to an image.And this particular image can be
labeled as animal, dog, or cat.The second one,
keypoints detection,involves finding multiple
interest points in the image,and can be used to find the
facial landmarks of an object.As shown in the example,
we use blue dotsto highlight the eyes, the ears,
and the nose of the animals.Object detection is to
place a bounding boxfor each object
in the image so wecan have the object type
and location simultaneously.The last one,
image segmentation,is a different approach.It looks at the entire
image, and assignseach pixel in an image
to pre-defined categoryso we can know the boundaries
and shape of each object.When it comes to the
new image scoring,there are several
reasons why we wantto do the scoring
on edge devicesinstead of on the cloud.The strongest driving
force is a low latency.The round trip data has to
travel between edge and cloudis completely avoided.And at the same time,
the risk of being hackedduring transmission is reduced.It also helps to
lower the budget,as we don't need to upgrade
our data center constantlyfor data storage.Now we have some basic
ideas about computer vision,as well as a need
for edge deployment.The next thing a user
wants to figure outis how those steps live in
the modern IT architecture.This slide presents a
typical architecturefor computer vision that has a
server side for model trainingand an edge side
for online scoring.On the left-hand side,
model training usuallyhappens on a server using
some raw images or videos thathave been previously collected.We load the images
to the server,pre-process them if needed,
like resizing or cropping.Then we use them to train
and deploy new models.Once we have the model, we
save it using a portable binaryformat called Astore, so
that it could be deployedto various types of devices.On the right-hand side, the
model deployment usuallycontains two flows, one
to the model and the otherto streaming the data from
either a local file or a cameraconnected to the device.I'm going to use the street
scene images for autonomousdriving as a demo to
show how it works.I used 4,800 images to train
a lightweight deep learningmodel which can assign objects
in the image to 13 categories.As in the central images,
objects in different categoriesare masked in specific colors.For example, all
pictures of vehicles,including the dashboard
of a driving one,are in medium blue, and
pixels of roads are in green.After having the
portable model, weuse different devices to
predict new images that are notincluded in the training set.Here we show a comparison of
computing power and scoringFPS of several GPU devices.Jetson TX2 has the
lowest computing power,while the scoring speed of
it is 10 frames per second,which is totally enough for
self-driving of daily use.Here I'm going to play a
video generated by Jetson TX2.[VIDEO PLAYBACK][MUSIC PLAYING][END PLAYBACK]I hope you liked that video.Next my partner, Juthika,
will show you another use caseof image analytics on edge.JUTHIKA KHARGARIA:
Thank you, Maggie,for the introduction
and the nice example.So in my section,
I'm going to talkto you about a use case using
computer vision application.So the specific use case
will be around surface mounttechnology, which is
basically a method in whichelectrical components
are mounted directlyonto the surface of a
printed circuit board.And electrical components
that are mounted in this wayare referred to as
surface mount devices.Examples could be a
resistor and capacitorsthat you see mounted on
a printed circuit board.So the particular use case
within this technologyis known as a
head-in-pillow defectthat we are interested
in talking about today.So this head-in-pillow is
essentially an assembly defectin which the bumps
from a ball grid arraydoesn't melt together with the
solder based on the printedcircuit board.An example of how
that looks likeis shown on the graphic on
the lower right-hand sideof the slide, where you can
actually see a good solderjoint being differentiated
from a head-in-pillow defect.You can see that in
the good solder joint,both the BGA and the
circuit board solderhave melted together,
they have joined very wellin the good solder joint.In the case of the
head-in-pillow,you'll see that there
is a distinct differencebetween both of these
solders, and they have notjoined very well.And when you look
at the picture,it almost looks like a
head resting on a pillow,and that's where the name comes
from, head-in-pillow defect.So this is the type
of defect we wantto understand using computer
vision applications.So in order to do defect
detection of HiP defects,we use or we work with
these machines knownas X-ray inspection machines.So these are big visual
inspection systemsthat manufacturers
have, and they'reavailable in
different wavelengths.For example we also have optical
vision inspection systems.And those visual
inspection systemshelp you to look at the
components themselves.So you can actually
look at the capacitor,or you can look at the resistor
on top of a printed circuitboard.In the case of X-ray
vision inspection,one can look at
the joints, whichis not observable in
the optical wavelength.So what happens is you have
a printed circuit boardthat you insert into
one of these machines.The machine has a camera
that takes several images.It scans through the
printed circuit board,and takes images of
the joins themselves.An example of how
that data looksis also shown on the slide.So essentially, what
you get from the datais this array of
those dots on a board.And those are essentially
representing all the joinson the printed circuit board.And some of these
joins are defective.Most of these joins are
actually very good joins.In this particular case, the
example of the defective joinis highlighted under
the red square.So that is basically
a defective join.In order to use
computer vision in orderto analyze these defects, we
need some samples of defects,and some samples of non-defects
so that we can build an imageclassification example.So in order to do that, we
ran through various samplesof this data.We extracted and cropped out the
defects as well as non-defects,and generated a training set.So the training set has
got defects, as well asnon-defects.And you can see examples
of defective joinson the top right-hand side.And you can see
examples of non-defectson the bottom row of the sample.And if I were to ask you to
look at this training sample,and tell me just
from looking at itif you can see any visible
difference right away,it's impossible to
tell if there isa difference between the
defects and non-defects,because it just looks the same.So initially, when we were
working with this datawe weren't even sure
if we were goingto be able to solve
this problem at all.But we tried different types
of pre-processing techniques,and we actually were able to see
certain features emerge when westarted working with the data.So to give you an example
of how that looks,one of the techniques
that we triedis known as histogram
equalization.So essentially,
histogram equalizationis used to improve
contrast in images.And the way it does
that is by spreading outthe intensity range of the
pixels over the entire image.So when we applied that
technique to the raw trainingset, we were able
to actually startto see some features
emerge from the data.So when I actually
look at now the defectsand the non-defects, I can
see certain differencesbetween them.For example, on the
top half of the slide,when you look at
the defects, you'llsee they're more circular
and more symmetrical,whereas when you look
at the non-defects,you can see how they
are more spread out.And also, in terms
of the size, alsoin terms of the
outer surface, youcan see that there are
some inherent differences.The other thing we did was
also look at image comparison.So you have a bunch
of HiP non-defects,and they represent the
non-defective samples.When you do an image
comparison using SAS,it returns a
self-similarity index.And that's a value that
ranges from 0 to 1.And if it is equal
to a value of 1,it means that the
images you are comparingare totally identical.And as the number goes
down, it basically gives youa metric for how
dissimilar the images are.So we took the non-defects and
then ran the image comparisontechnique.And you can see the spread
in the SAS similarity index.It's very close to 1, and
there is a spike near number 1.But when you do the same
thing with the HiP defects,you can see there
is a bigger spreadin this self-similarity index.So all these initial
tests were doneto see if there are
inherent differences justby looking at the
pixel informationof the images themselves.And we actually were able
to see that, yes, thereare some differences.So once we were able to
capture the differences,we pushed it through a
model training example.So what I am showing
you here is usingSAS DLPy to actually train
the model on those images thatwent through the
histogram equalization.So after going through
the pre-processing steps,we had a set of images for
defects and non-defects.We pushed it into a VGG 16
architecture using SAS DLPy,and we were able
to build a model.And the results of the model
were pretty good as well,so we were able to actually
get a mis-classification errorof 5.7%, which is
very good, givenwhat some of these
manufacturers are doing today.Because it's mostly a manual
process, where operatorsare essentially looking
at these defects,as well as doing certain
physical measurements wherethey actually measure
the the thickness,and the diameters
of these joins.And from that, they determine
whether something is defectiveor not.So just using computer
vision directly on the imageswill help their time, and
enhance their productivityquite a lot.And as you can see from
the validation data set,48 of the defects were
classified as defects.And from the
non-defects, there were82 non-defects that were
classified correctly,and then 8 of them that
were classified incorrectly.And on the right-hand
side, you can alsosee an example of applying the
model on the validation dataset, where we can see that
an HiP defect was classifiedcorrectly with 91% probability.And moving forward,
the idea and the hopeis that the as the software that
is installed in the machine,the operator would
have the abilityto override what the deep
learning models would suggestto them after scoring.So what you can see
here is an example.On the very left-hand
side of the columns,would be the answers
that the deep learningmodel would spit out.But the operator on
the left-hand sidecan either accept
those changes or itcan override the answers given
from the deep learning model.So it gives you
enough flexibilityto change things
if you wanted to.In any case, it really does
enhance the productivityof the operators.And the final thing
I want to talk aboutis deployment of models.Maggie talked to
you about how wework in the multi-phase
domain, wherethe model training-- where
we build the deep learningmodels-- happens offline.And once we get the Astore
out of those models,it gets pushed into a real
time deployment engine.We use SAS event stream
processing enginefor the deployment piece.So in this particular
example, the same scenariois applied, where the
deployment actuallyhappens on an edge device.In this case, the device
is the machine itself,where event stream
processing is installed.And the scoring logic,
which is the analytic itemsstored that Astore comes
from, the deep learningmodels that have
been trained offlineand in the offline setting.And then the inferencing
happens on the machine itself.And once you gather
more and more data,that data could be fed
into the training stepsto actually improve the
accuracy of models even more.Because you don't start out with
a lot of defects to begin with.So this could be a
nice feedback loop,where more and more
data could alsobe added to your model
training-- retrainingof your models as well.So this is how the whole
picture comes through,building your models offline,
using SAS deep learning,and then pushing the
results of deep learningfor edge deployment using our
real time streaming analyticsengine for continuous
inferencing of new datathat comes through.So that's all for you today.Thank you very much for
listening to our presentation."
78,"Hello, everybody.Welcome to SAS
Global Forum 2020.Today I'm going to discuss SAS
Viya Memory Footprint Analysisand Reduction my
name is Zhiyong Li.I'm a principal software
developer at Computer ServicesDivision of SAS Institute.I've been with SAS for 19 years.I support Java and several
other middle tier technologies.I have a PhD degree in computer
science from Duke University.I will cover the
following topics, SAS 9Footprint overview, introduce
the challenges of SAS ViyaFootprint, discuss Viya
Footprint reduction strategies,the current Viya Footprint
and the compressingof Viya Footprint across
several different releases.I will conclude with
a brief discussionon what we are doing right
now to further reducethe footprint.SAS 9 is a traditional
multi-tier web application.For the middle tier,
all SAS productsare deployed in the fixed number
of web application servers,or a fixed number of Java
Virtual Machines, JVMs.For example, if you deploy
SAS Enterprise BusinessIntelligence or EBI
only, your applicationswill be deployed into at
most two web applicationservers, or two JVMs.If you deploy SAS EBI plus SAS
Enterprise minor, or SAS EM,your applications
will be deployed into,at the most, three JVMs.If you deploy all SAS
products, your productswill be deployed into
14 web applicationservers, or 14 JVMs.By doing this, we avoid
overhead of JVM footprint.We're able to run EBI
plus EM in about 12 GB.And we are able to
run EBI in about 8 GB.Also, we are able to keep the
footprint ratio of middle tierat 75% of total footprint,
and other tiers at 25%.Now, let's look at
the Viya Footprint.Viya still uses a
multi-year architecture.However, it's middle
tier is completelyrewritten using micro services.As you know, each micro service
runs in its own web applicationserver, which in turn
runs in its own JVM.With this architecture,
SAS Viya Visual Analyticswill VA ended with
over 70 micro services,and hence, required 70 JVMs.SAS visual data mining
and the machine learning,or VDMML, ended with over 80
micro services, and hence,needs over JVMs.Each JVM requires at
the least 300 MB, hencebig footprint challenges
to our SAS Viya users.SAS Viya middle tier
requires more memory.Further, it takes a high
ratio of total footprint.For example, for SAS
Viya VA, middle tiertakes up to 92% of
total footprint.And others now take only 8%.There are a few strategies
for the footprint reduction.Some of them have been used over
the course of Viya releases.For example, we have
tuned the micro services.We adjust the size of the JVM
heap for each micro service.We try the different
GC algorithms,and pick the appropriate one.We also adjusted the
other JVM options.The largest reduction comes from
merging multi micro services.Our previous micro services
are too fine grained.We also noticed that if
we restart micro servicesafter the initial
deployment, we cansee a fair amount of reduction.For example, we see about a 4
GB of reduction for SAS Viya VAdeployment.We also researched
expanding Jar archive.That can give us
about 2 GB to 3 GBof savings for VA deployment.Using IBM Open J9 seems to
have achieved a large reductionas well.We have observed
about 5 GB to 7 GBof savings for VA deployment.In the next couple of
slides I will show yousome real footprint data,
and the footprint trendof the last few
SAS Viya releases.This chart shows SAS
Viya VA total footprint.You may notice there are
three lines in the chart.That's because we take
three sets of data points,one right after
successful deployment.A lot is during the load test.And the third is after
those tests all threelines show the same trends.So let's focus on the
black colored line.That's data collected
during the test.You can see that at 18W30,
which is Viya 3.4 release,the footprint is at about 38 GB.At 19W47 release,
which is Viya 3.5,that number has been
going down quite a bit.It is at 30 GB.After that, a few
minor Viya 3.5 releaseshave a stable footprint.This size shows the total
micro services footprint.It has a similar trend
as the total footprint.However, it is about
3 GB less than thatof the total footprint.The report on this slide
shows the individual memoryfootprint, the rows
of micro services,and the columns are deployments.And the numbers in the table
of the memory footprint in MBs.You may notice that all
of them are about 300 MBs.This last chart shows the
number of micro servicesover the course of the
several Viya releases.We'll notice that we have 83
micro services at Viya 3.4,and that number has been reduced
to 70 at the point of Viya 3.5release.This reduction is due to
the merge of micro services.The last four slides have
the similar chart and reportfor SAS Visual Data Mining and
Machine Learning, or VDMML.This slide shows the
total footprint for VDMML.This slide shows the total micro
service footprint for VDMML.This slide shows the individual
micro services footprintfor VDMML.This slide shows
the total numberof micro services for VDMML.So as of the last Viya release,
the micro services footprintis still very high.One of the focused areas in
the next few Viya releasesis still memory
footprint reduction.We will merge more
micro services.However, we also found
that micro services writtenin the Go programming language
have a much smaller memoryfootprint.It can reduce up to 80% compiled
with the Java micro services.Hence, we are doing
significant rewriting work.At the time of Viya
4.0.1 release for VA,we expect 36 Java micro services
are to be written in Go.For VDMML, we expect that
about 43 Java micro serviceswill be rewritten to Go.We expect a total footprint
reduction for micro servicescan be over 35%.However, to make the
expectation in check,Viya 4.0.1 is running in
the Kubernetes environment.The additional
footprint is neededfor the Kubernetes itself.Please keep looking out
for the Viya 4.0.1 releaseannouncement.That will have a final
official footprintnumber of Viya 4.0.1 release.That's all I have
for my session.Thank you for tuning in.If you have any
questions, pleaseemail me at Zhiyong.Li@sas.com."
79,"Hi, all.My name is Funda Gunes.I am a mission learning
developer at SAS.Today I would like
to talk about Howto Explain your Black-Box
Models in SAS Viya.In a health care study about
high-performing machinelearning, a model was trained
for predicting the riskof death from pneumonia.Based on this
prediction, the goalwas to decide which patients
should be hospitalizedand which ones should be
treated as outpatients.Among many input
variables, one variablewas whether patients had
a pre-existing condition--asthma-- or not.When interpretability
techniques appliedto this pre-trained
machine learning model,it revealed that
the trained modellearned a role, a pattern
in the training data,that having asthma lowers the
risk of death from pneumonia.This rule all sounds
counterintuitive,but this was actually a true
pattern in the training data.Asthma patients were more
resistant to pneumonia.Because of this rule--and possibly other
rules like this--the model was not
put into production.After further investigating
the specific rule,it's found that the
asthma patients actuallyreceived more
intense care due totheir pre-existing condition.So this was the real
reason why asthma patientshad lower risk of death.However, this information--
receiving more intense care--was missing from
the training datawhen the model was trained.And because of this
missing information,the machine learning
model incorrectlylearned that asthma
patients have lower risk.If the researchers didn't
use interpretabilityand executors decide
to deploy this model,the model will
incorrectly predictthat asthma patients
are not high riskand hence those high
risk patients would notget a chance to receive
the care they needed.This example, taken
from a paper by oneof my favorite researchers,
Rich Caruana, and a few othersshowed that model
interpretabilityplay an important role by
housing researchers or datascientists to detect this kind
of bias in the training data.Model interpretability
methods in SAS Viyacan help you see through your
black box machine learningsmodels so you can find out
important issues like thisin your data and models.Let's first talk about
why emission learningmodels are black box.Modern machine learning
algorithms study training datathat include a set
of input variablesand labeled the outputs
to learn the relationshipbetween the input variables
and the labeled output.When we use this learned
model to make predictionson new observation, in the cases
where we don't know the output.When machine learning models
study the training data,the learning process
includes many iterationsover the training data.And this exhaustive
learning processenables algorithms to learn
very complex relationshipsand patterns in the data.As a result, we get a complex
machine learning model.This complexity helps
machine learning modelsto make accurate
predictions on new data,assuming you had a lot of
data in your training set.However, the same complexity
makes it very difficultto understand the association
between the inputs and outputs.Because of this, many
machine learning modelsare seen as black box
models producing predictionswithout explaining why
and how they are made.One example of a black
box machine learning modelis a simple neural network model
with one or two hidden layers.Even though we can write out the
equations that link every inputin the model to every output,
we cannot grasp the meaningof the connection simply by
examining their creations.This has less to do with the
shortcomings of the modelsand more to do with
the shortcomingsof human understanding.Often the higher the predictive
accuracy of the model,the harder it is to
interpret its inner workings.This is where interpretability
techniques come into play--provides a lens
through which youcan read these complex models.Model interpretabilty
can meet different needsof different users--such as regulators,
executives, data scientists,and domain experts.Regulators need model
interpretabilityto make sure model
makes predictionsfor the right reasons.For example, if an individual's
loan application is rejected,the loan and agency need to
confirm that this decision doesnot violate any
laws that protectcertain groups of people.Executives need
some understandingof black box models so that
they can logically justifythe decisions they make.Lastly, data scientists, with
the help of domain experts,need model interpretability
for several reasons.These are to be able to detect
bias that's buried in turningdata-- similar to the
first pneumonia examples--extract new knowledge
that is originallyhidden in the data
and debug modelswhen they produce wrong
or unexpected predictions.We can collect all model
interpretability methodsunder two main umbrellas--inherently explainable
models and post-hoc--also known as model agnostics--explanations.Inherently interpretable models
incorporate interpretabilitydirectly into them into
the model structure,and thus these are
self-explanatory models.One type of commonly used
inherently interpretable modelis generalized
linear new models--also known as GLMs--which includes regular linear
and logistic regression models.The regression rates
of GLMs directlyreflect feature contributions
since these modelscan be explained through
those regression rates.Assuming an efficient feature
selection is performed,these models are useful for
models interpretability.However, if you have
complex relationshipsand nonlinearities
within the data,these models may not be
sufficient to capturethis complexity, and the
prediction accuracy may suffer.More recent examples of
inherently interpretable modelscan achieve interpretability
without much sufferingfrom prediction accuracy.These models use
various techniquessuch as regularization
techniques,by forcing models to
use fewer features,or by enabling features that
have monotonic relationshipswith the outcome.Another popular example of
inherently interpretable modelsis generalized additive models
with pairwise interactions.For more information and
reference of these techniques,please see our SAS
Global Forum paper,which has the same
title with this talk.In this talk, I will
focus on exploringpost-hoc interpretability
methods for explained trained,supervised machine learning
models such as boosted trees,forest, and neural networks.The main idea of this
post-hoc approachis to explain predictions
of a machine learning modelby treating the
model in a black boxand then generating
explanations without inspectingthe internal model parameters.So how does it do it?Post-hoc interpretations
are in generalbased on data perturbations.For example, they look
for how the predictionschange if the data is
perturbed in a certain way.Post-hoc
interpretability methodscan be grouped into
two categories-- globaland local interpretability.Global interpretability
focuses on explaining the modelfor the entire population.For example, it looks for which
features play an important roleon the overall
prediction of the model.In contrast, local
interpretabilityprovides explanations for a
specific prediction of a model.Here you see a screenshot
from SAS Model Studioapplication, which is
a visual interface thatenables users to build
predictive modelingpipelines in SAS visual data
mining and machine learning.In Model Studio,
model interpretabilityfunctionalities are provided
as post-training propertiesfor all supervised
learning nodes.This screenshot here
shows how simpleit is the request
model interpretabilityfor your pre-trained
gradient boosting models.Requesting model
interpretability propertiesdoes not require to retrain
your machine learning model.Next, I would like to show
a case study where SAS ModelStudio is used for
transforming post-hoc modelinterpretability.The data come from a
FICO XML challenge,which can be found at
this link shown here.And it's about home
equity loan applicantsthat are real home owners.The customers in this data
have requested a credit linein the range of $5,000 $150,000.The goal is to predict
whether the applicants willrepay their HELOC
accounts within two years.The data has over
10,000 observationsand includes a mix of 23
interval and nominal inputvariables.The input variables
include variablessuch as the number of
installments traitswith balance, the number
of months since the mostrecent delinquency, and average
number of months in file.The target variable
is risk performance,which is a binary variable
that takes value, good or bad.The value bad indicates
that a customer's paymentwas at least 90 days past due
at some point in the 24 monthperiod after the credit
account was opened.The value good indicates
that payments were madewithout ever being overdue.The data are balance with
around 52% bad observations.Since the focus of
this presentationis on explaining the
model interpretability,I will skip all
data pre-processingand feature generation steps.I will also skip details
on how to create a ModelStudio your project,
partition data,and assign various roles to
your variables in your data.For all these details,
please see our paper.Suppose you trained a well
performing gradient boostingmodel for the FICO
data in Model Studio.In order to
understand your model,you can now request global
interpretability methods,variable importance, and
partial dependence plots--as shown in this screenshot.Let's first start with exploring
the variable importance table.The variable importance
table shows the rankingof the importance of
features in constructionof the gradient boosting model.We see that external risk
estimate was the most importantvariable, followed by
another variable calledaverage month in file.Now that we know external risk
estimate was the most importantvariable, let's see its
partial dependence plotto further understand its
effect on the model predictions.Partial dependence plots
depict the relationshipbetween a feature and
the average predictionof the pre-trained model.In this figure, you can see
that the predicted probabilityof payments being
90 days overduedecreases manually as external
of risk estimate valueincreases.The text box to the
right of the graphexplains the graph by using
natural language generationanalogy in SAS Viya.All model interpretability plots
have this kind of analogy textbox they come with,
and these explanationscan help you
understand the graph.And they are especially
useful if you are notfamiliar with the technique.The heat map on the x-axis
shows the population densityat various levels of external
risk, where the blue showsthe less dense regions.You should be
cautious in explainingthis plot on the places where
the population density is low.Next, let's switch gears
to local interpretability.Unlike understanding the
general behavior of the model,local interpretability
methods focuseson explaining individual
predictions of the model.For this, we need to specify
individual observationsthat we need explanations for.If your do not specify
individual observations,SAS provides explanations
for five randomly selectedroles in your data.Here you see I specify five
roles based on the ID variable.Note that ID variable
is an input variable,and it must have the
role key to be used here.You can assign this role through
the Data tab in Model Studio.After specifying the
individual observations here,I request local interpretability
methods, ICE LIME,and Kernel SHAP.Now let's go through
these methods one by one.Let's start with the ICE plot.Here you see the ICE plot for
five individuals and clusteredPD plots shown in blue.While the focus of PD plots
was the average effectof a feature for
the entire data,ICE plots show the
effect of a featurefor a single observation.By examining various
ICE plots, youcan gain insight into
how the same feature canhave different effects
for different individualsor observations in your data.We can see that the change in
the model predictions for eachof these individuals
decreases as the external riskestimate increases--which matches the
behavior that we had seenin partial dependence curve.Each observation is affected
by external risk estimatesslightly differently.For observation 152,
there is a steep declinein the model's predicted
probability of late paymentswhen the external risk
estimate is between 60 and 70,whereas for observation--5429-- the decline is more
gradual between 60 and 70,and it's steeper after 70.Next, let's explore
LIME explanationof a false positive prediction.This instance was predicted
as a high risk application--with a predicted
probability of around 0.97.In LIME, the main assumption
is that machine learningpredictions in a neighborhood
of a particular instancecan be approximated by a
white-box interpretablemodel such as a regularized
linear regressionmodel like lasso.This local model does not
have to work well globally,but it must approximate
the behaviorof the pre-trained model
in a small local regionaround that particular
observation of interest.On the left, you see
the regression ratesof this black box
model that are usedto explain the prediction
of the pre-trained,and the feature values
of this instanceare shown in the
table on the right.We see that, for
this instance, thereare only two variables that are
on the left side of the plotdecreasing the risk of default,
whereas all other predictorsincrease the risk of default.Next, let's explore
a Kernel SHAPexplanation for a false
positive prediction.The truth for this
instance is good,but the model
predicts it as bad--with a high
probability, around 0.9.Different from
LIME coefficients,Kernel SHAP explanations
for feature contributionsdo not directly come from
a local regression modelwhere the feature is joining the
model only in a certain order.The SHAP explanations take
into account features--can join the models
in any order--so order doesn't matter.Hence, they are shown to be
more reliable explanations whencompared to LIME explanations.Even though the model has
such high confidence in itspredictions for instance-- which
was probability around 0.9--we don't see the same confidence
in SHAP's explanation.Because the top five SHAP
values are almost equallyweighted to the right and
left side of the plot.This explanation can
be used as a warningsign for a false prediction
and can be usefulwhen debugging the model.In Case Study 1,
we talked about howto request model
interpretability in ModelStudio application of SAS Viya.In Model Studio, we run
cloud analytic services,or CAS actions,
behind the scenesto pair for model
interpretability.If you prefer to use a
programmatic interface,you can cull all those actions
by yourself in a SAS sessionthrough the CAS procedure,
or to Python or R,through the SWAT library--or you can use REST APIs.Here is a simple
code snippet thatshows how you can pair
for model interpretabilityby CAS actions.You first need to train
your machine learning modelby your preferred algorithm.Here, I trained a
forest model and savedthe scorecard in SAS analytics
store called forest model.In the second box, I
used the score codeto request LIME
explanations for someof the observations specified
by this query variable.If you are interested in
learning more about SAS Viyaprogrammatic interface,
please see the second casestudy in our paper.This example uses CAS
actions through PROC CASto pair for model
interpretability.The whole code is
available at our datarepository that's
shown on this slide.In summary, we talked about
model agnostics, global,and local model
interpretability techniquesthrough a case study in Sas
Visual Data Mining and MachineLearning.I hope you can use those
valuable techniquesin your projects to
explain, validate, and debugyour black box machine
learning models.Here are some
additional informationand resources to learn more
about model interpretabilityin SAS Viya.Thanks for taking time
to listen to my talk.For more details, be
sure to check our paper.Also, feel free to reach
out to me through my emailif you have any questions.Thanks again for
listening to me."
80,"Welcome to the SAS BI platform
to improve use of analyticsin higher education.Do the stats match the intent?My name is Sean Mulvenon.I'm a professor of Educational
Statistics at the Universityof Nevada Las Vegas.A brief background on myself--I am, as I mentioned,
a professorof educational statistics.I am emphasized K12
educational and highereducational analytics modeling.My areas of expertise
include growth modeling,hierarchical linear modeling,
longitudinal data architecture,and various educational
policy models.A brief bit on my background--I was a professor
at the Universityof Arkansas for 22 years.I've been an associate dean at
the University of Nevada LasVegas during the
last 20-plus years.I also served three years
as the senior advisorto the Deputy Secretary of
Education in Washington, DC,where I worked on large-scale
data modeling, growth modeling,and educational policy.Anyway, moving on, the
emphasis of this presentationand consistent with the title,
do the statistics reallymatch the intent?One of the various
challenges that'sbeen identified in
education, and it'sbeen going on for
quite some time,is that we seem to
have a lot of metricsout there that don't necessarily
align with the goal of policy.For example, on the first
slide with background,I have the
definitions persistentand completion
highlighted in red.In academe, in particular,
in higher education,there is a tremendous
amount of pressureto increase both of these.Now persistence is a new
term, a relatively new term,and it refers to students
continuing to stay in school.Completion is more
aligned with graduation,but completion also
has a broader termthat can be aligned
with other programareas such as graduate
certificates, technical degreeprograms.So it's more encompassing, but
students complete the degreeprograms or the
technical programsthat they have been enrolled.Questions on metrics, on how
to use these key metrics,for example, I have a chart
here of graduation ratesand then some
degrees of college.And these become really
interesting metrics,because when you talk about
some degree of college--is that enrolling in school?Is that-- to what extent,
what do you by ""some.""Some is a fairly omnibus word.So it can entail any
number of things.It could be one credit hour
versus 116 credit hourswhich fall into
the same category.So the idea of some is a
great example of what we mean.What are we really
talking about,and how does that metric
really align with the goalsof persistence in completion?""Persistence"" is also
a fascinating term,because what do we
really by persistence?Presently, there is a lot of
research around persistenceand it's focused on students
actually completing--there's that word
""completing"" again--a single course.So they're looking
at metrics that startin the beginning of a semester.Look at your performance
during the first four or fiveweeks, subsequently
on an initial test,and using that information,
predict the likelihoodthat you will or will
not complete the classor were successful in the class.Furthermore, what
they're also doing isit's really an odd
kind of classification.They do A and B as
being successfuland C, D, or F as
being unsuccessful.But C and D count
towards your graduationin most academic
programs, in particular,in elective courses.So just the very model by
which they are definingor the process by which
they're defining ""persistent""becomes complicated.And from a mathematical
statistician's perspective,increases the amount of
measurement error associatedwith using both
persistence and completion.Paradigm shift-- I think
there's a greater challenge outin higher education now
where student expectations,personal responsibility,
prerequisites,preparation are actually
being marginalizedas a key element of
students being successfulboth in their persistence
and completionand are being replaced
with greater responsibilityor oversight of the
post-secondary institution.So as part of that, the
academic support systems,which truly aren't new or
paradigm shifts per se,but what they are are
greater expanded resourcesto support kids, including--virtually every group on
campus has their own supportmechanism, support groups.They have clinics you can go to.They have both social and
academic support networks.A new, invasive thing
on campus, they'recalled learning
management systems.You may have heard of them.They're Blackboard.Canvas is another example.And they are these
data support systemsfor students that are
aligned with classes thatallow the faculty to put course
notes and a lot of other thingsthat normally would be handed
out in a traditional classroomstyle.But they can also put
practice exams and thingson these websites that will
give the student greateraccess to information.They can also interact.So it's a much
more online method.But like the version at the
University of Nevada Las Vegas,they collect over
1,200 variables.So there's a lot of
information out there,but a lot of these variables
are a little dubious,and I'll talk about
that here in a moment.Another learning thing that's
out there is blended learningmodels, where the perception is
it's too difficult for peopleto get to campus and
actually attend class.And the point of attending
classes is the problem.So we want to make
it online wherethey can fit their schedules.And so you're seeing a lot
of online proliferation.This has also been
conflicted with there'sa lot of for-profit
online corporationsand things out there
that also have,quote, unquote,
accredited programs.Having participated in
the accreditation programand actually being one of the
peers to the Higher LearningCommission that
accreditates universities,I know that the process is--I think if you
produce enough paperand you are able to
sign enough checks,that you can get accredited.And I don't say
that to be cynical.I just watch the process.And so the blended formats
also raise some issuesabout really educational return
on investment, which I'll talkabout here in a minute also.Methods to address
these new challenges--data architecture,
and this is whereI think SAS is such
a powerful platform.Linking of data in the system--having worked with
educational data,I wrote my first SAS program
in September of 1988.So I've been programming
in SAS for a long time.And I like to think I'm--I wouldn't say I'm an
expert, but I'm very advancedand can do a lot of things.And I can do if from scale all
the way up to dissemination.And data architecture,
especiallywith educational platforms, it's
very difficult for any numberof reasons, but in particular
some of the vendor modelsare very troubling.Because as soon as a vendor
gets a hold of educational data,the first thing they
do is to strip offthe unifying identifier.Like for example, the National
Center for Education Statisticsdata structures-- they will
strip these things off,and they'll put in their own
identifier, which precludes youfrom integrating their
data, which they usuallycontractually are required to
put out in a public domain,and merging it with any other
data that you may have locally.And that will include stripping
student IDs and student names,which is required under FERPA.But the unit of analysis
which is most importantis the school or the classroom
and that gets stripped off.And so the linking of data
is really problematic,and in higher ed, we
have a tremendous numberof information.I know that business is
very envious of the datathat we have and would love
to get access to it to inventproducts, which
we don't do, whichis part of also what I'm talking
about with this presentation.We've got to do a
better job of that.And a single source solution
and in particular, by a SASis a really important
element in this.Also improving of the metrics--and I just spoke about that.The definitions,
the computations.I refer to this as
the analytics modeland the data architecture
in those key pieces,I refer to as the
elevator model.Now when you talk about the
definitions, the definitions--people don't understand
definitions alsoincludes data rules and how
you're putting things together.So for example, when you
talk about duration, whichis the common metric in the
LMS, or learning managementsystems, this data--and I talk about
this in the paper--if a student attempts the
practice test three timesand does it 30 minutes the
first time and gets 5 out of 10,and then does it a second
time for 20 minutesand gets 7 out of 10, and
then tries it a third timeand takes 10 minutes
and gets 9 out of 10,what the LMS system
will provide youis duration of 60 minutes,
which is interesting.It's a metric that
you would want to use.But the three metrics of 30,
20, 10 are incredibly valuable.And from those three,
you can actuallyget the duration of 60 minutes.They also report only
one score of 9 out of 10.So now I have a score of 9
out of 10, which is great.But there was a process
by which the student wentfrom five to seven to nine
that's all valued because it'sa measure of engagement
that is the only people thatown that are the folks
who own the system.They don't provide us that
information as part of it.And part of that's out
of compass learning tooSo how we find the metrics
or even graduation rates,for example, become
very important.And the data rules about how
we do that in defining themare really interesting.For example, at the
University of Nevada Las Vegasa couple years ago, they had
400 new freshman engineeringstudents.But 100 of these
students, approximately,were required to
take remedial math.So these aren't your
traditional engineering,first-year freshmen,
but they willcount in your graduation rate.So right away with
those 100 kids,the likelihood that they're
going to graduate as engineersis very low.So now, really
you only have 300.So the best that the
engineering collegecan do with a graduation
rate is really like 75%.And even if they get
50% of those kids,their graduation rate is
going to be 37 and 1/2%,which isn't very good with
a 48%, 49% national average.So they start behind the curve,
even though their 50% of thosetraditionally admitted
students did pass.So the metrics are incredibly--the data rules and
the definitionsbecome very important
and then the abilityto innovate resources
out of that.This next slide looks at
the data elevator model.And what I'm really
talking aboutis how you take information
from that first floorand really interact with
each of the succeedingfloors as a model.And the data goes in, is
processed, is kicked up.And it keeps improving as it
goes to the last level, whichis the ability to send out
more meaningful information.I think one of the other
embedded challengesis that there are a lot of
people that can point and clickand can develop analytics.But do the analytics
makes sense in the contextof whatever the purported use?The validity of the analytics
I think is a problem.Having been a
professor since 1993,having been in higher ed in
various ways as a graduatestudents since '86 and having
served as a senior advisorto the Deputy Secretary,
served on numerous nationalcommittees, been an associate,
being served on other--been director of
research and assessmentfor three years at the
University of Arkansas,understanding the metrics is
something that a lot of peopleclaim, even myself,
a knowledge but weneed to grow that
knowledge and understandingwith the examples of
the graduation ratethat I just shared that create
a lot of problems in our abilityto be more effective and
improve the actual persistenceand graduation rates.We started a pilot program.I actually wrote a National
Science Foundation grant.It's called REBELS,
and it standsfor Research Evaluation and
Building of Effecting WarningSystems.So we're taking that
learning management systemand we're trying to
develop it as a modeland platform about how to really
use that data more effectively,integrate it with
transcript data.We're trying to integrate also
high school transcript data.So we have more of a 9th grade
through completion model.And what we're
really looking to dois do some innovative
developmentpathway models
about how to succeedand some probability density
functions on the marginsthat really show that,
based off of this child'seducational profile, what
is the likelihood they wouldbe successful as an engineer?So those original 100 students--
we're not discouraging them.We certainly want
to encourage anybodyif they want to be an
engineer and you'vehad a poor educational
preparation,how can we help you
be better prepared?We're not trying to
limit or preclude,but we need to better
understand the likelihoodto get better metrics
and ultimately helpthese kids be more successful.And so we had a lot of things
in our file cabinet thereand they're underutilized,
or they're utilized in a way.A major goal in education
is to make things so simplethat they render
themselves meaningless.Predictive analytics--
a lot of great thingswe can do there with growth
models and group projectmodels, relevance of
analytics that I talked about.One of those relevance models
is rethinking graduation rates.So I came up with this
model, which I'm callingexpected graduation rates.And the goal of this
expected graduation ratesis to really take into account
the historical performancewithin particular colleges
and the likelihood of successof those students based
off their academic profile.So there could be multiple
groupings, traditionallyadmitted, marginally
admitted, and thenlook at those expected
graduation rates.And now when I calculate
the graduation ratesfor computer science engineering
or any number of physics,minor grads in nuclear
physics, if youwere to look at
those things, thenyou'd have a better idea
of what is reasonable?You know when you look
at the cumulative GPAin a college of educations,
they're a 3.5 and higher.When you look at engineering,
it's between 2.5 and 3.0.So the expected
graduation rates shouldbe different relative to those
two categories for colleges.Now big data and education--
man, this has been around.I was part of all this in DC.I was part of No
Child Left Behind.Did a lot of
accountability modeling.Built a lot of systems for
different state educationagencies.And the policy--I wrote a book chapter, actually
it's cited in the paper, on--what I'm always
afraid of is peoplealways blame either
psychometricsor they blame statistics and
testing, but the real moralof the story-- the
challenges that we most haveare not the mathematics
and statistics per seor the psychometrics, the tests,
and the process of evaluating,it's really the educational
policy that lays over the top.And so if you get a chance,
not to do a shameless plug,but that book
chapter is actuallyquite illuminating as to some
of the challenges and problemswe have.I love this.This is a great comic.Feel free to read
this at your leisure.But I love the last line
by Dogbert, which says,OK, let's pretend that
matters, when askedis the data accurate?I was always advised
by the Deputy Secretarythat I had two goals--statistically accurate
and legally defensible.That became kind of a standard
and can I really defend it?There's a lot of
things out therethat are almost indefensible.And so it's really important
that we get back from integrityand have the analytics
match the intent,because it is a challenge.And it happens in various ways.You know, in growth
models this is not growthbut legislatively, they had
to have a negative decline.So if a student
stayed above this linebut declined
grammatically, the parentswere still told that the student
made adequate yearly progress.And they still do this
today, even as part of ESSA.And so it's really
important that we get backto the business of
understanding and providingmetrics that really go toward
improving educational outcomesfor all students.In addition to that,
we want to make surethat we build systems that can
go through from bottom to top,that elevator model in a
single-source solution,so that we can control
the domain, the analytics,and the vetting and the
validity of the outcomesthat are produced.And so we're working on a
series of systems to do that.So anyway, those are the models.Thank you for your time today.If you have any questions,
please feel free to contact me.And here's my
contact information.Thank you."
81,hello everyone my name's Alice LDSRamirez and I am a senior decision andthe deletion of periodic s-- Universityof California San Francisco and today Iwould like to share our work on mixeffect smaller and complex or de datawith a Linux procedure at my positionthat you see yourself I work very oftenwith large-scale survey data in thedevelopment of our regression smallerfor very understanding of risk factorsor health outcomes all the adults theNew Zealand data that are widely usedI'm the clinical epidemiology fields andin that matter data from large-scalenatural Africa sensitive survey samplesare very useful in longitudinal studiesfor several reason why don't thing isthat respondents are usually followedfor extended periods of time a broadrange of health and demographicinformation can be collected and a largenumber of respondents are can beincluded in the analysis of survey datausing mix effect small well can have twomain components one component is thefixed effects par that describe how thepopulation means are different across ofsoldier characteristics and the secondcomponent is the random effects thatcapture the variability among subjectsfor following units typicallylongitudinal survey data you can havemultiple observations per subjectsacross wave and also solids argues intoprocessed and extra the mix effect smallneed to account for the survey featuresprobability weights closer and sadain this type of model the random effectsaccounts for correlation for salvationswithin the Saints audience andcorrelation of solids with the samecloser or sirif we fail to account for this surveyfeature weget biased population estimates and drawinferences about the population ofestelí so it's very important whenthey're using your feeling a mix of hadsmaller with survey data to includedisobey fishhowever this area is a still an area ofactive research and currently there isno survey analysis procedures in factfor mix mode these work have shown howto use and apx procedure to feed awedding nick smaller where i can't infor for closer however when you havemany levels of a strata and closer thesecompletely be very computationallyintensive and complicated to optimize soin this paper we show five methods ofaccounting for so they Fisher we comparethe estimates the 95% confidenceintervals and running time among thismethod our goal was to find the balancebetween obtaining accurate estimateswith Pharrell standard errors andmanageable processing son finally wemake some recommendation on thespecification of the final in this firstslide here I'll show you the right wayof feeling and makes a fat smaller withsoda fish this particular example I'mfeeling an exception model with twolines random intercepts and slaw and thesubway feature probability weights andclose I'll highlight here is thefeatures are more important in the Saudiofficial design the first one we have amethod equal equalitythis means that we are using thequadrature algorithm to get the verylikelihood function then the empiricalequal classical allow us to use thesandwich estimator to obtain thestandard errors for inferences on thefix effects then we have to run onassignments here the firstunderstatementwe are obtaining the random interceptsfor the variable closer bar this failureis a survey design very design variablethere is a combination of a starter andcloser and if accounting for thecorrelation of IDs with the same clusterthen we have the second randomassignment that we are training randomintercepts and random slow for each IDwe are represented by new IDnested within closed server and then wespecify the probability wavecorresponding to each of theserespondents in the sample W so this isthe right way of feeding the mix effectsmall while I can't afford this a veryfeature that I just mentioned howeverthis model can be very computationallydisease and how to optimize and this cantake a long time to finish so in thenext slide I will show you our tariffways of feeling these things affectsmall while acting differently fordifferent survey features in the firstslide here I'm showing you before wecall the reference method in this casewe are not accounting for any surveyfish so you see here that I have closeout the empirical classify all trianglesalso a probability way so basically herethis is the baseline model that we arecomparing with for the rest of the maththen the second molar we addleader by dealer first step by step eachof this or by feature so first we aregoing to add the probability weightingthe second random same as we saw in thefirst slide here and then we keep thismethod before Pilate rule so we aretraining the waiting a little functionand then a new feature here is theirfirst course of action so this optionwithin the pirate rulealgorithm allow us to cut to reduce thenumber of random effects that the modelmeans to fit and it reduces the numberthe number of random effects this alsoin turn reducesboth the computational demand and memoryusers and consequently we have shorterrunning time so this is very nicebecause we have seen before that themodel can take very long time toconverse right the problem here is thatthis method is not compatible yet withempirical classical object so we cannotput the two in the same thing but stillwe can get accurate estimates of thefeature of the fixed effects and also ofthe covariance parameter estimates inthe next method we have like a firststep process first in the first model wefit exactly the same mole as we haveseen the previous method to with theprobability way here and the firstperson option and we obtain in this asmaller they fix effect estimate then werun a second model where we changeslightlythe second random stain so basicallyinstead of using the probability weightwe use something called normalize weightthis normalized weight is calculated asdividing each respondent probabilitywave by is meet so it's basically thisformula here and what we're doing withthis is basically avoiding the forestimation of the sample size which willavoid having to a small standard errorsso once we have for the first model thefirst model the parameter estimates thetry parameter estimates and once wecollect it using the second model thestandard errors then we can calculatethe 95% confidence interval is a systemin this formula here at the bottom ofthe slide then the fourth method is whatyou have seen already like this is whatwe call the whole standard methods andit has all the all the features that areimportant for for this type of of phonethey're finally method 5 is also into astep process so in the first part wealso use the same method as method 2where we have the phosphorescent optionand the probability weight and then butthe purpose of this first model is alittle bit different from what we haveso we have seen beforein this case we fit this first modelwith a goal of obtaining the covarianceparameter estimates so we feed this morewill change the covariance parametersestimates and then we fit a second modelwhich is very light as the gold standardmethod with the difference that we addthe farm's statement as you see here inthis case the first step and sustainmenthas four covariance parameter estimatescorresponding to the one that wepreviously calculated in model 1 so wefeed me1 obtain the covariance parameterestimates and then we feed more to witha farm statement the sub function forequals 1 2 3 4 is telling hands to excelin the model to constrain these 4covariance parameters estimates to thenumbers that I have provided so now tonext to test our method or wind straightour method we use a sample and sampledata in this case we are using using thenational analogy to sensitive couple of1600 proximately 60 laundry communitieswelling seniors enrolling thecontrolling the health and retirementstudy for hrs and dangerous is anongoing longitudinal survey ofrepresentative sample of all personspersons in the United States over age 50and eithersalmon's changes in health and involvethey're serving features that aretypical for the hrs are a stratificationand we have now a study sample 52EstradaFalacci we have two closersthe differential selection probabilitywhich I have been mentioning before usprobably the way and we derived thisvariable from prior interview and thenfinally we have a derived variable thatyou have seen in previous slide and wecall it closer bar and as I mentionedbefore this variable is a combination ofa strata and closer and it has 104levels then the many predators of ofthese data would be a type of heartprocedure and we have too hard forserious W analyze the coronary arterybypass graft we call alesh orpercutaneous coronary intervention andwe call it pci electron is the compositememory school and this is a continuousvariable derived at interviews beforeand after the procedure we follow therespondents eye fiercely for procedureand ten years after the procedure thetotal sample size was always a fans ofeight thousand Conservation'scorresponding to the over 1600respondents and that we have now oursample size and finally our goal was toinvestigate whether there is a change inthe kinetic applause slow followingcourage for PCI and where this change inthe slope was different depending on theprocedure performed here we see in thistable the resource of the comparisons ofall these five methods each colorrepresents a method and then in thiscolumn here we have the fest of fillingthese mixes at small there's first thingthat I would like to highlight is thatmethod 3 and 5 have very similar stunt avery similar fix effects estimates andalso 95% confidence intervals confirmwith the gold standard myth and this wassuspected sinceboth method three and five theyincorporate they incorporatedprobability weights and also they adjustadjust the standard errors of the mowerin the case of metal three we use thenormalized weight which I told me for alittle you before that is used tocontrol for the over estimation of thesample size and getting to a smallerstandard efforts and metal five use asandwich estimator so we expected thatthese two methods were very similar tothe gold standard then we have methodtwo and three that have if we look onlyat the fit cells they have very similarfix effects compared to the goldstandard method and remember that thismethod both use the first person optionin addition if we look at the covarianceparameter estimates of metal two threeand fiveall of them have almost the samecovariance parameter estimates as thegold standard man so what this tells usis that using the first web suit optioncan be a ballad third channel if toobtain accurate fix effects estimatesand covariance are parameter estimateshowever one needs to be cautious aboutmaking inferences on these fixed effectsbecause as we have seen with method twoif we look at the facts this one thesetwo fix of hats we see that the 95%confidence intervals are very narrow andthis is because we did not use thesandwich estimators so these gave usvery nervous on their errors which cancan cause that we make some inferencesabout these three efforts so whenever weuse the first person option is to obtainthe right estimates but we need to becautious about the inferences that willmake on these estimates then betweenmethod 3 and 5 withfound that missile five actually haveflow so 95 percent confidence in soulsto method for compared also with metal 3so we prefer method 5 over method 3 andfinally we found that both method 3 and5 we use significantly the tronic timesthe mode you can see that model methodfor front for about four hours and bothmethods half I should use during timebut what we had in the method for thenext slide I'm summarizing what I justtold you about so we will skip throughthem then these figure actually we arenow focusing on answering and what isour research question so remember thatfrom last time from Jesus lion mentionedthat our goal was to see where there wasa change in the cognitive style treebefore and after procedure between thesetwo loops cash and PCI well we arefocusing now on these are fixed effectshere and this one tells us where thefree trial' tree slope is different inthese are two method and what you seehere is that regardless of the methodnews there is no significant differencein that slow prior to the procedurebetween Harshman PCI however we can seethat metal two through five are moresimilar estimates compared method onewhich did not use any probability weightalso is interesting that method one hasa different is a different size of thefat but also the sign of the fat isdifferent this one is positive and therest is now so it will be up to theclinical researcher to determine wherethis difference may be - and also inDirection is clinical and then we focuson seeing if this change in the slopefrom proprietary Australia 3 betweentheseto procedure corrosion PCI's issignificant has a significant differenceor not in this guy in this case again wedon't find any significant differenceamong the different method that we tryand in this case also defense the sizeof the facts is very similar acrossmethods that metal tube through fivewell we did incorporate probabilityweights are more similar amongthemselves compared to method one thisis the same that we found the previousslide one thing though that I what Iwant to point out here is that thismethod - the one where we only use thefirst Watson option and we do not useany sandwiches together or anything elsehas too narrow confidence interval thisis very important to highlight becausethese are a size is what I mentionedbefore that will be using those fastparts of functional we need to becareful on the interpretation of theinference of these effects that willchange because this one we might thinkthat it has very narrowcontent center when in fact that's notthe case as a conclusion we have seenthat video mix effect smaller thataccounts for so they feature is veryimportant to obtain accurate and preciseestimates however when we have mixerthat small with many levels of a strataand cluster this type of model can bevery computationally intensive andcomplicated to optimize which in turnshave the model will have long runningtimes we have seen in this presentationthat the first parts of options thefinance timing and the use of normalizedweights can be used to produce reliableestimateswhile at the same time reducing therunning time my colleague our boss Chengji who is the second author of thispaper and that I would like to send forthe support the UCSF pepper centerdevelopha and the National Institute on a andwith this I conclude my presentation andI thank you all for your attention
82,"Hi, this is Brian Kinnebrew
with the Global TechnologyPractice at SAS.And today, I'm going
to be discussingthe process of
migrating your existingSAS 9.4 jobs over to SAS Viya.In today's presentation, I'll
discuss the following agendaitems.First, I'll start
with an introductionto SAS Viya, followed by the
primary components of the SASViya ecosystem.I'll discuss how to best process
data and interact with CAS.I'll discuss some best practices
for migrating your existing SAS9.4 code to leverage CAS.Next, I'll discuss some
alternative solutionsfor functionalities
and capabilitiesthat are not yet
supported in SAS Viya.I'll introduce you to the job
execution service, what it is,and how it can help
you in SAS Viya.I'll provide you
with some codingexamples located on GitHub.Some training resources that are
offered by our formal educationdepartment, and then finally,
some additional readingmaterial and documentation
around SAS Viya.Let's get started with an
introduction to SAS Viya.SAS Viya is the third generation
in memory extension of the SASplatform.Our first generation was
high performance analytics.This had the
capability to speed uphighly complex algorithms,
scheduled jobs,and offered batch processing.However, it was limited
to single user accessand required that all
data must fit into memory.The second generation
in memory extensionwas the laser analytics server.This repeatedly ran
less complex algorithms,provided interactive
use of data and code.It did offer multi-user access.However, it did also require
that all data fit into memory.Our third generation is SAS
Viya, as mentioned earlier.SAS Viya is a single approach
to solve both use cases outlinedin high performance analytics
and the laser analytics server.It allows processing in
memory, as well as on disk.And it delivers highly available
fault tolerant processing,as well as multi-user access.What is SAS Viya?It's an open, cloud
ready in memory datamanagement and analytics
component to the SAS platform.It caters to users with both
varying proficiency levelsand coding specializations.It's uniquely designed
to enable multiple usersto simultaneously process
complex analytic workloadsand ad hoc data processing
and data management.It meets the resiliency,
governance, automation,and ease of administration
needs and demandsof any organization.Continuing on with our
introduction to SAS Viya,it provides the
following capabilities.Analytics and data management
accessible to anyone,regardless of their skill
level or experience.It provides an open
scalable environmentthat addresses the challenges
of any size and complexity,while applying the
same analytics and datamanagement to both streaming
data as well as data at rest.It also provides a single code
base and a unified environmentto curate both SAS analytic
and data management assets,as well as those that
reside outside of SAS.SAS Cloud Analytic
Services, pronounced CAS,is the in-memory runtime
engine for SAS Viya.It's suitable for both on
premise cloud deploymentsand hosted deployments at SAS.The runtime environment
is the combinationof hardware and software where
data management and analyticstake place.It can run on a single machine
or in a distributed environmenton multiple machines.The distributed
environment consistsof one controller, an
optional backup controller,and one or more workers.For both architectures
however, CASis multi-threaded for
high performance datamanagement and analytics.SAS Studio, which is the native
programming interface for SASViya, provides a
programming environmentfor developing and submitting
SAS programs to CAS.Finally, the SAS Scripting
Wrapper for Analytics Transfer,otherwise known as SWAT,
enables open source softwaresuch as Python, R,
Lua, Java, and RESTAPIs to run data management
and analytics on CAS.What are some of the technical
benefits of SAS Viya?First, it provides a huge
amount of shared memory.It holds all of the data and
persists it for shared use.Second, it's a specially written
multi-threaded software thatwas written from the ground up.It provides amazing agility.Many Base SAS
processes can executeas is, including the DATA step.It provides current machine
learning and deep learningtechniques.The platform is
specifically designedfor iterative analytics.It's also cloud ready.It's elastic,
flexible, and providesa scalable architecture.It also provides
increased accessibility,giving you the
flexibility to usethe interface of your choice.Let's take a look at some of
the benefits and differentiatorsof SAS Viya.It provides data management for
both data at rest and streamingsources.This includes ETL, data
cleansing, data manipulationand wrangling, as
well as analysis.It provides visual data
examination, exploration,and presentation.It provides a suite
of advanced analyticsfor both structured
and unstructured data.That includes machine learning,
deep learning, forecasting,optimization, statistics and
both data and text miningto name a few.It provides an inventory and
governance for both SAS models,as well as models
developed outside of SAS.It provides a ready
made in memoryruntime environment
that is automaticallymutable to constrained
processing.It provides code portability for
enterprise deployment options.For example, grid, cloud, single
cluster, multiple clustersand so on.You can access CAS from SAS
or open source languagesof your choice, such as Python,
Java, Lua, R, or REST APIs.It also provides centralized
administration, management,and maintenance of
all data managementand analytic processing
and activity.What does CAS provide?CAS expedites speed to answers
with optimized distributedin memory processing
that provides highlyscalable computational speed.It provides foundational
access to underlying methodsthrough REST APIs for
software developersto quickly adopt SAS into
their own applications.It includes an array of
innovative algorithmsand visualizations that
will continue to expandwith each release of SAS Viya.It provides common
code that is portable.The same code works in different
environments and scaleswith changing data.It's also engineered
and architectedto execute processing wherever
needed to optimize ROI.This includes in database,
such as Hadoop or Teradata,streaming data, data in
memory, also data in a privateor a public cloud
and also in device.Our next topic focuses
on the primary componentsof the SAS Viya ecosystem.This is a graphical
representationof the SAS Viya ecosystem.At the heart of
this ecosystem isCAS, the in-memory
processing engine,and micro services which
replace the metadataserver in SAS Viya.CAS can be accessed
by SAS Studio, as wellas the API of your choice.That also connects to
various data sources,such as Hadoop,
Teradata, file systems,or relational databases.You can also access CAS
through the micro servicesvia REST APIs, web
apps, other monitoringconsoles and other clients.As previously stated, the
in-memory high performanceprocessing engine of SAS Viya
is SAS Cloud Analytics Services,or CAS.CAS provides the runtime
environment for data managementand analytics with SAS.The CAS server has the
following characteristics.Data being transmitted to and
from CAS and stored within CASis completely secure.CAS can manage all of your
data easily and share datawith multiple users.Authentication is
used to control accessto CAS and its resources.Your identity must be
successfully authenticatedbefore your session is created.CAS can run on a single
machine or as a distributedserver on multiple machines.For both architectures,
as mentioned earlier,CAS is multi-threaded for
high performance computing.CAS executes on tables
that reside in memory.The source data for
the in memory tablescan come from a variety of
sources, including SAS datasets, server side files, event
stream processing, and databasetables to name a few.Depending on the
source, data can beloaded serially or in parallel.You can program using new
high performance SAS Viyaprocedures, some SAS 9.4
procedures, and languageelements including DATA
step, DS2, and FedSQL,or CAS actions.CAS actions are the smallest
unit of work for CAS,but offer the most flexibility.To program using
CAS actions, youuse the CAS procedure,
known as PROC CAS.Organizes user defined
formats in format librariesyou can use the format procedure
to specify a class formatlibrary to store a new user
defined format programminginterfaces to CAS include all
programming interfaces that youuse with SAS as well as
those used with open sourcelanguages, such as
Python, Java, Lua, and R.SAS Viya provides two
programming runtime serversfor processing data that is not
performed by the CAS server.This is commonly referred
to as SPRE, otherwise knownas the SAS Programming
Runtime Environment.This is exactly
similar to SAS 9.4.However, SPRE is housed
within your Viya environment.When your SAS environment
includes the SAS Viyavisual and programming
environments,your SAS administrator
determines the server.The SAS workspace server
and the SAS Compute serversupport the same SAS code
and produce the same results.The following table
shows which servercan be used based on the
SAS environment and the SASprogramming client.The following
categories showcaseprocedures that are
available for siteswith SAS Viya installed.Many of these categories
contain procedures thatare brand new for SAS Viya.SAS Studio is the native
programming interfacefor SAS Viya.The latest edition
of SAS Studio is 5.2.There are snippets
within SAS Studio whichprovide pre-written code to help
you get started on your journeyin writing code in SAS Viya.There are many different
categories of snippetsand within each category
are several examples.To use snippets, simply modify
the code to meet your needs.In addition to snippets, SAS
Studio 5.2 also provides tasks.Tasks provide a point and
click code generation menu.There are many
categories of tasksand within each category are
several examples of tasks.Simply click on the
task that you need,fill in the parameters,
and the codewill generate
automatically for you.Here are some of the new
features in SAS studio 5.2.Using the query tool, you can
create a query to extract datafrom one or more tables
according to criteriathat you specify.You can generate your query
using either SQL or FedSQLcode.However, remember that only
FedSQL is supported in CAS.Using the Import tool, you can
import several basic file typesinto data sets.With Git integration, you
can clone repositories,stage changes and
create commits,create merge and
rebase branches,and resolve merge conflicts
from within SAS studio.You can now access files and
folders on both the SAS ContentServer and your
server file systemfrom the Explore section
of the Navigation pane.You can use the new
DATA step debuggerto find logic errors
in a DATA step program.You can run and
create SAS Viya jobs.You can run a saved
program as a background joband the new scheduling
functionalityenables you to automatically
run your jobs or SAS studioprograms at a specified
date and time.You can use the new
command line interfaceto access SAS studio
using the keyboard.You can create a filter
based on a single columnby right clicking the column
heading in the table viewerand selecting quick filter.You can send a copy of your
results and the associated codeand log files by email.Files that you can send include
results in HTML5, RTF, and PDFformats, as well
as the code and logfiles that are associated
with those results.You can now specify
custom SAS codeto run before or after code
for programs, tasks, queries,and imports.This code persists between
SAS studio sessions.You can now customize
some keyboard shortcuts.SAS studio 5.2 includes many
new tasks for Cloud AnalyticServices, or CAS, machine
learning, and time seriesmodeling.And finally, the
common task modelwas enhanced to support
dynamic and cascading prompts.In the next section, we'll
discuss processing dataand interacting with CAS.Some considerations
for processing in CAS.When your data source and/or
your destination tablereside outside of CAS,
processing will automaticallyoccur in SPRE, the SAS
Programming RuntimeEnvironment.This is the SAS Viya
version of SAS 9.When your data source and
destination reside in CAS,processing will
occur inside of CAS.So it's very
important to rememberthat in order to process
in CAS, both your sourcedata and your target data
must reside inside of CAS.You can process
data and interactwith CAS using a
variety of interfaces.These include visual interfaces,
programming interfaces,or REST APIs.When processing data and
interacting with CAS,whether it be via visual
interfaces, programminginterfaces, or REST APIs,
CAS issues a class actionbehind the scenes.Each CAS action is
grouped into a CAS actionset based on functionality.There are three tiers of control
when interacting with CAS.First, GUI's provide the
least amount of control,but offer the highest
level of user friendliness.Next, procedures including new
SAS Viya procedures and CASenabled SAS 9 procedures,
allow more controlby using a SAS programming
interface such as SAS Studioto write code or
create proceduresvia a menu driven wizard.Lastly, CAS actions through
the CAS language and the CASprocedure, known as PROC CAS,
provide the greatest levelof control and flexibility.CAS actions offer
options, parameters,and capabilities that might not
be available in SAS Viya or SAS9 procedures.You can also create
your own CAS actions.CAS actions can also be called
from Python, Lua, Java, Rand REST APIs.It's important to note that
actions run only in SAS Viya.In this next section, I'll
cover some best practicesfor migrating your SAS
9.4 code to leverage CAS.One of the biggest
myths around CASis that will always
perform better than SAS 9.The truth is it depends on
your data, your environment,and your code.There is no magic bullet for
executing code and processingdata in CAS.Be very careful when running
code against small data in CAS,especially a distributed
CAS environment.Overhead from internode
communication and datadistribution can result
in performance loss.Remember, by default,
CAS runs multi-threaded.Some SAS Viya procedures,
statements, and functionsrun single threaded.Others run multi-threaded
and some are a mix of both.Several Base SAS procedures also
run multi-threaded by defaultin SAS 9.As a first step, ensure that
all of your code and programsexecute successfully in
SPRE using SAS 9 librefs.Secondly, create CASLIBS for
all of your data sources.This could be data
sources such as datathat resides in relational
databases or traditional SASdata sets residing
on a file system.Third, start with the most
important step or block of codethat takes the longest
amount of time to run,not necessarily an entire
program in general,so that we can reduce the
execution time and leverageCAS as much as possible.Finally, after you've done these
three steps if you discoverthat your code is
not running in CAS,contact myself or your
local SAS representativewho can reach out to
me for assistance.We've developed
several work aroundsfor capabilities and code that
isn't yet supported in CAS.I can also take a look at
your code and troubleshootand try to remedy why
it's not executing in CAS.As discussed in the
previous section,CAS actions provide the most
flexibility and capabilitieswhen executing code in CAS.Consider using the following
link to learn the languageand syntax so that it will
be easier when you need it.Remember that everything that
runs in CAS calls a CAS action.In addition to the many CAS
actions provided by SAS,you also have the capability
to create your own CAS actions.Please see my paper that I
presented at SAS Global Forum2019 on how to create
your own CAS actions.Additionally, SAS provides
several CASL built inand common functions.Similar to CAS actions,
you have the capabilityto create your own functions and
use them in the CAS language,or CASL.See my blog at this link
for additional details.CAS enabled Base SAS
procedures and the DATA stepcan leverage CAS, but
one must benchmarkto determine which
engine is fastest,whether that be SPRE or
CAS for the task at hand.Remember, some code might
perform better in SPRE.Some CAS enabled
Base SAS proceduresexecute partly in SPRE,
as well as partly in CAS.In addition, some
statements and functionsare not yet supported in CAS.You can use the CAS language,
or CASL, for 100% in CAS.Remember to purge CAS
tables from memorywhen they are no longer needed
for downstream processing.This is very
important and ensuresthat you have available
memory for processing.Otherwise, you might start
using CAS Disk Cache, resultingin I/O and potentially
slower processing.Remember, it is not necessary
to process all code in CASto justify SAS Viya.You may think that the
best way to get startedin migrating your SAS
9.4 code to leverageCAS is to convert all of
your code to leverage CAS.Please do not do this.Be selective in identifying
steps that can leverage CAS.Look for some of the following.Code blocks with
long execution times,whether this be 30 minutes,
multiple hours, or even days.Also, look for CPU
intensive steps.These can be
analytical proceduresthat are computationally
demanding,such as regression
analysis, mixed modeling,logistic regression,
neural networks, gradientboosting, and so on.DATA step also containing
computationally intensivestatements, functions,
and conditional logic.Also, look for
source data volumesthat are greater than
50 gigabytes in size.This is a general baseline
recommended by our researchand development department.You must also consider
the computational demandof your code when determining
where to execute your programs.Don't just consider
the size of the data,but also consider the
complexity of your code.With CAS, consider using
all of your data insteadof just a sample.When your source data is less
than 50 gigabytes in size,consider keeping these data sets
in traditional SAS7BDAT formatand executing your code in SPRE.Again, this is a general
baseline recommended by R&D.And you must also consider
the computational demandof your code when determining
where to execute your programs.It's always a good
idea to comparerun times between
both SPRE and CASto determine the most
efficient environment.Cardinality of BY variables.When dealing with
high cardinality,BY variables contain a
unique value for mostif not all observations,
such as loan level data.This might execute
faster in SPREbecause the data
is not distributedacross multiple worker nodes.The exception is to
generate a partitioned CAStable where the BY variables
match the partition variables.Lastly, data order
of CAS tables.BY statements will
group the datacorrectly across the CAS
worker nodes and threads.However, there is no concept of
ordering within a distributedcomputing environment.Therefore, ordering will not be
honored within the various BYstatements.The good news is
that BY statementsare executed on the fly.No sorting is required for BY
statement processing in CAS.This includes performing
operations such as merging,first./last.processing, and so on.When BY statement
processing is complete,data is then redistributed
across the CAS worker nodesand threads.The exception is to
generate a partitioned CAStable where the BY variables
match the partition variables.In this section,
I'll introduce youto some alternative
solutions for functionalitiesand capabilities that are not
yet supported in SAS Viya.In SAS Viya 3.5, the DATA
step with the descending BYstatement is supported for
numeric character and VARCHARdata types.However, the
descending statementis not supported on the first
variable of the BY statement.This also applies
to BY statementswith only one variable.Should you only have one
variable on your BY statement,the descending statement
is not supported.In SAS Viya 3.4 or older
versions, a workaround for thisis to create a CAS view
with new variables thatare the negative value of the
original numeric BY variables.Use the new variables in the BY
statement in ascending order.The original numeric
variables will nowbe in descending order.Continuing on, PROC SORT
options NODUPKEY and NOUNIKEYare supported in SAS Viya 3.5.Alternatively, you can use
the brand new CAS actiondeduplicate.In SAS Viya 3.4 or older, you
can emulate these with the DATAstep by outputting a record
for the first occurrenceof the last BY variable.You can also emulate
this with FedSQLusing a distinct and
group by statements.Lastly, you can use the
CAS action group by infoto remove duplicates.For additional information about
de-duping with the DATA step,please refer to
the following blog.In distributed
computing, rememberthat row order of a given by
group is completely random.Next, if you have a DATA step
with SYMGET or other macrofunctions, these are
not supported in CAS.You can emulate these
functions with formats.The following example is going
to run in SPRE because itcontains a SYMGET function.You can see that
in this expression,a macro variable is used.A way around this is to
use a format instead.This particular
example executes in CASby substituting a
formatted value insteadof the SYMGET function.With PROC APPEND, you can
emulate this with a DATA step,a multi-table set statement, and
the data set option APPEND=YESon the target table.This does require SAS
Viya 3.4 or newer.However, PROC APPEND in its true
form is not supported in CAS.In this section, I'll introduce
you to the SAS Job Executionservice.The SAS Job Execution
service is a webbased application
that allows usersto register SAS code as a job.A job can be scheduled
and monitoredthrough this application.The SAS Job
Execution applicationalso allows for registering
a HTML front into a job.Dynamic prompts
are also availablewithin the application.In SAS 9.4, stored
processes were typicallymade up of SAS code,
HTML, and prompting.Often, these items were
combined into one SAS programthat used to PUT statements
to generate HTML and populateprompts.This approach would
often intertwinethe job roles of SAS programmers
and front end developers, whichled to confusion.With the job execution
service application,these roles are now separated.This allows for
front end developersto work with the
HTML and prompts.And the SAS developer
only has to beconcerned with the SAS code.As we progress
through the overview,you will learn how to
create jobs, HTML forms,and dynamic prompts.In 9.4, we had a
prompting frameworkthat was built into
several of our products.Enterprise guide,
stored process,applications, et cetera.This has been replaced with
the custom tasks in Viya.Custom tasks allow for the
creation of data drivenprompts.Logic can also be built
to filter prompts basedon previous prompt selections.A side note to this
is the same frameworkto build custom
tasks in CAS Studio.With the openness of
Viya, the various APIscan also be used directly
in HTML or JavaScriptto populate visual elements.All right.Let's start with an overview
of the Job Execution WebApplication.On the upper left
hand side, you willfind the menu for the SAS
Job Execution Application.The content menu option
provides a logical hierarchyto store SAS jobs, HTML,
and dynamic prompts.This can be organized any way
your organization sees fit.The jobs menu
options displays jobsthat have been executed
within a select time period.This is handy to see
if a job was successfuland view the SAS log.A job can also be
scheduled from this page.The scheduling menu
option displaysa list of jobs that are
currently scheduled.From this page,
the scheduled jobscan be modified and/or deleted.The samples menu options
has roughly 20 job samplesto show various actions
that can be taken.These range from simply
displaying hello worldto creating an
interface that allowsa file selection and import.This is the detail of
the jobs menu option.The folder structure displayed
is a logical folder structure.It does not map to any
physical disk structure.Behind the scenes, SAS Viya
is using the folder, files,and job definition services.Essentially, all
of this informationis stored in metadata in
Viya's Postgres database.To create new content
such as folders and jobs,you can right click and have
a standard set of menu optionsor use the toolbar at the top.Editors are provided for
SAS code, HTML, and prompts.This is almost identical
to the programming editoryou're already familiar with
from Enterprise Guide or SASStudio.One key thing to
remember is that the codeis stored in metadata.So if you have a
program stored on disk,you will need to copy that
source program into the editorsince the folder structure is
a logical folder structure.It will not display
a physical filefrom a physical path or
physical folder structure.You will need to open
SAS Studio and copythe program from
the logical fileinto the job execution editor.Many samples are
provided out of the box.This is a good
starting point to learnhow various actions are taken
within the job executionapplication.Everything from a
simple hello worldscript to working with JSON
script to uploading files.A link to this
application will beprovided on a slide at the
end of this presentation.Although many samples
are provided for you,the samples cannot be run
directly from the samples menupage.They will need to be copied
to one of the logical folders.To do this, simply select
one or more of the samplesyou would like to copy and
then press the copy to button.This will bring up the
above dialog box thatallows for a folder selection.Once the copy is complete,
you can navigate backto the jobs menu and
navigate to the foldersthe samples were saved in.From that point, the code
can be reviewed and executed.To illustrate what
we've been discussing,I will now show three
different examples.The first example
will show a SAS jobthat requires no user input and
is scheduled to run every hour.The second example
will show a SAS jobthat requires
input from the uservia the dynamic
prompting framework.Lastly, example
number three willshow a SAS job
that requires inputfrom the user via an HTML form.To get started, select the
public folder and right click.The menu options will ask if
you want to create a file,HTML form, or prompt.For now, let's choose File.This particular job
will run a SAS programthat fetches current weather
data from an external service.The result of the job
execution is current weatherappended to a historical table.The file type in the dialog
box will be job definitionand the server will be compute.This weather code will
be provided via GIT.However, the concept is not
to learn about weather code,but to learn how to place
your own SAS code into a job.Feel free to pull the
weather code from GITor use a program
you have already.Again for SAS programmers,
this interfaceshould be very familiar.Once the code is
ready and pasted,the Submit button can be
pressed to test the code.Because this code
produced no visual output,there will be nothing displayed
if it ran successfully.If something went wrong, it
will say that an error occurred.To check the status of the
execution, go to the jobs menu.The jobs menu will show
all recently run jobs.This is where we would validate
that the weather job we justcreated and executed
was successful.We can also view the SAS logs
of previous jobs on this page,as well.For now though, we simply
want to verify the job ranand then schedule it
to run every hour.To schedule it, highlight the
job you would like to scheduleand select the
scheduled job menu icon,which is the last
one across the top.Once clicked, the above
dialog box will appear.This dialog allows the user to
provide a name and description.The executing user
can also be selected.A trigger will need to
be added that specifiesa when a job runs.This can be any time
frame from secondsto weeks, months, or years.Multiple triggers can be
applied to one job, as well.The scheduled jobs menu
displays all scheduled jobsthat the user has access to.From this page, users can
modify, delete, and viewthe properties of a job, such
as when it runs, et cetera.Administrators can also view
job details and the statusin the Environment Manager.In the second example,
we'll put a user interfaceon top of the
forecast weather data.This will allow an end user
to select a city and a weatherdescription.Using the previously
created job,weather forecast, right
click on the job nameand choose the Edit menu option.We would like to add
prompts to the program,so select the option prompts.This selection will
open up a prompt editor.Dynamic prompts are created
using the common taskmodel from SAS.This model has a lot of
functionality built into itand has a slight learning curve.A link to the documentation
on the common task modelis provided at the end
of this presentation.This example shows the
creation of a data sources tag.This allows us to set the
data set to pull values from,as well as the filters.Notice we are specifying
the forecast dataas our default table.We are also specifying
two filters.The first is the city name.This is a simple prompt that is
populated with distinct valuesfrom the city variable.The second prompt is
the weather description.Notice that it is populated
from the description column,but also has a where
tag applied to it.This means that the
values displayedwill be filtered on the
selection made in the cityprompt.So for example, if rain
is not in the forecastfor the selected
city, it will notbe displayed in the
description list.The common task model
is extremely powerful,not only for dynamic
prompts, but also the abilityto create custom tasks.This is very similar to
the tasks in SAS Studio.Now when the job runs, the
prompts are displayed first.The user makes their selections
and presses the Submit button.The selected values are
passed through the jobas macro variables and
integrated into the SAS code.Keep in mind that the first
example is now scheduledand runs every hour, which
updates the forecast table.This code in the second example
is simply performing querieson the forecast table.Since this job produces
output, the resultswill be displayed once
execution is complete.The following table
shows the outputbased on the selections
made in the promptsin the second example.The third example is
using a custom HTML form.Just like we did
to create a prompt,you'll now want to right
click on weather forecastand choose Edit HTML.An HTML editor
will be displayed.A few details to
note about this code.The table markup should be
straightforward for HTMLdevelopers.Things that are not
so straightforwardare the type equal hidden tags.The hidden value underscore
program referencesthe job to run when
the form is submitted.Hidden value underscore
action tells the jobto wait for the submission
of the form and then execute.And hidden value
underscore outputunderscore type tells the
job to generate HTML outputand results.This is the simple custom
HTML form appearance.Enter your values
and click run code.This is the output generated
by the values enteredin the custom HTML form.These are a couple of
key links for usersthat want to become more
familiar with the job executionservice and the job
execution web application.There are many coding
examples storedon GitHub for your reference.Some of these have
been discussedduring this presentation.Others are there simply to
guide you on your journeyfor migrating your
SAS 9.4 code to Viya.The links on this
page will guide youto the coding examples
located on GitHuband also, the link at the
bottom will provide youwith information on how SAS
Studio 5.2 integrates fullywith GitHub.There are a variety of
training resources on SAS Viya.The following link will take
you to the complete SAS ViyaLearning Path offered by
our education division.You can take the
entire Learning Pathor choose individual courses
and subjects that youwant to learn more about.The last section in
today's presentationprovides you with additional
resources on SAS Viya.The links on this slide provide
you with additional readingmaterial, including blogs,
communities articles,white papers, and
formal documentation.Thank you for watching
this presentation.If you have any
questions, pleasedon't hesitate to reach out
to your SAS representativeor to me directly.Thank you."
83,"KUMAR THANGAMUTHU: Hello.Thanks for joining the session.As cloud vendors add support
for commonly used databases,more and more
organizations are lookingat migrating or leveraging
their cloud databasesto reduce their on-premise
infrastructure overheadand boost data to insight
performance, especiallyin analytics platforms, such
as SAS Viya. on cloud as well.Today, we are going to talk
about SAS connection and dataaccess options for some of the
Microsoft Azure data sources.I'm Kumar Thangamuthu, part of
Global Technology Practice, oneof the data strategy
leaders at SAS.I help SAS customers globally
to solve business problemsacross various indices, offering
data management expertise.Here's our agenda for today.We look at data access methods,
five of the most used datasources--Azure Data Lake Storage,
which is the blob storagesource in Azure,
HDInsight Hadoop Data LakeStorage, SQL Database,
MySQL Database,and PostgreSQL database.Finally, we'll take a quick
look at the possible data forcesand connection options
for Azure databases.Azure Data Lake
Storage, or ADLS, herewe are looking at creating a
SAS via Cloud Analytic Servicelibrary, or in short, CAS
library to ADLS and SASlibrary or libref
at the same time.First and foremost, when
an ADLS storage instanceis created in Azure, it has
to be ADLS Gen 2 for SASto connect and access data.Other important parameters
are resource type,which is set to ADLS.It is the SAS
system defined type.So it has to be ADLS
for this to work.Few Azure specific connection
parameters, file system.This is analogous to
a schema in RDBMS.Here it is set to
data Tenant ID.This would be the ID setup
for your organization or teamin that organization,
depending on whether thereis just one Azure account
for the whole organizationor each team.And then the application ID.This would be an application
registration ID in Azure.Typically, this would be
set as a rich applicationfrom the organization that's
trying to access the Azureknowledge storage.And path here is set
to look from routewith a backward slash.Also a SAS library was
created using libref.So traditional SAS
data sets as wellas in-memory tables
can be loaded or sent.If you look at the SAS log
of a successful execution,this SAS Viya in memory has
three nodes, one controller,and two worker nodes,
which now has a CAS libraryreference to the ADLS Gen
2 data storage in Azure.Now let's look at an example
to save data to ADLS from SAS.Here, we are saving the data
to Azure Data Storage in ORC,which is Optimized
Row Column formatset, using SAS ORC engine.We are utilizing the CAS proc
casutil to do this execution.Going from the top of the code,
we are loading a SAS data setto in-memory cluster first.Then saving the in-summary
table to ADLS in ORC format.Key information here, the
highlighted CAS out parameter.The file name should have
.ORC to save the file in thatformat.As we look at the
log, SAS ORC enginesaves the class table in this
example to ADLS in ORC format.Another point to note,
the SAS ORC engineis independent of
the support for ADLS.What that means is you can
just create an ORC fileoutput to be persisted just
to the local path as well.Second, AZURE HDInsight
Hadoop Date Lake, thisleverages SAS/ACCESS
Interface to Hadoop,to connect and access data
from HDInsight Hadoop.URI is an important
parameter here.This points to the Hive
Server 2 in HDInsight.After a HDInsight cluster
is created in Azure,this URI can be retrieved
from the properties.The file is the name of a hive
schema they are pointing toin this example.Two environment
variables have to be set.One, to a local folder
containing all the Hadoop JARsretrieved from the
HDInsight clusterThat we have specified here
using SAS_HADOOP_JAR_PATH.And the other config XML
files for the Hadoop clusterthat we have specified here
using SAS_HADOOP_CONFIG_PATH.SAS or Hadoop admin would use
a SAS Hadoop tracer script--it's a Python script--to extract these files from
the Hadoop cluster to SAS.From the log, you can
see this example usesSAS/ACCESS Hadoop, with the
engine parameter pointingto Hadoop.Third, you're looking at
the Azure SQL database.Here we use SAS/ACCESS database
through Microsoft SQL Server.That is specified by setting
source type parameterto SQL Server.The connection information
is added to ODBC.as seen by
sqlserver_dsn parameter.It is set to Azure SQL
database in this example.Here's an example to save data
from CAS in-memory clusterSAS--I'm sorry-- Azure SQL database.This is similar to the ADLS
example we have looked at.A couple of changes.Output CAS library is said
to Azure SQL database.And the CAS out parameter
set to just cars,to save the data
to an RDBMS table.Now saving data to
other two data sources,MySQL and PostgreSQL
are almost similar.So we'll just look at the
connection information.The only parameter
a user has to modifyis the outcaslib, pointing
to the specific data source.Like for example in
this Azure SQL database,we are pointing to
outcaslib azsqldb.That would be modified the MySQL
database library or PostgreSQLlibrary for those
specific examples.Other features, such as SQL pass
through to Azure SQL databasesare also supported here.Fourth, we're looking
at MySQL database.Here we use SAS/ACCESS
interface to MySQL.This is specified by setting
source type parameter to MySQL.The connection information
is specified directlywith host and user credentials.In this example, I have the
global parameter commented.What that means is the CAS
library reference to the SQLdatabase will only be available
to the current session,current CAS session.Essentially, other users
won't be able to see it.The other source
type's set to MySQL,but this source
type, other flavorsof MySQL databases
available in Azurecan be accessed as well,
such as MariaDB and MemSQL.Fifth accessing Azure
PostgreSQL database.The code is almost
similar to MySQLwith the source type
set to postgres.By that, we are
pointing the SAS systemto use SAS/ACCESS
interface to Postgresto establish the connection.Other features, such as SQL
passthrough to PostgresSQLare supported here as well.In this example, we have
specified the connectioninformation and the credentials
directly in the code here.We could also certify
using a ODBC file as well.Here's a summary of all the
possible databases in Azurethat can be accessed from
SAS using possible SAS/ACCESSinterfaces.If you have Teradata
database in Azure,you can leverage SAS/ACCESS
interface with Teradata.If you have Snowflake
in Azure, youcan leverage the new SAS/ACCESS
interface with Snowflake.Now SAS/ACCESS interfaces
to ODBC and JDBCcan be used to connect
and access datafrom virtually almost
all data sources.So database specific
SAS/ACCESS interfaceis recommended for
optimal performanceand additional features, such
as SQL passthrough, whichis specific to individual
databases to execute the code,SQL code inside the database
rather than pulling the datato SAS and then execute it.One of the advantages of using
that is minimizing the datamovement, as well as
boosting the performanceof the whole data
access functions.That's my presentation
for today.Thank you all for joining us.If you would like to reach
me, here's my email ID--kumar.thangamuthu@sas.com.I'm happy to chat with you.Have a nice rest of your day."
84,"Hello, everyone.My name is Usha
Srinivasan, and Iam a data scientist at Credit
Analytics at Ford Motor CreditCompany.And today I'm going to talk
about CECL IFRS9 implementationprocesses that we have
gotten working at FordCredit collaborating with SAS.And I'm going to talk
about drivers for quarterover quarter change
in allowance.I'd like to first start off
with some acknowledgments--the team I work with,
Maria Mutuc-Hensleyand Jill Bewick, who's our
manager at Ford Motor CreditCompany, as well as our
collaborators SAS, Gaurav Singhand Danny Noal.I'd also like to
acknowledge Yi Lu, whois our director at Credit
Analytics at Ford Motor CreditCompany, our business
partners Suzanne Mayberryand Shlomo Kleinman,
Margaret Mellottwho is our global
communication manager,as well as Rahul
Reddy who helped uswith the IP infrastructure and
implementation of the IFRS9variance process that I'm
going to talk about today.So with that I'm going
to briefly go overthe outline of my presentation.I will be going over the CECL
IFRS9 background and modelfieldwork, talk about overview
of the different attributionmethods, including loan
level and methodologyfor variance attribution,
as well as relatedaverage variance attribution.I will then briefly go
over comparisons of methodand talk about
business insights,and conclude with some summary.So as a lot of you
may already be aware,the IFRS9 CECL process
for expected credit losscalculation requires
us to go overthrough the cycle process for
calculating the expected creditloss to a point in time process.And this happened because in
the most recent financial crisisfrom 2008 and '09, the
loss impairment models onlyrecognized credit
losses as they occured,or after there was
evidence of default.So the International Accounting
Standards Board issued a newaccounting standard-- the
International FinancialReporting Standard 9, or IFRS9,
in July of 2014 to kind of getover this problem.And under this new process,
the expected credit loss--that ECL--should be recognized
at all timesfrom studying from
the originationthrough the lifetime
of the contract.And so the ECL is therefore
recognized at each reportingmonth or date, and
is updated to reflectthe changes in credit risk
associated with that portfolio.And as part of this
IFRS9 requirement,macroeconomic
indicators are includedas parameters in the models that
covered up the ECL calculationso that we can generate
forward-looking estimates thatwill take into
account any expectedchange in the
macroeconomy, and howthat would be likely to
influence that expected creditloss.So what is current
expected credit loss?So the ECL calculation
is shown in the slide.It is a product of our
exposure at default,or our cash flow, the
probability of default,the loss given default
or the severity,as well as the discount
factor that takes into accountthe time value of money.The probability of
defaults can furtherbe broken down into its
component factors, whichare the probability
of default historical,the probability of
default macroeconomic,the probability of default
with a performance adjustment,and the survival
factor that takesinto account the number of
contracts that are currentlyexisting in our portfolio.The PD macroeconomic and
further be broken downas a target of the different
macroeconomic variablesthat were significant
in the PD macro model.Similarly, the LGD is a product
of the different macroeconomicvariables that were found to
be significant in the LGD macromodel.And each of these models
are portfolio specific sothat we can calculate
the expected creditloss as a function of
these lists metricsfor different portfolios.So what is variance attribution?At its very simplest form it
is taking the difference in ECLbetween two reporting times--maybe two different months
or two different quarters--and breaking that change in
ECL into the underlying riskfactors.So what the variance
attribution allows us to dois identify the
different risk factors,and find out those factors
that are most responsibleor the biggest drivers
of change in ECL from onereporting time to another.And so these risk
factors are shownin the pictorial on the left.So these include portfolio
growth, change in the PD,and change in the LGD.And recall I had mentioned that
the macroeconomic factors area part of the PD and
LGD change, so wecan account for how
macroeconomic changes willaffect our ECL change from
one time period to the other.We also take into
account changesin the historical default
rate, and changes in dayspast due so that we can account
for a significant increasein credit risk as a
portion of the accountbecomes increasingly delinquent.So ECL variance methods can be
determined in different ways.That is the step by step method,
the contract level method,and a weighted average method.The step by step method
is the most comprehensive,but it's also computationally
the most intensivebecause it requires us to
change one risk factor at a timeand calculate the ECL,
and attribute the ECLvariance to match this factor.But then we need to do
this over and over againfor every risk factor.So this is kind of the gold
standard, but like I said,really computationally
intensive.In addition, we have the
contract level method,which is the most granular
mathematical attributionthat we can perform because
it tracks all the changesin the ECL at a contract level.We also have the
weighted average method,which is a more aggregate
level methodology thatcan be used to calculate
changes in the rate of defaultand severity, but not
at a contract levelbut rather that an aggregated
segment, product, or business,or even at a portfolio level.So I'll be going over
these three methodsin the rest of my talk.So the contract level method
basically takes the differencein ECL between times T1 and T2
and looks at the shared portionof the portfolio
between times T1 and T2and attributes them into
the different factors.And these are shown
by the yellow bars.And so the sum total of
these three yellow barswould represent
the change in ECLfor the shared portion of the
portfolio between T1 and T2.To this we would add the
change in the portfolio volumeitself, and how that
affects the ECL.And so the sum total
of all this wouldgive us the attribution for the
ECL change from time T1 and T2.So the first step that
we worked with SASto validate this contract
level methodologywas to compare the results that
we get from this contract levelmethod to the step by
step on one at a time,changing one risk
factor, and tryingto demonstrate that the
results from the two methodsare comparable.So this slide here
shows that whenwe change the risk factors--which are shown on the
left one at a time--we can obtain the results shown
in the one at a time column.And when we perform the
same analysis using the SAScollaborative contact
level methodology,the slide here shows that our
numbers were very similar.And importantly, all
the directionalityof the changes that
we were attributingto the different risk factors
are consistent between the twomethods.So this gave us confidence
that our contractlevel mathematical
attribution is giving usthe expected results
as if we were changingone factor at a time--
or very close to whatwe would get if we were
changing one factor at a time.So this next slide
here is just a reallygranular representation
of how wecan correlate the
directionality of the changesin the macroeconomic factors
with their contributionto the ECL.So on the left side you will see
a blue line and an orange linethat show the unemployment
rate at time T1by the orange line and
time T2 with the blue line.And you can see that around mid
2020, the two lines cross over.So after 2020 June
the unemployment rateis higher for T1 than T2,
whereas in the peak 2020 frameyou can see the blue line is
higher than the orange line,showing that the T2 time
period had a lower unemploymentrate than the T1 time period.And so despondently
when we calculatethe ECL that is attributable to
this unemployment rate change,you can see that pre 2020
there is a dip in the ECLwhich is consistent with a
lower ECL at time period 2,and post 2020 we see an uptick
in the ECL variance, whichis consistent with an increase
in unemployment in time 2.So this also demonstrates that
we can use the contract levelanalysis of ECL variance
to get to ECL changesat a very granular level, which
is what was shown in the slide.So one of the take homes
is that the ECL attributedto macroeconomic
changes is intuitive,and it aligns with
business expectations.And this happens even
at a contract level.We can further aggregate
this to a portfolio leveland get the macroeconomic
related changes in ECLfor the portfolio.This next slide now
shows how we canuse the weighted average
methodology to also come upwith a similar ECL
calculation and attributethe different risk
factors in a methodanalogous to the contract level.So for the weighted
average method,the ECL is a product of the sum
of exposure, or the cash flow,times the weighted
average life, whichis the average
number of years thatare left to recover the
principal outstanding amount.The weighted average PD, which
is the PD of the portfolioor of the segment depending
on which level of analysiswe are performing,
but it's weightedby the outstanding balance.Then the weighted
average LGD, whichis LGD that is weighted
by both the PD,and the outstanding balance, and
the weighted average discountfactor which is weighted by
the exposure, the probabilityof default, as well as
the loss given default.So using this weighted
average methodwe can calculate the ECL.And further, we can break
down the weighted average LGDinto a product of the weighted
average macroeconomic factorsthat are in the LGD model.The weighted average PD can
also be broken down into itscomponent factors, that being
the historical baseline PD,the macro PD-- which
is the contributionof the macroeconomic variables
to the PD model of performanceadjustment--and the weighted average
survival, which takes intoaccount the active
portfolio thatexists at each time of review.This next slide here shows
the different calculationsthat go into determining
the componentsof the weighted average
method for calculating ECL.So we would calculate
the e field for the twodifferent time periods, and then
take the difference of this ECLand attribute them to each of
these contributing factors.So as shown in the next slide,
we demonstrate in this slidean example of how the weighted
average PDs can be calculatedfor the sample portfolio.So the slide here shows
in dotted green linethe expected cash
flows at a reportingtime that are expected to
fill out of the portfolio.So you can see the
decrease as the cashflow contractual
obligations are met.And the four solid lines here
represent the four componentsthat feed into calculating
the weighted average PD.To begin with we have the orange
line, which is the baseline PD.And this is the PD
that is expectedto flow out of this portfolio
based on historical defaultrates.But remember, in the
IFRS9 CECL framework,we have a macroeconomic
component thatoverlays all modified
this historical PD,and that is shown in the
dark blue colored line, whichis the bottom of the four curves
that are shown in this graph.So the macroeconomic
forecast is bringing downthe expectations for the
probability of defaultfor this sample portfolio.In addition, we have a
performance adjustmentwhich allows for an overlay to
this macroeconomic probabilityof default that makes it more
in line with current defaultsthat are flowing out
of that portfolio.And the last, which is
the light blue line,is a representation
of these PDs as theyapply to an active
portfolio-- soall the contracts that are
active at the reportingmonth for which this
cancellation was performed.So this gives us a
nice representationof how the weighted average PDs
change over the time horizonfor the ECL calculation.This next slide shows
the weighted average LGDcurve for the sample portfolio.And you can see as the
macroeconomic forecastchanges in the forecast horizon,
the weighted average LGDalso responds to that change in
the macroeconomic conditions.And we don't see a
flat line, but rathera gently rolling
line that representsthe weighted average LGDs as
they come out of this portfolioto the time horizon that we
are forecasting the LGDs for.So this slide here
shows a comparisonof the weighted average and
the contract level methodology.And the big take homes here are
that the two methods are verysimilar in their attribution.However, they are not identical.And part of the reason for this
is that the SAS collaborativeeffort that we performed for
the contact level varianceattribution mostly
focuses on the sharedportion of the portfolio
in terms of breaking downby the different PD risk factors
and the different LGD riskfactors, whereas the weighted
average methodology canbe used to just look at the
entire portfolio at these twodifferent time frames and
calculate the different riskfactors that are acting on
the portfolio at the twodifferent times.So you see some
differences in the valuesattributable to
the different riskfactors, however the
directionality of the changeis quite consistent.So in conclusion,
this slide herecompares the advantages of
the two different methods--the contract level and the
weighted average methodologyfor ECL variance attribution.So one of the advantages of
the contract level methodis that it is very
comprehensive.We can drill down to
each primary component--the PD, LGD, EAD.We can also isolate
portfolio effects whichare due to a change
in the portfolio,or a change in the
aging of the portfolio,and separate them from
the PD and LGD factors.Of course because it's a
contract level methodologyit is very granular,
and so it alsoallows us to figure it out if
there's any data issues whichare associated with
certain groups of contractsthat allows you to track
and fix them accordingly.Also I didn't talk
about this, but stagingor significant
increase in credit riskis part of the
IFRS9 methodology,and the contract level
method can track contractsas they go from one stage
to the other becauseof change in their delinquency.And in contrast, the
weighted average methodcan provide us with
certain other insightsinto the ECL
variance, one of thembeing that we can calculate
the macroeconomic impacton the portfolio at two
different time pointsregardless of whether
the portfolio isshared or unique to one
or the other time period.Also, the weighted average
methodology is quantitativebecause it's a portfolio
level calculation,so we have the ability to
extract portfolio level PDand LGD rates between two
different forecasting monthsand compare the values.So the key takeaways from this
very productive collaborativeeffort that we had
with the SAS teamis that business needs
to track changes in ECLunder these new
regulatory framework.And we need to be able to
attribute changes in ECLdue to portfolio,
LGD, PD risk factorsso that we can help
explain to business whatthe main drivers of variance
are from one reporting dateto the next.Also because we
have incorporatedmacroeconomic
forecasts in the ECL,we can look at
different scenariosfor downturn and upturn
forecasts into the future.And this allows us to
provide quantitative numbersto finance for the
expected creditloss under different
scenarios, and thisallows them to anticipate
the magnitude of changein allowances that
could potentiallyhappen when the macroeconomic
conditions change, and givesthem the ability to allocate
capital accordingly.So with that I would like
to end my presentation,and I thank you all
for your attention.And I thank SAS
GF for this chanceto present to the audience.Thank you so much."
85,"SREE VUTHALURU: Hello, everyone.Welcome to the ""Analytics to
Fight Opioid Abuse"" use caseat the SAS Global Forum 2020.My name is Sree
Vuthaluru, and I ama systems engineer supporting
our government businessunit at SAS.I'm excited to talk
to you all todayand show you a demonstration
of our analytical capabilitiesthat have been tailored to
help fight the opioid epidemic.Before we go into the
demo, I will pass itto Dr. Steve Kearney
to give us an overviewof the problem at hand and
the approach SAS has takento help fight opioid abuse.Dr. Steve Kearney.STEVE KEARNEY: Thank you, Sree.Hi, everyone.I'm Steve Kearney, and
I have the privilegeto work with an exceptional
team here at SAS and with allof you to solve some of the
world's most challenging healthcare, life sciences,
and government problems.Sree is one of those
brilliant SAS colleagues thatworks tirelessly to make
the world around her betterevery day, and I look
forward to her demonstrationof how to use analytics to
fight against opioid abuse.As we deal with the
current COVID-19 pandemic,there is also an epidemic that
continues to rage across the USand other parts of the
world, the opioid epidemic.At SAS, we have been working
with all the key stakeholdersin this fight for years.Traditional or base SAS is used
by every department of healthand federal agency involved
in this daunting problem,but many times it is with
incomplete, latent datathat is often in silos.In recent years, we are proud
of the innovative solutionsand partnerships that we've
developed in this battleand we had signs of
improvement in 2019.However, many are concerned that
the current state of our healthsystem will erode the great
strides that were made.The numbers before
the 2019 improvementslook something like this.More than 12,000 people were
treated in ERs every dayfor opioid abuse and
opioid overdoses.Now with COVID-19,
those patientsare struggling to access
treatment and manythat were doing well now feel
isolated and without resources.Also, more than 130 people
died every day in 2018from an opioid
overdose, and it wasestimated it could have been
as high as 180 people per day.There is grave concern now
what the new trends look like.There was a decrease in
prescription opioid deaths,but unfortunately,
there was a 10% increasein synthetic opioid
deaths, like fentanyl.The majority of those
that were using fentanylreceived it through
their transitionfrom prescription drugs
to heroin that nowhas become laced with fentanyl.The SAS approach
promotes interoperabilityand combining data sources.It allows for a
common data languagein an analytic
environment that promotesdata-driven decision-making.Through the use of
visualization, networkanalysis, risk scoring,
and real-time alerts,this analytic environment
utilizes AI and machinelearning, along with the
ability to develop or compareany model including open source,
to turn numerous disparate datasources through APIs and
advanced data managementinto actionable data that
can empower law enforcementand health officials to get
the services to the peoplereal time and most
importantly, save lives.Now, I will turn it over to
Sree for our demonstration.SREE VUTHALURU:
Thank you, Steve.Let's jump into the demo.This demo uses some
publicly-available data,along with some demo data,
and represents informationthat may be collected
by law enforcementand prescription-drug-monitoring
programs.As Steve mentioned, combining
data from multiple agencieshelps us gain the most insight
in tackling opioid abuse.I've already cleansed the
data and prepared it in orderto get it to the
dashboard formatyou're seeing on
the screen today.I've done this within
the same SAS platform.And some of the steps
I've taken includestandardizing
addresses and names,joining disparate data
sets together, and removingduplicates across my data sets.We'll start by looking at
an overview of the city,along with some key values.Here, you can see the number
of fatal overdoses by county.I also have the number
of drug-related arrestsfor the entire state and
naloxone administrations.The line chart below shows
me fentanyl-related deathsin the yellow line and
oxycodone-related deathsin the orange line.Let's jump into the next
page for more details.This Dashboard page
is split in two.On the left-hand
side, we'll havemore law-enforcement-focused
insights,and on the right-hand
side, more healthand PDMP-revolved insights.In the middle, we can see the
same map broken down by countyfor fatal overdoses, where
Essex County has the highest.The next map underneath
shows naloxone deployments,where Camden has the highest.And the last two maps are drug
possession arrests and drugdistribution arrests,
again, at a county format.The purpose behind
both of these mapsis to better understand if
the areas where individualsare purchasing drugs
are different than thosewhere they're using drugs.Now, let's jump to
the left-hand side.Here, we have a
table where we'vecalculated risk of drug
overdose at an individual level.Now as you can
imagine, this consistsof highly-sensitive
information or PII.We would recommend and
enable role-based accessto allow only certain
users or groups of usersthe ability to view
and interact withthis sensitive information.This can be done throughout the
platform at a data-set levelor even at a report level.The specific risk score
here takes into accounta few different metrics
ranging from the number of drugoffenses, previous convictions,
total MME the individual hasbeen prescribed, as
well as many others.Now, the risk score can be
altered to add other metricsor take out certain
variables dependingon what is most relevant
for that specific state,municipality, or country.The model to create these risk
scores is created within SASand can be automatized to output
results as the data is updated.Let's move to the
byline chart below.I have two byline charts
here, one for strict countand one that takes into
account the population,and is standardized
per 100,000 people.Looking at this chart, I
can see the orange lineis the number of drug-related
arrests per 100,000and the blue bars are the
number of fatal overdoses.As you can see in
Atlantic County,there are many more overdose
deaths than arrests.The opposite is true
for some other counties,like Monmouth County here.This small insight helps
policymakers better tailortheir approaches for
different parts of the state.One of the counties may
need more treatment centersand naloxone kits,
whereas the othermay need more law enforcement.Now on the right-hand
side, we've once again riskscored, but now for providers.And we've put them into
five different riskgroups, which are the bars.The yellow line
shows the risk score.The risk score here can
take into account metricssuch as the number
of patients thathave overdosed under the
provider's care and the numberof MMEs the provider has
prescribed as comparedto others in their specialty.Again, this is
something that is highlycustomizable for
any region, and wewould work with
local domain expertsand incorporate their
input into this model.The risk scores so far
have been at a provideror individual basis.However, identifying
hotspots and knowingwhere to focus efforts,
especially with low resources,is another key insight.Let's go back to the map
and focus on Essex Countyand drill into a
zip-code-level view.Here, I can see the top few
zip codes by both risk scoreas well as fatal overdoses
and fatal overdosesper one million.So far, everything
we've looked atis from a high-level situational
awareness perspective.This approach can be taken for
not only this opioid use case,but many others--for instance, the current
COVID-19 pandemic.It's important to understand
where the problem is.However, this will
take us only so far.So let's go back and
investigate furtherat an individual
perspective and seewhat actions we can
take on this data.I'm back on my main
dashboard page.And focusing on
this table here, Ican see an individual with
an obviously high risk score.I'll double-click to take me
into the individual scorecardpage.Now, the scorecard
gives me a lotof information about
this individual,including their
previous offenses,whether or not they went
to jail, and if they weredrug dependent at the time.Again, this is a page with
very sensitive information,and we would assure that only
certain individuals have accessto it.Now, I would like to
know more about John Doe,so I'm going to jump into
our investigative pieceof the platform by
double-clicking and linkingto Visual Investigator.The investigative
side of this platformwill allow me to investigate
John Doe and any involvementsthat he's had, as
listed down here.This takes into account multiple
data sources and multiple dataentries.However, this is all done
through entity resolutionbehind the scenes,
so I'm not actuallymaking any connections.It looks like there's an
open burglary investigation.So I'll go ahead and
open that involvementand see some basic
information about this entity.Here, I can see that
the dwelling was burgledand prescription
drugs were takenfrom the medicine cabinet.This happened back
in March of 2019.I'd like a better picture about
John Doe, so let me go backand open up a network view
in a new analyst project.The network view will give
me a better understandingof all of the entities that
John Doe is connected with.Although these entities may come
from disparate data sources,SAS is able to connect them.I'll go ahead and expand
these links one level.I'm not manually drawing
any of the lines here.Again, it's the entity
resolution done in the backend.Here, I can see that
John Doe is connectedto multiple prescriptions,
some arrests,and some naloxone deployments.I'd like to know more
about the Vicodinand oxycodone
prescriptions, so I'llexpand these nodes one level.I'll start with the Vicodin.Doing so, I can see that
all three prescriptionscome from the same
doctor and same pharmacy.In other cases, we have
seen multiple prescriptionsfrom multiple doctors,
which has been indicativeof doctor shopping.While investigating
providers, wefound cases where
prescriptions were alwayssent to a specific pharmacy.This was an initial
clue to understandingthat the doctor was
colluding with the pharmacyand further
investigation was needed.In a similar way, alerts on
high-risk-scored pharmaciescan help uncover pill
mills through similar typesof investigations.Back to John Doe, I'm seeing
many other types of entities,including arrests and
naloxone administrations.I'm curious and want to know
more about the timeline here,so let's take a look at that
by opening the time slider.I'll start at the
beginning, whichis back in December of 2018.As I scroll through
this timeline,we'll see the entities pop
up in chronological order.So we see that first was the
Vicodin prescription, followedby oxycodone, and a
second oxycodone scriptthe next month.Continuing to scroll, I'll
start to see in March that therewas a possession of prescription
opioids arrest for John Doe.Continuing to scroll, I
see that burglary entry,where he was a suspect, as well
as a loitering charge, and thena naloxone deployment followed
by an assault followedby a second naloxone deployment.I'll continue scrolling
here, and we'llsee that there was a third
naloxone deployment justa few days later, and finally,
a drugs overdose charge,and he has passed away according
to the toxicology report.Now, all of these
insights and any notescan be saved as part of
an ongoing investigation.The platform itself allows for
alerts on different entities,perhaps in this case, risk
scores on patients, providers,and prescribers, as
well as assigning tasksin a workflow to claim,
investigate, and escalatecases.Throughout this
demo, we've triedto give you a high-level
illustration into oneof the many ways SAS
is using its technologyto better understand
the opioid epidemic,as well as make data-driven
decisions and actionsthrough investigations.Thank you for watching and
please connect with Dr. SteveKearney and myself on LinkedIn.And feel free to email
us with any questions."
86,"Hello.I'm Mark Jordan, your
instructor for running SAS 9Programs in SAS Viya.As a SAS programmer,
you may haveheard about the massively
parallel processingcapabilities
available in SAS Viya.Perhaps, you've gotten
access to SAS Viya at work.Maybe you're taking
a university classusing SAS Viya for Learners.Or it could be
you're just wonderingwhat SAS Viya is all about.In any case, I'm really looking
forward to spending a few hoursshowing you how easy
it is to get your SAS 9code running in our
high-performance SAS Viyaenvironment.In lesson one, we'll
learn a little bitabout what SAS Viya actually
is and how it's architected.And then we'll talk a
little bit about how to workwith data in CAS libraries.So let's dive right in.SAS Viya is a great addition
to the SAS platform.The part of SAS Viya
that excites me mostis the Cloud Analytics
Services, or CAS,provides us an in-memory
processing engine.That means that you can
lift data into memoryand leave it there
while you conductseveral subsequent
processes on itwithout having to write it
back to disk in between.And you can do this
safely because of the waythe system redundancy is setup.This allows for
extremely fast processingwhen you're doing things like
manipulating data, and thendoing some analysis
on the results.In addition, the way CAS
loads data into memoryallows it to handle
data of any size,even if the data is larger
than the physical memoryavailable to you.And it handles it all
with lightning speed.As we mentioned before,
SAS Viya is an additionto the SAS platform.And the SAS 9 engines still
exist within this platform.This course focuses
on those of uswho have been programming
and SAS 9 for yearsand are wondering how we can
best leverage the new additionsto the SAS platform.SAS Viya's an open platform.You can run SAS Viya in
a variety of environmentsin a way that makes it very
flexible to meet your needs.The main question
on our minds is,in this new massively
parallel processing, or MPP,environment, can we still use
the traditional SAS codingmethods that we've
become accustomed to?And the answer is,
yes, as long as youare aware of how SAS Viya works.And some of the differences that
make in the way things process,you can certainly use
SAS code that you'refamiliar with in the
Viya environment.So in this course,
we're going to learnhow to connect the SAS Viya
from SAS Compute Server sessionthat would be in any
session like SAS 9or the Compute Server associated
with a Viya platform itself.And then we're going
to write code that willallow us to access data in CAS.And we'll run SAS
9 programs and seehow we might modify them
to better adapt themto the SAS Viya environment.SAS Viya is exciting because
of the way it accesses data.You can access data
from any source.You can access data that lives
on the on-premises hardwareor data that you read
in from the cloud.You can process relational data.You can reprocess
unstructured data.And your data can be saved to
a variety of formats with ease.There's a new format for saving
CAS tables called SASHDAT.This is an exceptionally cool
format for this environment.Basically, it's memory mapped
so that it looks on disk prettymuch like it looks in memory.And it is stored
distributed across the nodesin your environment.This data can be
loaded in parallelof the ram at lightning speed,
and is the preferred modefor storing tables from cars
when you write them to disk.In this class, we're going to
use SAS Studio on a pure Viyainstallation.You're going to see that
the code runs there exactlylike it does in a traditional
SAS 9 installation.With a modern
version of SAS 9, youcan also use SAS Studio or
Enterprise Guide or the SASwindowing environment even to
connect the SAS Viya in CASand execute code up there.But it's more open than that.You don't even need a SAS
client really to run code Viya.You could do it from
Jupyter Notebook.You could do it from R Studio.You can do it from a variety
of different client interfacesdepending on what you need
to build your application.In this course, we're focused
on running in CAS from SAS.The architecture for
SAS Viya is differentthan our traditional SAS
Compute Server installation.We're going to focus
pretty much on this areahere, Cloud Analytics Services.Cloud Analytics
Services is designedto provide a
distributed processingmodel in which
pieces of the dataare loaded across
several machinesand are processed in parallel.The data is lifted into
memory, processed once,processed again.Subsequent analysis
can happen allwithout writing the
data back out the disk.And only when you wish
to save out the dataand not work with anymore, you
explicitly save the data backto the offline storage.Working with CAS is made
significantly easierin SAS Studio by the
presence of snippets.Snippets in the SAS
Studio interfaceare accessed by this
icon you see over here.And their broken up into
folders for snippets designedfor different purposes.In the SAS Viya Cloud
Analytics Servicesthere are snippets
like templates.They allow you to
connect to CAS,to load data into
the CAS memory,to save data back out
at the offline storage.Or just to drop it out
of memory, if you like,and to disconnect
from your CAS session.So before we can do
any processing in CASfrom our SAS 9
environment, we firsthave to establish a
connection to a CAS session.So let's start
working with SAS Viya.The first thing we
need to do, of course,is get into SAS Studio.And you can access SAS Studio
by starting the Chrome browserdown here on the Task Bar.When the chrome
opens up, there'sa folder appear with bookmarks.We'll just click on SAS Studio.User ID and password are student
and Metadata0, capital M,e-t-a-d-a-t-a, with a
numeric zero on the end.We won't need administrator
authority for anythingwe're doing in this class.So when we log in we're
going to choose the default,no about assuming
SAS admin authority.Once SAS Studio is open, you
can see that it's very light.I am an old submarine
sailor and Iam very fond of the
new dark themes thatare coming so much in vogue.SAS Studio has a dark
theme that I can set.So I'm going to go up to my
Profile here click on Settings.And here, we have a
chance to choose a theme.You'll notice that this light
theme is called illuminate.If you ever want to get back
to it, that's what you use.The dark theme is called ignite.And I'm going to
go ahead and choosethat to make things a
little easier on my eyes.Now remember, you're in a
you're in a browser, right?So if you hold down the Control
button and scroll your mouse,you can make things bigger.And then if you
scroll the other way,you can make things smaller.So it's quite easy to
adjust this environmentto work as you would like
it to work with your eyes.Now let's work with snippets
to get access to the CAS.Down here on the
Snippets bar you'llsee that there's a whole section
on SAS Viya Cloud Analyticservices.And a New CAS Session is the
one that we'll be needing.Now, you can Right-Click
and say Open As A Program,or you can just Double-Click
this and have it OpenIn A New Tab.This is a template for
starting a session.If we leave it
alone, our sessionwill be called My Session.But you can name
your session anythingyou'd like as long as
it's a valid SAS name.And then in the
Session Options, youcan set some differences from
the default, if you want.This determines how national
language system formatsand things are interpreted.And this is the
active CASLIB that wewant to use when we
start up our CAS session.Now we could change any of these
things, but we don't need to.We can just run this code.And in the log, you'll
see that my session hasbeen connected to the
CAS and that the CASuser is the active CASLIB.Now that we have a cast
session established,eventually, we're going
to need to turn it off.So to terminate our CAS session,
we have another snippet.And I'll just Double-Click
Terminate The CAS Session.And you'll notice that because
we accepted the default sessionname, this is all set already
to terminate my CAS session.So I'm just going to go
ahead and run this code.And we can see that My Session
was successfully terminated.Now, to access files
during the course,you'll need to be able
to find those filesand determine the
path to get to them.Here we're going to go
into the Server, Home,and in the Workshop
folder, and sgf20s9v.And you can see that
these are the folders thathave our data in them.For example, if we wanted
to write a SAS program thatwould access this Excel
file, we could Right-Clickon that, choose Properties,
and there's the nameof the file and the location.Now you notice, we had
to click an awful lotto get down in here.We're going to be using
these files a lot,so it'd be really helpful to
have a shortcut to these filesand make it quicker and easier
to get to our course data.We can do this by
taking the filethat we want to make a
shortcut to, Right-Click on itand choose Add A Shortcut.Give it a reasonable name.Now, I'm going to click OK.And when I do that, up here
in the folder shortcuts,you'll now see a folder
shortcut for sgf20s9v.And you'll see that it's
a quick way of accessingthose same folder and files
that we saw in the workshop.So from now on, I'm
going to use the shortcutto get at the files for
this particular class.Now that we've connected
to a CAS session,the next step would be
to access some data upin the CAS library.In a traditional
SAS environment,we use a LIBNAME to point to
collections of SAS data filesthat are stored somewhere in a
network location we have accessto from our SAS Compute Server.So for example,
in Linux your pathmight look like this, to a
folder named MySAS, whichcontains several SAS data sets.We'd like to refer to
this aggregate locationfor SAS data sets as MySAS.And so we use a
LIBNAME statementto access that folder
location using the Base SASengine to interpret the data
sets that are stored in there.Our CASLIBs work a
little differently.A CASLIB consists of
both an in-memory spaceand the physical data
storage space that's offline.When you're working
with CAS data,only the tables that
are loaded in memoryare available for processing.The data source can
store many different typeof data that can be loaded
into memory quickly and easilyin the CASLIB.Those sources are not yet
available for processingand cast until they're
loaded on the membrane space.The data source can
store physical files,like Excel files and SAS
data sets or SASHDAT files.But it can also store
connection informationthat allows you to make
high-speed connectionsto relational databases
and other data sources.Now, CASLIBs have
two different scopes.A Session Scope means that
when you assign that CASLIB,it will be active only for as
long as the session remainsactive.And as soon as you
terminate this CAS session,the session-scoped CAS
library is terminated also,and its tables
disappear from memory.This works much like a work
library word, for instance,in traditional base SAS.Conversely, a library
can be assigned in CASwith Global Scope.Now you have to have
permission to createa globally-scoped library.And you may either
belong to a group thathas been granted such permission
by the SAS administrator,or perhaps, you're the SAS
administrator yourself.CAS libraries created
with Global Scopeare visible to all cast
sessions that have permission.You can set
permissions on these.They would be visible
to all cast sessionson that particular CAS server.And they persist in RAM even
when you disconnect from CAS.So when you reconnect
to a CAS session,those Global Scope
tables are alreadystill loaded in memory for
you to continue processingin your next session.Remember, again, that all of
this talk of data persistingis persisting in memory.Data that you saved to an
offline storage location,of course, persists.It can't be processed
until you load it backinto the memory space.So what happens to a
Session Scope CASLIBwhen a CAS session ends?And the answer is that, it's not
available next time you log in,because Session Scope
libraries are visibleonly to the person
who created itand only for the duration
of the CAS sessionin which it was created.Now, when you connect
the CAS, thereis always an active CASLIB.And there can only be one CASLIB
lib active at any one time.Generally, if you connect
CAS and don't specify,CAS will automatically make your
CASUSER library active for youat the start of
your CAS session.It's a place for you to
store your own private files.And you're the only one that has
access to the CASUSER library.At the invocation of
CAS, or at any timewhile you're in a CAS
session, you can alwayschange the active CASLIB.So if you're starting a CAS
session with a CAS Statement,you can use the CASLIB= option
to specify a different CASLIBthan CASUSER.Conversely, if the session
is already in processand you want to
switch active CASLIBs,you can use a CASLIB
statement to do that.This kind of breaks out
into three different typesof CASLIBs.Your personal CASLIB,
which is global.Your personal CASLIB will
always be available to youwhen you log back in,
restart a CAS session.If you don't specify
a CASLIB whenyou start your cast session,
it will automaticallybe the active CASLIB.And there are a
predefined CASLIBs.And these are CASLIBs that
have been defined by someonewith administrative privileges.These are generally
global in scope.And they have access
controls on themto determine which users
can tap into these CASLIBsduring their CAS sessions.And these are generally created
for popular data sourcesthat many different people would
require access to frequently.And finally, there are
manually added CASLIBs.Now, you have to have
permission to do this.Not everybody will
have permissionto manually add CASLIBs.But your user in this
class has this permission.And when you add
a CASLIB manually,you can create that
as a session CASLIB,which will only exist for
the duration of your session.Or you can create it as a global
CASLIB, which will persistafter your session disconnects.Now, the administrator
can controlwho has access to that CASLIB.And we usually use these
for ad hoc data access.So we frequently do
have authorizationto manually add CASLIBs.So now that I have a CASLIB
active in my cast session,how can I see what's in there
in the SAS 9 side of things?We could connect
to the CAS sessionwith a light LIBNAME statement.We provide the LIBREF that we
want to use on the SAS sideto refer to that data.In this case, we're
using CASUSER.And we specify the CAS
engine to connect to that.Now, you can stop
there, put a semicolonand the active
CASLIB about the timewill become connected
to your CASUSER LIBREFthat you see here.Or you can go a little
further and explicitly specifywhich CASLIB you would like
that LIBNAME to connect to.When you've
connected to a CASLIBwith a line LIBNAME
statement, only the tablesthat have already been loaded
into memory are visible.Remember, we said, that in
order to process data in CAS,the data has to be loaded
into the memory space.So it is quite possible that
you'll have several offlinepieces of data stored
in this CASLIB,but when you connect
with the CASLIB,you'll see nothing
because none of themhave yet been lifted
in the memory.Now rather than
assign a CASLIB oneat a time with a
LIBNAME statement,you can use a CASLIB
statement to assignLIBREFS for all of the CASLIBs
available to you in your CASsession.You do this with the
_ALL_ naming conventionand the keyword ASSIGN.If you don't want them all, but
you have three or four of themthat you'd like to connect at
once, rather than running threeor four separate
LIBNAME statements,you can just say, CASLIB, and
list the CAS library namesthat you're interested in
and the assigned keyword.This makes all those
CAS libraries availableusing LIBREFS that are the
same as the CASLIB name.And they'll be visible
in your SAS session.So let's take CAS
for a test spin.In SAS Studio, I'll
find the demo program.In this program, we're
setting a path macro variablethat kind of points to the basis
for all of the courses here.And then we have a
LIBNAME statementthat assigns a LIBREF to
point to our SAS filesthat are located in
the MySAS folder here.And then we're going to start up
a CAS session named MySession.We're really using all
the default settings.And we're going to
list the CASLIBs.Let's go ahead
and run that code.And you'll notice in the SAS log
that we are connected to CAS,that our CASUSER library
is now the active CASLIB.And then a list
of all the CASLIBsthat are available
to me in my session.Even though CASUSER
is the default,there are several other
CASLIBs available out there.So we have connected the
CAS, and we've actuallyassigned a LIBNAME name
to our base SAS data.So here in my
LIBNAMEs I see my SAS.But I don't see anything
about CAS, no CAS libraries.Well, early on we said that
we could use a line LIBNAMEstatement to assign a LIBREF of
our choosing to a CAS library.Because CASUSER is my
personal library in CAS,I'm going to call the
LIBREF CASUSER also.I'm going to go ahead and
assign that LIBREF here.And now my libraries
show the CASUSER library,but there are no data files
in memory at the moment.Now, if I wanted to assign
LIBREFS to all of my CASLIBsall at once, I could use that
CASLIB statement instead.Remember, CASLIB_ALL_ASSIGN.And when I do
this, you'll noticethat there's a lot of CASLIBs
that did get assigned,but there's a couple
of notes in herethat said, basically,
the CASLIB name was notvalid for a use as a LIBREF.You remember that
there's a restrictionon LIBREFS that say we can only
have eight characters in them.And the CASLIB model
performance datawas too long to make
a LIBREF out of.And so the SAS LIBREF was
not automatically assigned.We'd have to do that one by
hand and give it some shortcutname of our own if we'd like.But it did expose all of
these other CASLIBs to me.You'll notice that most of
those don't have any data loadedin them right now either.Well, I wonder if there's
anything in my CASLIB CASUSER.You'll notice that there's
no content showing.Well, I can list the files
in the outline storagewith a list files
and in PROC CASUTIL.PROC CASUTILs are
very useful procedurefor fiddling around with
stuff in the CAS environment.We can list the
files and the tables.The tables would be
things that are in memory.I'm going to go ahead
and run that list.And we've noticed
that it gives ussome great information
on where the path isto the actual physical
folder that is associatedwith the CASUSER library.And then we have a boat
load of files out there.But when we did the list for
the tables, nothing showed up.So no tables are
loaded in memory.Let's have a look at
the Public library.We can change the CASLIB to
Public by restarting our CASsession with a new
CASLIB= specification.And then we can list the things
that are in the Public CASLIB.You'll notice that that does
set Public to my active CASLIB.Now, this one says
local equals no.That means that this
is a global library.We could just as easily list
the files and tables in Publicby changing this,
if we were curious.We found that there's no
tables available in Public,but there are a couple
of offline filesthat are available out there
we could load into memory spaceif we wanted.Now we can change the
CASLIB without havingto reset the CAS session
with a CASLIB statement.So we're going to make a
new CASLIB called MyPath.And this is an ad hoc CASLIB
that is local in scope.This is the data folder.We look at our folders
out here in data,there's a folder called MyCAS.We're going to point
our CASLIB at thatand use that as our offline
storage for the CASLIB labcalled MyPath.And when we create a CASLIB
with a CASLIB statement,it automatically becomes
the active CASLIBfor my CAS session.With this SAS
function, GETSESSOPT,you can see that the
active CASLIB is MyPath.Now what happens
when I clear it?It disappears for
my CAS session,and you'll notice that the
CAS session never leaves youwithout an active CASLIB.Soon as we cleared the CASLIB,
MyPath, CAS automaticallyset CASUSER, our personal CAS
library, as the active CASLIBagain.And that's our first dive
into using CAS libraries.Hi there.It's Mark Jordan again.In lesson two of
Programming for SAS Viya,we'll take a look
at how to load datainto CASLIB from the
offline storage associatedwith the CASLIB.And also, loading data into the
cast lib from files and datasets available to us
in the SAS Session.So now that we've learned a
little bit about accessing datain CASLIBs, you'll probably
want to load some filesend to the memory space and
do some analysis in CAS.Now, before we can do any work
with tables in CAS, of course,we have to have the
table in memory.And as you can
see in this CASLIBthere are no in-memory
tables loaded.But we have plenty of
files in the serverthat we can use to
load tables from.The server-side files reside
in that offline storage spacethat belongs to the CASLIB.And they could take
many different forms.In order to do analysis
with any of those files,we need to load
them into memory.Once the files are in memory,
we no longer call them files,we call them tables.And tables are
what CAS work with.Just like CASLIBs can have
a Session Scope or a GlobalScope, so your tables
can also have a SessionScope or a Global Scope.The default is Session Scope.What this means is that
you'll see the promoterequals no when you look at
the contents of the table.And that table will
be visible only to youor whoever it was that created
that table in their CASAnd when they
disconnect from CAS,that table is automatically
offloaded from memory.What if I need that table
and subsequent sessions?What if others in my group
need to share the data with me?In that case, I'll
need a global table.A global table can be promoted
to global as you load it,or it can be promoted from an
existing Session Scope table.You'll know that a
table is global in scopebecause in the contents, it
will show promoted equals yes.This table will be visible
across CAS sessions.You can disconnect and
reconnect, and stillsee the table in memory.It will also be visible
to any other usersto whom the administrator
has granted permission.And it will have all
the necessary controlslike row level locking in
order to allow that tableto be used simultaneously
by more than one user.And as we said before,
these tables are notdropped from memory when
you disconnect from CAS,but remain in memory so
that next time you connect,there they are.So just as a quick
refresher check.If we create a Session Scope
table, is it only for usand are we the only
ones that can see it?And will it go away when
I disconnect from CAS?The answer to that
is, yes, of course.Session Scope tables are
only visible to the userwho created it.And they're automatically
unloaded from memoryas soon as the CAS
session disconnects.Global tables, of course,
are visible to multiple usersand will persist in memory
even if you disconnect your CASsession.Now maybe the data you want to
analyze doesn't live in CAS.It's not in the CAS offline
storage space or the datasources, but we have access
to it in our SAS Session.You can think of the SAS
Session here as the clientthat's talking to CAS.And so we refer to these types
of files as client-side files.We can load these
client-side filesinto CAS tables with the PROC
CASUTIL LOAD DATA statementwith a data equals and
your SAS data set name.Hi there.In this demonstration,
we're goingto show you how to set up your
autoexec file in SAS Studioso that your CAS session
and CAS librariesautomatically reconnect
themselves every time youstart SAS.Then we're going to load
a client-side file, a SASdata set into CAS
and investigatethe contents of the in-memory
table that we've created.In the previous
demo, we showed youhow to use the path macro
variable, LIBNAME statements,and CAS and CASLIB
statements to assignLIBREFS, point to local SAS
data, and to CAS tables.We're going to copy this
code, and in the Optionswe're going to
choose Autoexec File.We're just going to
paste that code in there,and then we're going to save it.Now to prove this
works, I'm goingto go ahead and log out of SAS
Studio, and then sign back in.Now you'll notice that
all of my CAS librariesthat showed up in
the previous demowere also showing up here, as
well as the MySAS library thatcontains the SAS data
sets for this class.So let's load a SAS
data set into CAS.Now remember, when
we do this, it'sgoing to take data from
an existing SAS libraryand it's going to load
it up into the CAS memoryspace as Employees.So that means that
when we do this,it should show up in the
cache user library here.The load went well.And now, we can actually
see the CASUSER table thathas the employee information.And it matches the
information thatwas in our original Employees
table here in the SAS Session.Now before, when we listed the
tables in our CASUSER library,we got nothing from the
listing, if you remember.But when we run the
list of the tables now,we can see that the
Employee table is indeedloaded up in CAS.And you can see the path to the
folder associated with that CASlibrary.Now you can use PROC
CONTENTS on that tabletoo, because CASUSER is also
a LIBREF here on the SAS side.So we've looked at it from the
CAS side using PROC CASUTIL.Take a quick look at the same
information on the SAS sideusing PROC CONTENTS.And you'll see that
we could accessthat data set either
way from the CAS sideor from the SAS side.Now, let's compare PROC
UTIL and PROC CONTENTS.Notice that the information
that we get from PROC CASUTILis fairly extensive.And PROC CONTENTS generates
the standard information.Now, we're not going to
turn off the CAS session,but we can remove
the employee tablefrom our CAS
library memory spacewith a DROP TABLE command.You see that it is
no longer listed.And just for good measure, we
can reload the Employees tableinto the CAS library and
manipulate that tablewith a standard BASE
SAS TOOL PROC DELETE.Notice that either
way we do it, usingPROC CASUTIL or
standard SAS procedures,we can manipulate,
access, and describe dataon a CAS server from
our SAS Session.Now, perhaps you want to load
files from the client side thatare not SAS data sets.And we could do that too.We use the load file equal
version of the load statement.Don't forget that
in the file name,you'll need to use a fully
qualified path to the filein your local SAS session.And then you're
going to be requiredto specify the name of the table
that you're going to create.What about those
external files thatare stored in the
CASLIB data source?Things like Excel
spreadsheets and text files.Well, you'll also
use a LOAD statement,but this one uses the
CASDATA equal option,indicating that the file
can be found in the CASLIB'soffline storage space.It's not the active CASLIB.You can specify in
CASLIB to let usknow which CASLIB's
offline storage hasthe file you're interested in.And, again, if you want to
load that table into a memoryspace in a different CASLIB,
you can specify the OUT CASLIBequals option.You can specify a table name
that's not the same as the datasource name, if you wish.And, of course, you can
promote that table on load.By default, the name
of the data sourcewill be the same as
the name of the tablein the in-memory space.But the tables are not
replaced by default.So if there's a table
existing already,you must specify replace
or you'll get an error.In a previous demo,
we took a SAS dataset from our SAS Session and
loaded it up as a CAS table.This is kind of referred
to as loading a client sidetable into CAS.In this demo, we're
going to take a lookat loading a file from the
CAS's local offline storageup into memory for processing.The file we're going to
use as an Excel file.You're going to see how easy
it is to load of Excel filehas a CAS table.And once we've
reassured ourselvesthat everything's well with that
table in our personal CASUSERlibrary, we'll promote that
table into the public spaceso that others can
access the data too.The CASUSER CASLIB
offline storage spaceincludes a Excel
file named sale.xlxs.We'll go ahead and
use PROC CASUTILto load that file up
into CASUSER memory spaceas a table called SALESXLXS.Then we're going to take a
quick contents of that to seewhat the data looks like.And in our results, we can
see that the columns came outpretty much as we'd expect.Notice that all the
character columnshave come out as VARCHAR.Nice space saving measure.And that this is a
local scoped table.Currently resides in
our CASUSER library.Having satisfied ourselves
that the data looks good,we would like to share this
data with the rest of the peoplein this CAS Server.Now there's a CASLIB
out there namedPublic that contains a bunch
of Global Scoped tablesthat remain in memory
for others to use.What we're going to do
is promote the tablefrom our CASUSER library
to the Public CASlibrary using the same name.Now when we do this, the actual
table is moved, if you you.It will appear in Public and
no longer appear in CASUSER.Now, because I've already
run this demo before,that table already exists
in the Public library.And one of the attributes
of a global tableis that you can't replace it.In order to replace the
table in the Public library,I'm going to have
to drop it first.Now you can see that the table
has been replaced with a tablethat we used to have
in our CASUSER CASLIB.And just to verify that
everything went as planned,we'll take a list of the
tables in both CASUSERand in the Public library.From the output, we can see
that in the Public CASLIB,the SALESXLXS table exists.But if we scroll back and
look at the CASUSER CASLIB,there are no longer any
tables listed there.So when we promote a table
from one CASLIB to another,the table is moved and then
promoted to a global table.Now in this class, you're
using a virtual lab.You'll be logging in a student.This user ID is a
data manager whohas permission to
load data into CASand set access
permissions for others.You may need to identify which
files are needed to be loaded,whether their client side or
server side, determine the CASlibrary or libraries
where the in-memory tablesshould be stored.Finally, you'll want to
take a look at table scopeand see whether the tables
scope should be local or global.After you have manipulated
and processed the data,you may wish to say
the modified dataoff to the offline storage.And if you do that, you
may want to determinewhether you want to save that
data in its original formator in SASHDAT that
format for rapid parallelloading when needed again.So after processing the tables,
we might want to save it.And we'll show you
how that's done.Now, there's a lot
of tables loaded upin the memory space for
this particular CASLIB.And we may want to
drop them from memoryor want to know when
they are droppedfrom memory automatically.If you have the CAS Server
restarted, of course,all in-memory tables will
be dropped from memory.But you can also use PROC
CASUTIL with a DROPTABLEstatement to deliberately
or explicitly remove tablesfrom the memory space.Before you do, you may
want to save that table offinto the data source
area for later reload.And an excellent format for
storing your data is SASHDAT.This is a format that allows
the data to be quicklyloaded back into
memory in paralleljust by reading the header.It's typically much, much
faster than transferring datafrom the client side
to the server side.Now, let's have a look at how
we might save a table thatis in a memory space
in a CAS libraryout to the permanent
storage space of the CASLIBin SASHDAT format.Do this we're going to use a
code snippet to help us out.First, let's have a look
at the files that alreadyexist in the CASLIBs
we've been dealing with,CASUSER and Public.And we can see that the CASUSER
library has the expected files,but there is no
SALESXLXS SASHDAT file.Now, you remember that
SASHDAT file was nicebecause it made for very, very
fast loading of large datasets.Now, SALES isn't a
particularly large data set,but it will do for
our demonstration.In the Public
memory space you'llsee that the SALESXLXS
table exists.So we want to take this
table and start backas a SASHDAT file.So I'm going put my cursor
right here in the code window,and I'm going to find me a
code snippet that will helpme save a table to the CASLIB.Now, if I Double-Click it, it
opens up in a separate window.But if I Right-Click
and choose Insert,it will insert code
into my current program.So the table name that we
want to save is SALESXLXS,and the CASLIB like
that it's is its Public.The CASLIB where I want to
store it really is my CASUSER.And we're going to call
it salesxls.sashdat.Now that file
should be availablein my CASUSER library.And a quick run of PROC
CASUTIL should prove our point.And now we see the
salesxls.sashdat file is storedin our CASUSER library,
And then ratherthan loading from an Excel
spreadsheet, next timewe can load it in parallel
using the SASHDAT file.Hi.Mark Jordan, again.And in Lesson Three of
Programming for SAS Viya,we'll take a look at our
traditional SAS code, DATA Stepand SQL, and see how
we might have to modifythat to run in SAS Viya.Now, if you're like me, you
have extensive experienceprogramming and DATA
Step and Base SAS.You maybe wondering, have
my skills become obsolete?Does everything I know
go down the tubes?And that is a good
question, but the answeris that it's not true.You can use much
of the same syntaxthat you're very much
used to using in DATA Stepand run that in CAS at
much faster speeds becauseof the parallel processing
capabilities we find in CAS.Now, the very thing
that makes CAS fastalso changes a little bit
the behavior of programsthat we write and run in CAS.So we'll need to be
aware of those changesand figure out how
we want to strategizeto mitigate the
effects that we don'twant, while keeping all those
high speed effects that we do.So to understand what
causes the difference,a DATA Step itself is a
single-threaded process.It reads the data sequentially
one row at a time.And the instructions that
you write in a DATA Stepare really instructions on
what to do to each row of data.And so this concept of
sequential single-threadedprocessing shouldn't
come as a surprise.But in CAS, we have
more than one workerthat can be executing
that same code at once.And so the data needs to be
distributed amongst the workersso that we can maximize
our throughput.And CAS and Viya take care of
all that stuff automaticallyfor us.And this means
that our DATA Stepwill run now in multiple threads
instead of in a single thread.Now, the data is
partitioned out in blocks.And this means that
not every threadis going to receive exactly the
same number of rows of data.Another interesting
thing about itis that when you have
multiple people workingon the same thing in parallel,
some will finish quickerthan others.So that the order
that you get thingsback in from the processors is
not always the same every timeyou run the process.So we'll look at some
of these examplesand how, in some cases,
the parallel processing iscompletely transparent to us.In other situations,
where we needto take some action
because of the waythat threads produce
their results.Let's start with a
simple Base SAS DATA Stepprogram using a single thread.And then we'll run the
same thing straight upin CAS and Viya
and check it out.So the DATA Step is used
to manage or manipulatetables in preparation
for further analysis,generally, with some type of
analytical procedure in SAS.We can modify the values that
already exist in columns.We can compute new columns,
and we can conditionallyprocess and produce extra
rows or only the rowsthat we desire.And we can combine
tables in a DATA Step.Here's a relatively
simple DATA Step.It reads in a data set
called MYCUSTOMERS,and it writes out a data
set called DEPARTMENTS.All we're doing is checking
the value of CONTINENTto set the appropriate
value for the DEPARTMENT.If both the output table and
input table in a DATA Stepare CAS tables,
then this DATA Stepwill run in CAS automatically
without any further need for meto do anything to my code.Now let's take a look at how
to take an existing DATA Stepprogram and modify it so
that it can run in CAS.The big thing that we're
going to have to rememberis the DATA step has to read
from a CAS in-memory tablebefore it could run in CAS.And ideally, any
data that it writeswould also write
out to a CAS table.So first, let's take a
look at the data programand see how it works.The data program
creates an output dataset called WORKDEPARTMENTS
by reading in the customerdata set from our SAS library.The select group sets
the value of DEPARTMENTdepending on the CONTINENT
in the particular record.And the output
data set will onlycontain the variables CITY,
CONTINENT, and DEPARTMENT.And writing the values of
THREAD ID and N to the logwill let us know how many rows
were processed by this threadas we run a traditional DATA
step just to see how it works.You can see that
the THREAD ID was 1.In other words, there's
only one thread associatedwith traditional a DATA Step.That's true.Now we've processed
almost 200,000 records.Now we'd like to run
this program in CAS.The first thing we're
going to have to dois to make sure that the
data set that we want to readis in CAS.You can't run it in CAS
unless the data you're readingis in CAS.So I'm going to go ahead and
load the CUSTOMER data set upinto my CASUSER library.And I'm going to promote
this to a global table.This means that every
time I connect to CAS,my CUSTOMER table
will already be readyfor me to use in my
personal CAS library.And that seems to
have run quite well.So the next thing
that I need to dois change my DATA Step to
read from the CAS library.Now we have assigned a LIBREF
to the CASUSER library calledCASUSER.And my table should be
available there as MYCUSTOMERS.So I'm going to change
this to say CASUSER,and I'm going to change
this to say MYCUSTOMERS.Now we also said, in
order to get this babyto run in CAS all the
way, would be goodif we wrote the result
out to a CAS library.So I'm going to write
this out to CASUSER.And that should be
all the modificationsI need to get this
thing to run in CAS.Wow.A lot of processing
going on there.You can see that because we
were reading from a CAS table,writing to a CAS table, the
entire process ran in CAS.It's noted in the log.And that there were 16
threads involved in processingthis data.But what about the output?Has anything changed?Well, in order to make this
work so we can compare them sideby side, I've made a copy of
the original DATA Step programover here, and I've executed it.We can see that the output
data contains Leinsterand its first row of data.And if I rerun this
code, you'll notethat the output data
always comes outin a consistent order.Leinster's always the
first city listed.When I ran the
identical code in CAS,the first city was Hanover.If I rerun that code,
the order of the rowsthat is coming out of
the DATA Step in CASis different each
time due to the factthat rows returned as
soon as a thread hasfinished processing them.And our log confesses that
we've got 16 threads workingon this problem.In the original DATA Step,
we had only one threadworking on the problem.This is just an artifact of
multi-threaded processing.All right, so we've
seen in some cases,that the DATA Step
runs sequentiallyin a single-threaded
Base SAS environmentand SAS Viya
environment producedpretty much the same results.Maybe in a different order,
but the same results.Let's talk about
conditions that maycause the actual calculated
results to be different.Let's take a look
at an example thatuses a SUM statement to create
an accumulator variable.When we as a SUM
statement in a DATA Step,the expression on
the right-hand sideis added to the accumulator
variable on the left.And the value of that variable
is retained automatically,so it doesn't get set back to
missing every time the DATAStep iterates.Now here, we're using the
SUM statement conditionally.So the expression is only added
to the accumulator variableif the condition was true.This DATA Step creates
the accumulator variable,NAcustomers.It counts the number of
customers from North America.Both of the input and
output data sets are in SAS,so this code is going
to process sequentiallyin a single thread on
the SAS Compute Server.When we're all
finished, only one rowwill be output that has the
total value for the numberof North American customers.If we change this so that the
table we're reading and writingfrom reside in CAS, then
this will run in CAS,and so it will also execute
in multiple threads.You can think of many copies
of this program, right?Each running independently
in multiple threadson their own blocks of the data.And when you look
at the result, youcan see that each
copy of the DATA Stepran on its own thread.And when the thread had
completed processing its data,it wrote a record out.So rather than
having one record,having the total for the
entire data set MYCUSTOMERS,we have 16 different
rows of data output.One for each thread that
was doing the processing.So this is an artifact of
threaded processing general.SAS programs are
processed and CAS.The data and a program
is distributed across allof the nodes in the system.And after each node
finishes up its processing,it returns the results to
the system for writing outto the output table.Now when we divide and
conquer the world like this,the work process is
much, much faster.But in this
particular case, it'sfast but we don't have our
final result that we were reallylooking for.So what do we do?Well, if you think about it,
if this was a giant table,you have only 16 rows now
of pre-summarized datathat you can further
summarize to get the answer.So it might be simple as
writing another DATA Stepand summarizing those results.But how can we make
sure that DATA Stepdoesn't run in multiple threads
and exacerbate the problem?We want a way of making the
code run single-threaded.So with a single equal
yes data set optionwe can force the
processing to occuron a single thread even in CAS.Now there are still some DATA
Step statements that cannot runin CAS.If connecting to CAS from
a SAS 9.4 installation,then those DATA Step programs
will just run on SAS 9.4.If you're operating in a
pure SAS Viya environment,SAS Viya includes the
SAS Compute Server.SAS Compute Server can
run your code just asif it was running and SAS 94.Things that run in the Compute
Server run single threaded.Even though your DATA Step, in
this case, is running on Viya,it's not running in CAS, it's
running on the Compute Server.And it will run in
a single thread.Previously, we've seen
how running a DATAStep in multiple
threads of CAS affectsthe flow order of the output.Now we're going to
turn our attentionto a different problem.When you're summarizing an
entire data set with a SUMstatement and you run that
process in 16 threads,you get 16 individual
answers for subgroups,not the one single
answer for the total dataset as you had anticipated.We'll see this in
action, and then we'llsee a couple of ways we
can address this problem.Now if this DATA Step was
running in the SAS ComputeServer, it would all
run in a single thread.And as a result, we would
only get one output rowwhen the entire data
set had been processed.But if this DATA Step
is running in CAS,it will run in multiple threads.And each thread will detect
when it runs out of dataand produce an
individual row of output.If we look at the
output data, wecan see that rather than one row
with a sum for the entire dataset, we have a series of 16
rows produced by each threadas the code was
executing in CAS.So what could we do to
resolve this problem?Well, when you're running
a DATA Step in CAS,you have the ability to
add an option to the DATAstatement that specifies you
require the process to runin a single thread.Now, if we did this,
that would alsohave produced a single output.But there would have been
a impact on performance.So let's take a quick look at
the total cost of running bothof these DATA Steps
and compare itto what it would cost to
run the entire processin a single thread all at once.So in the two DATA Step process.We used to 0.3 seconds
and another 0.01 secondsfor a total of 0.04 seconds
to do the processing.By running the entire process
in a single DATA Step,we did get the answer
without writingtoo much additional code, but
it required 0.09 seconds to run.That's almost
twice as much time.And this is not a
very large data set.So you can see that
there are benefitsto running distributed in CAS.And even if it does
require a second DATA Step,frequently, the overall savings
are well worth the effort.For a quick comparison, we'll
look at the 2-step process.A single row of output and
the single-threaded versionof the same process.And notice that the answer
returned as the same,but the total time
required to acquirethe answer in the two DATA
Step process was much less.You could use a DATA Step
to process data in groupsor to merge two
data sets togetherbased on the contents of
one or more variables.But the data has
to be sorted first.If the data is
sorted, you can dofirst-out last-out
processing to identifythe first and last
observation in each group.This can allow you to
do grouped aggregations.Sorting's a pretty resource
intensive thing to do.And especially, as
your gets large,this can bog down the process.In any case, after the
sorting is completed,the data is processed one
row at a time single-threadedfrom the data sets in question.Now, in CAS, the
default behavioras to load your data
in a distributedfashion across the nodes.And this has no
regard for the valuesin any of the rows themselves.The DATA Step is executed
in different threads.So if you're thinking
about by groups,there could be members of those
groups in each of those nodes.This is the default processing
if there's no BY statementin your SAS DATA Step.In CAS, if you do have a BY
statement in the DATA Step,there is no need
to presort data.BY in CAS will ensure that
all of the rows associatedwith a particular group
wind up on a single nodeso that the group processing
that you do will be accurate.This still allows the data
to be distributed in groupsacross the multiple nodes.So parallel
processing can happen,speeding up your throughput.It is no longer
necessary for youto think about sorting your
data before doing thingslike by group processing with
first-out last-out or merges.If you apply a format to a
variable in the by group,the ordering is in accordance
with the formatted valueof that by group variable.Now, the DATA Step
with the BY statementcan execute in each
thread with the assurancethat all of the values
for a particular groupwill be on an individual node.And as each thread
finishes processing,it will return its
result. In this case,the yellow DATA Step node had
the fewest rows to process,so it returned
its results first.And then thread 4,
and then 3, and thenthread to 2, and so on.So the order that the
values are returnedmay be different each time, but
the by group processing valuesare completely accurate.Previously, we've
seen the effectson a DATA Step which was
doing an accumulationfor the entire data set.Let's take a look at
a similar process,but when we're
accumulating in by groups.Now being before we can
use it BY group processingin the traditional
data program, we'regoing to need to the data.Because we want to do
processing by continent,first, we'll sort the
customer data by continent.And then we'll take
the sorted datain with a BY statement
in the data programand do first-out
last-out processing.This basically will allow
us to make a single outputrow by the summarized values for
each unique value of CONTINENTin our original data.In order to be able to take
a quick look at this datawhen it's done, I'm going to
add a quick PROC PRINT step.Run our process and see
what the output looks like.As you can see here, we've
got one row of outputfor each unique value
in the CONTINENT column.And because the data
was nicely sortedand the process runs
single-threaded,the observations came out of
there in alphabetical orderby continent.What would be different if
we ran this program in CAS?Well, if you remember,
in CAS, we'renot going to need to
preserve the data.So I won't need
the PROC SORT step.I'm going to copy
the DATA Step out.And I'm going to change
this so that it runs in CAS.Now you remember,
we already havea copy of the CUSTOMERS table
up and my CASUSER library.I'll just modify the code
to read from that table.And then because I want the
whole process to run in CAS,I'm going to write the results
out to a CAS table too.You might remember
us talking earlierthat said that when we do
by group processing in CAS,we don't have to
preserve the data.The CAS engine will take
care of that for us.All I have to do
is run the program.And you'll notice that we got
the same five rows of output,but they're not necessarily
in the same order.We can switch back
and forth and seethat the values are the same,
but that the order of the rowsis not the same.And once again,
this is a functionof parallel processing.Hi there.Mark Jordan, again.In the fourth and final lesson
Programming in SAS Viya,we'll take a peek at some of
our favorite Base SAS procedureslike, PROC TRANSPOSE,
PROC REPORT, PROC MEANS.And see how they have
been unable to do muchof their computations in CAS.And then we'll take a look at
a new procedure, PROC MDSUMMARYthat is built specifically
for creating summarystatistics in the environment.Beginning with SAS 9 M5,
many Base SAS procedureswere enhanced to
process data and CAS.The goal is to have CAS do
the heavy lifting, summarizingand analyzing larger data.And then downloading to
the SAS Compute Serveronly the much smaller summarized
data for post-processing.And post-processing
might include thingslike computing
additional statisticsor displaying the results via
the output delivery system.The documentation for
each of the procedureswill identify what procedural
functions are supported in CAS.If you're programmed using
one of these proceduresincludes a statement that
is not supported on CAS,then the data for
that in-memory tablemust be transferred down
to the SAS Compute Server.And then that procedure will
run on the Compute Serverusing the transferred data.A lot of the procedures that
do data management things like,read table metadata,
PROC CONTENTS, and PROCDATASETS, procedures like,
PROC COPY, which make copiesof the data, and PROC DELETE,
which can delete tablescan all run in CAS.But the key to getting
this to run in CASis to make sure that you're
working with CAS tablesthrough a CAS engine LIBREF.So you can see here
that for PROC CONTENTS,the data equal
should have a CASLIBas the LIBREF for the contents
procedure to work on CAS.PROC COPY, the N
equal, and so on.Many procedures that
manipulate data like,PROC APPEND, DS2, FEDSQL, PROC
TRANSPOSE are also CAS enabled.And they can take advantage
of that massively parallelmulti-threaded environment.Now PROC APPEND allows you to
append one table to another.The BASE= table gets the data
in the data equal table appendedto it.But because the
CASLIBNAME engine does notsupport updating a
data table in place,you can't really upend to
an existing table in CAS.You can use PROC APPEND to
create a whole new table if youspecify the name of the BASE=
CAS library table as a tablethat doesn't exist yet.Then the DATA= table is copied
up in place with a new nameassigned to it.As SAS programmers,
we're used to usingPROC APPEND to append more
data to an existing data table.We're going to see
that in CAS, pendingto a table is not permitted.But we can use a DATA Step with
an APPEND equals yes optionto produce a very
similar effect.First, we'll create a couple of
smaller subsets to work with.Now, this data program
breaks up informationfrom the CARS data set into
output data sets, CARS1 and 2.One continuing only Honda,
and the other only Acura cars.I could see that one
has 17 observations,and the other has seven.Now, in order to append the
data from the second tableto the first, in a traditional
Base SAS environment,we would just run PROC APPEND.And when we were done,
the seven observationswould have been added to
the existing observationsin the first data set, for
a total of 24 observationsin the resulting data.Now, let's load those data
sets right up into CAS.And we'll try the same
processes in PROC APPENDto append the data in the
second table to the first.We quickly see that update
access is not supportedfor tables in memory in CAS.And as a result,
PROC APPEND fails.Now one thing you can
do with PROC APPENDis you can make a
copy pretty easily.If you append an existing
table to a table thatdoesn't exist yet,
then PROC APPENDmerely makes a copy of the
original table for you.Let's go ahead and
take a look at that.I'm going to use PROC APPEND
the copy a data set up into CAS,and then compare it
to the original table.Because there was not
yet an existing table,PROC APPEND was
successful in creatinga new table named CARS.How are we going to append
the CARS2 data set to CARS1?Using a traditional DATA Step,
writing to a CASUSER CARS1table, we're going to set CARS2.But we have this APPEND equals
yes data set option applied.And the end result is
that the final caslibtable has the 24 observations
in it that we wished.And we have achieved
the exact same resultswe would have been able to
achieve with PROC APPEND.Now, I love SQL.But our old friend PROC SQL has
not been enabled to run in CAS.What this means is that PROC SQL
procedure can read CAS tablesthrough a CAS-styled LIBREF.But the processing
won't happen on CAS.Instead, that CAS table has to
be brought down to the ComputeServer and processed there.And because CAS tables
can be pretty large,it's likely to have a
pretty dismal effecton your performance.So what's an SWL lover to do?Well, you can execute
SQL queries in CAS,but you must use FEDSQL instead.PROC FEDSQL is a SAS proprietary
implementation writtento the ANSI SQL-3 standard.In SAS Viya 3.3 and above,
besides the traditional SASdouble-precision floating point
and fixed width character data,FEDSQL can also support
int64, int32, varchar.So it has some advantages.Some of the other
differences includePROC FEDSQL is much more ANSI
compliant than PROC SQL was.And so things that
are very SAS-oriented,like the mnemonics we use
instead of operator symbols,are SASsy in nature and they
don't work in PROC FEDSQL.Now the calculated
keyword, the remeergingof calculated statistics,
all of those thingsare not ANSI standard, and so
they don't work in PROC FEDSQL.Other than his lack of support
for non ANSI extensions,you'll find that FEDSQL has a
really similar syntax to PROCSQL.However, not all the
statements and functionalityavailable in FEDSQL
can run in CAS.The CREATE TABLE statement
that creates a tablefrom the results of a query
expression is supported.And the DROP TABLE
statement is available alongwith a SELECT statement.So most of the things
you use most oftenwill execute just fine in CAS.Some of the things
that don't work in CASinclude, sat operators like
UNION, correlated subqueries,which are terrible for
performance anyway.And a CREATE TABLE statement
with an ORDER BY clause.Dictionary queries don't
work on the CAS server,but you can query the
dictionary using FEDSQLthrough a CAS-style
LIBNAME statement.And the VIEW statement doesn't
work in CAS to make views.Now, if you want
to tell if you'rePROC FEDSQL code is
actually running in CAS,you can turn on the MSGLEVEL
equal I system option.The default value, N,
only prints and notes.I will add additional
informational messages,which will include information
on whether your code ranin CAS or not.We've learned that PROC SQL
code runs single threadedand will not run in CAS.We've also heard about
this new PROC FEDSQL.That FEDSQL code is
more ANSI compliantand does run fully distributed
in the CAS environment.So let's see what it takes
to convert a simple PROC SQLquery to running CAS
using PROC FEDSQL.So here's a simple
PROC SQL querythat takes distinct combinations
of customer name and cityand produces a report
ordered by the city value.We're only going to
include those wherethe continent is in Africa.Let's have a look at
the original programand how it runs.Now we can see that the values
do indeed appear to be unique.Now, how can we get this
process to run in CAS?We would just take the query
and convert it to PROC FEDSQL.Now we'll would change
the SQL to FEDSQL.Of course, I'm going to add the
SESREF=MYSEESIONOPTION to pointthat SQL out to the CAS session
in which we wish to process.And then I'm going to run
the query just as it is.I'm using the option
message doubleequals I to ask SAS to tell
me whether the PROC FEDSQL ransuccessfully in CAS or not.Oh, no.When I ran my query directly as
FEDSQL, I ran into a problem.And the problem is that the
EQ mnemonic operator is notrecognized by FEDSQL.Remember we said that FedSQL
is very ANSI compliant.And the EQ, LT, GT type
mnemonics are not ANSI at all.They are specifically
valid only in SAS.But let's go back and change the
FEDSQl code just a little bitto use the equal sign
instead of the operator.And that really should
resolve our issue.And here we can see that the
process did indeed run in CAS.And that the
results produced areidentical to those
produced by PROC SQL.So keeping your rescue all
code ANSI compliant as much aspossible will make
conversions to FEDSQLmuch simpler in the future.Now, several rather
Base SAS procedureshave been enabled
to process in CAS.One of these is PROC TRANSPOSE.Running PROC TRANSPOSE and
CAS had several advantagesover processing on the
SAS Compute platform.It reduces network traffic
and it processes much faster.If your input table and output
tables are in the CAS server,then usually the transpose
is performed entirely in CAS.Two other procedures
that are CAS enabledare PRODUCT MEANS
and PROC SUMMARY.When the input table
is a CAS table,a lot of PROC MEANS
processing can be formed righton the CAS server.If the results are directed back
to CAS using OUTPUT statement,sometimes, there are
some intermediate stepsthat need to be calculated
on the SAS Compute Server.So some of this aggregate
data may be passed back to SASfor processing before being
written out to the CAS table.On the plus side, if you
are using a BY statementand your PROC MEANS
or PROC SUMMARY,you won't have to
sort of the databefore you use the BY
statement any longer.There are a few
things that you shouldavoid when processing in CAS.OBS=, FIRSTOBS, and RENAME= data
set options are not supportedCAS.And they will prevent
processing there.Not all statements for a
PROC MEANS and PROC SUMMARYor supported than CAS.If you use only the
supported statements,then processing should
happen completely in CAS.If the statement is
not supported in CAS,the data has to be pulled to
the SAS Compute Server in orderfor processing to occur.You will get the
correct results,but performance may suffer.In addition to some
unsupported statements,there are some
unsupported statistics.PROC MEANS and PROC SUMMARY
provide a bunch of statisticsthat are not supported in CAS.This means that if you use
those statistics in your code,processing of the
intermediate aggregatesmust be performed on
the SAS Compute Serverin order to get you the
answers that you need.If you want all of the
processing performed in CAS,you can invoke one
of these actionsdirectly using PROC CAS or one
of the many other CAS clientsand languages available to you.Another way of getting
summary statisticswith all of the
processing in CASis to use PROC MDSUMMARY instead
of PROC MEANS or PROC SUMMARY.The MDSUMMARY procedure
is designed specificallyto work on CAS tables.It could compute a wide variety
of basic descriptive statsfor variables across all the
observations or within groups.The MDSUMMARY procedure
can only read CAS tables.And it doesn't display output,
it produces an output table.You can use that table
for further processingas you would use any other table
produced by a SAS procedure.Now let's take a look at how
to use that new MDSUMMARYprocedure to produce summary
statistics for tablesin memory in CAS.At first glance, the
syntax for PROC MDSUMMARYis so close to PROC
SUMMARY, that weare tempted to just change
the word SUMMARY to MDSUMMARYon our code.But if we do this, we'll
quickly find that MDSUMMARYrequires OUT equals.It does not produce a report,
but only an OUT equal data set.So we'll add the
output statementwith an OUT equal specification
to write the resultsto CASUSER user PRICE COSTS.And then we'll run a
quick little FEDSQL queryto show us the results.You'll remember that we needed
a SESREF to get FEDSQL outto run on the CAS server.A quick look at results
shows us that PROC MDSUMMARYhas produced a wide
variety of statistics,retail price, and cost.Starting in SAS 9.4M5, both
PROC REPORT and PROC TABULATEsummarization can be executed
on the CAS server if your inputdata, of course,
originates in CAS.Like PROC MEANS
and PROC SUMMARY,PROC REPORT and PROC TABULATE
both have a long listof statistics that they can
analyze to produce a report.Like PROC MEANS and SUMMARY,
not all of these statisticsare supported in CAS.If you use all of this
sort of statistics here,the analysis can be
performed in CAS.If you choose other
statistics, then itmay have to be pulled onto
the SAS Compute platformin order to complete
your report.If your PROC REPORT
program contains variablesof usage display or order,
then the data probablywill have to be pulled back
to the SAS Compute platformto finish creating your output.If you intend to run your
PROC REPORT code in CAS,you should avoid using
DISPLAY or ORDER variables.Well, I've lectured
through all the lectures.And now, all that's left is for
you to complete the practices.While you're doing
your practices,I'd like to keep a
couple of things in mind.Number one, you should do
the practices in order.Sometimes, the output
of one of the practicesis required as input for
the next practice exercise.In addition, there's a couple
of typos on the PDF course notesfor this course, particularly,
in the exercises.On Practice 3.1, it tells you
to open the starter programsv03e01.sas.But it should be sv03p--as in papa--01.sas.Now, the same issue with
the second exercises.Program name sv03e02,
should be sc03p02.So just make sure that you
open the right program file.All of the programs on the
computer image should be fine.And I hope you enjoy
these exercisesand they help set in your
mind all the things you'velearned during
the last few hoursabout programming in SAS Viya."
87,"ISAIAH LANKHAM:
I'm Isaiah Lankham.MATTHEW SLAUGHTER: And
I'm Matthew Slaughter.ISAIAH LANKHAM: And
this is everythingis better with friends using
SAS in Python applicationswith SASPy and
open source tools.We could not be more
excited to bring youthis virtual tutorial
for SAS Global Forum2020 in which we
are going to presentthe best of both worlds.We're going to show you how to
take SAS and use its strengthsfor data analysis, data
management, and statistics,and how to combine them with
Python, the world's mostpopular programming language,
or at least soon to be so,which has strengths,
including scripting,web development, and a
huge open source community.In fact, the community is
so big and so welcoming,that it's what allows
us to have SASPy, whichis a package for
the Python languagethat acts as an interface
for the SAS system,allowing us to use SAS
and Python together.MATTHEW SLAUGHTER: Before
we explain what that means,let's take a step
back for a momentand think about the
way we usually use SAS.There are a number of different
interfaces to the SAS system.Many of you may be familiar
with the SAS display manager, aswell as SAS enterprise guide.And some of you may even be
using the latest SAS studio.Of course, many of
you may also haveto submit SAS code
in batch mode,possibly from a command line.Ultimately, all of
these interfacesoperate in a similar fashion,
in that the interface submitsSAS code to the SAS kernel,
which executes that codeand reads and writes SAS data
sets from disk as needed.Then the SAS Colonel returns
the results of that codeand a log for the execution
of it to the SAS interfaceand reports it all back
to you, the programmer.What we're going
to show you todayis a way to use Python to
interact with the SAS kernel,rather than one of
these SaaS interfaces.ISAIAH LANKHAM:
And in particular,what we're going to
do in this tutorial iswe're going to start with
a Python interface, nota SAS interface.And the Python
interface we're goingto start with is
called JupyterLab,and it is a way of using SAS
University Edition, whichis free software that you
can download and install.We have complete
instructions for howto do that on our GitHub.So if you go to
github.com/saspy-bffs.BFFs standing for
best friends forever,which is what we hope you'll
believe SAS and Pythoncan be after this tutorial.So we're going to use
JupyterLab as a Python interfaceto submit Python code
to a Python kernel,and then through
the power of SASPy,the Python kernel
on our behalf isgoing to submit SAS
code to a SAS Colonel.The SAS Colonel is going to
do what it normally does.It's going to read and write
SAS data sets from diskas necessary.It's going to munge all
the SAS code for us.It's going to return SAS
log and SAS results for usto the Python kernel.And then the Python
kernel is goingto pass along the SAS output
to the Python interface for usto display, allowing
us to indirectly useSAS from inside Python script.MATTHEW SLAUGHTER: So
in order to demonstratethese principles,
we're going to take youthrough a series
of code vignettes.First, we're going to teach
you the basics of Python.So introduce you to Python
modules and imports,as well as basic Python
syntax and object types.Then we're going to take
your data on a round trip.Importing a SAS data set
from SAS into Python,doing some data
manipulation, and thenexporting it back
to your SAS session.Then we're going
to introduce youto the SAS data method,
which allows you to interactwith SAS data sets
where they liveon disk using Python syntax.And we're also going to show
you how you can use Pythonto learn SAS if you want to.Finally, we're going to show
you how you can use Pythonas a scripting language for
SAS using SASPy in orderto imitate the SAS
macro facility.All right, this is section
1, Python code conventionsand data structures.For our first
example, we're goingto meet the Python environment.So on line one here, I'm going
to import the platform module.Then we're going to
use the print functionto display both the
Python version, whichin SAS University
Edition is 3.5.5,and the operating system
information, which again,in University Edition,
is this Red Hat Linux.Then on line four,
the help functionis going to be used to
display this whole list of allof the modules we have
installed in Python.Do you recognize any of the
ones in the list here, Isaiah?ISAIAH LANKHAM: Yeah, so
they're calling from the left.I'm noticing the name
Jupyter several times,including JupyterLab,
which I believeis the Python package that
generates the interface you'reusing right now.MATTHEW SLAUGHTER: That's right.ISAIAH LANKHAM:
I'm also noticingpandas, which is in the
same column a little bitfarther down, which we'll talk
about in just a little bit.And then I think at the
very end of that column,you'll see SASPy, which
is kind of the highlightof our tutorial.MATTHEW SLAUGHTER: Along
with the SAS Colonel.ISAIAH LANKHAM:
That's right, whichis running behind
the scenes allowingus to connect to a SAS instance
and run SAS code inside Python.So I am noticing
in terms of syntax,that this looks a lot
different than SAS,which might be something for
SAS programmers to adjust to.Let's talk about the
case of the code.Would it work like
a SAS program, whereI put it all in uppercase?MATTHEW SLAUGHTER: Right, so
this is important to know,that Python is case sensitive.So for example, if I say import
platform all in upper case,that will cause an error.Theoretically, there could
be a module named platformall in upper case, in
which case, using platformand uppercase would work,
although import in upper casewould not.ISAIAH LANKHAM: Yeah,
so case matters.What about semicolons?My pinky finger's
getting a little itchybecause I don't see semicolons
at the end of the lines.MATTHEW SLAUGHTER: Right,
so semicolons are normallyoptional in Python.I can add one at the end of this
line and that works just fine.The only situation where you
do need a semicolon in Pythonis if you want to put multiple
statements on the same line,in which case, a semicolon
is needed to markthe dividing between them.But in general, those
are not necessary.But they don't cause
any problems either,so you're welcome to
exercise your itchy pinkyfinger to your heart's delight.ISAIAH LANKHAM:
That's good to know.Let's also talk about
all the dots everywhere.That's a little different
than SAS as well.MATTHEW SLAUGHTER: Right, so
Python is an object orientedprogramming language.And this dot notation shows
you when one object isnested inside another object.So we imported the
platform module,and the sys object is
nested inside of platform.Then the version object
is nested inside of sys.So when we bring platform
into the namespace,we get all of the objects
nested inside it as well.ISAIAH LANKHAM: Yeah,
which is really helpful.I like to think of Russian
nesting dolls or turduckens,where you have a chicken inside
of a duck inside of a turkey.MATTHEW SLAUGHTER:
Sounds delicious.ISAIAH LANKHAM: Well, I'm a
vegetarian so I wouldn't know.Let's also talk
about quote marks.So I'm noticing you
have single quotes,it looks like, around modules.What happens if we add
double quotes instead?MATTHEW SLAUGHTER:
Good question.Single quotes and double quotes
are usually the same in Python,and that doesn't
cause any problems.If you want to use
double quotes instead,that is purely a stylistic
choice in most situations.ISAIAH LANKHAM:
Just like in SAS.But are there cases in Python
where single and double quoteswill have different behavior?MATTHEW SLAUGHTER: No,
so there isn't anythinglike the equivalent of
the SAS macro resolution,where you have to remember
that macros will onlyresolve inside double quotes.ISAIAH LANKHAM:
Yeah, and I thinkthat's important
for SAS programmers.Because I think
typically in Python code,the community convention is
to always use single quotes.MATTHEW SLAUGHTER:
That's good to know.ISAIAH LANKHAM: One final thing.Let's talk about
the Python batteriesincluded philosophy, since we're
talking about the open sourcecommunity around Python.So we have to import
packages because wedon't want to use
up a lot of memoryhaving them loaded by
default, so we load themas we need them.But why do we have so many
packages to begin with?MATTHEW SLAUGHTER:
Well, while it's greatthat you can go out
on to PyPI or GitHuband download whatever
packages you want,it's also really nice to have
a large number of packagesavailable just when
you download Python.It means whenever
you install Python,there's a really wide
variety of things you can do,which I think is nice.ISAIAH LANKHAM:
Yeah, without havingto go and buy separate things.You can just automatically have
a package for drawing thingson the screen or recording
audio, all sorts of thingsby default.MATTHEW SLAUGHTER: Right.All right, let's move
on to exercise 1.2.In example 1.2, we're
going to introduce youto a basic data structure
in Python, specificallythe string, or str object.So first, we create a Python
object named hello world stron line one.And then use the print function
in order to display it.Then we use the print function
again to display a blank line.And finally, on lines
four through seven here,we have some conditional
logic to test the valueof the hello world str object.And print one message if
it's equal to hello Jupyter,and print another
message if it's not.In this case, we
just set the value.So obviously, we get
the type function,which we're using to figure
out that yes, hello worldstr is of class str.ISAIAH LANKHAM:
And clearly, thisis just an excuse to show
off conditional logicinside of Python.But what would happen if,
say, after line three,we change the value
of hello world str.What if we made the value 42?Would that cause a problem?MATTHEW SLAUGHTER:
Good question.Let's try that.So obviously, we get
our error messagethat we ourselves
wrote here whenthe value of hello world str
doesn't match what we wanted.But this doesn't actually
cause an exception in Python.This is because Python
has dynamic typing,so I can create it
as a string up hereand then assign it
another value down here.And Python will just allow me
to do that and change the typeof the hello world str object.So for example, if I
change this condition hereto check if hello world
str is equal to 42,we can see that the result
is an object of class int.This is obviously quite
different from somethinglike a SAS data stack,
where a variable onceinitialized as one type can't be
given a value of another type.ISAIAH LANKHAM: Even though
the name of the variableis now a lie in Python,
the syntax still allowsus to assign a new type.MATTHEW SLAUGHTER: Yes,
hopefully the audiencewill forgive us this small lie.ISAIAH LANKHAM: Indeed.Let's also talk about the use
of the single and double equalsigns.Do those mean different
things in Python?MATTHEW SLAUGHTER: They do.In SAS, a single
equals sign is usedboth for assignment statements
and also for conditional logic.But in Python, if you want to
assign a value to an object,you have to use a
single equal sign.And if you want to check
the value of an objectto test a quality,
you have to usetwo equal signs, which does make
things a little less ambiguous.ISAIAH LANKHAM: Which
is always a good thing.I'm also noticing in
terms of ambiguity that wehave some indentation here.What would happen
if we got rid of it?MATTHEW SLAUGHTER:
Yeah, so this isone of the most interesting
features the Python language,which is that white
space actuallyhas syntactical significance.So this indentation here is used
to delimit scope and indicatethat this statement will execute
if this condition is true.And if I get rid of
that, I will get an errorbecause Python is expecting
an indented block.ISAIAH LANKHAM: Which I think
is really important for SASprogrammers who aren't
used to whitespacebeing a significant part
of the language syntax.MATTHEW SLAUGHTER: Absolutely.I think a lot of SAS
programmers enjoybeing able to choose
how they indent thingsand express themselves
artistically.But I think there's
a lot to be saidfor this approach in Python.ISAIAH LANKHAM: Absolutely.I think it's good.It forces us to
have good habits.MATTHEW SLAUGHTER: All right,
let's move on to example 1.3.All right.In example 1.3, we're
going to introduce youto some basic Python
concepts, specificallythe list data object,
and also the conceptof an index in Python.So on line one, we
create an objectnamed hello world list, which
is a list of two strings, helloand list.And then we print the
value of that object,print another blank
line, and we thenuse the type function to display
the fact that our object isof class list.ISAIAH LANKHAM: So let's
talk about when we say list.Is there a SAS
equivalent to that?MATTHEW SLAUGHTER: Yeah, so
the most direct equivalentis SAS would be a
data step array, whichis to say it's a grouping
of variables or data valueswhich can be accessed
by using an index.So for example, if I want to
print just the first elementof this hello world list object,
I can use the print functionand then say hello
world list bracket zero.And that'll display hello.ISAIAH LANKHAM: So why zero?MATTHEW SLAUGHTER:
So a list in Pythonis always indexed
starting at zero.So that is one
important differenceto keep in mind between a
Python list and a SAS array.Because in a data step, you can
choose to index your data steparray starting at pretty
much any numeric value, whichis pretty convenient sometimes.But fortunately, there are
other data objects in Pythonthat have more versatile
indexing conventions.ISAIAH LANKHAM: Which I think
we'll see in the next example.MATTHEW SLAUGHTER: All right,
let's move on to example 1.4.In example 1.4, we're
going to introduce youto the dictionary,
or dict, whichis an object in
Python that allows youto map values to keys.So on lines one
through five here,we create an object named
hello world dict, whichis a dictionary with three
keys, salutation, valediction,and part of speech,
which are mappedto three values, each of which
is a list of two strings.Whenever you see these
curly braces in Python,in between them you'll find
the definition of a dictionary.Then on lines six
through eight, weprint the value
of the dictionary,as well as the type of the
hello world dict object, whichof course, is class dict.ISAIAH LANKHAM: So you
mentioned that thisis a dictionary, which is a
fundamental data structureinside of Python.In our last example, we looked
at another fundamental Pythondata structure list, and
we noticed that therewas a SAS equivalent.Is there also a SAS
equivalent for dictionaries.MATTHEW SLAUGHTER:
Right, so a dictionaryis an example of what is
called an associative array.And the examples of
associative arrays in SASinclude both SAS formats, as
well as data step hash objects.ISAIAH LANKHAM: Right, and
I think also another synonymis the word map.And that, I think,
is maybe helpfulbecause we see this as a
mapping between the keysand the values.So we have this one
to one correspondence.MATTHEW SLAUGHTER: Right.ISAIAH LANKHAM: Yeah, so let's
talk about the idea of a key.So how does a key
correspond to the ideaof indexing a dictionary?MATTHEW SLAUGHTER: Yeah.So if I create a new object
named hello world salutation,and then I assign that the
value of hello world dict indexto salutation.Then print the value
of that new object.You'll see that by
using the same indexingnotation from a list
with the brackets,and just passing
it the value of oneof the keys of
the dictionary, wecan display the value
mapped to that key.ISAIAH LANKHAM: That's great.And then I'm noticing
that the value is a list.So could we actually then use
list indexing on the result?MATTHEW SLAUGHTER: Yeah, so
if in the print function here,I add brackets and zero, like
we did in the previous example,then we can print specifically
just the first elementof the list, which is hello.ISAIAH LANKHAM: So that's great.And then we could change
it to one, I'm guessing,and also get out the word dict?MATTHEW SLAUGHTER: We
can change that to one.And we can change
salutation to oneof the other keys
like valediction,and display the word list.ISAIAH LANKHAM: That's great.So it's sort of like I can think
of a dictionary in this senseas like a matrix.And I can sort of index
things like rows and columns,in a way.MATTHEW SLAUGHTER: In a
way, and in fact, we'llsee in the next
example that you canturn a dictionary into an
object with rows and columnsin the way you would expect
in a matrix or SAS data set.ISAIAH LANKHAM:
No, that's great.Before we do that, though,
just one final point.I noticed that when we
create our dictionary,we have salutation come
first, then valediction, thenpart of speech.But then when we print it,
part of speech comes first.MATTHEW SLAUGHTER: Yeah,
that is a good point.This is actually an artifact
of the version of Pythonwe're using.You'll recall from
the first examplethat we're using version 3.5.5,
which is actually a little bitold.In the most recent
release of Python,you'll find that the insertion
order of a dictionaryis actually preserved.But in this version of
Python, that's not guaranteed.ISAIAH LANKHAM: And hopefully,
not important in most cases.MATTHEW SLAUGHTER: I hope not.All right, let's move
on to example 1.5.In example on 1.5, we're
going to introduce youto the concept of a Python data
frame, which is the Python dataobject, which is the closest
analog to the SAS data set.So because the data frame
object is not part of Pythonby default, we do need
to import the data frameobject from the pandas
module, which fortunatelyis included with all of the
modules in SAS UniversityEdition.Then we can use the data
frame constructor function,pass it a dictionary definition,
and create a Python objectwhich is a tabular data
structure similar to a SASdata set.On lines nine through 13, we
print the hello world df objectfollowed by a blank
line, as well assome information about
the shape of our dataset, another blank
line, and finallysome additional metadata.ISAIAH LANKHAM: That's great.So let's talk about the hello
world df dot shape output.So I see an ordered
pair, two comma three.What does that correspond to?MATTHEW SLAUGHTER:
So two correspondsto the number of rows
in the data frame,and three corresponds to
the number of columns,which as you can
see, also correspondsto the number of keys in the
dictionary we had originally.And two corresponds to
the number of elementsin each list in the dictionary.ISAIAH LANKHAM:
So in other words,I can think of part of speech,
salutation, and valedictionas being like the names of
variables in a SAS data set?MATTHEW SLAUGHTER: Absolutely.ISAIAH LANKHAM: Yeah, and it
looks like the zero and oneare sort of like the row
labels when I use print.Except I'm starting
with observationzero as the initial observation
instead of observation one.MATTHEW SLAUGHTER: Yeah,
although it is alsopossible in Python to use a
column as an index by default.You'll get an index
from zero to one the wayyou would in a list.ISAIAH LANKHAM: Right,
which I think is important.And so does that tell us,
then, that we can use indexingjust like we've seen in the last
couple of examples on a dateframe?MATTHEW SLAUGHTER: Absolutely.So if I want to create
another object here.So I can once again
use the salutation key,or in this case, row label
to create an object thatjust contains that column.So if I print my
new object, you'llsee that I have selected
just the hello and data frameelements of the
salutation column.ISAIAH LANKHAM:
So we've extractedjust the single column
name salutation.MATTHEW SLAUGHTER: Right.ISAIAH LANKHAM: And
so can we then uselist indexing on that column
to get specific elements?MATTHEW SLAUGHTER: Sure,
which would you rathersee, hello or data frame?ISAIAH LANKHAM:
Well, data frame,that's the start of the show.MATTHEW SLAUGHTER:
That makes sense to me.Yeah, so we can use the
list indexing with brackets,just like we did before, and
display just the value dataframe.ISAIAH LANKHAM: So that's great.And so that sort of corresponds
to then like row index columnindex, like I would think
of with like a matrixor with a SAS data set.And so we can do this
then nested indexing, justlike we did with dictionaries.And so we can sort of
treat, then, a data framekind of like a dictionary.But in our definition, I noticed
if we scroll up a bit that whenwe were defining our columns
we always had two elements.What would happen if we tried
to have one of the listsbe a little longer or shorter?MATTHEW SLAUGHTER:
Good question.Let's try adding an
element to this list.That does cause an
error, unfortunately.ISAIAH LANKHAM:
Right, so I guessPython is not smart
enough to fillin missing values in the third
row we're trying to create.MATTHEW SLAUGHTER:
Not smart enough,or perhaps the
designers of this modulethought they were protecting
us from some potential problemsby enforcing that.ISAIAH LANKHAM:
That's totally fair.So let's maybe then talk
about one final thingabout data frames that makes
them different from data setsbefore we move on to section 2.So I think it's really
helpful to understandthat a data frame
lives in memory,and that's the reason
that we can indexand get specific elements
out of it quickly.Whereas a SAS data set
always lives on disk,and we have to process
it row by row loading itfrom disk into memory
and then back outof memory back to disk.MATTHEW SLAUGHTER: Right,
although the potential downsideto the data frame approach is
that as your data approachesthe size of available
memory, thingsdo become less efficient.And in fact, you can
run into problemswhere you don't have enough
space to put all of your datainto a Python data frame
in memory all at once.ISAIAH LANKHAM:
Yeah, that soundslike a really
important trade-offto me between how Python does
things and SAS does things.So in Python, you get
this immediate accessto things in memory.You can do this
pinpoint indexing.But you can't work
with things that exceedthe size of physical memory.So SAS has an
advantage there in thatyou can work with arbitrarily
large data sets, and youcan't necessarily in Python.MATTHEW SLAUGHTER:
Yeah, I think thisis a good example of an area
where SAS and Python covereach other's weaknesses.ISAIAH LANKHAM: Absolutely.And so then that's
where we're soexcited to have the
best of both worlds,to get just about
anything done by usingthe right tool for the job.MATTHEW SLAUGHTER: Absolutely.ISAIAH LANKHAM: And this is
section 2, ""SASPy Data RoundTrip,"" where we're going
to start with example 2.1and connect to a SAS kernel
from inside of Python.So in this example,
what you'll noticeis that we start with
an import statement.So we've seen import
statements before.In this case, I'm using a
relative import statement,where I'm saying from
the SASPy package,I want to just import the single
object called SAS session.This is different than if
I just said import SASPy.And if I were to say that
instead as an absolute import,I would have to put
SASPy here in orderto delimit the namespace where
the SAS session object lives.But going back to the
relative import statement.On line two, I'm
creating a new Pythonobject, which I'm calling SAS.And I'm setting it equal
to a SAS session objectthat I'm initializing
with an option calledresults, which tells
me how I want my SASresults to be returned.Here, I want my SAS results
to be in HTML format, whichis sort of the default you can
imagine that you get insideof something like
display manager.And then on line
three, I'm printingthe type of the
SAS object, whichyou can see has this very long
name with some dots in it usingthe Python object
oriented notation.So the type of object
SAS, the definition for itis inside the SASPy object,
and then the SAS baseobject nested inside of that.And then the SAS session
object definitionnested inside of SAS base.And so in the background,
what happened herewhen I executed find
two specifically,is that Python went out and it
used a default configurationthat comes with SAS University
Edition that tells Python,here's how to go to a SAS
kernel that lives insideof SAS University Edition.And here's how to create
a connection to it.The intention being that with
that connection established,I'm then able to submit
SAS code to that SAS kernelfrom inside of Python.And so in effect, what
lines one and two do isthey set up a
connection that we'regoing to use in future
examples inside of section 2.MATTHEW SLAUGHTER:
So what would happenif you were to invoke
the SAS session object,but not store it
in another object.If you just said SAS
session results equals HTML.What would happen?ISAIAH LANKHAM:
Well, so Python wouldbe happy to establish
a SAS session.And so you can see
here, SAS connectionhas been established.Here's my subprocess ID, if
I'm interested in keepingtrack of that for some reason.But I'm no longer going to
be able to access that SASsession since I
haven't given it a nameto use in future examples.So in other words, the SAS
session has been created,but I'm now no longer
able to access itbecause I didn't give it a name.MATTHEW SLAUGHTER: So
what are all the optionsfor this results parameter here?Do you have an option for every
ODS destination, or just HTML.ISAIAH LANKHAM: So HTML
is definitely thereas an output destination.I can also use text, which
is handy for example,if we're working
in an environmentwhere we don't have
access to rendering HTML.And I believe there's
other options here as well,like pandas if you want to
get a data frame out insteadof some type of static
text or HTML to display.There's definitely really
good documentation for SASPywhere all of those
can be looked at.MATTHEW SLAUGHTER: So if you
use pandas as your results,that's sort of like
using ODS output in SAS.ISAIAH LANKHAM:
Exactly, to capturea table inside of a data set.Right, well, let's go ahead
and move on to example 2.2,where we can actually use
our SAS session object.In example 2.2, we're
going to load a SASdata set into a data frame.In other words, we're
going to take a SAS dataset, a physical file
that lives on disk,and we're going to load it
into memory as a Python objectin the form of a panda's data
frame like we saw in section 1.If I execute this
code, you'll noticethat I get quite a bit of output
that's going to appear here.And when we compare the
outputs to the code,you'll see that it's in fact,
the first eight lines of codethat are going to define
an object called fish dataframe smelt only.Line nine is going to print
the type of that object,and you can in fact see it is an
honest to goodness data frame.A panda's object
type whose definitionis nested inside of frames,
nested inside of cores,nested inside the pandas
module that we know and loveand already saw
previously in section one.Then line 10 prints
a blank line,and then line 11 is going to
print by default the first fiverows of the data frame,
fish DF smelt only.That was created above.You'll notice that there is
also a little bit of outputthat I get about the
objects SAS, the SASsession, this connection
to a SAS kernel.It's doing some work
in the backgroundto make sure it has
a connection that itcan send SAS commands to.We don't have to
worry about that.That just sort of happens
as things start and stopin the background.We get different process IDs.We don't have to keep
track of any of that.But the point is that I
am using this SAS objectfrom our previous example, this
connection to a SAS kernel,this SAS session object.And I'm using its method,
SAS data to data frame,in order to load a physical
file, a SAS data set from disk.And the SAS data set that I am
loading lives in the SAS helplibrary.And the exact data
set name is fish.And so you can
imagine this beingthe same thing as writing
SAS help dot fish inside SASitself.In addition, I have a third
argument to the SAS datato data frame method,
which is called DS opts.And this is short
for data set options.And so what you can
imagine is that thisis like how in SAS, if I were
to write SAS help dot fish,and then in parentheses
put somethinglike where equals species equals
smelt, comma obs equals 10.I can specify that I want
to have SAS subset for mein real time the data set
to just get specific rows--here where the
condition species equalssmelt is satisfied-- as
well as just a fixed numberof observations, at
most 10 rows where thatwhere condition is satisfied.Here, I'm only
printing the first fivebecause I'm using
the head function.But if I wanted to
see more than that,I could say head 10 to see
in fact, the first 10 rows.In fact, all 10 rows, so
if I change this to 20,you'll see that I again
only get 10 rows indexedfrom zero to nine.As we saw before with zero
being the default initial index.MATTHEW SLAUGHTER: Yeah,
so I guess it's importantthat we're not
writing out SAS helpdot fish here,
because otherwise youcould easily get that confused
with python dot notation, huh.ISAIAH LANKHAM: A
slightly different meaningof the dot notation, indeed.MATTHEW SLAUGHTER:
So for the DS opts,could you also use
other data set optionslike keep equals or drop equals?ISAIAH LANKHAM:
Yeah, absolutely.So I believe that there are
analogs for most, if not all,of the data set options
available to you in SAS.So if you wanted to
just keep, for example,let's say the species column,
and say, the weight column.You can provide a third key
to your dictionary of optionshere, with the key being
keep, and then the valuecorresponding to that key
being the list of strings whereeach of the strings is one
of the names of the columnsthat you want to keep.So here, I'm saying just
keep species and wait,which you can see reflected
in my output down here.MATTHEW SLAUGHTER: And
what if I want to--instead of seeing
the first coupleof rows of the
data frame, I wantto see the last couple of rows.Is there a function for that?ISAIAH LANKHAM: Yeah, so
that would be somethingwe would do in Python.And maybe it's
important to note herethat as we're having
SAS and Python interact,some things we do in Python,
some things we do in SAS.So here, the data
set options aredoing filtering on the
SAS side, so that I'mgetting just a small amount
of input into Python,instead of getting
all of the inputand then having to do the
filtering inside of Python.However, when I'm displaying
things down here, and I say,for example, let's
say head two justto get the initial two rows.That filtering is
happening in Python itself.And so if I wanted to
do different filtering,I could either change the
number here, like we saw before.Like let's say, two to four
to get the first four rows.If I want to get the
last few rows instead,there's the corresponding
function tail.And so these are then the
last four rows in the resultsthat SAS is giving
me that correspondsto all of these conditions.And so with that, we'll
move on to example 2.3.So in example 2.3,
we're going to learnhow to use nothing but
pure Python operationsin order to manipulate
a panda's data frame.And we're going to do that
by loading the same SASdata set that we looked
at in the last example,SAS help dot fish.And we're going to
do that using SAS,which is our persistent
connection to a SAS session.So in other words, a
way for us to submitcommands to a SAS kernel
to get things back.And we're going to again use the
SAS data to data frame methodin order to say to take this
SAS data set that lives on disk,and to load it into
memory as a panda's dataframe, a Python object.So once I have done
that, once I have loadedSAS help dot fish
into a data frame,I'm then going to go through
a series of three manipulationsteps.The first step is going to
use a group by operation.And so what this is going
to do is it's going to groupthe rows in fish underscore
DF, my data frame,by the values in
the column species.The next thing
I'm going to do isI'm going to subset just
to the weight column,and then I'm going to aggregate
the values in the weight columnby these aggregation functions,
count, standard deviation,mean, min, and max.And the idea here is that I'm
creating a series of temporarydata frames-- so I'm taking up
more and more system memory--where each of these stores
an intermediate resultsin the process of first doing
a group by, and then subsettingto a specific column,
and then aggregatingvalues in that specific column.The results allow me to then see
this new data frame down here,fish underscore DF
underscore GSA, GSA for groupby subset, and then aggregate.And I can see that for
each value of species,I can see the number of rows
in the original data frame thatcorrespond to that value.I can see the standard deviation
for the values of weightin those rows corresponding
to species BreamI can see the mean,
the min, and the max,also for the rows in the
original data set thatcorrespond to species Bream.MATTHEW SLAUGHTER: Yeah, so what
we've essentially done here, Iguess, is to build the Python
equivalent of a proc means,or procs equal with a
group by clause, right?ISAIAH LANKHAM:
That's right, yeah.And so you can see
the correspondencehere between group by in Python
panda's syntax versus groupby in SQL.But you can also
see it as you said,like being the
equivalent of proc means,where here, I've used species
as my classification variable,and weight as my
analysis variable.MATTHEW SLAUGHTER:
One thing we sawearlier is that in Python,
you can often chain thingstogether, whether dot
notation for nesting objects,or indexes.Is there a way to chain some
of these commands together?ISAIAH LANKHAM:
Absolutely there is, Yeah.So here, we decided to break it
up across multiple lines justto make it clear that there are
three discrete operations thatare happening.But there is nothing
to stop me from,in fact, doing it all at once.So I can do my
group by, and then Ican chain that together
with my subsetting, and thenthe results of that, I can chain
together with my aggregation.And in fact, if
we do that, we'llsee that we get the
exact same output.And not only do we get
the exact same output,but it's actually
much more efficient.When I create new data
frames one after the other,it takes up a lot
of system resourcesbecause I have to
allocate new memory,I have to copy things
over, that kind of thing.But when I change
things togetherinto a string of
operations that are allhappening on the
original data frame,fish underscore DF, what happens
is that Python will actuallyapply each of these in memory
in place so that things actuallyare much faster because I don't
have to do all that memorymanagement behind the scenes
to create multiple dataframes in sequence.MATTHEW SLAUGHTER:
So just like in SAS,you might try to combine
multiple operationsin a single data
step for efficiency.In Python, you can
try to combine thingsin a single statement
the same way.ISAIAH LANKHAM:
Absolutely, yeah.And you can also see this
kind of as the analogbefore when we
talked about puttingmultiple lines of
Python on the same linewith a semicolon between them.Sort of the same idea of
just stringing togethera bunch of commands all
on a single data frame,all in a row.All right, and so with
that let's go aheadand move on to example 2.4.And lastly, in
example 2.4, we'regoing to see how to complete
our data round trip.Where first, we loaded a SAS
data set into a Python pandasdata frame.We did some manipulation
inside of Pythonusing only Python operations.And now, we're going to
take the results of allof our manipulation
inside of Python,and we're going to stuff it back
onto disk as a SAS data set,so literally a round trip
from SAS to Python back to SASagain.And we're going to
do that by again,using our SAS object
that we created before,which is a connection to a SAS
session, meaning a way for usto submit commands
to a SAS kernelsomewhere living on disk
from inside of Python.And we're going to use
the analog of the SAS datato data frame method that we saw
before, this time called dataframe to SAS data, to reflect
the fact that we're startingwith the data frame, an
object that lives in Python,and we're going to get
the result of a SAS dataset, a file that actually
lives physically on disk.And the data frame that we're
going to turn into a SAS dataset is the fish
underscore DF underscoreGSA data frame that we
made in the last example.We're going to give the
name fish underscoreSDS underscore GSA SDS for
SAS data set to the filethat we're creating.And we're going to create it
in the default work librarythat SAS defines whenever
we start a SAS session.So that's what's
happening on line one,is to basically do the
inverse of what we sawin the previous two examples.So then, just to prove to
you that we actually didcreate a SAS data set--again, a physical
file living on disk--with line one.If I execute the rest of
this example, what you'll seeis that lines two
to eight executehonest to goodness SAS code.So if you've been
sort of saying,well, I can kind of see what's
happening with the Pythonstuff, but I'm really much
more familiar with SAS.Well, this should look
very familiar to you.This is honest to
goodness SAS code.This is the same proc print
that you know and love,where I'm specifying
a single data setthat I want to
print out, and thena run to say that I'm done
with that program step.And you'll see that the
name of the data setthat I'm going to
print is in fact,the same as the name of the
data set that I created hereon line one.You'll also see
that I'm specifyingthat I want the results of my
SAS submission of this codeto be returned in text, which is
why I just have nice text downhere, like you might
see in listings output.And the way that I'm
actually submitting just raw,honest to goodness SAS
code to the SAS kernelthat I'm connected to
with my SAS session objectthat we called SAS, is I'm
using its method called submit.And then you'll notice
that in quotes, I'mputting the actual SAS code.And you'll notice that
I'm using a trick herewith what are called
triple quotes.So triple quotes are
a way of creatingstrings in Python
that are allowedto have embedded line breaks.And so that's why I can have a
line break at the end of linethree, a line break at
the end of line four,a line break at the
end of line five,so that this is all one
single string, so the partsthat I have highlighted here,
but with embedded line breaksso that I can make my SAS code
much, much easier to read.And see that it is, in
fact, an honest to goodnessfriendly proc print
statement here.Then, once I get the results
of submitting this SAScode to the SAS kernel, and I
store it inside of this objectcalled SAS submit
return value, Ican then process that new
variable that I've created,and I can get things out of it.The first thing to know about
the SAS submit return valueobject that's created is that it
is in fact a dictionary, whichis why you can see that I'm
using indexing notation herewith a string for
the index value,just like we saw before
with dictionaries.And so this string,
LST, is the nameof the key that corresponds
to the value of the SASresults, the things that are
actually printed right here.So in other words,
this is the key,this is the value
in the dictionary.And I am extracting
that here on line nine,and then I am printing
the results on line 10.MATTHEW SLAUGHTER: So if LST
is one key of the dictionary,can you show us what's
in the other key?ISAIAH LANKHAM: So, in fact,
there is exactly one other key,and that key is
log, which allowsme to see the log
of the SAS sessionfrom submitting this proc print
statement to the SAS kernel.So LST gives me the
results, the actual thingsthat I would see inside of
SAS if I were to execute this.And log shows me the log
of executing that code.And those are the,
in fact, the only twokeys of the dictionary that's
returned from the SAS dotsubmit method.MATTHEW SLAUGHTER: And so if you
can submit arbitrary SAS code,presumably, you could
run whatever procedureyou want on the output of
our pandas operations, right?ISAIAH LANKHAM: That's right.I could change
this to proc means.I could change it to proc
core, whatever I want.And I will get honest
to goodness SASresults back in this LST--corresponding to this LST key
of the SAS submit return value.MATTHEW SLAUGHTER:
And the SAS objectwe have here is the
same as the SAS objectwe created all the way at the
top of our round trip, right?So if we wanted, we could
easily start this all over againand import from that
session back into Python.ISAIAH LANKHAM:
That's right, yeah.So it's a persistent connection
to a SAS kernel, whichis why, in fact, I am able
to take this object thatwas created above and reuse it
all within the same session.MATTHEW SLAUGHTER: All
right, this is section 3,""Executing SAS Procedures
with Convenience Methods.""So up until now, we've shown
you how to connect to SAS,and how to use that
connection to bring SAS datasets into Python, and
take Python data framesand take them back to
SAS's SAS data sets.In example 3.1, we're
going to show youhow to connect directly to a SAS
data set using Python syntax.So on first line, I'm going
to establish my SAS session,again, just like we did last
time using the SAS sessionobject from the SASPy module.Then, on line four, we're
going to use the SAS datamethod, which is different
from the SAS datato data frame method we showed
you in the last section.The SAS data method creates
an object of class SASPydot SAS base dot SAS
data, and that objectis essentially a file pointer
to a SAS data set on disk.So previously, we
showed you how youcould read a SAS data
set into memory in Pythonand operate on it.But this method,
the SAS data method,allows you to point Python
at a SAS data set whereit lives on disk and
operate it on SAS data setswithout importing
them to data frames.The column info
method for our fishSDS object which we've created,
the output of the SAS datamethod, executes the
SAS contents procedureon a SAS data set, and
displays the column informationfrom the contents
procedure output.Then on line nine,
the described method--again, of our SAS data object,
which we've named fish SDS--runs the means procedure and
displays summary statisticsfor our SAS data
set, similar to whatwe created in the last section,
but without importing datato Python.So all of these what we
call convenience methods areessentially Python
functions that allow youto run some SAS procedure
or snippet of SAS code,and can operate
directly onto SAS datawithout importing or exporting
anything using only a filepointer at the SAS data set.ISAIAH LANKHAM: I think
that's really cool.And I like that we can work with
the SAS data set without havingto load it into a data frame,
especially when we wantto actually use these
convenience methods to runSAS procedures without having
to directly code them ourselves.That seems really
convenient to me.MATTHEW SLAUGHTER: Yeah,
it is really convenient,because it allows you to
interact with both the SASsystem and SAS data sets
using Python syntax,and without having to explicitly
import or export your data.ISAIAH LANKHAM: Agreed, yeah.The tricky part,
though, seems to behow would I know that
column info correspondsto proc contents?How would I know that describe
corresponds to proc means?And how would I know all
the available conveniencemethods and the SAS
procedures they correspond to?MATTHEW SLAUGHTER: Yeah,
there is, of course,online documentation
where you canread through the available
convenience methods,but that's not a great answer.The output we get
from the column infomethod and the described
method do tell usthat this is the
contents procedureand this is the means procedure.But again, if you had slightly
different output settings,it might not say that.So we will see in
the next sectionthat there is a
convenient way to getPython to report to you the SAS
code that is being executed.ISAIAH LANKHAM:
And we might alsouse this chance to
plug our GitHub page,where we have all of these
notebooks that we are showing,and we actually have detailed
notes for each example,including a link to
the SASPy documentationand all the
different conveniencemethods that are available.MATTHEW SLAUGHTER:
In example 3.2,we're going to show you how
to display the SAS code whichis generated by Python,
which hopefully will helpget at the answer
to Isaiah's questionin the previous example.The teach me SAS function
included with the SAS moduleis activated by passing
it a value of true.And when activated, it causes
any convenience methodsto not execute and instead
just print the SAS code whichis generated by the method.This means that if you
don't already know SAS,you can learn SAS in part by
getting Python to show youthe SAS code it's writing.But even if you
already know SAS,you might find this
useful because you'dlike to know exactly
what Python is doing.ISAIAH LANKHAM: Which I
think is really great.One thing I'm noticing
is that true and falselook a lot like things
that we have in SAS.Do the Python versions
of true and falsealso correspond to the
values one and zero?MATTHEW SLAUGHTER: That's right.So instead of true, I could
say one, and instead of false,I could say zero.And that would work just fine.Personally, what I find trips me
up as a person coming from SASis the case
sensitivity we alludedto earlier, where
false with a lowercasef is not actually false.So if I were to, say, teach
me SAS false with a lowercasef and try to use other
convenience methods afterwards,we would find that I
would get an error,and teach me SAS would still
be turned on the next timeI tried to use a
convenience method.So it might be easier
just to use one or zero,or you can just try
to remember you needto capitalize true and false.ISAIAH LANKHAM:
Which is interesting.These are some of the few
default Python objectsthat are capitalized like that.So another thing I'm noticing
is that this constructionlooks kind of similar to some
things that we see in SAS.In particular, I'm thinking
of the ODS sandwich, whereyou turn on output to
a specific destinationand then turn it back off.So should we call this
the teach me SAS sandwich?MATTHEW SLAUGHTER: Yeah,
I think that's a good wayto explain it.It is very similar to
turning ODS on and off,or perhaps turning
an option on and off.Some of you might consider
using m print this way in SAS.Before you execute a macro, you
might turn options m print on,and then turn it off afterwards.So you can think
of this as beingeither like an ODS sandwich
or an options sandwich.ISAIAH LANKHAM: Which
I think is really cool,to sort of see that parallel
between how we do things in SASand how we can do those similar
types of things in Python.One thing I'm wondering is other
than learning some SAS syntax,is there any value to the
teach me SAS sandwich.And so I'm wondering
for example,here, we're saying that
the described method--we get the proc
means equivalent.Could I use that code somehow?MATTHEW SLAUGHTER: Absolutely.So in the previous section,
we saw the submit methodthat lets you submit arbitrary
SAS code that you write outin a quoted string from Python.So for example,
let's say you wantedto select a specific variable
out of this SAS help dot fishdata set, but the
describe methoddoes not allow for any
arguments to set the variables.Well, you could use teach me
SAS to get the code that'sbeing created by the
described method,copy it, then create
a new output objectand use the SAS
dot submit method.Submit your code
generated by teach me SAS.And then print the LST
object of your SAS output.And that way, you're
able to modifywhat is being done by the
convenience method, whichotherwise, you might not be
able to do unless it alreadyhas arguments for the
parameters you'd like to set.ISAIAH LANKHAM: Yeah,
that makes sense.So for example, if the
describe convenience methoddoesn't have a way to do
a classification variable,I could get this code and
then add a class variablelike species to the
proc means statement.MATTHEW SLAUGHTER: Absolutely,
you can do exactly that.I'm going to break this
out across lines because Ithink that'll look better.But yes, you can add
a class statementand look at things by species.ISAIAH LANKHAM: So
that's really cool.So now, I feel like
Python is teaching me SAS.MATTHEW SLAUGHTER:
Well, there you go.All right, let's move
on to example 3.3.So just like we saw in
the previous section,you could specify
data set optionswhen using the SAS data to data
frame method to import SAS datasets into panda's data frames.You can also use
SAS data set optionswhen you're using
the SAS data method.On lines one through
four here, I'vecreated a Python
dictionary namedclass underscore DS
opts, which specifies thewhere equals and keep
equals options in SASto keep the rows of the data
set where sex is equal to F,and the variables age and sex.Then when I invoke the SAS
data method on line five,I specify my dictionary
for the DS opts argument.And then, when I subsequently
use convenience methodson this SAS data object,
you can see that it onlyhas observations where
sex is equal to F.And it only has the
sex and age columns.ISAIAH LANKHAM:
And then I noticedyou're using the dot head
method that we saw before.Before though, we used
it on a data frame.Here, we're using it on
a SAS data set directly.So that's kind of
a cool equivalence.MATTHEW SLAUGHTER: Yeah,
so this is a good point.Earlier, we mentioned how
different Python moduleshave their own name spaces.In this case, head is a method
nested inside the SAS dataobject, whereas
previously, we sawthe head method nested inside
the panda's data frame object.Technically, these are
two different methods,but they've been given the
same name because they'reintended to do the same thing.So this is an example both of
how the Python namespace iswork, and also of
how you can operateon SAS data using essentially
regular Python syntax.ISAIAH LANKHAM:
That's really cool.So before, we saw also that
there was a tail method thatcorresponded to head.Is there a tail
method here as well?MATTHEW SLAUGHTER:
Well, let's try that.Sure enough, there
is a tail method.And we can see that
we we're now lookingat the last couple of
observations in the SAS dataset.Although you can see that we
are skipping some observationssince the observation number
jumps a couple of times.ISAIAH LANKHAM: And I'm
guessing that correspondsto the where filtering to
only get specific rows.MATTHEW SLAUGHTER: Exactly.ISAIAH LANKHAM: Well,
that's really cool.Yeah, so again, I think
this is a good pointto mention that there
are links to allthis documentation
in the example fileswe have on GitHub.And last but not least,
this is section 4, entitled""Staying DRY:"" Where DRY is
an acronym that stands fordon't repeat yourself.And in particular,
we're going to lookat just a single
example in depthwhere we're going to imitate
the SAS macro processor.So this should hopefully
be exciting to you,even if a bit more advanced
in some of the thingsthat we've done in the
previous three sections.So in this example, I'm
going to do something thatshould look entirely familiar.I'm going to start by using
a relative import statement,and from the SASPy module, I'm
going to import the SAS sessionobject.I'm then going to
create a SAS session,and I'm going to call
that resulting SASsession just SAS for short.And I'm going to specify that
the results that are returnedto me should be in text format.If I execute this
cell, it's goingto take a moment to
connect to the SAS kernel,and to get some output.But what you'll notice
is that the outputlooks like good old SAS output.And in particular,
that I have the resultsof calling proc means twice.So let's look at
how we got that.So on line three,
what you'll noticeis that I'm doing nothing more
than creating a string objectinside of Python.The only difference from
the previous string objectsthat we created are that I
have this special interpolationpart with a percent
sign followed by S.So this is a way of doing
what's called string templatinginside of Python.This is a bit more advanced
than the types of thingsthat we do with strings in SAS.But you can kind of
think of it like thisis like me using an
ampersand and the nameof a macro variable inside
of a string inside of SAS.The difference being
though that here,you'll notice that I'm
doing this reference,but that I'm not
actually specifyingwhat this value should
actually be anywhere.Instead, what I'm
going to do is I'mgoing to fill in a value
in place of the percent Sfarther down below in the
code when I use the SAS codestring here, and I use
the percent sign againto say that whatever value
is in this new variableDSN should be put in
place of the percent S.So in other words, I'm
going to reuse this string.I'm going to not
repeat myself by havingall of the other
parts of the stringwritten out more than once.And I'm going to fill
in values repeatedlyin place of percent S.And those values that I'm going
to put in place of percent S,as I do what's called
iteration using a for loop.It's the example of like
a do loop inside of SAS.Is I'm going to iterate and have
DSN take on first, the valuefish, and then the value class.By having DSN take on the
values in this list of stringsin sequence, and then to
execute the body of the for loopfor each value
that DSN takes on.Just like in an example
we saw previously,you can see here that the
indentation is significantbecause it's delimiting
this block of code that'sgoing to be repeated
multiple times, oncefor each iteration
of the for loop.And so this then gives
you a full exampleof how to do every
type of flow controlyou might want to
do inside of Python.First with if then branching,
like we saw before, and nowwith looping using for loops.Python, by the way,
also has other waysof looping like a while loop.But it's typically more
common to use for loopsand to always have our loop
go over a fixed length listin order to avoid any
type of infinite loops.But that's just an aside.So the idea here is that for
each iteration of the for loop,you'll notice that I'm again
using the SAS submit methodthat we've seen in
multiple examples,and what I'm going to submit to
SAS is in fact a single string.And again, that's going
to be the SAS codefragment with the
value of DSN in placeof the percent S in my string
template that I created here.And then what I'm
going to do is I'mgoing to store
the results of SASsubmit inside of this variable,
SAS submit return value.So we saw this before where I
have this intermediate variablethat stores the results
of the SAS submit call.And then I'm going to remember
that the results of SASsubmit--so in other words, the type of
the variable, SAS submit returnvalue, is a dictionary.And then I can index that
dictionary and I can get outthe value corresponding
to the key LST, whichmeans the SAS results
that are returned,as opposed to the log, which
I could get out insteadif I wanted.And in fact, you can see here
that if I change this to log,I get two SAS log instances,
each correspondingto proc means that has been
run, first on SAS help dot fish,and then on SAS help dot class.And what you should notice
is that line four, whenI defined my string template
and I say proc meansdata SAS help dot something
run, looks exactlylike what I'm getting
in the log here.Except that in place
of the percent S,I have fish, the first
value that my index variableDSN takes on.And then, I get proc
means data equalsSAS help dot class, where class
is the second value that DSNtakes on in the second
iteration of my for loop.And so basically
what I've done thenis I've written the
Python code thatallows me to create
a string templateand make repeated
calls to proc meansin order to stay dry, to not
repeat myself by writing outall of the boilerplate
parts of the proc means callover and over again.And instead, just
change the dataset that I want to apply proc
means to, just like we mightdo with the SAS macro facility.But you'll notice with
quite a lot less codeand with quite a few less
ampersands and percent signs.MATTHEW SLAUGHTER:
Yeah, and we'recalling this imitating
the SAS macro processor.But this actually
does some thingsthat would be relatively
difficult in a macro loop muchmore simply.Because we don't need
to scan across a listor create a series of
separate macro variablesto iterate across.ISAIAH LANKHAM:
Yeah, that's right.And in fact, I think this
is yet again a good timeto plug our GitHub,
where I'll mentionthat you can find a
downloadable Jupyter notebookfile of this example, along
with a complete versionof the equivalent SAS code that
you would write using the SASmacro processor to do
this exact same thing.And what you'll
notice is the codeis like three to four times
longer because it takesa lot more work to in Python--or, excuse me, it takes
a lot more work in SASto do this kind
of indexing trick,where I want to iterate
across a list of strings.MATTHEW SLAUGHTER: One thing
I'm struck by looking at thisis that if you are
learning SAS using SASPy,you could use teach me
SAS, which we showed youin the previous section, to
get the SAS code generatedby a convenience method.Edit it to do exactly what you
want with the submit method,and then you could
easily iterateon that to operate across
a series of data setsor variables using a for loop.ISAIAH LANKHAM:
Yeah, absolutely.So there's no reason
that you couldn'thave gotten the code behind the
convenience method describedin the last section of examples,
pasted that right here, putin this string
templating element,the percent S. It's always just
percent S. Just use that as is.And then when you
want to put somethingin place of percent S, just
again, use the percent sign.And then whatever string
you want to put in place,a percent S afterwards.All right, so and with that.I know that we've gone
through a lot of examples.We've covered a lot of ground.But I do want to,
again, encourageyou to go to our GitHub.Download all of
our example files.See all of the detailed
notes, and even the exercisesthat we've included
for each example, whichwill allow you to--at your own pace--explore and further
enhance your understanding.And we do want to really
take this opportunityto thank you for the opportunity
to work with you on this,and to keep in touch.In fact, on our GitHub, we
have complete instructionsfor how you can replicate
all of these examples,including how to join our Gitter
community, where you can talkwith us, ask us questions
about any examplesor anything beyond that
you would like to ask.So thank you, and we will now
move on to our closing slides.MATTHEW SLAUGHTER:
All right, thankyou so much for viewing our
virtual SAS Global Forumsession.At this time, we'd like to
issue you a call to action,encourage you to look up
our GitHub repo linkedat the bottom of the
slide, which will show youexamples that allow you to
replicate all of the exerciseswe've shown you today.Also, please feel free to
reach out to us on Gitterto ask any questions you have.And finally, we'd
like to encourageyou to please go check out all
of the other great SAS GlobalForum 2020 virtual
sessions on this channel.ISAIAH LANKHAM: Please do.And if none of that
made any sense to you,all you have to know, all
you have to remember is goto github.com/saspy-bffs,
where once again,BFFs stands for best
friends forever,because we hope that we've
convinced you that you shouldsee SAS and Python as friends
that will be inexorably linkedtogether and used each to the
best of their strengths movingforward in all of the work that
you might try to do with dataanalytics or otherwise.And in fact, we have a but wait,
there's more moment for you.Because you've been such
a great virtual audience,we do have a very brief demo.And instructions for
replicating this demo are alsoon our GitHub page, link at
the bottom of the slides--github.com/saspy-bffs--where we show you how to break
outside of the SAS UniversityEdition environment, and how to
replace JupyterLab as a Pythoninterface with PyCharm.Where PyCharm is one of
the most popular IDEs--integrated development
environments--for using Python all on
its own, all by itself.And in particular, using the
instructions on our GitHub,you'll be able to take actual
production Python code,run it inside of PyCharm,
and have Python talk to a SASkernel independent of what's
provided in SAS UniversityEdition.And this allows us to combine
the strengths of Python herefor web development-- as
we'll see in the demo in justa moment--with the strengths of
SAS for data analytics.So without further ado,
let's go ahead and transitionto our demo.So what you're seeing
now is PyCharm,which is an integrated
development environment, a.k.a.an IDE for the Python language.And at first glance,
it probably looksa lot different than
the SAS display manager.But I wonder, Matthew,
do you see anythingthat looks similar to you?MATTHEW SLAUGHTER:
Yeah, well, wedo have a code editor
window with colorbased syntax highlighting and
a browser tab on the left.So in some ways, it's
superficially fairly similarto the SAS display manager
or maybe SAS Studio.But the function of that
browser window on the leftis a little different
from the wayit would be usually
in display manager.ISAIAH LANKHAM:
Yeah, absolutely.And I would say that the main
difference is that in SAS, I'musually wanting to see
a list of data setsthat have been created
within my session.But here, I have
a bunch of filesthat I've written
that are intendedto work together to create
a Python web application.And so the emphasis
is really thenon making many files
that work together,rather than many data sets,
because Python is a much moregeneral purpose language.And in particular, we
have two files herethat we should point out.One is called app
dot py, and this fileis a Python script that
looks a bit more involvedthan any of the Python
we've seen so far.We'll come back to it though,
and talk about its contentsin detail in a bit.The other file is called SAS
config underscore personal dotpy.And this is the file
that does all the magic,because this file allows us
to have a freestanding Pythoninstallation and to
configure it to knowhow to talk to a freestanding
SAS installation.And in fact, that's what
all of this is doing here,is it's telling Python
where to look for SAS.And it's saying these are
the Java middleware piecesto use to talk to the SAS kernel
in order to submit code to itand have it give us
SAS results back.And so these two files,
along with some others,work together so that we get a
fully running web application.And so I will run that
application for you now.And you'll notice that there
is a lot going on here.This kind of looks
like the SAS login a way, where I'm
seeing that thereis a Python interpreter
being initiated,and it's running a
certain Python module.I'm seeing here the
same kind of messagewe saw in University
Edition about a SASsession being created.Here's its subprocess ID.Make sure you pay
attention to that.There will be a quiz later.And then the rest of this is
output from the flask module,which is Python--one of the many,
actually, Pythonmodules for writing
web applications.The point, though,
is that once it'sgotten all stood up and
bootstrapped and all that,I can click on this
link and it will take meto a web browser that
is connected to a webserver running on my laptop.So you might recognize
the IP address 127.0.0.01as being local host,
meaning your local machine.So again, there's a web server
running on my local machine.And so this is the way I
access it on port 8000.So this is a web
application that Icould put on a web server and
run for anyone in the worldto access.But here, the web server is
just on my local machine.And I'm accessing it
with my web browser.So what you're seeing
right now is all Python.And the first part of how this
script is intended to workis by providing a
full directory path.And so here, the directory
path that I'm going to give itis for the SAS help folder
on my Windows machine.So this is where the SAS
help data sets all live.And I'm going to click submit.So again, this is all
Python that's working here.The Python script will look
in the current directory,and it's going to find
all of the SAS data sets.And it's going to do that by
finding all the files thatend in dot SAS seven bdat,
which is the extension,the file extension
for a SAS data set.And then the way
this applicationis intended to work is
you pick one of these datasets using this
HTML form element,and to then click
one of these buttons.And it's only when
these buttons areclicked that SAS is springing
into action in the background.And so what's actually
happening at this very momentis that SAS is being told by
the Python web applicationto go and find this file,
fish dot SAS seven bdat,this SAS data set
that's living on disk,and to run proc contents on it.Similarly, I could run proc
means on the same fish dataset, so SAS help dot fish.I can also run proc freq
on SAS help dot fish.And this is called
data set explorerbecause it's a relatively quick
way of being able to quicklyexplore the data
sets in a directory,and to be able to switch
between different data setsand understand them
all in sequence.MATTHEW SLAUGHTER: Yeah, if
you're an experienced SASprogrammer, you can
probably imaginehow something similar to
this might help you automatesome of the common
data explorationtasks you have to do every day.Or alternatively,
you could adapt thisin order to make it easier
for some of your co-workerswho aren't SAS programmers
to interact with your data.ISAIAH LANKHAM:
Yeah, absolutely.And so this is just intended
to be a proof of concept.And in fact, the code for
all of this is on our GitHub,which we've talked
about several times,so github.com/saspy-bffs,
BFFs standing for best friendsforever.Because by the time you've
watched this tutorial,we really want you to see that
you can use SAS output togetherwith things like Python based
web applications in orderto get literally the
best of both worlds,and have SAS and Python be
literally best friends forever,allowing you to leverage
the strengths of bothto create something even better
than you could do with oneor the other.So let's go ahead
and talk a little bitabout the magic behind
how all of that works.So this is the application code.It's the same code that's on
GitHub, the same GitHub pagethat we were just looking at.And I'm curious,
Matthew, after havinggone through four sections
of Python examples, whatparts of this Python code
stand out to you as familiar?MATTHEW SLAUGHTER: Well,
on lines one through five,we can see a series of
relative imports, whichif you'll recall, we did
several times during our sessionto import various modules and
objects for use in a Pythonscript.There is also down below,
a couple of instanceson lines nine through 15
and then again on lines 18through 22, where we were
defining dictionaries,which you should
remember from section 1as being a list of
keys mapped to values.What are the function of
these two dictionariesin this app here?ISAIAH LANKHAM: That's
a good question, yeah.So when I'm writing
a web application,I'm always concerned
with preservingthe state of the application.So in other words,
keeping track of whathas the user clicked on,
what information have Ihad to collect and
compile togetherin order to respond to
user commands and input.And so here, the response
contents dictionaryis my way of
storing all of that.So you remember that
the application asked usfor a directory?Well, the directory will be
stored as the value associatedwith the directory key.And sorry, PyCharm is trying
to overly be helpful here.So we have the directory
key and the directory value,which currently is
an empty string.But you can imagine
that later on, Iwould want to change the value
associated with the directorykey in order to keep track
of the directory the user hasentered.Similarly, there's
the directory listkey that's going to be
starting as an empty list,but later it will be
a list of file namesthat were found
in the directory.There's a specific data
set that's been selected,and maybe there's
errors that happensalong the way,
that kind of thingthat I want to keep track of.This by the way, is
where we see some trickswith HTML, where I
want to use what'scalled an iframe to
embed SAS output, whichcomes to me as an HTML page
inside of the main page.So that's why it's iframe file
name as the name of that key.Then this dictionary is
actually a little bit different,and perhaps more interesting.Because you don't need
to play with the responsecontents dictionary
if you're going to tryto download and modify this.Instead, you're
probably going to wantto play with this dictionary,
which maps these strings thatare just arbitrary
strings that correspondto the text in the buttons
in the web applicationas the keys.And it maps those keys to
values that are actual SAS code.And its templated SAS code,
just like we saw beforewith the percent
S. So this shouldlook very similar
to section 4, whenwe looked at our example of
having Python imitate the SASmacro system.And so the idea is that
these percent S's are goingto be placeholders that
I'm going to fill in belowwith actual data set
names, and that'swhy this is a library temp lib
dot, and then a SAS data setname is going to be put
in place of the percent S.And so you can then I could
add additional SAS commands,like maybe I want to run proc
core or proc reg or somethinglike that, just by having
additional dictionary entriesin the SAS command's dictionary.And then below, I
could imagine expandingthe logic of the
application, whichis a bunch of if then else
kind of constructions, whichexpand on the example
from section 1 of if thenelse kind of things.Where based on what the
user does-- like they resetthe page, or they
submit a directory--you can see that I'm updating
the contents of the responsecontents dictionary by setting
values for specific keys.And that in the
end, what I'm doingis based upon what
the user selects.I'm then using a SAS
dot submit, just like wesaw throughout this tutorial,
and I'm using the SAS commandsdictionary.And I'm indexing it by the
command that was given.So this is the key that I'm
passing to the dictionaryin order to retrieve the value.And then I'm doing the
string substitution--just like we saw in section 4--in order to do a submission
on a specific SAS command.And really, as long as you were
to update any of this businesslogic that had to be changed
to have specific new SAScommands available to the
user, all you would have to dois update the business
logic and thenalso add those possible commands
to the SAS command dictionary.So that is sort of in a
thumbnail some of the thingsthat we've seen that
are very familiar,based upon the
tutorial contents.Do you see anything
else that maybeis out of scope from the
tutorial itself, Matthew?MATTHEW SLAUGHTER:
Well, for example,I don't think we've talked
about what's going on on line 25with the at sign.And I don't think we've shown
a lot of examples of defininga function in Python.So there are some
interesting Python features,which hopefully people
will go and learnabout on their own time
if they're interested.ISAIAH LANKHAM: Absolutely.And I think the
interesting thing to note,though, is that I've tried to
write this application in sucha way that you really
don't need to understandthe concept of decorator.So decorators are when
you have these at signs.You don't really
have to understandthe concept of
writing functions,so that's when you write this
d-e-f, for define a function.And you don't have
to understand thingslike what is a global variable
in Python, that kind of thing.You don't have to
necessarily even understandhow the flask application
framework works,where you have this way of
building web applications.But instead, you
can sort of see thisas a boilerplate, a
template for beingable to add new SAS
commands at will,and being able to have the
Python application expandto your needs.And so that has been a very,
very quick whirlwind tourof this demo.We do want to remind
you that the demo isavailable on GitHub, and that it
is intended to be aspirational.Because we don't
want to just show youtoy examples, but
instead we wantto show you how you can actually
legitimately build thingsthat you could use in the
real world for your projectsthat combine things
like SAS outputswith Python based web
application frameworks.Because we think that's a really
exciting and interesting thing.And in fact, in my
day job I actually usethis data set explorer
on a regular basisbecause it's a lot
faster than writinga bunch of proc contents
calls, personally.The other thing I want to
mention is that on our GitHub,we're currently in the process
of building a code repositoryspecifically for this tutorial.And so by the time this
video is on YouTube,this repository should be all
built out and ready for you.And as mentioned, it will have
complete instructions for youto be able to replicate all
of the examples that you saw,all of the toy examples
that we started with,as well as how to replicate
the data set explorer demo.So that if you really want to
take the PyCharm challenge,if you want to download PyCharm,
if you want to download Python,if you want to write a config
file that tells Python, here'show to connect to my
local installation of SAS.You, too, can actually
get to the point whereyou are using SAS and
Python together, literallygetting the best of both
worlds, either as a hobbyor even in your day job.So I think that's where
we'll leave it for today.We really, really
appreciate all of your timeall of your energy.And if you do want
to get a hold of us,if you ever want to talk
anything with SAS or Python.You can probably tell that we're
pretty excited about sharingall of this with you.We do want to mention
that on each of our GitHubco-repositories, we do have this
button called chat on Gitter.Gitter is a way of
talking with people,sort of instant messaging,
Slack, that kind of thing.But it's specifically where
you can't authenticateusing your GitHub credentials,
so that you don't haveto create yet another account.And we do hope that
you all after seeingthis will be encouraged
to go and GitHub accountif you don't already,
and to interactwith the incredibly vibrant open
source community that's there.And to log into
our Gitter, there'sinstructions for how to do
that also on GitHub, like youcan click this button,
or there's instructionsthat you'll find in the tutorial
repo that will be built outby the time you can see this.And let us know.Hit us up and we'd love
to talk more with you.So, thank you.And anything to add, Matthew?MATTHEW SLAUGHTER:
Nothing, except to saythank you so much
for staying with usfor this virtual session.And we hope that you
will take the challengeto try some of this
stuff in the wild.ISAIAH LANKHAM: Absolutely.We know this isn't the
same as seeing us in personat SAS Global Forum 2020.Because of current events,
we had to do this virtually.But let's keep in
touch, and we reallyhope to see you at SAS
Global Forum 2021 in person.So thank you."
88,"MIKE LEWIS: Good
afternoon, everyone.Welcome to our presentation
on forecasting health carestatistics.My name's Mike Lewis with
the Cleveland Clinic.I'm the senior director in
our Enterprise AnalyticsDepartment.And with me is Michael Bromley.MIKE BROMLEY: Yeah, my
name is Mike Bromley.I'm a senior analyst and data
scientist at Cleveland Clinic.I have worked with Mike for
the last six years here.MIKE LEWIS: So let's talk a
little bit about the businessstory here.So one of the things that
we were working on doingis trying to improve
our analytic maturity.So many of you have seen this
driving analysts maturity curvewhere we're starting at
reactive decision making.We're all really good
at this stuff, right?We've all done our ad hoc
reporting and standard reportsfor years.And now we're really trying
to drive towards that moreproactive decision making.So in order to do
that, we producea lot of corporate statistics.And when thinking about
the corporate statistics,we've used very basic models.So in order to try
to drive maturity,we decided to implement just
some forecasting techniques.And we had a lot
of different thingswe needed to address
with this project.First were some
institute challenges.And those were really
just around the end usersthemselves.How could we get everyone to
understand the informationand interpret it the right way?And then also, how
did we train the usersin their ability to interpret
and use these results?Again, for the
organization, thisis the first time that we
were kind of introducingall these new methods.And we were trying to
move forward with a wayto help the
organization understandhow we can use these new
methods going forward.So when you think about
where did we start,you're thinking about we
had a very basic model.So this yellow line was
kind of our projection.And you could see a
very flat line at top.At the bottom was
a flat line there.And those kind of represented
the week and weekends.And you could see the
actuals in the blue kindof how the model would
perform against that.So we'd have peaks and valleys.So we were always
running into issues of,why did we miss the projection?Or why did we come in
under the projection?So when you think about
where we needed to move,it was really getting
back to thingsaround what was
our ultimate goal.We had situations
where we couldn'taccount for variables in
conferences and thingslike that.So what we did was we introduced
a financial statisticsdashboard.And the goal was
really to figure outhow could we replace our current
projections with some timeseries models.And we wanted to
develop somethingsimple and streamlined so that
the end users could forecast--take the forecasted results,
consume them, and then improvetheir operational statistics.So again, I'm going to turn
it over to Mike Bromley.He's going to talk a little bit
about the featured engineeringand model development
of this process.Mike?MIKE BROMLEY: All right.Thanks, Mike.So I'm going to kind of talk
through the whole sort of lifecycle of the model build.I'm going to start with a pretty
generic slide on the modelbuilding process.So it's going to
seem very linear whenI step through these slides,
like we did step 1 through 10and then we were done.But it's not.I'm sure most of the
people watching thisknow it's really not
like that in reality.We went around this
circle 100 times maybe.I mean, we kind of restarted
and rethought of ideasand realized they were
terrible and then started overand revalidated.So just kind to
keep this in mindas I step through because it's
not as linear of a processas it may seem in here.So we know that we're
working on forecasts, right?But what are we
actually forecasting?We talked about
corporate statistics.It's what we call
kind of key metricshere at the Cleveland Clinic.But really it's those
six kind of metricslisted on the top left, key
health care volumes thatessentially drive the business.So what we're going to
need is daily outputfor at least 12 weeks.If we can predict 10
years, they would love it,but let's kind of
keep it realistic.So we're going 12 weeks for
those different metrics.And when you start to think
about what do we actuallyneed to understand
to make a decision,maybe there's a
decision that we couldmake with high-level surgery.From an organizational,
enterprise standpoint,maybe we can use
that for something.But what we really need to know
is where are they happening?What facilities?What institutes?What departments?What subsets?That's kind of what this
decision tree-looking thingat the right is.It's an example of these are
the things we need to understandand get out of a forecast.But if we start to split
levels that detailed,we get to that 7,500
number, which is, OK, wehave 7,500 forecasts.Wow.That's a lot.I mean, it is certainly
possible to takesome of the commercial tools
out there and throw data at itand have it spit out
a bunch of forecasts,but you start to lose a
little bit of that signalat the bottom levels, demand--you know, your volume
is sort of intermittent,and it's a lot harder to build
and pay personal attentionto that many model levels.So what we're going to do
is kind of a combination.We're going to use SAS
Forecast Studio, which is--it's one of those
tools I mentionedwe can take a bunch of
data, throw it at it,and have it spit out models.But we're also going to
apply Monte Carlo simulationtechniques to kind of get
us where we need to go.I'll talk a little bit
more about the simulationin a later slide here.So we talked about
model structure.We know we don't want
to build 7,500 models.So this is kind of where
we ended up, a little bitof a different
structure dependingon the statistic or the metrics.So some of them
were by institute,some by a combination of
institute and facilityand sub statistic, and so on.So this kind of
gets us to I thinkit was 498 models,
so about 500 models.And any level that isn't going
to be output from this modelstructure will be generated
in the simulation,in the sampling that I'll
talk about in a little bit.So we kind of
understand, OK, thisis what our model
structure is going to be.But, you know, what
about the data?We don't have data, and I
don't know many people thathave data up to this minute.In our data, we have a lag.So we talked our data
stewards and came upwith this table on the left--here if I go back,
I go back one tic--all right, this table on
the left and started there.So we pulled data in, and we've
built some forecast models.And it kind of worked
for some stats.It didn't work so
well for others.And so we started to
investigate, OK, going forwardnow, what's the true lag?Like, where does
our data truly startto be reflective of actuals?How far back do we have to go?And so those numbers
on the right sideis kind of where we ended up,
increasing lag for admissions,discharges, and some
of the different statsto account for coding.So we may know volume
three days ago,but we don't know
which institutesand things like that.So coding took a little
bit longer to complete.And things like ED and
observation, observationwas OK, but out-patient
E&M needed a little bitmore adjustment that I'll
kind of talk about now.So even with a
longer lag, this kindof an example of it
already showing up now,we don't really
have complete data.So what we started to do, we
wrote a program to track, OK,data that's 13 days
old, how does itchange as it gets
older and older,as we move further in time?And we say, OK, it's
13 days old, nowhow does it look when
it's 20 days old?How does it look when
it's 100 days old?We start to see
this sort of shiftup as data is actually
completed, until we finallyget-- oops, I went one too far.So we see this shift up.And this is kind of what
we started to notice.And we don't want
to put data thatrepresented by the blue
line into a forecast modelbecause we're really
going to project low.It's going to look like
we're trending down.So we need to somehow
get this yellow lineto be reflected in our actuals.So there's two kind of
ways we could potentiallydeal with that, right?We could increase
the lag period.So we would need to
realistically change itto almost 40 days.So essentially our
first projectionfor true forward
projecting data wouldbe the 41st day in the model.I don't know how many people
use the weather forecast for 41days from now, but I don't
think too many people actuallybelieve what they see.So you can probably
guess we didn't reallygo with that method.It is an option, but
we didn't go with it.And the second option was can we
track enough of this phenomenonto actually adjust the
data up before we feed itinto the model?So that's definitely
what we ended up doing.And what you ended up seeing
was this kind of shiftof as we applied this
change to the blue lineon the bottom right,
we pushed it up to--it was almost always up.Actually there was a few cases
where we adjusted it down.But we ended up
adjusting data sothat when we feed
into the model,we get a more
accurate projection.So that's what it
will end up lookinglike when we shifted it.OK.So we've kind of talked
through some of the componentsthat we needed to finally
get this model in.And at this point,
like I said, wewent around that circle a lot.We talked about the model
structure and data lag.We need to figure out
what's the time frame.We talked about structure.We need to now talk about
independent variables and someof their adjustments, where
we've kind of adjustedour dependent variable, but
we still have to think, OK,what kind of
independent variablescan we use to improve from
that sort of flat line model?Can we put things in
there like holidays?Can we put schedules and
things like that in there?So this is actually
the final listof some of the
important variables.The list is a lot
longer than this,but these are really the
ones that are most impactful.So some of your standard
variables in there,like a flag for weekends,
flags for holidays.We had some holiday waiting,
depending on the effect that--how they had on volume
before and after,and things like that.But the major ones here
are those last three.I call these the
schedule variables.So we have actual
surgeries scheduled.We have what's
called patient slots,which is representative of
a time slot on a physiciancalendar, where they are
available to see a patient.So this really drives
out patient E&M volume.And then PTO.So I don't know if
that's really a schedule,but it's when physicians
go in and say, OK, Iplan on being out
at a conferenceor whatever in two weeks.That definitely has an effect on
surgeries and admission volume.So we're going to talk a little
bit about those three variablesnow.So how can we actually
leverage these?When you throw the surgery
schedule into a surgical model,for example, it's great.Like your model
stats are awesome.Your r-squared is at 0.99.Your MAE is way down.And you think, great.My model's going to be amazing.But then you kind of
see this orange linebecause the future
schedule is not populated.And so your model
stats may be amazing,but you actually get
this, unless you reallythink your volume is going to do
that, which could be a problem.You're probably thinking,
yeah, this isn't right.So we need to do something here.If we're actually
going to use this,we have to think
about adjusting,kind of like our dependent
variable earlier.So again, a couple of
different solutions.We could take the schedule
and forecast it, and thenuse it as a learned feature
in our original model.Or we can do something
similar to whatwe did with our
dependent variable, whichis track the shift, so track
how does the schedule changethe closer we get to
it being tomorrow.So we actually kind of
do both of these things.We have a combination of using
actual available patientsslots along with a projection.And then for the physician
PTO and surgical schedule,we do a little bit
of an adjustment justlike our dependent variable.OK.So now we've kind of applied
a similar adjustment.We've adjusted our dependent.We've adjusted our
independent variables.We kind of go around on that
circle a whole bunch of times.And now we're finally at
this point where we're like,OK, what are the actual
models we're goingto use to get our projection?And it's kind of--we use that forecast to throw
a bunch into the blender.I guess this is a funnel.And out comes a champion model.So for us, it's
going to be modelsthat have very correlated
independent variables,so schedule for surgery,
position PTO for admissions,and slots for
established, or new,or consultation,
E&M home visits.Those are pretty much all going
to be ARIMA or ARIMAX models.Models that really don't have
a great independent variableto inform them but
have a lot of volume,for example,
emergency department,there's a lot of randomness
associated with that,those are going to fit best
to ESM, Exponential SmoothingModel.And then that last one
in the funnel there,those Intermittent Demand Models
will be pretty exclusively usedfor those lower levels
or those model levelswhere there's very little volume
and kind of sporadic demandcoming in week to week.And all of these are kind of
thrown at Forecast Studio.And based on the holdout
MAE, which is 150 days,a champion model is selected.So not a lot to talk
about on this slide,just thought it was pretty
important to put out sortof the high-level model stats.Like I said, models that have
a schedule variable in themhave really nice model
stats, but you can't alwaystrust them since no matter
what, the models stats aregoing to be amazing
and the forecast mightbe completely awful.Luckily, when we apply the
adjustments, the surgeries,and outpatient E&M,
the things likethat, still has a pretty nice
projection to go with it.So real quick, kind of talk
about the simulation portions,and I mentioned this
a few times earlier.And this is kind
of our techniquefor getting to
those model levelsthat we don't have--
actually, I'm sorry,getting to the
output levels that wedon't have a model level for.And so, for example, if we
have a surgical forecastby a facility, so by
hospital, but we reallyneed to understand where's
the volume by instituteand department, what we can
do is get the facility outputfrom the model.And we can then
use that forecastas the mean of a
normal distribution,and then generate some random
sample sizes and samplefrom our actual historical
data to generateall the level of detail that
exist in reality, so whatdepartments is the volume going
to be in, what institutes,different things like that.So the big advantage
for this is wecan build less
models, which is--it's more manageable to
manage 498 models than 7,500.But we can also change the--we get the levels of detail
we need in the output,but we can also
change either or.So we can leave
the models exactlyas they are if we think
they're fit perfectly,but we can change
around the outputwe get from the
simulation, or vice versa.We can leave the output the
same but change our model levelswithout screwing up
the whole process.So that's kind of
the flexibilitythat we get from
using this technique.And kind of just a fun
fact, every time we refresh,we do about 4 million
samples with this technique.OK, so I think that kind
of covers the model build.And now, let's turn back
over to Mike Lewis, who'sgoing to close it out with
some of the output and userengagement slides.Mike?Thanks, Mike.MIKE LEWIS: Thanks, Mike.So, you know, again,
the big key hereis really, how do we make
this easier to consume?And we try to follow
these principles.We don't really ever want to
try to show the complexityor show the math.It's really about how do we
make it easier to consume itintegrated in their workflow.So when you think about that,
we integrate this informationinto their workflow,
and then it'sreally about keeping
end user engagement.So we have weekly
discussions with our teamson this, where we talk
about the model build,and as we talk about
accuracy, we ask themfor things that are going
on in their workflowto help us understand are there
other variables we can do.So this user feedback and user
engagement is a critical stepas we move forward.And when you really
think about it,we're ultimately asking them
to change the discussion.It's how do we change
the discussion?And when you think about the
forecast, that's ultimatelyaround, can we make a decision?So it's not, is the
forecast right or wrong?It's looking at
the forecast, it'slooking at the
projections, and saying,is this accurate
enough to make youchange your decision knowing
this kind of information?So again, we also continue
to do the long termmodel tracking that we help
the end users understand.So we track all
this stuff over timeso we can understand
model degradation.And finally, is this
ready for production?I mean, it's ultimately
that final slide that you'reseeing here, which is, you know,
we've done all these steps,we deployed it, and we are
engaged with our end user.So again, Mike and
I want to thank youfor joining our presentation
on forecasting statistics.And we hope you enjoyed
our presentation.Thank you."
89,"RICK SHOOMAN:
Welcome, everybody,to today's session on fighting
financial crime using SAS.My name is Rick Shooman.I'm a Charlotte-based
managing director at EY.I have more than 20
years of experiencein the technology
area and mostlyaround financial services,
including financial crimescompliance with risk,
fraud investigations,and digital delivery.Before EY, I previously worked
at one of the US's largestbanks, running their
global technologyarea focused on global
financial crimes compliance.And currently, I'm leading
the financial crime systemsintegration practice
here at EY including,our alliance
relationship with SAS.As part of this practice,
we are leading effortsboth at regional
and global banksto perform vendor selection,
system integration,independent testing
of KYC, whichis Know Your Customer,
transaction monitoring,and sanction screening.Next, I would like to
introduce my colleague, Andre.ANDRE CAVALEIRO: Hi.Thanks, Rick.So my name is Andre Caveleiro.I'm a senior manager on the
EY regulatory complianceteam on the technology
side based out of New York.So a major part
of my entire yearwas dedicate to financial
crimes prevention mostlyon the tax side but also acting
as a liaison between businessand technology.So from development
through optimization,I'm always looking for ways
to enhance the detectiontechniques, improve efficiency,
and also reducing costs.So I cover basically all the
financial service sectorsand major geographies.I'm here today to share along
with Rick our experienceon this area using
the SAS platform.So thank you, again, and
let's get started, Rick.RICK SHOOMAN: Thanks, Andre.So let's just kind of start off
with why the financial servicesindustry cares about
financial crimes.And when we talk about
financial services industry,we're talking about banks,
wealth and asset management,broker dealers, insurance
companies, and evenmoney service brokers--anywhere mostly where
money movement is involved.So why do they care, right?So some financial crimes
affect the financial servicesthemselves.Think about fraud
and money loss.But some affect the
society as a whole.They can affect the
economy, national security,human rights with trafficking.And really, all
of this is coveredthrough when we talk
about financial crimesat our financial
services organizations.Now banks alone, if they only
care about fraud and moneyloss, why are they
really interested?And the reason is the
Patriot Act and, after 9/11,the Bank Secrecy Act.The regulators mandate
that financial servicesorganizations need to
keep pace with the changesof criminal activity
and help themin the fight against
illicit behavior.And it's an enormous problem.Almost $110 billion
annually is estimatedin terms of tax evasion.In the UK, they estimated
that every 15 seconds,a financial crime is committed.And in the US, it's most
likely higher than that.We're talking about $2
trillion in illicit fundsin circulation.And what does that
mean to the banks?Well, you know, if
they don't comply,then they can face
large regulatory fines.There's been $12.4 billion
in the seven largest fines.That's only seven
of the largest.There's been plenty more.The average financial
crime operations budgetis in excess of $1 billion
for the largest global banks.And they say this is
about a 70% increasesince the last financial
crisis around 2010.So with that said,
let's go to Andre,and let's talk about
what we actuallymean by different types
of financial crime.ANDRE CAVALEIRO: So thanks
for the background, Rick.Now I'd like to provide you
an idea about a different typeof financial crimes, the
detection and controlsto prevent them,
and the challengesfaced by institutions on
implementing and operatingthese controls.Starting from the
top on my slide,we have some examples
that can be also expandedconsidering there are
light variations for eachof these items.But detect and stop
any sort of crimeis a priority for all of us.And some controls operate
more on the real time mode,and others depend
more on informationavailable for detection.So here we have
some good examplesthat can serve as a base for
most of the fin crimes typesand also as a reference to
discuss the challenge relatedto the detection and
the monitoring controls.So for example, fraud,
transaction screening,market abuse, and
insider tradingcan be more on the
real time side as wherelike money laundering, bribery,
and corruption and tax fraudmore on the post-event side.But there are also situations
that can be somewherein the middle, like electronic
crimes, terrorist finance,and others.So to mention some challenges--
and we'll cover in detail someof these items in
the next slides--I would consider,
number one, data volume.So it's growing fast with
the new products and servicespopping up day after day.Digital banking and
open banking can be justlike a couple of examples.Another topic would be like a
complexity of global bankingnetworks as well cross-border
risks so as a resultof the global
economy, consideringthe international presence of
institutions and connectionsin addition also to the
dynamics of the geopolitics.So Rick will mention and cover
specifics about these kindof controls in a bit.Older topic will be inefficient
frameworks and processes.This can have like
a significant impacton productivity and costs.And not to repeat,
but the increasingregulatory requirements are
also an important factor.So challenge will
be always present,but technology, methodology,
and techniques are here to help.So I'll hand over
back to Rick so hecan talk more about trade
base and money laundering.Rick, back to you.RICK SHOOMAN: So
let's take a lookon how SAS helps in the fight
against financial crime.Let's take an example
around trade finance.When we talk about
trade finance, whatwe are talking about
is the movementof goods cross-border-- a trade
between companies, shipping,trucking, anytime that
we're moving goodsfrom one place to the other.And that's ripe
for financial crimebecause there's a large
dollar amounts, with about $17trillion in trade every year.And the way that the
illicit behavior happensis either around money
laundering-- thinkabout moving goods that don't
match the line of businessor there's an incorrect
inventory of goods.Think about shipping
a empty containerfrom one location to
the other, moving goodsthrough different geographies
to hide the origin thatmay be a sanctioned
country like North Korea,or even fraud, when a number of
excessive parties are involved,that they're trying to hide
the origin of the funds.And it may be looking
at incorrectlyshowing the goods
in order to tryto defraud someone of finances.So how does SAS
specifically help?And it's really
around the analytics.Trade finance involves a
number of different data sets,and Swift is one of the
main transaction systemsthat we're talking about.So you see a transaction.But some of the fields
within the Swift messagesthemselves are
unstructured data.A lot of information
about the bill of goodsis included in the
messaging, free text fields.We've got core
KYC systems, whichtalks about the risk of the
different entities involved.Letter of credit-- this
is between the exporterand importer.You think about this as
a guarantee of delivery.The exporter is selling goods.An importer doesn't want
to pay until they receive.Banks provide letters
of credit to providesurety of delivery and payment.You have the
payments themselves.You've got all the different
types of documentationand, like I mentioned,
bill of goods.What is the documents of
what's in that shipment?And there's other
third party data.And this is typically
done manuallyin terms of trade finance.If you want to look
for illicit behavior,someone needs to collect all
these different data points,whether they be structured
or unstructured,and make a decision as to
whether a suspicious activityis anticipated or viewed
So what we've doneis we've partnered with SAS.We've created an
application called Track.And it's kind of used as
explainable artificialintelligence.So it combines with
the risk policyto come up with 206 risk and
compliance checks in a way thatmeets regulatory standards.It is a proven methodology
that the regulators like.It matches with a bank's risk
governance defensibility.And really, what it uses in the
middle is this analytics layerprovided by SAS.There is structured
business rules.There's machine learning to
learn from previous decisionsas to whether information is
indicating a suspicion basedon a risk policy, contextual
analytics to look at documentsand see whether terms
used in the documentsindicate that again, there's
movement of goods thatare either illicit, may be
related to military and defenseuse but other things that might
throw up red flags, and thena network analysis to connect
individuals to show whatis the relationship and indicate
if there's rings of activityor goods are starting to become
sourced by similar means thatwould indicate that there's,
again, illicit activityhappening.So with this, you're covering
a lot of different areasin trade finance prime really
around money laundering.And as I mentioned
before, a good exampleis shipping an empty
container but payinglots of money for that.And so the money is
movement, but really, there'sno goods exchanged.Fraud, military dual use,
economic sanctions-- again,hiding the origin of the goods.And you're looking at
the network analysisto see if you can determine
whether it includes sanctionedcountry, again with boycotts,
which is similar to sanctionsand should-- you know, is there
a boycotted area that shouldnot be allowing
this trade to occur,and then meeting local
export controls as well.And the outcome of this, if
it comes up with a transactionrisk, looking at all of
the data that's coming in,using the analytics
to create a score,applying it to
their risk policy,and looking to make
automated decisions.As you can imagine, and Andre
pointed this out earlier,the amount of transaction volume
happening both in the industryand in terms of trade combined
with the number of increasedregulations that are
coming in at local, state,and national levels
means that there'sa lot of decisions that
need to be made basedon all of those risk areas.And the tool is really meant
to expedite the process.SAS greatly increases the
productivity and the efficiencyof a firm's financial services
organizations and their riskand compliance areas.So let's go on to another
example that uses SASto help fight financial crime.ANDRE CAVALEIRO:
Thank you, Rick.So as mentioned on the previous
slide, in previous slides,we have a bunch
of challenges thatwere like presented
from regulatory pressureto operational aspects
and also the complexityof the listed schemas that we
are always trying to detect.And the trade finance
is a good example--very, very, very complex, right?So there is one element that
is always in the center that wecannot ignore, which is data.So I could spend the
entire day talkingabout different aspects of
data and multiple approachesand solutions.But today I'll focus on this
specific slide on one approach.It's called the
entity resolution.So entity resolution
basically dealswith duplicate entities,
internal or external entities.So this can affect not only
the detection process but alsothe investigation of the
case and reporting results,impacting not only the
efficiency but alsoproductivity and costs.So without solving
entities, we maybe losing the ability to
identify potential suspectsituations.And you also won't be able
to provide a holistic viewfor the investigators that
are responsible to analyzeevery single case
that is detectedby the controls and the systems.So this issue can be caused
by simple, like, dataquality issues--I mean, caused by data
collection or transformation.But it can also be part of
a fraudulent schema, wherethe bad actors are
trying to obfuscatethe connections between
parties and transactions.So the entity resolution
approach or solutionconsists in a number of
techniques and algorithmsto identify similarities
that can resultinto a single entity or
a group of entity relatedor related parties.So the steps of this process
would cover not only the datastandardization and
enhancement but alsothe matching and variety of
methods to identify connectionsand similarities--so direct comparisons using
like fuzzy logic, phonetics,and others.So the resolution of
entities add not only qualityto the process but
can also use to detectother type of behaviors
based on simple data or datacontrols are not able to--we are not able to identify.So as you can see, this
is one of the solutionsthat we work with SAS based
around the SAS platform thatcan help the financial
institutions notonly to enhance the
quality and the processbut also to identify other
possible suspicious activitiesthat are not identified by
traditional AML platformsbut also trends, meaning
identify ahead of timewhere are the
suspicious activities.So back to you, Rick.OK.So how does SAS fit
in this picture?So EY and SAS are working
together, as Rick mentioned,developing solutions or
implementing the SAS platformto deliver service to help
the financial institutionsaddressing not only the
issues that we coverin the previous slides
but also on the endto end financial crimes
detection and provisionprocess.So by one hand, we can
have the SAS platformthat covers all the elements--I mean, from fraud,
anti-money laundering,customer due diligence.But on the other
hand, we can alsouse the different
modules to leverageand show specific issues.I mean, it can be used during
like an implementation,optimization, or even
to address reallyspecific problems like the
Visual Analytics module,the model risk manager,
and FCA recentlylaunched by SAS that,
for example, can increaselink activities, related
data integration,the analytics itself--for example, segmentation,
parametrization, scoringand others--or also to validate
existing modelsand identify points
for improvement--for example, in a model
validation approach.So this is like overview
of everything that you haveand everything
that we are doing.I'll hand it back to Rick
for the closing notes.RICK SHOOMAN: Thank you, Andre.As you can see, we're using
SAS to fight financial crime.And it really is vital to
provide the financial servicesorganizations a tool that they--
and a set of tools that theycan use that are both
reliable, that areexpedient, that are real time.So thank you for
your time today.Hopefully you've
been able to geta sense of what it's like in
the world of financial crimescompliance and how financial
services are fightingfinancial crime to
protect our economy,to protect our national
interests, protect themselves,and how SAS is playing a big
part in leading the industry.So thank you for
your time today.We have our contact information
shown if you have any questionsor follow up that
you'd like to do.Feel free to contact
Andre and myself,and thank you for your time."
90,"Hello.I'd like to welcome you to this
virtual session for SAS GlobalForum 2020, modernize your SAS
Visual Analytics environment.My name is Gregor
Herrmann, and I'mworking for the global
technology practice.Let's first have a look on
what I have prepared for you.First of all, we want to have a
look at how to promote contentfrom a SAS 9 system--a Visual Analytic 7.x
release probably--to SAS Visual Analytics
8.5 on SAS Viya.In the second part, we
are having a quick lookon how the promotion from
within one system on SAS Viyato another system on SAS
Viya would look like,and the third part
covers the promotionof other objects from SAS 9
and a small outlook on what R&Dis currently working for.Let's get started.Before you promote content from
SAS 9 to a SAS Viya system,you should consider
the following topics.First of all, check your
hardware and your softwarerequirements.Of course, it is possible
to use the same softwareto run your SAS Viya environment
than you are currently usingfor your SAS 9 environment.There are no major changes
in hardware requirements.From a software perspective,
you can run SAS Viya systemson the Linux platform or
on the Windows platform.With regard to viewing
Visual Analyticsreports on the browser, there
is a significant change.As you might already know,
we are moving from the flashtechnology to HTML5.This has some consequences.One consequence, for example,
is that we are no longersupporting Internet Explorer
11 to view Visual Analyticsreports.The reason for that is that
Internet Explorer 11 has notbeen designed to run
HTML5 applications.So make sure you have one of
the supported browsers availablelike Google Chrome, or
Microsoft Edge, or Firefox,or Safari if you're
running on a Mac.If you have a significant
or large environmentto convert from Visual Analytics
7 to Visual Analytics 8,you might consider running two
versions of Visual Analyticsin parallel for a limited time.This gives you, of course, more
time to move the content over,and allows you to do
it in smaller chunks.For example, you are
converting some of the reportsto the new version,
and then you havetime to check if everything
is working as expected, wherethe majority of your reports
are still running on your VisualAnalytics SAS 9 environment.There is also something
to consider with regardsto viewing reports.If your information consumers
are already using a Windows 10laptop or desktop,
then you shouldconsider using the
Visual AnalyticsApp for report viewing as well.I strongly recommend that
you have a look at it,because it allows the
information consumers to viewthe report or to get
their results morequickly because of the fact that
this Visual Analytics App doesnot have the need to download
the HTML5 components before itrenders the report.This results in significantly
reduced first timeto view the report--time needed to view the
report for the first time.One more thing that
is not on this slidehere that I should mention--if you move reports from
Visual Analytic 7.x to VisualAnalytics 8.5, also
consider the capability gapof the two versions.It might be more efficient to
create some report from scratchin the new system,
because we have provideda large amount of
new capabilitiesin recent releases in
Visual Analytics 8.So be aware of the capability
gap between the two versions.This might have consequences
for your reports as well.First of all, you should
look at your inventory, whatyou have currently in stock.What content types do
you have to convert?Are there reports, explorations?Are you using custom themes?Do you use data preparation
jobs or custom graphsin your report?What's also important is knowing
the size of your inventory.So are we talking
about 15 reportsor are we talking about
several hundred reports?You can also compare
different environments--for example, a dev environment
versus a prod environment.R&D has developed a
tool in recent yearsto support you in
assessing your content.Let's have a quick
look at screenshot.The name of the tool is the
SAS Content Assessment Tool.What it actually does--it runs a job in your
existing SAS 9 environmentand gathers all the necessary
metadata informationto provide as a result
a very detailed reportof your inventory.We are currently looking at
the cube and info map detailsinventory.It shows you that your current
system-- or the system that weare currently looking at has
more than 1,000 informationmaps, some 115 OLAP
cubes, and a large amountof relational expressions
and OLAP measures.There are much more
details in this report.Of course, this is only
a screenshot of one part.You would also see, of course,
Visual Analytics reportsand what's being used within
those reports as well.I strongly recommend that you
get in contact with your SASrepresentative if you
are interested in runningthis content assessment
tool in your environmentto get a better view of the--
of how you can move to SAS Viya.There are some things
to consider as well.There are objects that
are not being promoted.First and foremost, I
have to mention the data.So data has to be loaded
to the target systembefore you do the promotion job.This is, of course,
not very difficult,but you have to be aware of--there are other things that have
to be done in advance as well.For example, if your data
are using custom formats,you'll have to load them
to the Viya system as well.You might have been
using custom themesto customize the layout of your
reports in Visual Analytics 7.These custom themes have been
created in the flash world.Flash is no longer available in
Visual Analytics on SAS Viya,so you have to
recreate your customthemes with the new HTML5
Theme Builder on SAS Viya.You can do that also before you
move the reports to the targetsystem.Two more objects that are not
part of the promotion processare Visual Data Builder jobs
and Visual Statistic projects.Visual Statistics
projects especially,they are coming from
a very early releaseof Visual Analytics.You can first promote them
to explorations of the VisualAnalytics 7 release, and then
they are eligible for promotionto Visual Analytics
on SAS Viya as well.How does the execution of the
promotion actually looks like?So if you're promoting
from a SAS 9 system,you first have to create
your export package--namely, the SPK file either
in SAS Management Consoleor via the batch export utility.There are some
changes with regardsto authentication in SAS Viya.Therefore, there is
something that youhave to consider before
you export the packages.If you want your authorization
to be part of the promotionas well, you have to make sure
that, on the new system, allthe users and all the
groups that you are usingto define authorization
settings for specific objects,they must already be present
on the target system.This is something
that you can checkby create mapping
files for-- or itmight be create export
packages for users,and then check on
the target systemif all those users and
groups are already present.Once you have create
your export package,your SPK file, you can
open the package filein the import window of
SAS Environment Manager.your target files do not
have exactly the same names,for example, or your
CAS libraries havedifferent names on
the target systemthan the original SAS
libraries or LASR librarieshad on the source
system, then youmight have to adjust
the mappings, if needed.You can also import the package
using a so-called command lineinterface on SAS Viya.During the import, when you
do it in Environment Manager,you can also decide to
save a mapping file that--for later usage.You can then use
this mapping filewhen you do a batch import for
more reports or other packages.If you are promoting
within SAS Viya,it looks pretty
similar, except the factthat we no longer use SPK
files as the package format.We are now creating .JSON files.These can be created in
SAS Environment Manageror via batch export.I should mention that, if you
are-- let's say, if you'reupdating your SAS Viya
environment in place,of course, there is no
promotion necessary.But maybe you do a new
installation on another machineor you have done
a backup and youwant to do a clean
install of a new version.Then you have to create your
content or you have to exportyour content in .JSON file.That content on the new
system can be imported, again,using the same mechanisms--
either SAS Environment Manageror the command line interface.In this case, mapping
files might notbe needed because you might
have exactly the same structureon the target system that you
had already on your sourcesystem in SAS Viya, but
you can neverthelessuse them to modify target
data sources, for example,if necessary.Let's have a look at
promotion of other objects.Lots of our customers are still
using SAS Enterprise BI Server.One important object
that is being used in--for example, in
Web Report Studio,reports within such enterprise
BI Server are OLAP cubes.OLAP cubes are a
multi-dimensional datastructure that
allows you accessingdata on different
hierarchy levelswithout recalculating
everything on the fly.OLAP cubes are no longer
available within SAS Viya.Nevertheless, we are
able to visualizein cross-tabulations
similar reportsthat we had in Work Report
Studio now in Visual Analytics,because Visual
Analytics also knowsthe concept of hierarchies,
and the computing powerof the in-memory
technology allowsus to calculate those different
hierarchies and the numbersrelated to them on the fly.If you are still using
those OLAP cubes in SAS 9,you might be interested
in converting them,because it's now possible.OLAP cubes will be converted
to Visual Analytics data views.Those cubes can be either
based on detail tables or starschemas.The initial support
is restrictedto 11 most popular
MDX functions,and more are being discussed.R&D has looked at a couple
of big customer environmentsto decide which MDX
functions are favorites,or are most being used
at our customer sites.Again, the SAS
Content Assessmentcan support you in
checking the eligibilityof the cubes for
promotion, because you'llget a detailed report on which
MDX functions are being used.This is a screenshot
of the import--the first import
step, when you'reimporting a package that
contains an OLAP cube.You currently see that
there is-- this is the cube,and we have a couple of tables.In this case, we are
importing an OLAP cubethat's based on the
star schema, whichhas an ORDER_FACT table and
a couple of dimension tables.Another object that
has been used in SAS 9are SAS Information Maps.SAS Information
Maps in SAS 9 havebeen representing
the business metadatalayer for report authors.R&D has also looked at
converting SAS InformationMaps.The development
work is currentlyrestricted to relational
information maps,meaning information
maps whose target--or whose data structure behind
it are relational tables.Information Maps
will be convertedto SAS Jobs in SAS Viya.After you have imported the
information, map the SAS Jobneeds to run.It creates a CAS table and a
Visual Analytics Data View.So the Visual Analytics
Data View especiallycontains, for example,
the calculationsthat are being made within
the Information Map.Another important object to
look at are stored processes.Stored processes are no
longer available in SAS Viya,but there is something that
can replace the functionof a stored process.Stored processes are being
converted to jobs in the SASJob Execution Framework.This is already available
in the current release.It can be executed using the SAS
Job Execution Web Application.Currently, they are not
out-of-the-box usable withinVisual Analytics, but
development is underway to makethis capability available.It will be available in the
SAS Visual Analytics releaselater this year.So let's do a short summary.First of all, make
sure you have completedall your preliminary tasks.Secondly, review your
inventory and do itwith a focus on redesign.I'm strongly
recommending this, and itrequires a good knowledge
of the capabilitiesof the most current version
of SAS Visual Analytics.Of course, this also
depends on the fact--or on the version
of Visual Analyticsthat you are currently
using, but if youare using an earlier version--the earlier version you use, the
bigger the capability gap is.Last not least, if you are
still running SAS Enterprise BIServer, check out the new
promotion capabilities,meaning promotion of OLAP cubes,
promotion of Information Maps.R&D will not stop working on
those promotion capabilities.They are also currently looking
into converting the first WebReport Studio reports
into SAS Visual Analytics,so stay tuned.And think about replacing
SAS Enterprise BIServer with Visual Analytics.That's all from my side.If you want to contact
me, write me an email.Thank you."
91,"STEVEN MILLS: Hello.Thank you for
tuning into my talk.I hope everyone is well.My name is Steven Mills.And I am a research
statistician developerwith the SAS Institute.Today, I wanted to talk
to you a little bitabout neural
network-based strategiesin SAS Viya for forecasting
with time series.So machine learning models
like neural networkshave often been dismissed
as potential modelsin forecasting.There's a couple of
reasons for this.One is that they're not
very good at learningother regressive features
common in time serieslike seasonality or trend.And the second is that they
require quite a bit more datathan classical models.However, the M4 forecasting
competition resultswere dominated by ensembles
of machine learningand statistical methods.In fact, the first and
second place contestantsincluded machine
learning techniques.And nearly 3/4 of
the best resultswere ensembles of
different models.An ensemble of models
often generalizesbetter to new data
than a single modelbecause different models
are better at identifyingdifferent features in the data.And additionally, noise or
error in any of the modelsis less impactful when it's
averaged with other forecasts.From this point of
view, it makes senseto use diverse models when
we are creating ensembles.Neural networks can make
valuable contributionsto these ensembles because
they're very good at modelingnon-linear behavior.And they are also very
good at identifyingcomplex interactions
between variables.With that in mind, I'd like to
talk about the neural networkmodeling strategies
that are designedfor panels of time series
in SAS Visual Forecasting.We designed three
modeling strategiesthat include neural networks.They are the panel series neural
network, the stacked model,and the multi-stage model.To show you how to use each
of the modeling strategieseffectively, I'm
first going to explainhow the data is preprocessed
differently from classical timeseries applications.This is to overcome
some of the shortcomingsof traditional neural networks.Then I'll give you
a brief overviewof each of the three modeling
strategies available in VisualForecasting.Next, I'll show you a case study
comparing the neural networkbase and the classical
forecasting methods.And finally, I'll cover a
few tips and common problemsthat you might encounter.One big difference between
machine learning and timeseries models is that
machine learning algorithmstypically don't care about
the order of observationsand will partition
the data randomly.Since time series
are ordered data,and the more recent observations
have more predictive power,ordered sampling makes
more sense instead.So when we are
partitioning the datafor forecasting with
neural networks,we put the oldest
data in the trainingpartition shown in the yellow.And then we use newer
data for the holdoutand the absolutely
newest data for testing.Neural networks also require
significantly more datathan a typical
time series model.With time series
processing, if weare modeling a panel of
series, each BY groupis diagnosed and has its
own model fits of the data.However, a single
time series typicallydoesn't contain enough data
to train a neural network.To increase the amount of
data available for training,each BY group if
concatenated togetherto form a single long
time series that'sset to a single
neural network model.So in a slide here, you can see
yellow, blue, kind of brownish,green, and purple blocks, each
representing a different timeseries.So we'll have repeating
time stamps that start overat the beginning of each color.The individual
series are delineatedwithin the neural
network algorithmby using BY variables
as categorical inputs.This lets the neural network
do things like determineif one particular series
has a lower average level,or perhaps some
features are lessprevalent in a
particular series.Feature extraction
is another techniquethat we use to increase
the amount of dataand also to help overcome
some of the shortcomingsof neural networks.So neural networks
I mentioned arevery bad at detecting
autocorrelated featureslike seasonality or trends.But if we create lagged versions
of the variables in our inputdata, then we can allow
the neural networkto use past data
when it's tryingto evaluate each observation.In the table below,
you can see that I'vecreated lagged versions of
the dependent variable yin the first row
of yellow columns,in the first two yellow
columns, and laggedvariables of the x variable
and the second group of yellow.Additionally, a trend or an
exponential smoothing modelcan be fit to the
dependent variableand included as an input.This helps a neural network
learn level shifts trendsover the course of the
series and adjust for those.Finally, seasonal
dummy variablescan also be generated.These can be generated
at any intervalthat you like, any
standard SAS interval.And they don't have to conform
to the interval of your data.So if you had weekly
forecasting data,you could generate quarterly
seasonal dummy variablesin order to keep your--or in order not to have
redundant information.There are three strategies
in Visual Forecasting thatuse neural networks and these
preprocessing techniquesas I mentioned earlier.The panel series neural
network is the first.The block diagram at
the bottom of the slideshows the typical workflow
through the neural networkmodeling strategy.First, any transformations and
data standardization is done,and then the feature extraction
process that I describedpreviously is performed.Next, the neural
network fits the modelto the data using a
training and validationcycle before it
finally reverses anytransforms and standardization
to generate the final output.The second strategy
is the stacked model.This is a hybrid of a neural
network and time series models.First, a panel series neural
network is fit to the data.Then the error or the
residuals leftoverare modeled with time series.And finally, the two
forecasts are added togetherto generate the final forecast.This takes advantage of the
advantages of machine learningand time series techniques.And finally, the
multi-stage modelis a bit of a twist on
hierarchical forecasting.The lower levels in
a hierarchy whereyour data may be much
more noisy and nonlinearcan be modeled with neural
network or regression models,while the upper levels can
be model with time series.Finally, the forecast
would be reconciledto the user specified level
to generate the final output.To illustrate the forecasting
power of these modelingstrategies, I'm going to show
some results from a case studynow from the Array of
Things project in Chicago.The Array of Things
project is a bunchof sensor nodes distributed
around the city thatinclude environmental
sensors like gas sensorsand light sensors, temperature,
humidity, microphones.And they continuously
collect data.And I've downloaded
from their websitethe data from October 2018.And I've accumulated it
to an hourly time series.There was a lot of
bad data in there.Of course, raw
data is often dirtyand needs a lot of cleaning.But after cleaning and
selecting variables,I was left with 27 series
of 720 observations each.The dependent variable that
I'm going to be forecastingis the ozone sensor output.And to predict that,
I'm going to usefive independent variables--temperature, humidity,
and three light sensors,one for ultraviolet
light, one for infrared,and one for visible.Ozone exposure is
an interesting thingto look at here
because it's closelylinked with respiratory
problems like asthma.In fact, the risk of
having an asthma attackafter you've been exposed
to high levels of ozoneremains high for hours.So it's an important thing to
be able to monitor and knowwhat you've been exposed
to throughout the day.In addition, with
COVID-19 in the world now,this could further complicate
respiratory symptomsfor those patients.The data only has
two hierarchy models.So I'm going to use
the panel series neuralnetwork and the stacked models.If it had more levels
in the hierarchy,or if the lower
level was more noisy,than multi-stage strategy
would be a good candidate.The extracted features that
I include that I've includedare three lags of
all the variables,seasonal dummy variables
for the hour of the day,and a damped trend model
for the dependent variable.So here are the results from
some of the forecasting.The plot on the left
compares the outputfrom the panel
series neural networkwith hierarchical forecasting.The shaded area, the gray
shaded area on the right,is the forecast horizon.So that's the future
predicted data.And to the left, in the white
area, is historical data.So we can see the
historical fit and thenthe fit in the forecast region.The tables on the right shows
the weighted MAPE measurementsmeasuring the error
in the model fit.In the top table, you can see
the stacked model significantlyoutperformed hierarchical
forecasting within sample data.And I should mention
hierarchy forecastingis really just the auto
forecasting version of thisbecause there aren't very
many hierarchy levels.I was not able to calculate
out of sample weighted MAPEfor the stacked model because
it doesn't yet supportincremental forecasting.However, the panel series
neural network does.So I was able to
easily calculatethe out of sample weighted
MAPE in that case.And here, we can see
that both the in sampleand the out of sample MAPE for
the panel series neural networkare lower than that of the
hierarchical forecasting,indicating that we have
achieved greater accuracywith the neural network model.Now, you can calculate
the out of sample MAPEfor a stacked model
type of problemin the coding environment.But it's not supported
in the UI yet.So cool.We have good results.But these results are only
good if we can use them.And I know training neural
networks can take a long time.So how long does it take
as we scale up the data?If you recall, the
original table that I hadwas about 20,000 observations.And here, I have scaled the data
up to two million observations.And we can see that the
training time on the leftincreases linearly as the
volume of data increases.So that's a good sign that our
training time isn't blowing up.So that's great news.But what about if our series
are short or our data is small?We know that neural
networks need a lot of datato train a good model.But how much is a lot?So I did another
experiment with a studywhere I shortened
the time series.So starting on the right
at 720 observationsand decreasing the number of
observations in each series,we start to see an
increase in the errorright around 350 or
400 observations.So this gives us an
effective estimateof the lower bound of
how long our series needto be to use the
neural network models.Now, reducing the number
of BY groups in a seriesdoesn't affect the
error the same way.It's not really deterministic.And it depends on the similarity
between all the different BYgroups in your panel data.This should give you a
pretty good foundationto start experimenting with
the neural network modelingstrategies.So I'm going to
give you a coupleof tips and common problems to
look for while you're doing so.And those should
help you out as well.One of the things that
you should keep in mindis the effect of missing values
in particular in combinationwith generating lagged
versions of variables.In the table here, you can
see that the first observationfor lag one of y is
missing because therewas no historical data to create
that time-shifted value from.This propagates
further down in timeon the second lag
and the third lag.Notice there's also a missing
value in the third observationfor the x variable.As we create lags
of this variable,the missing values also
propagate forward through timeand cause more of our total
number of observationsto have missing values.The problem with this
is that any observationwith a missing
value is going to beexcluded from the
neural network training.If you're not careful, and if
you have many missing values,then you can quickly reduce the
amount of data that you have.And you may not have enough
data to train a good model.Let's see.Another important
thing to understandis how the training time is
affected by the selectionsthat you make in modeling.For example, using the data from
the Array of Things case study,I had one time ID variable,
one BY variable, one dependentvariable, and five
independent variables.After creating three
dependent variable lags,three lags each for all of
the independent variables,adding a trend and
seasonable dummyvariables for each
hour in the day,I now have 51 effective
input variables,which causes a large
increase in complexityand the training time
required to fit a good model.This is further compounded
by the structureof the neural network.So here on the left,
I have a diagram,a simple example of a very
small neural network--one input layer, one hidden
layer, and one output layer.And on the right, I've
shown an enlarged viewof the central or
the center nodethere to illustrate
the math thatis calculated for each node.So the input layer
is going to haveone node for every
effective input variable.So this has already increased to
51 based on my case study data.And now, every
one of those nodesis going to be connected
to every single nodein the hidden layer,
the very next layer.Each connection has a connection
weight parameter W sub i.In addition, the different
nodes aside from the input nodeshave a bias parameter b.So you can see how quickly
these parameters multiplyand can get out of hand.The increase in
train time can beexponential with the number
of nodes in your hidden layersand the number of
variables that you are--or the number of features
that you're generating.So I think that that should
give you a pretty good amountof information.I encourage you to read my paper
for more details and more tipsfor using these strategies.So I've shown you how these
forecasting strategieswere designed for use--rather, these neural
network strategieswere designed for
use with time series.I've shown how they can
improve the forecastaccuracy for certain
data characteristics.And finally, I'd like to
leave this point with you.Parsimonious models
generally converge fasterand generalize better.There is often a
tendency to makea large neural network and lots
of inputs and lots of nodes.But multicollinearity will
damage your forecast results.And keeping your model simpler
is usually the best bet.If you have any
further questions,I encourage you to
reach out to me.And I look forward
to hearing from you.Thanks."
92,"LUCY SMITH: Hi.Thanks for joining.My name's Lucy.I'm a senior consultant
at Amadeus Software SASconsultancy company.I do a lot of work with
SAS Visual Analytics.My paper title is
Raising the Bar!Developing World-Class Reports
with SAS Visual Analytics.SAS visual analytics is an
Interactive Reporting toolthat can be used to create
dynamic reports and dashboards.This paper's about raising
the bar with the reportsyou're creating.I'll walk through
three techniquesthat you can use to exploit
the functionality availableand generate the best
reports possible.It's important to
create as best reportas you can, world class,
sophisticated, good lookingreports.The more care you take
to create your reports,the more the user
will get out of them.I'd like to think about the
information the user cangain in the first five
seconds of seeing a graph.If they're still trying to work
out what on earth the graph isshowing them, then
there's definitelyscope for improving the design.So essentially during
this presentation,we'll look at three
key techniques.After a quick introduction,
the first techniquewill be enhancing
a default visual.By that, I mean taking a
graph that you can create outof the box from SAS
Visual Analyticsand exploring all the
options and settings thatare available to make
sure the graph reallyshows what you want it to.The second thing we'll look
at is the SAS Graph Builder.It's an application that allows
you to define your own customgraph templates and add them
to the list in the Object's tabto be used when you're
creating reports.The last thing we look at is
the Data-driven Content object.It's a really powerful
object and allowsyou to embed third party
visualizations inside your SASvisual analytics report.So that's what we'll cover.We'll start off with
the introduction.When you first create a report
in SAS Visual Analytics,the objects type will show you
around 40 different visuals.Absolutely loads of choice
for the graphs that youuse to create your reports.A few of the most
common ones areincluded, including pie
charts, heat maps, histograms.As discussed, it's important
to create the right graph.But there are so many
different types of datathat you could be
working with, thereis no single visualization
to present all of your data.Before we have a look
at an example of that,the case study we're using
throughout this presentationis based on a UK
gym company lookingboth at the total
number of membersthat each gym
currently has and alsothe targets that have been
set for the number of membersin each gym.So as we look at the examples,
we refer to all of those.OK, so we'll first look at
what makes a graph good.And then we'll look
at an example thatexplores different options
and settings in SAS VisualAnalytics for a graph
that we'll create.So to start off with,
I've got two graphson the screen, graph
one, graph two.I'm going to go through
a couple of questions,and I'd like you to
think about which graphyou'd use to try and
answer these questions.Which gym has the most members?The second question is which gym
has more members, Hackney Townor Islington?Hopefully, you picked graph two.Both these graphs are showing
exactly the same data,but graph one,
although colorful,is quite hard to get
any meaning from.You can't tell the differences
between each of the gymsquickly.Graph two, on the other
hand, shows each of the bars.The length determines
the number of members,and the number is
also printed on it.It shows that there
are completelydifferent ways of
presenting the same data,but not all are good.So whenever you're
creating a graph,it's really good to consider
how you're presenting it.We'll now look at an example in
SAS visual analytics of a graphthat we've created, the
problems that it shows,and how to overcome those just
with the settings and optionsavailable.OK, so the the following
graph is a targeted bar chart.It's showing each gym
across the bottom.The bars are showing the
total members at each gym,and the black markers
on each of the barsare showing the targets that
have been set for each gym.Now the first problem that
I decided with this graphis that the gym names
are not fully visible.You could hover over and
get some more information.Visual analytics is interactive.But really, you're trying to
let the user see as much as theycan right from the
beginning, so that'sthe first thing we'll address.A second thing is you cannot
easily see how many gyms arebelow their targets.You could go through and
study each of the bars,but there's lots there so the
information is getting lost.The last one is it's
not easy to see whichgyms are performing the worst.That is, which gyms are furthest
from meeting their targetsif they are not
meeting their targets.So let's have a look at what we
can do in SAS Visual Analyticsto overcome some
of those things.Things to consider are generally
found on the right handside when you click on a graph,
including Options, Rules,Filters, and Ranks.So Options are generally used
for controlling the propertiesof the axes, the titles, the
legends, the overall lookand feel that helps the user
understand what they're seeing.The Rules, also known
as display rules, are--allow you to create logic-based
rules to dynamically colorparts of your graph.Styles should also be used.They sit under the
Options tab, but theycan set standard themes
throughout your reportsto make consistency and make
that a way of allowing usersto see what's happening
straight away.The last couple of things to
consider are Filters and Ranks.Filters allow you to
create logic-based rulesto control how much data is
being presented in the graphand Ranks are similar.You're controlling how much
data is shown in the graph,but instead of
based on some logicit's simply a top
or bottom count.So, for example,
if we're showingtotal number of
members, we couldset a rank to show the gyms with
the lowest number of members.And we'd pick how many we
wanted to see, so 10 or 15.So let's have a look at
a few of those in actionfor our targeted bar chart.First of all, the Options.The first one I highlighted
is setting the title.If the title's clear and it
explains exactly what the graphis showing, users will
understand quite quicklywhat they're looking at.We'll also set the x-axis
option to rotate the gym names.You can create as many
display rows as you want.I've created two
different ones herebased on a column
called Difference.The column called
Difference is a calculationbetween the total number
of members take awaythe target that we set them.So if the difference
is a negative number,the gym has not
met their target.if it's a positive number,
they have met their target.So here we've set
one Display rule.If the difference is positive,
the bar will be gray.If it's negative, we'll make it
red so it quickly stands out.And last but not least,
we'll add a rank.We'll limit the amount of
information shown in the graphto only those gyms that really
aren't performing very well.So we've created
a rank on gym namelooking at the bottom count.We want to see 15
gyms, and we'llevaluate what counts as
a bottom by the columns.And so the most negative
values will be shown.And the result of
that is this graph.So we've got the title.We've got each of the bars
for the 15 lowest performinggyms, those gyms that have the
biggest negative or differencebetween their targets and the
amount they have at the moment.Across the bottom
are all the gym namesquite clearly
presented, and the colorcoding on the bars of those that
aren't meeting their targets.To summarize, whenever you're
using SAS Visual Analyticsyou do have lots of options
for the graph that you use.So the first thing is
to pick the graph thatis most suitable for the
data you're working with.Once you've done that,
there's absolutelytons of settings
and options that youcan use to customize the
output that you're creating.Always consider
these steps wheneveryou're creating a new
graph, regardless ofwhether you've decided it's
not good enough or not.Always explore the options
that we've discussed.OK.So next up is the
second techniquethat we'll cover to enhance
how your reports look.This one's using a slightly
different applicationthat comes with your SAS Viya
environment called the SASGraph Builder.This technique's about creating
your own graph templatesand adding them to
the list of objects.As you can see in
the screenshot,the standard list of graphs
that you can create are there.And the highlighted one in blue
says line chart with time flag.We've created that
in the Graph Builderand then brought
it into the objectso that we can use
it as a normal graph.The SAS Graph Builder will allow
you to join graphs togetherand steal properties
from two different graphsand create a new
graph using them.Or overlay graphs on
top of each other,or just create graphs that
share rows, maybe x-axisor y-axis rows.So lots of scope.And let's have a look
at a practical example.OK.So here we've got a line
chart, nothing special,out of the box line
chart showing join monthacross the bottom and total
number of members up the side.In SAS Visual
Analytics, line charts,there's no way of annotating
data points unless youtook a screenshot of it
and started scribblingon your important data.In this example,
sometime around JuneI've highlighted as important.So I've scribbled it on.However, SAS Visual
Analytics is a reporting toolfor dynamic data.The data should refresh,
and you don't needto recreate report every time.So that's not the
ideal solution.What we can do is we can use
the SAS Graph Builder to definea new chart template, get
the role of a data label in,and it will allow us to add
annotations to the line chart.So here's a screenshot
of the SAS graph builder.It's an application.It looks quite similar to
the Visual Analytics layout,so it's user friendly.On the left hand side, you've
got your Graph Elements.Each of those are the templates
that you can start with.You can drag them
onto the middle,overlay them on
top of each other,and then use
properties from bothto create your final graph.The graphs we're
using for this exampleare a line chart
and a series plot.We're using the series
plot because it doesn'tallow your date on
the x-axis, so wecan't use that straight away.However, it does have an
extra row for a data labelso you can annotate data points.We'll combine the two, take
the rows from the line chart,the data label from
the series plot.So we've added them
both into the middle.On the right hand side,
we can control whatlines-- what rows are
going to be in the output.To start off with, the line
chart we'll leave as it is,and the series plot
we want to add a row.So we can add any extra rows.Each chart will have
different rows available.On this one, the row
we want is data label.On top of that are loads
of different optionsyou can specify.In fact, there's
plenty more optionshere than by default
in the Report Designer.So at this point,
you'd go throughand choose all the
properties, howyou want the x-axis, the y-axis,
and the data itself to look.We'll color the line
chart as normal,and we'll color the
series plot as whiteso it's hidden in
the background.We don't want our
lines duplicated.We only want to highlight the
labels used in the series plot.When you're done,
save it, and then youcan share that object between
any users that you want toand import it into
your objects pane.So I've saved it as line
chart with time flag,and now it's ready to use,
just like any other objectsin your report.Drag it on and add
your data to it,and here is our
embellished line chart.We've got our line
chart data from before,but we've added onto
it our data labels.The rows on the right
hand side, as you can see,are very similar to any
other objects you'd use.At the bottom, we've got
the data label we added.In this example, we've chosen
to flag up June of each year.So we've got June 2016, '17,
and '18 highlighted as markers.Essentially, you could
have put any datainto that to highlight
whatever was required.To summarize, the
SAS Graph Builderis a really good application to
help you enhance the graphicsthat you're creating.The new templates can
be used in your reportand also shared between
any number of users,and they can use them as well.So it's a one time task,
creating all your new graphstemplates.It's a quick way, and it's also
in your native SAS Interfacethat you should be
familiar with if you'reusing SAS Visual Analytics.So it's a really
good place to startto try and expand how much
you can do in your reports.OK.So the last thing that we look
at is data-driven content.Data-driven content is an
object in the default SAS VisualAnalytics that allows you to
embed a third party applicationvisual inside your
Visual Analytics report.It allows you to embed basically
any JavaScript based visual.You can develop your own from
scratch if you have the skillsor know someone
that has the skills,or you can use ones that
are already available.There's tons of libraries
of shared graphsalready out there.Essentially, you'd use the
data-driven content objectto do one of two things.One, maybe bring in a graph
that already exists but hasproperties that aren't
available by defaultin SAS Visual Analytics.The other option is bringing
in completely new graphs.So if your graph
doesn't exist, the wayyou want to present
your data does notexist as a default graphic
in SS Visual Analytics,you can create it
and bring it in.The great thing
about this objectis the graphs you create work
exactly like your SAS VisualAnalytics objects.When you import them into
your report, they'll interact.They'll be able to be filtered.You can add the
data exactly as youare when you're using a normal
SAS Visual Analytics object.OK, so we'll have a
quick look at an example.First of all, we've picked D3.There's lots of JavaScript
libraries you can use,but we've found that D3
brings probably the mostnew functionality to reports.And if you are
familiar with HTML,it's not too difficult to learn.So I've got a quick
video of a D3 graphicthat we'll be using in a
SAS Visual Analytics report.The graphic at the
moment is not in VA.It's just in an
internet page and ithas hard-coded data in it.When we bring it into VA,
we'll want to address that.We'll want to embed
it in the report,and we'll want the
data coming from VA.So first all, let's have
a look at the graphic.OK.So here's a bar chart.You can't have a bar
chart in Visual Analytics,so we're using a graph
that already existsbut adding functionality
that isn't therein a default bar chart.In this case, as you
hover over the barsthe surrounding bars
dynamically change color.And as you move the
mouse away, they'llchange back to their
original color.So the functionality
we're adding inis called On Mouse Hover.As you hover your mouse,
something happens.I said that that graph
has hard-coded data in it.What you could do if
you want to bring itinto SAS Visual
Analytics is we needto make two changes to the code.We're not going to go into the
code used to create the graph.That code and the
code that is adaptedis all available on the
SAS Global Forum GitHubif you want to have a
look at it in more detail.But I thought I'd pick out
the two really important bitsof information that you'll need
to know if you are creatingD3 graphs yourself.First of all is a listener.That graph I just
showed you reactsas your mouse hovers over it.The listener is part
of the code that'slistening for the movement, the
reaction and the interactionfrom SAS Visual Analytics.So that needs to be
part of your codeto make sure the
graphs are interactive.The data receiver is the
second important thing.We want to be able to pass
data from SAS Visual Analyticsinto the object, and
we want the objectto render the graph showing
that data dynamicallyand every time we
look at the report.So that data receiver
replaces hard-coded dataor wherever else
the data is and willallow the data to be passed
from SAS Visual Analytics.OK, so getting that
graph into your report.So last of all, we need to
save the code on your SAS Viyaserver.There is a specific
folder to save it in.Once you've done that, add
the object into your report,specify the web content
URL in the Options tabto point to your
graph that you'vesaved on your SAS Viya server.The last thing to do
is at the data rows.So it's just like a normal
graph now we've embedded it.The data rows expect variables.The variables are dependent
on how you've coded the graph.So in this case, I've added
gym name and total members.In the report, the
graph will automaticallyinteract with any other objects.So we'll have a look at a
quick video of a bottom barsimply interacting with
the graph we've created.As you click on
the bottom bar, itwill filter down
to the bar chart.The bar chart looks like a
native SAS Visual Analyticsgraph.So to summarize, D3, SOAP, and
any other JavaScript libraryyou can use really drastically
extends the opportunitiesyou have with how you present
your data in SAS VisualAnalytics.Leveraging the power of third
party graphics or graphs,you can choose to add
functionality to graphsalready exist or bring
in completely new graphs.They'll work seamlessly
in your reportas if they were native
to SAS Visual Analytics,including filters, interactions
with other objects,ranks, and many other things.OK, so that brings us to
the end of the presentation.To summarize, it's
really importantto engineer a great
graphic and to considerhow your audience is
perceiving what you've created.Thanks for your time
and thanks for joining."
93,"hello everybody my name in function andthis is pre-conference tutorial to SASGoogle forum 2020 it's touching theBayesian analysis so this is the outlineof what I will be discussing today it'sit's a pretty high-level overview ofwhat beta status this and do some of thesimulation methods that's being commonlyused in dedicated paradigm to doestimation and in the modeling and tothe end I'll introduce to SAS proceduresparkland is bayesian generously nameexited model and the park MC MC is ageneral sampling algorithm basedprocedure that has been out for a whileand III is a relatively new procedure sothis is the overview of the Vedaparadigm and we started by stating whate statistics in Bayesian statistics sothe idea of statistics is the science oflearning from data from collectinganalyzing and interpreting and to theinference and one key is to communicateand as we all know point estimate meanssomething but usually it is theconfidence interval to quantify theuncertainty is what we care about whatvarious statistics in statistics is asubset of statistics subsets ofstatistics in a larger sense of theworld and the key point and I want tocarry is to point out is that alluncertainties in bailant statistics aresummarized probabilities confidenceinterval and it's not a probabilityfishing and beta statistics or incidentis awesome so we can using this threeitem bulletin points to summarize fadein paradigm so they in the beta paradigmprobability describes degrees of beliefwhat that means is what you personallythink how likely something is going tohappen so it's a rather personal thingit's not a limiting frequency so it's bydefinition is very subjective this hascaused a lot of controversy inthroughout the development of statisticsbecause people find that to be not adesirable thing when it comes to doingscience but from a Bayesian perspectiveprobability certainly is very much to dowith how personally one believes thelikelihood of certain event that's morepoverty means and then parameters cannotbe determined exactly they are randomvariables and you can you can makeprobability statements about them youcan say the probability of theta isgreater than zero is 27 percent unlikelyis 27 percent was lower than 55 percentand influence about theta are based onprobability distribution of theparameter so it's key to haveprobability distribution describingabout the parameter on the other handwhat we are more familiar with thefrequency stochastic approach well theunderlying assumption is thatprobabilities are objective propertiesof the real world cell the first twolimiting relative frequencies that yourepeat an experiment pass the point manymany many many times then roughly 50percent of time will come ups head intherecome to be the tail and the parameterssailors are fixedthey're just unknown they're fixedunchanged so doesn't make any sense tosay what's the probability of somethingfixed is greater than a certain valueeither 0 is not greater than 0 andbecause of the definition of theprobability based on law and frequencyor statistical procedures should bedesigned to have these well-definedlong-run frequency properties such as aconfidence interval that if you were torepeat your experiment many many timesand 95% of the realized confidenceintervals cover the true parameter sothat's the compressor this is a simpleexample of how Beijing thinking reallife works and the suppose that yoususpect you might have a feverthat's your parameters just unknownunknown let's say and don't say no theycapture what's the body temperature andyou want to take your take yourtemperature and because you don't knowwhat's your body temperature years thenyou can have prior guess well in anormal situation if you are mostlyhealthy then it's centered around aneight point six degree and possibly sickand it has a range say between you know104 degree and it is used to captured bythis - the distribution and now you takea measurement from a thermometer and thethermometer says 101 degree so this isyour life favorite this is what wecalled is that the data is generated bysomething and normal theta and hopefullythat the summer middle is very accuratethen you know that it's probably normalcentered at the true lab true value andwith a small small standard deviation orlivesand you put the two and two together andthen you get your posterior and thenjoin us it though you do that and thisis what what bathing thinking we're likethis however on the other hand supposeher - you took a measurement and if themeasurements is rather than reasonableeither they will all or sitting in highand if it's very low and let's say justto be the temperature of 93 degree andit is so in conflict with what yourprior believed of your reasonable bodytemperature wrong is then you will checkthe data and then the posterior wouldmean the same will see prior and byrejecting datawhy would you do that because there isso low our belief that you could have abody temperature of say 90 degrees or 80particles so that reflects on theprobability and you that's how yourejected in and your posterior is indeedand so this is the basic paradigm sortof a war statistical term and supposethat we want to conduct a study and thenthe parameter theta is of theinfluential interest and it can be manyexamples in the regression coefficientit can be missing data and it has aratio it can be just a probability ofsomething so just anything that you wantto find out and now and let the data thenotes Let X denote the data and thenthis model will be called a density of Pof X ADA is the data generatingmechanism we also called a samplingdistribution if how the X comes aboutgiven fear and we also say that thefunction itself that proportional tothis sampling distributionis the likely we all know that that'sactually extended statistics so that sothe pay them my sense is that we want tostart with this star is thisdistribution or likelihood thatdescribing data generating mechanismsthat control by some unknown parameterand we want to say something about theparameter itself so it's the switchswitch given X that given theta we wantto say something theta given what wehave observed and the switch from datagiven parameter the likely function orthe sampling distribution to theparameter given data which posteriordistribution is the essence of theindependent and now immediate questioncomes about is what is the likelihoodand what is the posterior and are wesimply switching the like function andjust called the posterior distributionso so we just this is a picture of youknow you can look at the picture twoways and you see completely differentlytwo young lady or and all the ready butessentially the same thingare we and it's almost just like thatthat's almost a subject and there's aswitch the key to switch pumps was usingthe base theory that's attributed to theRomans base back 250 years ago so stateset is a posting redistributionmodernity switch the posteriordistribution of theta given X isproportional to f of x given theta whichis likelihood of P of X at skinningtheta times a prior distribution and infact it disappears it is constant so ifPI of theta is one then the posterior isindeed the same as a likelihoodfunctional form okay so the caveat iswhat you are in fact doing is you areswitching the random variables from thelikelihood the likelihood is more aboutthe daygiven theta since anything about thetheta just says how like the behavior ofthe outcome of the data is governed bythe sampling distribution now we makethe switch the random variable is thetaand nothing to do with the data the datais given and then this distribution aredescribing the behavior of the parametertheta given X and that's why they'rejust posterior distribution is adistribution on the theta and not alikelihood with the distribution on theXeven though functionality are the samefor and in order to do this you have togo through the joint the marginal andintegration with respect to theta at thethis is called a normal running constantor my own fishing boats and this is theformal basis even is following butmostly people using the wind on top forit and this is also the requirements todo Balian statistics is that you mustalways use a prior distribution itcannot have a analysis that bypass theneed of using prior distribution and andunfortunately it also cannot be constantall the time because the posterior isnot guaranteed it proper with respect toit's flat or a constant prior the reasonis because they likelihood distributionor sampling distribution require to bedistribution with respective X so it's acouple with respect to X but it doesn'tsay anything about theta that's that'snot that's not the job of the of thesampling distribution of the likelihoodfunction but in order to make this thesame proper as a distribution there mustbe something that and and so that's whyyou cannot always use once you have thedata in analysis framework on influencesbased ondistributions then you can say oh howabout this sumerian concept of ourdifferent point estimation in influenceemission and an estimator essentiallyjust a point a point an estimate of thebest guess from a distribution and theeasiest typically use posterior me theexpected value of theta with respect toa distribution and you can use them allbut mostly people use expected value oftheta and then the interval estimatesthe Balian concept is called crediblecells and it is the area under the curve5% and you can many many different waysof curve but most commonly people useequal tail to side alpha minus 2 alphasince distribution doesn't have to besymmetrical and high posterior densityis the smallest interval that contains95% of the probabilities as theinterpretation here is there is anapproach of percent chance that theparameter is in this interval theparameter is random it's nothing so youcan say that in the classical approachthe estimator is a confidence intervalcertainly has very nice properties suchas convergence probability coverageprobably written a convenienceprobability coverage probability meaningfor skullery and biasness so on and asthe interpretation is that a 95%confidence interval a sir sets if youwere to repeat experiments and that manymany times95% of roughly the realized confidencewith all colors with your parameter youcan now say the true parameter is in theconfidence is growing popularity 95% thetrue product parameters either in oroutside of a confidence interval but nowis any measurable probability so theinterpretation reflects the uncertaintyin the sampling procedure because theparameters cell itself is a fixed andinterval is ready and now we just goingto discuss a bit about twomajor criticism on phasing paradigm as away to understandso the first criticism is should aparameter be fixed or random and thedebate is perspective is that whether aparameter is fixed or random is somewhatbeside the pointI just fix toss random we still need toquantify the uncertainty about and thebest way we know how to do it is to usea distribution to describe the incidentand as that has many benefits to it nowsay that we don't really know but theimportant thing is to quantify itselfand then the second critical criticismon the beta paradigm is that a priordistribution contained too much issubjective information as in the sensethat we are being too subjective and ourown belief is driving the analysis you'dbe surprised how often this happens andthe fact of the matter is thatscientific enquiries are subjective bynature we in fact don't look all at thesame data and which identical conclusionif we do then there is no point we allhave to agree but in fact that we don'tagree and the Bayesian just being honestabout the whole process and the simplyreflects how general scientific inquiryworks which really rely on eachindividual bring their own opinions inthe knowledge in the process and youbelieve something works or you areskeptical something doesn't work and weall get together to see and based ondata to improve or correct our opinionand Anna so there is this famous a by byzhang heinous is that when the factschange I changed my mind and as old asthat's reflecting the updating the priorto the posterior just resetand on the other side of the equationyesmuch of the statistical modeling we gotto remember that is subjective theselection of the likely function in thearbitrary cutoff p-value 0.05 so thecompletely elimination of subjectivityin retain objectivity is really not it'sdesirable but it is not really practicalin any sense of the world and we canalso have a another look at a p-value sothat this contracts between Bayesian anda frequentist and so let's say this is asurvey done by and by 33 not for 40years ago at this point what would youso the survey says what would youconclude if a properly conductedrandomized clinical trial but treatmentwas reported to have resulted in abeneficial response of P is less thanqualified significanceokay so what are these things okay so umhere are the four choices one is havingobtained the observed responseso having seasoned data the chances areless than 5% the therapy is noteffective so the probability of therapyno effect is less than 5% that's alittle statement about on the therapyand it's likely to be wrong and nowthere's too confusing and equallyconfusing statements is the chance isless than 5% of not having obtained theobserved response if the therapy iseffective so you set a therapy to beeffective you assume to be effective andyou say that there is less than 5%chance of not observing or you set thetherapy the now to be not effective andto say the probability is less than 5%of seeing the thing that you have okayalthough the fourth one is thereally and the survey asked 24finishings and half answered incorrectlyand all had difficulty distinguishingthe subtle differences what does themean you assume CeraVe is effective whatI mean yes syrup is not effective but ofcourse to Train statistician this isBrennan pada you set the now to be notto be to be not effective and then youlook at the chances of something happensand then the correct answer to all thequestions then three okay now this is aright and this is even wrong and this iswritten by wolf for polygon submit thepaper and the correct answer is that thechances are less than five percent ofhaving obtained the observed response orany more extreme response if the therapyis not effective okay this is reallyimpossible to interpret what is lessthan five percent means but that is adefinition you can integrate out theobserved likelihood of seen observedresponse and a more extreme responsethat's what that's what he values andthis is the incompatibility that bathinga frequent is often differ on that isthat you can reject the now you cannever accept alternative and so on butand the results it is often being beingmisused in practice and people reallythink p-value and the confidenceintervals indicates the probabilityabout the hypothesis and the use of itsevidence for against the question ofinterest but that's not that's reallywhat Bayesian approach does and that iswhat's advantage of having so theBayesian inference in essence and inaddition to the interpretability bathingalso offers so the modeling advantageswhich I won't go through here butbecause of treating unknown quantity asparameters can be estimated any to avery easy solution to come to thesetting of complexproblems so far and bypasses need toderive asymptotics for that and it'salso formula of learning which our skiphere because it just says that as youupdating your data you were becomingmore and more certain or not certainabout a parameter and distribution willbe updated so there are some additionalBayesian advantages which I'll leave theattendees to look at look them because Ithink you all have the handouts and Iwill go through them at this point and Iwill move on to the next so next we willtalk about prior distributions which isimportant aspect of pervade analysisprior distribution as we had mentionedearlier represents COPD before seeingthe data it's what should even in thepainting probability measures of thegrease of the leaves the certain are youhaving a random event so why thisdefinition probability is subjective inago and has no work or on it it followsthat all fires are subjective priors andthe people don't like the word ofsubjective even though we had discussedof the subjectivity or the sort ofscientific inquiries and an inferencebut people still want to see and obtainthat data results that are objectivelyvalid and so the thing is that the dataspeak for itself and don't don't templewas with the analysis there's no forceand this approach it's knowinginformative prior they sometimes calledflat in poplin chapters priors and thenthe other side of the coin subjectiveapproach advocates in informal apply itto say that combinedwhat's the knowledge that you have inthe analysis is just how we advance ourunderstanding so general speaking theamount of data grows you know in a modelwith fixed number parameters so thelikelihoodtypically overwhelms impact of the priorgenerally detectors thought to be trueand I want to discuss a few firethat's for English section and now yourformula prior a prize my informativeit's had a very big definition and theword see wider with the definitioninvade a priori is 90 for me with it's aflat the little relative to the lightwave function and under the desire is tohave minimal impact on the posterior ofthetathe reason our people like a 19-4multiplier because they appear to bemore objective but it had been provennow at this point 100 25 years ago thatLarry Wasserman and Rob cast within thepaper to say it's unrealistic to thinkthat you could have a truly no inform ofthe prior that represents totalignorance about a parameter of interesta common example is the following if youare to model a regression model heightas covariates and predictor to theresponses of weight you must know whatthe real question coefficient need to beyou cannot possibly be anywhere fromnegative 10 billion to positive 10billion it's you probably shocked ifthey hide in a way to you know ratio andit's greater than say a hundred you knowdepends on the union of course and nowif you have a flap fire you think thatyou are no informative and you want tohave been on completely non formativeabout things you have a uniform priorfrom negative or temporal in to depositten billion to say this is anywhere mymy coefficient could be and I don't knowwhat values I take so I have equalpreference ahead of my analysis to saythat they are all they can be anythingperform a distribution or perspectiveyou know that you you're puttingthat's a massive amount of promisegreater than 99.9% how is that apossible range that just means that apriori you already don't believe theresults that you will see there you youare much more likely to believe that thecoefficient of a regression coefficientis greater than 100 and the less thanthen negative hundred Engler from therange of negative 10 billion to theopinion or negative temperature causeintended in or something so that's theconcept of you can have no known form ofthe prior representing total ignoranceabout the parameter of interest andthat's a heat point and but that's notstopping people from using it ourcalling is to be no informative in itand the frequent Stein phone the prioris prior is proportional to 1 which issine equality to all possible value ofthe parameter and that's just the way itis and if people do call in ourinformative ENSO that it does contain itcontains information that's key pointI'm trying to get across and here I'mgonna use an example at the binomialexample and suppose that you seefloating heads in 17 courses and yourlikelihood is binomial was X is 14 and Nequals 17 and if he is unknown now we'regonna do a flat prior because I don'tknow what what what what the theta isgoing to be before it could be 50/50 itcan be you know can be worse than thiscan be higher than a I don't really knowI'm having a flat prior of he isproportional to one and now I multiplythe two together I get a posteriordistribution and now you can see onetimes this gets to be here but nownotice that this the random variable isX so P is is fixed X and only plugging Xand M to be a likelihood and now the Xis known and n is known and appears andalso thethe variable is G so this is a binomialand this is a beta cone over beta andthis is the this is the the part that wecan see is that you have a binomialmodel with a flat tire the posterioridentically likely function is just andand then just a few observations I'dlike to state and first as we hadalready observed the posterior is equalto the writer who was with price flatthen why not use the flap by all thetime the reason is that the flap wiredoesn't always guarantee the problemposterior distribution this disc wantedthey need to be a distribution and youasked why is that we start with thelikelihood function with a properdensity and you slap something flat youend up with something improper that'sbecause the density function is aproposrespected X but but if you translate tolikelihood you often has a mode but thelikelihood function doesn't care aboutthe tail the tail doesn't need to godown so the likelihood function areguaranteed to be into global and if youhave that's why if you have flat priorit's not always guaranteed to be in theyou don't have tail or tail therewasting its own and so when you note youshould always use a proper prior toweight the tail of the life literallydown to make the thing a properdistribution and then the secondobservation is encased with thelikelihood function in the properposterior distribution or I can to go tolook at the same also this is the zoomin picture of this binomial around themall here and you can see the two curvesone is the black curve the posteriordistribution is also identical to thelikelihood and then this is Emily andthis is a normal process emission aroundmmm that's the classical inference thatthey placed on asymptotic essentially inthis area known population and thefading inferences based on exploring theentire distributionwhich that's why it has skewed to theleft so key takeaway is that you alwayshave to defend something by that I meantis that everyone in a sense slave to isa likely function this is the foundationof all both paradigm g x given theta isa data is how we used to do inferencebut given that in painting paradigm youoften you to justify the selection ofyour prior but in the classical paradigmyou need to you need to just as inphotos you need to know you need to knowwhy this curve is becoming normal okayand that's quite a bit of work theunderlying assumption behind asymptoticsis there exists an infinite amount of anobserved data that i just like the oneyou have seen okay whether that's trueor not is up for debate and this need tobe justified if you want to use theclassical paradigm if it's true then ofcourse asymptotics or follow-through butthe bayesian makes no such claimBayesian does now make a claim that youwill see more data and what you havealready seen if i see 14 courses orfourteen heads out of 17 courses that'sall the data i see and i don't assumewhat's gonna happen with the 18th pauseor 19 stops or simulate horses inlessons and then the third observationis when it comes to flap fire is itstrapped for a truly informative okay solet's suppose that in the binomialexample what we did is a flap fire onkey and we get to likely in theposterior and suppose that in or insteadof modeling on p I choose to model on alarger p okay so a uniform prior on Pwhich is the left hand side as you cansee it's a flat line between 0 & 1 1 ifyou transformtransformer variable to gamma of largerP the prior is a logistic distributionthis by no means is going to be flatright you can't say this is very fun Iskipped the way to do transform orvariable coherency and on the other handif I were to choose anomaly informativejust being assign equal likelihood toall parameter varies on the largestskill I translate back to this proteinprior is PT on both side so becausethere's so much waits outside of logic Imean anything outside the reasonableload yet like you know five and negativefive is significant and now if I say I'mgoing to be impartial to all logitshundreds negative thousand one hundredten thousand then just means that I'mmuch more prefer to do the analysis bythinking it is either 0 or 1 okay sothis is the way that and why is it whyis that the the so hard to have annoyingformulashe said if flat prior often implies theunit sort of a measurable scale on whichyou sign equal lightly village and thenwhen you say it's one you are saying atheta is the right to dip into on the 1and the 1 and one 1000 and 1000 10001000 1 but if you want it to be uniformon the log scale then it's likely to bebetween 1 to 10 10 to 100 hundred to athousand thousand to ten thousand thattype of unit when obvious difficulty isin justifying when you pick a specificflap prior to explain the choice of theunit which deprived being on informativepong and that's a that's a tricky thingto do and there you go we'll always haveto bear in mind when you do thatand as all people would say - we have apride at someone our informative at thesame time is invariant to transformationand then the answer Jeffrey is quiet andas Jeff is prior I'm just going throughthis very quickly it's just depends onthe fishes expect efficient informationplace on likely function okay so thelikelihood is telling you how toconstruct a prior and you can you cansee the example of how oh go through themath it's not very difficult we take thederivatives and plugging the value andthere you can compute it and then theplot I'm seeing here is this pot andthis is a Jeffrey's part it tailsupwards cause the opposite of thelikelihood likely goes down it tells upbut it is flat relative to thelikelihood and so the likelihood issolid the posterior shifts a bit to theleftHey and then the nice thing aboutJeffrey's the Jeffrey invariance it'sjust not you are not informative on onescale you are not informative boy or orskills there are some pros and consabout Jeffrey's prior which I will skiphere because it's only a two hourtutorial and this is really for forsomething that you would want to take alook maybe later onwhat's it has its rights and on how aviolates are likely principle and so onthere's some pros and cons of the of themessage which our our course but whatI'm what I want to what I want to finishwith this session is not just aregression example with zero occurrenceto see that Jeff is how Jeff is probablyutilize this life learning function itcan be quite quite quite useful so thereare four treatment groups three millionone two three four and the responses ofwise and then more patients giving thefour different treatments improved outof total number of n so the ratio okayso so it could for having no incidentsof events or zeros and now if you cannotdo this model standard IQ based methodsare now usable in case of because ofzero occurrences and you gotta haveother ways to associate computes tenand now we can compare using the uniformprior and the chairs prior analysis inhere and this is it just a data that Isee they can see for group one two threefour and one two three four and thenthere is the the the number of responsein each of the end of trials it's abinomial model or with treatmentlogistic and and the Jeffers prior isthis piece okay X transpose M transposeX and M is a after I have no elements orPN P one minus P I and now this is theblue line yes is the jeffries posteriorwhich you can see that when the and thenthe red line is using the uniform and itcan't estimate the law school okay justno information is in there whatsoeverand you can see that the two things topoint out Jeffries line followsinformation from some other group toinfer to least give you some signal onwhat you likely to believe given theinner line model something on the otherside it doesn't change much of theposterior distribution of otherparameter to have uniform cry so soJeffries prior in the sense is designedto do what it does it's been noninformative in many situations andhelping by borrowing information fromthe likelihood to do some inference ofother groups and that is the last is aconjugate prior and we're done with aniphone5 just moving into the conjugateprior and a conjugate prior means that aprior and a posterior are of the sameform so if you have a likelihooddistribution of the binomial a conjugateis a beta prior because the posterior isalso a beta distribution and people liketo use conjugate novel computationalregion for the reason but also becausethat you can look atunderstand the amount of informationassociated in the prior so the datacontains access success and out of ntrials and uh and the beta prior wasAlpha and the beta scale and shapeparameter indicates half of success outof alpha plus beta trial so you look atbeta 2 2 and a beta 3 17 you knowclearly what information that theyrepresent and they're on the same ideaof of looking at the data by containingthe information is this thing calledunity information carrier the design tohave a prior contain roughly the samehonor information as a one data point sothis is also an agency is using usingJeffrey's Fisher information based onone observation part kind of to do thisand the last thing I want to say is thatinformally prior in a way when it's usedproperly you can use existinginformation to carry through in youranalysis and when it's used properly itcan be very powerful but of course youcan also give very misleading resultswith it the last thing about prior thatI just want people to be aware of ouralso treat this as a skip for people toread through is called a priorelicitationso the idea prior to the recession isthat you know there is expert opinionson something that is increasinglyimportant but they don't know how totranslate their knowledge to the modelof the distribution on the parameterthat you have so they might have someknowledge in the field how somethingworked with something compound a in factthe outcome of B but because they can'ttranslate that information to the priordistribution of parameters in the modelso this idea is called prior elicitationthat's expert only be asked to assessobservable quantity but you want totranslate information to the parameterand I walk through aideal predictive distribution and thenhow to elicit 8a coefficients which ifyou want to connect study on height toweight what prior you can use beta and Iwill skip this this part due to lack oftime at this pointso for beta computation there are twotasks at hand the important thing is toestimate the posterior distribution hereof theta you don't why what's new to beaccurate because this is the basis ofall Bayesian inference and then thesecond task is to finding expectedvalues point estimates which can come intwo formsboth are both based on this distributionwhen you see expect a very old SATA justthe parameter of selves okay or expectedvalue of the function of theta withrespect to the posterior distribution soI could say that I want to know theexpected difference between twoparameter beta 1 minus beta so these arethe two things in a phone pole fordecades the most popular method forestimating the posterior distributiontask oneyes well I'm CMC Markov chain MonteCarlo and explain why that is to be onceyou you get this MCMC once you get theMarkov chain Monte Carlo from estimateis distribution the second task becomesrather trivial you can just plug in forvarious in this tag and it's it's it'svery much doable so so essentially markof traumatic results two things and thenthe multi coral aspect of second part isis to find in an expectation of G oftheta with respect to sample from adistribution fee of theta is it justgenuine many sample from thisdistributionevaluate them and take the average andthey find expected so that's why it'skind of a chilly this is a plug-inmethods andyou can do it is for big dimension thatyou want that's accurate going back tothe simulation based estimation so theidea is is simple by proportion this isthe distribution of beta distributionthe true density is the black line andthe red line is the estimate density andthe histograms are the samples of thevilli samples and you can sample rightyou can estimate distribution to anydegree of accuracy of course there's twomajor obstacle one years there is thisnormal and inconstant so for adistribution you know it's proportionalto some kernel over a numerical constantfor a normal distribution we know whatno more than constantly to be but forarbitrary distribution it's unclear andit in a second is the theta at emissionis high it's actually very very so thisdifficulty was the first difficulty ofthis problem sampling was I don't knowknow you have a talking distribution butyou don't know how to sample from thisproblem was solved by Alma and ametropolis who essentially says that tosample from any talking distribution youneed to just construct a Markov chainthey have the stationary distribution tovisit a desirable distribution soinstead of generating directly from atarget distribution so an example ofthis is the normal random numbergenerator function oh no mean not thenorm function the rand function or inSAS and uh no mean are you can generousyou can similarly from a Markov chaininstead okay once you do that you endedup with sample from the targetdistribution of course how to generateMarkov chain secondary but the problemthey solve is data stated that if youcan construct the Markov chain with astandard desired distribution the targetdistribution to be the posterior theMarkov chain is guaranteedto be the sampling of that distributionand the whole part is metropolisHastings algorithm and then the Gibbsalgorithm changed the landscape of thesimulation based the computationessentially that's why everything isgoing through the MCMC going back moreformally to define Markov chain MonteCarlo is just a stochastic sequentialway of generating conditionallyindependent sample according to somedistribution that means that the you youstarted from a sample theta 0 giventheta 0 your sample Zeta 1 the SATA onecan depends on theta 0 say that youcan't depend on theta 1 and theta 0 andso on so forth but the Markovianproperty is that the sequence ofconditional data that you have had onlydepends on the mother the most recentones so given the most recent ones yousample the new one given the new oneyour sample another one is forming thebasis of a Markov chain and here is alist of a lot of papers and the articlesand the books that devoted to 10 cm seeand you can check about it now you saywhat does the thing idea Gibbs comes inthe picturehe keeps come in the picture because MCMseeing high dimension problem typicallydifficult so Damon given a paper in 1984showed that you can break a largerproblem into smaller problem muchsmaller dimension and you can still getwhat you want the collection of thesample from the Joint Distribution andthen they also show the conditionaldistribution can join three to Tommy aunique distribution so everything workout it's actually a very good and that'sjust to show a picture of how the stepand I think many people are familiarwith this concept of gift something sosuppose I want to draw a sample from offon a beta distribution and that startwith alpha and alpha zero so this is avalue of alpha zero just pick a randomrandom random of course it's got anyvalue and condition on this piece andyou can you can see that there is aslice over the Joint Distribution thiscalled conditional distribution this isthe beta this is going sample on a betaemission and of course probably highhigher here and the lower there to thetail and you have the distribution andit is simple and you get a played-outzero if the straw sample and you turnedaround and you look at the other way andjust give you the marginal distributionof alpha given beta and your samplealong this marginal distributionconditional distribution you feel one byone and you repeat this many many timesand this is what gives sampling does heyI was going to do this simple example ofa normal hierarchical model deepsampling but our skipped is because dueto the lack of time that we have this isthe material it's not too difficult tofollow through you can get an idea ofhow to do gives in this particularsituation the key quantity that I wantto point out is that each of theconditional distribution is of astandard distribution and the standarddistribution is you you can you knowutilizing a standard distribution foreach of the step and that has been donemany many years in in Bayesiancomputation this is what conjugacysampling became so important but ofcourse now our problem can haveconjugacy and you can certainly haveproblems that conjugacy doesn't applyand then you get a distribution that youdon't quite know sample form becauseit's not a standard distribution andthis is a require of need a generalsampling algorithm and this is what themetropolis algorithm comes inso the metropolis algorithm essentiallydoes have it's a general samplingalgorithm that puts minimum amount ofrequirements on what we'll need to knowhow was a distribution that you aresampling from so sort of like a magicalthing your it just all you need to knowis the density of the distributionthe functionalist or the distribution assome random number generator you'd beable to to do this and then here is apicture of it and let's say that I wantto draw a distribution from PI of thetagiven X I don't know what the safe upthere it is it's certainly not a in anydistribution that I know ofbut I still need to sample and knowproportion so what I do is I startedwith some some get some initial value oftheta and then I say I'm just going torandomly draw Center the world in from anormal distribution of thetaNing as theta 0 given say that thisstandard deviation of Sigma and now Iget a theta stop and now what I do aregonna make a decision if I want acceptedas a sample of your checking and there'sa decision rule is such that if thewhere I'm going to the state our starhas a higher posterior density variationa preferred place I always moved it okso I'm moving from theta one to becomesay de say that 0 to become say thatwere so that's the second sample of theMarkov chain but if I propose to somemore words lower and I computed theheight of the density of this height andthis height and I know this is lowerthat I accept the sample with aprobability ratio between the height ofthis to the height of them okay that'smetropolis and then I keep going arepeat same process and there's an ideaas a Markov chain always move to thearea they have higher density but it canstill explore tail area with low low lowdensity and if I do this repeatedly I'mguaranteed to sample by proportion ofthis arbitrary distribution and this isa random walk machopper solvent so thisis the trace plot of a random walkmetropolis for noticedto stand to sample from a normaldistribution you see that started let'ssay that I started from a very far fromnormal zero like around zeroI started from negative twenty theMarkov chain gradually move quickly tothe center of the distribution and thenit stays there you can see it mostlystay around mean zero but it goes to thetail and any of the results is thisdistribution that's normal density ifyou do this long enough it canapproximate normal 0 1 2 arbitrarydegree occurs so there the pros and consof it that pros and cons of metropolisalgorithm it is requires many manyminimal requirement of knowledge aboutdistributions or spiritual and the roadseasy to implement its robust and it canreadily be slow and the certainlyrequired convenience diagnostics and thewhatnotbut the key point that I want to comeacross is the Gibbs sampling algorithmbreaks a larger problem into a smallproblemthe all muscle umma and the metropolisidea of using Markov chain to sample anyarbitrary distribution has simplified adifficult problem and the Gibb stepstells you that you can break the biggerlarger problem a bigger Markov chain tovery small pieces and the much harpersalgorithm takes the weights ofrequirement that you need to know aboutcertain parameters distributions inthere Gibbs conditional distributionsand that's important in a sense now manymany many problems becomes doable andthere has been the development for past25 years and still ongoing it is stillyou can see a variety of variety ofgeneral sampling algorithm to to defenseand the newest kid on the blockis Hamiltonian Monte Carlo that came outso maybe ten years ago but recently moreand more people are getting into it sonot a nurse alternative way ofgenerating samples for and how quicklygo through this too is against it's veryhighlight highlight concept and showsome parts in the pictures and for youto get a sense of obvious how this thingworks okay so Hamiltonian Monte Carlo isuses two things that's expensive moreconvey comparison much more expensivethan metropolis one is it asks for firstorder gradient information cost gradienttypically helps metropolis does notrequire gradients information at all andsecond yet it generates it creates thisthing called altering momentum variableand it's like a latent variable kind ofa thing to help you to sample from ahigh dimensional Joint Distribution theidea of Hamiltonian because of thephysics the Hamiltonian dynamics to andup because using utilize a Hamiltoniandynamics you can january samples it's avery far away from the current sampleand making efficient human in many casesokay so so the gist of idea is you haveparameters theta and they say that canbe high dimensional say fifty thousanddimension and the agency introduced amomentum variable R which is of the samedimension so it's fifty thousand moredimension you essentially double yourparameter sampling parameter space andhope that you would do better and oftenit does and now on very form a resultingjustice this this density which is goingto assume this has form of exponentialof your theta and the K of our and andthe U of theta is to be the negative logposterior distribution which is okay andthe K of R is going to have this kind ofthis form quadratic form half of ourchoice transpose R and you can see thatthis just camewas a normal kernel that's why thisthing it is aren't to be this way theirverse different terms for it butessentially the two things you shouldthink if one is a positional and one ismomentum so you think of theta W iscalled potential energy and Korrepresent kinetic energy and a term toremain to be constant because on physicsthat is all you remain constant youenrich in equilibrium so the key is howdo we put them in making them constantsand let's let's forget about this H h MCleapfrog this is a whole one actuallymoves in the Markov chain but let's justsee a picture of it so you have thetaand you have a density gives just up toa picture representation of the shapefestivities and I have theta zero and Ihave this l theta and again this is thispiece it's think of this is your thetathis is the potential energyokay and let's say that this ispotential energy and now I have a sampleof R of 0 okay this is the kineticenergy and how to do it how to do itlet's suppose I would if we now to do itand if there was that's what theLeapfrog same thing was for that weskipped in the previous two slidesthere's a formula how did itnow turn to to add together you create aHamiltonian equilibrium hey and now thekey is you gotta find a clever way ofmoving along this Hamiltonianequilibrium which translates it's notquite like this happen tonio but on highdimension it's like a contour we'rewrong the edge is all very theequilibrium so you can move anywherealong how to move along is the Leapfrogand you leapfrog run it to get to a newpoint and once you get to the new pointyou creates additional potential energywhich is a density variation and akinetic energy and you form a newcontour they let you sample and then theMarkov chain moves more rapidly becauseof this and here is a picture of it tothe left is a Hamiltonian Monte Carlo tothe right is a metropolis and it'sgenerated from a regression model youcan see that to the left the chain movesmuch more rapidly than to the right eventhough they all commit to the same thingeventually but the mixing is better andHamiltonian which others there are somedetails in the Leapfrog oh how manysteps that you have to do how how bighow wide of stepping the Leapfrog youhave to do and how many step you have todo now comes the no uten sampler sort ofautomated way of generating HMC sampleswhich i will go through in here and thisis one of the more popular becoming morepopular methods in sampling from a jointposterior distributionthe next part will discuss about somebenefits of sampling based methods sametime based measures final knowledge isnot the fastest algorithms in the worldby comparison there are much much slowerand then maximization and optimizationbecause optimization is nice very longsearch you want to go to the table vanyaand low e and quantification qualifyingof uncertainty came from coming from asin Horrocks and the formulas and easy tocalculatebut bayesian is responsible for samplingthe entire parameter space and thattakes time it's slow so but there areandnotable a lot of benefit in simply basedmethods and here I am going to providesome respectretrospective so the simulation basedalgorithm can provide like I said butthey just tend to be slow a moreimportant question is when you doinference what you want is a marginaldistribution so if the interest is on avariance you want the posteriordistribution of the variancedistribution here theta Y Sigma square Ygiven Y while the Gibbs samplingalgorithm generates samples from theJoint Distribution of everything so herejust this is the notation coming from anearly example of normal hierarchicalmodel sampling with all these parametersvia essence what you want he is P ofSigma square Y given y y integrate outall other parameters which can beexceedingly high dimension this is thenumerically this is a hard thing to dookay but at the tenth all is fine MGMT'sconcerned this is a tribute so here isthe idea of joint distribution that yoursample off on the beta so pairwise theycame from its own distribution how farin the beta this is this is alphaemission is dissipated division okay soyou have to bring a Markov chain issimple it gives you simple alpha and getbeta and you can send power and you getbeta and so on so forthand now the marginal distribution of fof X is in essence the collection of allthe other samplesand you can serve all were debaters thisis almost magical in the sense that youthrow away any Markov chain you throwaway since you don't want and you keepwhat you have you got your marginaldistribution so this numericalevaluation is trivial if you have aMarkov chain Monte Carlo chain gifts ofthe entire parameter space and you inessence you throw away all these samplesto keep what you have you got yourself amarginal distribution so yeah you canyou can you can there's a there's areference paper or there's a paperexplaining the get simpler CassandraGeorge in 1992 is I explain this thisphenomena and and so this this is whatsaves a day because for every any Markovchain Monte Carlo if one has to do someintense numerical marginalizationnobody would ever deal with MC MC but MCMC is you get to the sample from theJoint Distribution you get the rest ofthings for free in addition that you cancompute any functions of rendersdirectly such as a difference intreatment okay what that means is afollowing see magic in a classicalstatistics approach YouTube pluggingmethods if you want estimate alpha minusbeta okay so you plug in mo e and it'scall plugging method and now how do youdo with standard arrow is you have touse Delta methods central limit theoremyou gotta make sure that the tail goesdown correctly and things work well thebig ol and square over two and sand andcuba and squirrel will not let the tailgo down sufficiently fast fall from asample size but was MCMC you don't haveto deal with any of this you don't haveto deal with if this dysfunction itselfis exceedingly difficultit isn't clear would he be able tofigure out the clever way of to do theother methods and or notand with MCMC sample you just plug inyour plug in one by one and you get aposterior distribution of the entirething so for example if I have alpha anda beta and I can compute thedistribution of alpha minus beta just bypairwise of computes the function ofeach one and you can do is as complex ofdifferences as you want and you justplug in you have your function and youplug in every value and you get adistribution okay and then what theMonte Carlo average also lets youcompute the expected value as well youcan say the probability differencegreater than 0 given data is the numberof samples that fall to the right handside of 0 is a 49% so MC MC is you paythe difficult price ahead of time andonce it's done all subsequent showanalysis is almost trivial for thatreason that's why MCMC samplingalgorithm had been so popular becausethe stuff that you can do with it bypassa lot of needs to do difficultcomputation statistics right the lastthing I want to talk about is Markovchain convergence which is an importantaspect of it the convergence just meansthat the Markov chain need to get to thepowder distribution and it stay thereand sampled by proportion and of coursethe faster the betterit's called Quinn mixing and randomWalker algorithm can explore all theparameter space but it can power take along time to do ithowever as that's a question ofefficiency okay that's not fatal in thesense that as long as a Markov chain issampling the posterior distributionchoose appropriate density thoroughnessdoes not lead to biased estimate so sothe goal is until you tell me if youhave gathering of samples to accuratelyestimate distribution unfortunatelythere's really no definitive test ofconvergence but that's more or less issame true with all numerical methods andlike we never know we've reached aglobal maximum for example I mean thereis stabilises computational a same ideais running simple Markov chainDiagnostics that you need to have somenumerical support or visual support thatthe Markov chain has stabilizes and asampling from a stationary distributionand transpose still running supreme forthis reason you want to have adistribution marginal density the smoothand you want to trace prod thatstabilizes has whisks that go to thetail means explores the tail area and itcomes back to the high densityconfidence on and you can a verystraight spot you can have a good tracepaw on top and the second line probablyindicates you need some burning burningjust means that the initial value of theMarkov chain I need to be marketingvalue and you could discard it and thenthe remaining stays but if you do seesee a trace plot like this it'sindicating non convergence and youprobably need to deal with it modeling adifferent sampling algorithm and so onso forth that's a more serious problemwhat I kind of want to go through thecase is here if you ever trained if if aMarkov chain that looks between a goodand the bad something like this itstabilizes but it's moving slowly whatto do and do people after a lot ofpeople think that they want to do thingso the idea of thinning years to reduceautocorrelation so on top over channel50000 and as high autocorrelation to dothat and now if I think that by tenwhich means I keep one of every tensamples I go the first sample andelements and vote on a core sample Idiscard the remaining that the rest Ican reduce Oracle ratio all thecoalition just indicates where I am herecan Lawless tell where I'll be next butif it's all coalition zero means ajumping jumping randomly around iconsomehow produced so so it's just meansto change more slowly and you can dothis you can't you can't really singhere but on the other hand you got toknow that you are reducing the samplefrom fifty thousand to five thousand anda whilethis is have a decreasing ofautocorrelation means sampling sample inthe sample that less correlation andalmost pseudo independence you are throwaway ninety percent of the samples sothe thing is what introduced did thatsomething should do or not so there aretwo things to keep in mind one is youwant to ask are there any danger ofkeeping the remaining nine copies dothey do any damage to the end ifdelivering and it's unbiasness or changethe value of the expected value in yourMarkov chain estimates the answer is nookay all the coalition does body to biasthe marker essence - has been cooled onand it says paper that you can readabout it the second point that I want togo through is you when you think I canit's a very arbitrary number you'reessentially saying that I'm keeping 110percent of the sample any 10 percentwould do the remaining 90 percent isgiving away not because they are badjust because they are unlucky and anyeach one of them were just to be as goodas this current copy searching imaginedenied as a copies any other copieswould just be the same good asso they collectively would in fact bringsome information to you in estimatingthat in addition when you think aboutMonte Carlo the order doesn't reallymatter it's not asking how these samplescome to use the order matters inestimating the point estimates so far inthe barn the lodge you want to you youwant to think it is way that these arethe samples including them don't reallyhurt the estimate and excluding themreduce efficiency the general advice isunless storage becomes a problem you arebetter off keeping all the samples forestimation so that's to sing or not tosee I will skip this is quantitativemeasure of mixing this is a concept ofeffect sample size and I liked usingsynthesizes a way to quantifyconvergence but but this is somethingthat you can read and the next I will goon to talk about the soft world prop Bclaim and park emselves okay park p clipmy starter was a an overview of theprocedure so pork big lamb you can thinkof what we claim is a baby cousin to themixed and appendix procedure specializesin general as linear linear and thegenerous in image effects models but itjust does raising raising some supportit's just quick review of mix modelcontains fixing randomly parts and thenormal model the representation that weare familiar with is y is X betacosecant minor what can be a vector sothat being mixer repeats that means theerror terms are multi-dimensional andnormal and if this is a single dimensionthis which is a bonus and the camerasare the random effects in the beta's ofthe fixed effects fixed just means it'sthe same foror subject in the population and againmy just means you have cost us and as agammas as a cluster sigh indicates azero there is there's the G and it canbe called Aaron's shifty when in thefacts themselves are better informedwe call this G R the G side and odds aresidedistinguishing the two differentcovariance on the random side on avertical side in part mix we use randomstatement his specific you side and therepeated statement is important X forthe ridiculous comments the extensionwith generalized linear model again isthis you should same say same linearpredictor explained uh past Z gamma andthen links to the link function to theexpected value of the y lo g link forbilling one not and log link and thenthe likelihood is exponential familythat makes a channels linear fixedeffects model ding in proc mixed andproc Linux you integrate out the randomeffects to get the marginal likelihoodand in maximizing to get the solutionfor beta the Chi and the residuals areasked and that's how you do estimationand inference typically normalapproximation goes in quadrature and soon so forth to pay them you need tosound prior distribution and theparameters single generalizing in mixedeffects model our forecasts the betaarchaeology and are the cameras havenormal zero on variance G so then youhave to get prior for beta a private keyand probe lawand once you specify that if and then ifit's a generalized linear mix effectsmodel and you can send both on theclient posterior that's what the cargobrought to be claimed on it's justsimple Fonzi different joint posteriordistribution of such things Hey andpakka claim has seen simple syntax verysimilar to mixed an apartment and theessential three statements the largerones are the model the random andrepeats and we'll see examples of themyou specifyresponse variable and covariates fixedeffects response distribution linkfunction that's the job of the modelstatement and the random you add on therandom effects of Z and then you want toknow what type of the G sided covariancematrix may be their prior R and then ifyou have repeats you specify the outsidecovariance structure for the P statementyou have a car statement is acategorical variable you handle categoryverbal and then you have estimatesdiamond2 computes a linear combinationof parameters after the there's someadditional features of be claimed ifthere's a lot of covariance which theremight be 13 of them for both G side andoutside before and you can because youhave our side and you can do repeatedmeasurements and it supports balancedand unbalanced assembles the converseheterogeneity is a way to model you canhave different subgroups that havedifferent covariance structures that thesame within the group but differentbetween groupsOh covariance heterogeneity assignsdefault knowing formally prior or and ifyou if you think that the analysisrequires additional inputs or differentprior there's suite of priors that youcan choose fromhe also confused di see you can handlemissing data missing compare random ormissing at randomthe are the algorithms themselves simplealgorithms are paralyzed to utilizemulti-coreto speed up our computation here aresome example the if supervised is amodel specification and we started by asimple inning of a question was with allinteraction between two class variablesa and B this is just familiar syntax isthis a standard sass syntax for anyregression model was a bar b of Lintroduction between categoricalvariable a and a categorical variable Band a proctor p claim you have inputdataset and you have a random seed therandom seed is for reproducibilitybecause you're not Ubuntu if m2 to runsand you want the same numbers otherwiseOh generally different seeds you'll getdifferent different numbers fordistribution wise they should be thesame and we have some default priorthey're putting on the beta and a sigmasquare and then now we can change theprior things are indicated here usingyou using Google since we add on saylet's say I want the scale prior Sigmascore to be inverse gamma shape point orone scale is 0.01here's which is change it in here so PIof Sigma square Z inverse gamma and thenI'm adding a random intercept model thenormal regression was when an interceptis then I add a gamma J according tosubject ID and subject I did ID must bea cost variable here Heidi must be thereand is a covariance prior is the AUSA Iknow that she said G site prior for forSigma M r-squarethat's the gamma J is normal zero SigmaM R square and a PI of gamma square ishalf Koshi with the possibility and thenif I have the random intercept modelbecome a cost variable D in here andthen my random you can just changing theD to be here and it just becomes amulti-dimensional random effects theyhave a r1 structure to have this side ofthe Chi covariance matrix in here andthen you can add a repeater measurementsthat just hypothetically to say that mymother why repeats over hours and andthe repeater statement I have type equalto un and subject equal to person thenyou have a very easysignification of in this case so withoutlooking at the P claim a procedure nameper se the remaining is very similar tomixed procedure and clinics procedure inspecifying the kharsa model the randomrepeated statements fit in there onceyou switch to a different distributionyou can do poise on the differentdistribution it's the same idea you canchange different link function you canhave offset you have no intercept and soon you can add random statement with asubject equal to analysts in the Englishexample and you can have a coupe optionto model covariance solution ad that thefemale might have a different covariancestructure as a male and then you canhave nested on all nested random methodsthis is a you have one level for analystand whatever what one was in addressedso anything you can specify differentprior distribution on a combination ofdifferent things that you can also usefor Victor and next we will talk about alogistic regression so the idea here isto build up from simple to show you thesymbology to questions below just randomeffects model to repeat the measurementswith the head healthy Canadian is anextend to embarrass the repeated remarksthe logistic regression example isfairly simple by Nature it's just totest of treatment in the placebo orcompared response variable is whetherreported paying or not and thecovariance includes age and the genderof six patients and the duration ofcomplain and such and so onso the treatment of a B and a P and thegender is offender M in the female andthe male and as their the pain is yes orno so that's that's a set and thelogistical Wersching is considered to besimple rotate local link between gendertreatment and interaction between thetwoage and duration and there are dummyvariables for all categorical predictorsand now we're going to consider a normalprior with large variance as an onlyform of higher or aggressioncoefficients so the beta 0 to beta 7 8perennials so the following fits alogistic regression similar to what Ihad seen before and I'm just going toexplain them in a bit more detail thedata is input dataset random see numberburning is a thousand MV I then littleMarkov chain is 20,000 and I'm going tosame by two you don't have to sing but Ijust want to use it to indicate the howto use the syntax and then the output isequal to data set sent out it's thecollection of posterior samples of allthe model parameters the random effectsparameter and so on the model let's lookat a model statement so the modelstatement models of probability of painno pain and pain you can have the sametrick as we want to model the povertythe pain equal to yes then you specifythe descending keyword after theresponse in the model step and then theinteraction between gender and thetreatment and you can see the treatmentand the gender are all class variablesand then specify was a referencereference level and age and duration arethe cost are the continuous variabledistribution in the binomial and thecoefficient prior the beta prime normal0 and variance equal to 1 e to the 6power and the reference coding it's oneof the coding options in the prom optionin the customers to create a designmatrix for categorical variables and theoutput are the following you have moreinformation response binary logitfix effects yes is no random effectsjust yet and uses cameraman samplingautism with a thousand burning pointsout of simulation sighs - singing arandom seed and then most threats andmost righteous means that how many CPUsthat you are using to to do this but IIfor it is one but you can certainly useor utilizes all the CPUs that you haveavailable to your TV it produces thesummary statistics on the intercept thegender treatment and interestinteraction the term the age anddurationit has posterior main postural standarddeviation and 95%HPT intervals for all of them and thenit also produces these things as traceautocorrelation and a density partsbecause we simplify cortina tea-traysort columns as the plots of the traceall the correlation and density forevery parameter I just picked theinteraction term between treatment a andand it's actually equal signal and thenyou can see that the day as I havesymbolism you know just to be brushingyou can compute log of odds ratio thislog loss ratio is just a logit of x ofthe ratio between x equal to y equal to1 and y equal to 0 given X it's the logof this is just the linear regressionterm and suppose that you are interestedin how can the ratio of odds for afemale population to the male populationthen it's the difference is simply thebeta 1 coefficient if you work out theform you know the difference and thenthe the the odds ratioexponential of the log of odds and theexponential one you can just use theestimates team to compute the femalegroup and the male group between the twoby accounting for the treatment effectsand there's some explanation here whatis numbers one and negative onethings and how different values are forpeople who are familiar with estimatestatements then this is fairly standardstuff but if you are not you can maybewith the explanation and see how peopleuse estimate statement to compute thelinear combination of parameters andthen the slash of e gives the equationthe L matrix that's being displayed andthis is this is the estimate statementsay it's a coefficients is on the signas being all the weights of being usedon the coefficient and two computationand you can you can also have posteriorsummary statistics for a GOMparametrization the Chi informatizationis different from the summary statisticsof this reference parameterization Ithink this is just to indicate that Ican run the same analysis with GML GOMparametrization versus the referencefrom tradition and is supporting eitherway when notice that they are thingssuch as you know zeros these ourincluded in the model because it's a GMand the results of the log of odds ratioof a female and the male the reason Isay sorry this this probably had to comeout as appear to be nowhere but what I'mtrying to say is that some peopledepends on the parametrization somepeople see how to compute the log allodds ratio more familiar one way versusthe other but by the end of the day youcan compute the log of odds ratio to beone point six five between the femaleand the male group and you can you cancompute the odds ratio by exponentiatefrom the sample out data set of thefemale versus malea treatment a variable and this variableis what what this variable generates sothis statement generates a linearcombination of all the parameters andit's named f versus M at treatment athis variable is going to be saved as asample out so if you want not the logabout but the exponential of the log ofthe odds odds then you take these sentout data set and you just compute oddsratio to this explanation of thatvariable okay and then you canafterwards use the sum int mackerelsto computes same such as the posteriormean standard deviation and HPD's of theodds ratio or even but the density partso here is how you would do autisticregression and also infer on linearcombination of functions of parameterusing the results of the simulation thisis just a list of building Bayesianmacros that computes various differenttype of statistics and a Diagnosticsbased on the outcome of posterior dataset it also works similar here's a macroof the date data set and the variablethat some in computes summary andinterval statistics but you can you canuse you know effective sample size MCSEand so on to confuse different lazy andrelated quantities now we're going toextend this to a logistic random effectsmodel the logistic burn effect model isjust the random effects logistic wasrandom intercepting here in this examplewe are investigating two medicalprocedures we called a and B there are15 centers the center goes from 1 to 15and there are two rows - sorry longerdata set here 15 of them are given a andand the 15 of them are cumin be in thesame center and you know the number ofpatients and then the number of patientswho reported side effects data and we'regonna do similar logistic regression butin here we going to have the sideeffects rash and this is events and overevents type events or events of endssyntax automatically puts this into abinary likelihood with low chilling andcold is the categorical variable and aknowing option is used for you to bettercompare group a versus group II don'thave to account for the intercept andthen there's a random intercept modelwith a group subject very well okayagain it's the same saying here is themodel information table the posteriorsummary statistics table you have thegroup able to school be random var isthe measures variability of the centerlevel intercept this is the G sidevariance parameter for the randomstatement and at each centers interceptthey don't come out by default becausethey're 15 of them the men attempts itcould be even more so in the posteriorsummary and interval statistical tablethey only display the regressioncoefficients and G sides covariance oran outside if there are side play sinceit's a binary model with no residualvariance we only output- regression parameter and AG sidevaries term the random effects parameteror summary statistics don't come out bydefault but you can request them byspecifying a model monitor option in therandom statement same as the solutionoption in the M mixed procedure yeahit's a same same thing here is you cansee the trace part of Group A and thennow you can look at the probabilitydifference between Group A and Group Eby computing the largest logistictransformation of the group Bcoefficients - Group A and the computesof probability look at the summarystatistics of that distribution and jointhe density of that the the shade inrule or the area that of the probabilitydifference is greater than 0 and theprobability the probability that thedifference in color video greater than 0is greater than 99% it's a mouthful butyou can see that how easy it is to dobeta analysis to directly make astatement to say that the the common theprobability that there is a treatmentand then the treatment difference isgreater than 0 that probability is 9 anemphasis so it's part of the power ofthe beta analysis next I want toillustrate a case of repeatedmeasurement model was heterogeneity andthe repeated measurement model justmeans a patient or subject has repeatsover time and again this is a twotreatment trial for patients with alsowriters right Ryan no I'm sorry I do notpronounce that word brand annoyedunalloyed no sorryalso writers so there are 67 subjectswho enrolled and then they were followsthree time points one two and three andin the data you can see that thesubjects the first wheel rolls come fromsubject 26 at the time they had abaseline measure of the strength oftheir hand grip and then the retreattreatment one and then some receivetreatment to the male subject and thetimeline is 1 2 3 and then the grip is161 at time 1 to 10 times 2 & 2 3 & 3 sothese are the over the treatment timeyou want to see if the grip hasincreased over time as a result of thetreatment and then you can you can youcan fit a three-week interaction withvariable gender treat and time and alsothe baseline measure as a continuousvariable we can use within subjectheterogeneity so the instructor typeusing the residual side is the outsidecovariance matrix to see if says patternand such so the cerebral interactiongender treatment in a time is specifiedin the model statement the baseline is acovariance and we know intercept themodel now we add the repeated statementof time with the subject equal tosubject subject is and it's this is thefirst column so 26 is one subject 27 isone subject 71 and so 72 and and type isun is the covariance all sided type butthe repeat variable time must bespecified because time is used indicateswhether the key is taking place you theoption our and our core prints theestimated our matrixand estimate our correlation which thereare is a bit confusing but it's thestandard the R is not the variance tovariances IG side and all situs is ourside covariance matrix and alsocorrelation which shows and then and ifyou plot the data you can see thatthere's of course there's a genderdifferencefemale tend to you have weaker handgrips and male but the the strengthchange vary greatly over time probablymore with a variability in the malegroups and with a female group and thenyou can argue saying that maybe onesingle or repeated residual side of thecovariance matrix is not sufficient incapturing the two groups and you putthem into a a group equal option to takeagenda to say that each have a differentcovariance structure and now you can seethat for female you get one covariancematrix with represented in group so hereare the posterior density parts of thecovariance matrices of carve one one forfemale which is blue and a for malewhich is green I mean which is red andso the female has variance three around300 and the male have 900 900 and youcan see that the terms are different andthen the female have a diagonalvariances law has to be constant around300 but the male variance term increasesover time and you can see that thevariability across indicates a greatamount of variability as as you go alongtheI'm dimension of course you can then adda more between subject had heterogeneityby adding a random statement that has awhen an intercept by subject and a groupby gender wears a uniform covariancematrix and so on which I won't gothrough in explaining the output of thedetail but a big claim is enable you todo these things at East by exploringdifferent capacity of a generalizedlinear mix effects model by adding pieceof things and you don't have to takemuch of the computation but being inconservation so the last example forproc be claimed is unbalanced repeatedmeasurement bottles and I'll explaineven though comparing models syntax whyspecifying it's the same as a repeatingmodel the the procedure does a lot ofthings and the nice there's a cover hereI'm explaining how the unbalanced thingworks out okay this is again is a bitsnowed world Peter measurements modejust means the pieces are kind we haveseen before to set up to explaining howwe underneath doing handling theunbalanced the nature of the things youa bit set up and the set up is essentialreading indicates that why I is a vectorof dimension their covariance of R or isconsidered to be M by n colors metricsresidual and the P is a number ofparameters in the size and I have to sayI have I capitalized subjects and allcan certainly takes different differentshape different types not hey so in aregression model with repeatedmeasurements says parameter vectors ispaidtennouji there's no random effects forconsiderations idiot Adam it's not veryissue we have the prior where theposterior and then you sub sequentiallysimple beta giving oh and argument betaand this is this is okay this is just aa multi-dimensional linear regressionthings you have beta and the residualreligion data sampling that's okay andhere just to show there are these areall had known formula when is the motorvery normal when your inverse push itand that seems kosher now we are talkingabout unbalanced the things and bad itjust means that some subjects have somemissing values in in V which is thatthere are two two situations as far thedata representation is concerned one ison the left hand side you can see caseone time is one two three four subjectequal to 1 and I have covertsinformation X and then Y has somemeasure and there's some measure ofmissing so this is situation were I theX information or no you can think ofaccess code there with it's informationthat's no but somehow in some case Yjust cannot be observed okayand to the right hand side to the leftwhat I want to point out is left handside everybody still has subject samenumber of repeats in every sub subjectover time one two three four althoughit's not it doesn't have to always acase is that she's a second case to theright person one had measurement of ageat 8 10 12 14 and had had responsemeasure by subject person to missingmeasure from age 14 so have differentnumber of repeats three dimensionsso the difficulty associated withlaundering and variants repeatedmeasurements is that the dimensionchanges so for example in first personthis wide 3 second person wise to inthis case y y1 has 4 dimension what werethose three dimension and conjugatetypically is now available in regressionit gets to be complicated and that thealternative approach is to assume thatwe have ourselves a mission data poleokay that's the power of the Padian isthat you don't see things in here whereyou don't see why you can assume that Ycame from a missing value but theyactually normal distribution ofautomation or normal but you just have amissing value in here that can obtainsome information from within the groupand between the group information okayso we can treat the data that we haveseen as the missing data and then becomereferred to as missing a randommechanism and at least thing about it isthat it creates sets the residual oughtto be of the same dimension and it makescomputation easy so if you can note whyI is missing in the missing part of whyI observers of zero part of why I thenall the missing values are treated as arandom variable so they are sampled inthe Gibbs sample they are filled infirst giving the observed an X and thecurrent sample beta and are and thenonce a fill teen this becomes a completeregression complete the data set up forlinear regression which we sample thetheta and r sub sequential ok otherwords I have a missing value herebecome missing value parameter and theseparameters then become part of thelarger MCMC chain and the sampling ofthe missing value of y is the result ofa normal normal theory whatever normaltheory that's conditioned on theobserved colorants matrix and so on withme and the Sigma square in a normalregression if it's non normal then canbe sampled using Michalka sirhey steam it's a remaining of the betaand odd and careless booth is in thebalanced case for that reason okay sothis is the first situation if we don'thave we have only missing missing whybut the covariance made information isknown we can include the missingresponse mu and a sample missing why aspart of the Markov chainbut what if situations arise there'smissing covariance okayso here's example case 1 I have 1 2 3 4the fourth observation I am missing bothcovariance and what right some casesthat I can already right the missingcovarianceas a case to the right where the firstperson gender is female and secondperson the general mail and you canassume that this is just male and thenthe linear regression computers why butnow if I don't have this piece thelinear regression in here knew of themissing value depends on the covarianceinformation if this is missing thenobviously I cannot compute this value sothe trick is to look at the residualsinstead of instead of the covariance bymoving the regression piece from rightto leftand so the residual itself is missing bythe centered at zero instead and and inhere we with covariates missing we justmodel instead of the model missing valueof y which which is covariance and anepsilon additional missing value andsampled and say they are sent to a zerowith right covariance matrix as aresidual of the missing covariance okayand then everything carries through wecan see that they they in fact reach thesame same also and this is the plot thatone shows fitted line of the regressionline and the fifth line on the residualI will project everything here down tozero and you get the same answerfourth yeah the actual value of thecovariates become irrelevant because ofthe normal model that we have you canextension to the mixed model is verytrivial you add the random intercept orrandom random effects parameters in themodel to make it to be the same as alinear regression then it just addsadditional sample of the random effectsat Seaside and so on and the distancehow internal a proper people in handlesmissing data or missing covariance datain the residual missing cases you can gothrough this example but it's idea isthe same here where we have patientbaseline drug and our wizz data clearlyare not balanced here three three drugsfor different patients and there somepatients have the early and missing andsome have later ones that are missinghere just patient no profile well wecan have the unbalance the data okay sothey expand it to the balance data butwe don't need to being proc a big limbbut expanding this is just to show thatthe data have missing values in in hereand you can fill them in by the hourthe initial model that you can just fita regression model with our as repeatssubject as patient was injured pipe isun and the patient the nested effects ispatient within drug indicates therepeats occurred for each drug treatmentwheezing a patient of cell three times24 in totalgoing back this is the data that myfirst person is given given a drug a atimes but the same person is giving drugB and different treatments as wellthat's why there is a patient withindrug options we have different groupsand then estimated the outsidecovariance matrix for this again it'sall done under the another thing for thecorrelation matrix as well and you canproduce these things as a heat map ofthe correlation matrix and you can evenmodel different covariance structures ifyou want so so you can also indicatewhat I didn't show here is we can trydifferent type of the covariancestructures on the our side andidentifies which one is a reasonable fitbut hollow decide and you can use of theIC of the way to to handle it and thenadding the random effects itself is notthat difficult again you have the renderintercept and subject is patient withinjob it's the same thing by adding avendor statement then that produces theestimates and so on so that was the endof this example that's going to show andfor the next few minutes we will just goon and talk about more general sempronII always and protein seems this lastpart is on protium CMC and there I'mpushing very close to to our limit andthis this part will just be a very highlevel from going through what's in itthe the procedure by itself is now moreprogramming oriented procedurecompletely apart between Pro Tem andfixed a specific set of homes Jenpainting Chennai's being index effectsmodel that it has a lot of built-infunctionality so syntax specificationyou don't do a lot but MCMC is the onlyother side of the spectrum that you canfit many many different models but itrequires and then even supportsflexibility through support in the datastep program in which you can define ownsample function click on your home youngdistribution and if any prediction andalso on but it certainly requires acertain programming capability this isvery similar to end all mixed what I amgoing to go through for the remainingfive minutes is to highlight theseslides and hopefully the slides are selfexplanatory where you can go and look atthem at your leisure and try out some ofthe examples in using MCM to fitdifferent types of models for the firstparty to explaining what this is and thethrough a linear regression model toemphasize on a point that thespecification of a MCMC program shouldmimic how you specify the model on paperyou know the question is what rights youcan change different components of themodelcry distribution likelihood you canintroducing data step change point toregression model and so forth and thenext session explaining looping throughthe data sets whole program CMCaccumulateit's posterior distribution forcomputing and the prior statement meanswhat model statement means and there's adifferent hyper models that they can fixnot only of the simple ID model but alsofor data for different subsets of thedata or the correlation auto regressivetype of data and data that havedependency complex dependency amongobservations so what the produce ofprocedure produces that's similar to beclaimed and then the next section goesto the introduced syntax programmingstatementwhat is tribution what is tributing thatit does so I'm going through this veryhigh level including the samplinghierarchy how it is a multi threadingthe being done the palm statement theirfunctionality and of course they say howcan use a different option to choosedifferent Stephanie algorithm a study ofconvergence Diagnostics in here theyalso discuss you know things as a nutsalgorithm we had covered earlier andwhat the prior statement doesn't specifyprior distribution how the model formodel statement how you can use a modelsimilar for model multivariate data andthen how you can utilize programmingstatement and what to look out for inusing the programming statement and atthe type of distribution that issupports and some notes on the gammainverse camera and push yourdistribution three of Faerie confuse todistribution in their working withtruncated distribution working withcensoring data and working with arrayfor multivariate distribution and so onour finish with two example one isworking with non-standard distributionsa distribution that is neither normalbeta or poison or what now how tohow to do that and what to look for howyou can use it for for direct simulationto understand for example what the priorlooks like you know it's important ifyou have a prior you don't know what thelooks like we can just using em seems tosimulate the read play from the priorand you can simulate from complex jointdistribution a multi dimensionalsituation and then the last part is arandom effects model from the setup ofit to introducing how the setup relatesto the random statement in constructinga random effects in a model anddifferent type of random prize that youcan have in the random statement ofdifferent options and the importance ofthe subject variable how to understandit and making a link to some user whoare who comfortable with whim box andhow they link together the conceptualizeand the complexity of using flexibilitythere multiple random statement editoroffers with different usages and asimple example of three ways of codingthe same model with the joint modelingof two to outcome of one why to you cando long data format you can use a shortdata format and so on and this is theend of the tutorial I know the end is abit rushed but giving that we have onlytwo hour of the timeline there is Ihoping to provide as much information asI can and as always my if anyone wouldlike to get in touch with me my emailaddress is in here and you can just sendme an email if you have any questionsthat you have so I wish you all the bestand thanks for listening to the tutorialbyeyou"
94,"CHARLES CHASE: Hello, everyone.Welcome to the SAS
Virtual Global Forum 2020.My name is Charlie Chase.I am the executive
industry consultantin the SAS Retail/CPG
line of business.Today, I'm going to talk
to you about the grapeto grocery, SAS's demand
sensing and shapingcapabilities for a winery,
from farm to table.As the executive
industry consultant,my role is thought leader
and trusted advisor,responsible for delivering
analytics-driven solutionsto improve SAS customer
supply-chain efficiencies.I have more than 20 years of
experience in the consumerpackaged-goods industry, and
an expert in demand forecastingand planning, market response
modeling, econometrics,and supply-chain management.Today, we are representing
a fictitious winerycalled the Bonne Nuit
Family Winery and Vineyard.However, the data is
real and has been masked.The Bonne Nuit family
moved to Sonoma County,California five-generations ago.They feature a
high-end selectionof varietal wines, that are
made in a traditional Bordeauxstyle.The winery uses SAS
Analytics, Machine Learning,Visual Analytics and
Customer Intelligence,to forecast consumer
demand for their wines--as well as provide
sales and marketingwith real-time
consumer intelligence,delivering wine from the
winery direct to consumers.So how does SAS deliver value?SAS software is helping the
winery to consistently delivervalue, by helping the winery
balance demand and supply,ensure consumers find the right
products on-shelf across everychannel when they need it,
enable high service levelson customer orders, empower
sales and marketing withreal-time consumer
intelligence--improve product quality in a
rapidly changing digital world.And finally, bring
new products consumerswant to market in the
most efficient way.More and more people
are buying wine online.Did you know that
direct-to-consumer wine marketin the US grew by
over 11% in 2019,with sales of over $3 billion?And volume sales
were up over 9%,with wineries shipping over
6-million cases of wine directto consumers.This is a real growth market
within the overall winecategory.So let's see how the
Bonne Nuit marketingteam uses SAS
Customer Intelligenceanalytics to better understand
the consumer experienceand forecast demand.Using SAS Customer
Intelligence analyticsthe Bonne Nuit marketing
team can utilize analyticsto better understand their
consumers, and which consumersthey plan to target for their
overall product-marketingstrategy.The marketing team can
create customer segments.In this case, they have
created four segments,to determine more information
about wine consumers.Taking into account the results
of an A/B test run in the past,you can see different marketing
and promotional campaignsfor varietal wines being
offered by the winery.The marketing team can
utilize this informationon the website, to better
understand and discovernew segments of customers.The marketing team is then able
to visualize the informationrelated to the performance
of the A/B testing,gaining a better understanding
within different segmentsand overall campaigns--and information about them.As you can see, different
demographic informationis available.For example, they can gain
additional informationregarding average income,
revenue for the last 12 months,average age, as well as
other demographic informationabout each segment.Now the winery marketing
team can engage directlywith the consumer,
with the abilityto personalize that experience
driving consumers to pathwayto purchase.In this case, the
consumer can seedefault content regarding
what's trending,as well as other content.As the user begins
to click deeperinto the website to categories,
they find wine offerings.As you can see,
the website startsto track and personalize
a consumer's activity,on the left.When a consumer comes back
to the home-page promotions,varietal wines are featured.As users log into the
website on the left,they go from being an unknown
visitor to a known visitor,based upon the information
regarding consumer loyalty.We know that Dennis is now
logging onto the website.We know his email.We know his age and
demographic information.We also know he is a
high-value, loyal consumer.We are now able to tailor
the content on the websiteto content that we know will
resonate well with Dennis.We know he is not shopping
for other productsduring his visit to the website.So we now change the display to
high-end white wine varietals.As we're able to interact
and personalize the consumerexperience on the website, we're
able to understand and drivemore additional insights into
our consumer's preferences.Behind the scenes, data-driven
consumer segmentationis applied.The segmentation
takes into accountsimple things like gender,
more advanced thingslike analytically-driven
profiles,in addition to
past-response history.Now, we can create
and orchestratea multi-channel
communication chain.This includes website and
mobile, as well as email.The email and mobile
communicationsare personalized to align with
the system known as Next BestAction, based upon
Dennis's information.The white-wine page
view has triggereda multi-channel
consumer-communication journey,including website content
personalization relevant emailcommunications, and
mobile-application-contentpush.Using SAS, the Bonne
Nuit Winery hasbeen able to focus less on
transactions, or shipments,and more on the consumer,
taking advantageof the point of sale and
syndicated-scanner data beingcaptured from their
retail customers-- as wellas real-time digital
consumer intelligence.Using advanced analytics
and machine learning,SAS's to accurately capture
consumer-promotion liftsand forecast demand
is unrivaled,whether it's optimizing
the marketing programmingor measuring the impact
of consumer promotionsto accurately forecast
demand using point of saleand syndicated-scanner data--then supplying that demand
on-time and in-full,either directly to the
consumer or to grocery stores.The key differentiators
are the abilityto sense and shape demand using
advanced analytics and machinelearning, integration
and visibility across allthe planning functions,
understanding a consumerexperience to more-accurately
forecast demand.So how does SAS help the winery
accurately predict demandin this changing environment?So as a demand planner,
a marketing planner,at the end of each
forecast cycle,I'm going to review the actuals
and see how well we did.As the demand planner for the
Bonne Nuit Family red wines,I'm going to click
on the red wines--also, the Le Bijou red wines--to see how well we did
as of the last period.I'm going to check inventory
across the country.And as you could
see, we're buildingin maturing the Northeast and
starting to build inventoryin the Upper Midwest.I'm then going to look
at my forecast accuracy.And as you can see
in the Northeast,we have the lowest accuracy
and the highest error,as well as the
highest forecast bias.We tend to over-forecast by
around 7% and under-deliver.And when I go deeper
into this to seehow well we're forecasting
overall the Le Bijou red wines.And as you can
see, we're runningaround 81%, which is much lower
than all the other productgroups, who are in a
high 80s-- close to 90.We're also carrying or building
over 59 weeks of inventory.I'm want to drill-down deeper
to determine what regions.And as you can
see, we're buildinginventory in all the regions
except for the South--not just the Northeast.As a marketing
planner, I'm goingto look at how well our
promotions and pricing aredoing.And as you could see in this
waterfall chart on the left,it helps the marketing
team determinehow much demand volume is
associated with base volume,price, and promotion lifts.This chart is the result of
the statistical models thatwere created using advanced
analytics and machine learningby the demand analysts.The marketing planner
can drill downby product group and region,
to see how promotions and priceimpact each product group.And as you can see, it's not
impacting Le Bijou red winesas well as it should be.The chart on the right plots
forecast error and sell-throughrates.The planner can see that those
products with low forecasterror tend to sell through
the retail channel.And those products in the
upper-left-hand corner,which have higher-than-normal
forecast errortend to have low
sell-through rates.Here, the planner reviews these
exceptions and collaborateswith the marketing planner
and a demand plannertogether, to come up with a
plan to improve performanceon these products.Now, I'm going to
switch hats, and I'mgoing to be the demand
analysts and startto update all the models
for the new cycle.As a demand analyst, the
first thing I'm going to dois update the point-of-sale
syndicated-scanner models,because that's
driving shipments.So I'm going to click
on that product group.Here is the data
that's being used.And as you can see,
there's all typesof data-- pricing, promotions,
merchandising capabilities--even temperature, in this model.I'm going to go to the
pipe that we built.We call these forecasting pipes.And you can see we used
an open-source pipeto bring in a Python
random-forest model.We also built a SAS
random-forest modeland a linear-regression
model, and then combinedthe SAS forecasting
random-forest modellinear-regression model to
create an ensemble model.So let's see how well they did
competing against each other.And as you can see, the ensemble
model won the competition.And this is not a normal.In fact, that there
is research hasbeen done that says these
ensemble models combiningmachine learning with
traditional advancedtime-series models
actually outperformthe machine-learning
models by themselves.And this is the case.So I'm going to close out.And now I'm going to go
to my shipment pipes.And as you'll see, we have
all the shipment information,and we brought in
point-of-sale data.And we created three additional
point-of-sale factors,that pull forward one week,
two weeks, and three weeksthe point-of-sale
data, so it matches upwith the shipment data.Also, ran a different
type of pipe.And this time we ran
automatic forecasting.It always does bottom-up,
but uses all different kindsof advanced models--but only time-series models.Hierarchical
forecasting, which looksat forecasting and
hierarchy-- and then astacked neural-network
model with time series,which is an ensemble model.And again, in this case,
you'll see that whenwe look at the comparison that
the auto-forecasting modeloutperformed the ensemble model.And this is not unusual, either.So let's go see how
well the forecast looks.And we're going to go
to the forecast viewer.As you can see in
the forecast viewer,here's the impact
of the promotions.They're running a
holiday promotion.This is the first week and
this is the second weekof that promotion.And then here's some
carryover, whichwas modeled using the
machine-learning models.So now I'm going to switch hats
and become a marketing planner.And I'm going to go to
my planning workbook.As a marketing planner, I'm
going to open up my workbook.As a marketing planner,
I have my own view.And as you can see, I
have a point-of-sale dataof both total point-of-sale
baseline and promoted.And the baseline
and promoted arebroken out using those
machine-learning models.So that's done
through analytics, notthrough judgment.Here are all the
different promotions--the holiday promotion that
we're running, currently,that runs in the last
week of February, March,as well as other promotions.Also, the revenue
generated from that--baseline and promoted revenue--cost of goods sold, and margins.We want to maintain
at least a 42% margin.And we're pretty close
across the board.So as the marketing planner,
after this promotionhas run the second
week-- which isthe week of the 9th of March.And here's a carryover-- some
residual impact from that.That's also been modeled.I want to try to
assure that we sellthrough more of that wine that's
been building in the Northeast.So we're going to run
a 10% price reductionon a case of wine promotion.I'm just going to
put a 1 in here.1 turns it on and
a 0 turns it off.We're going to calculate it.And as you can see, that
adds an incremental 900 casesper customer.And the revenue went
from 118,000 to 130,000and we maintained a
44% margin from a 45.So we're well above 42.So this promotion not only
generates additional volume,but it also increases
revenue and it's profitable.So now I'm going to close out
and get approval for this.And once it's
approved, the impactwill automatically
be transferredto demand-planners workbook.So let's close out of here.And now I'm going to
log-in as a demand planner.It was approved to my workbook.As a demand planner, I have
a slightly different view.I don't have to
have the same view,but I do have a point of sale.And again, it's broken out.And if we go over
to week number 10,you can see there is the impact
of that 10%-off price reductionon a case of wine--not only on the POS data, but
how it's impacting shipments,as well.And here's the shipment volume.We also have open inventory
balance receipts comingin-- open and closing
inventory balance--also, weeks-forward coverage
based on the forecastand current inventory.And what we want to do
is we want to maintaina two-week forward coverage.And as you can see,
we're just barelyat one-week forward coverage.So I found out that
as the demand planner,I can expedite around 5,000
cases from the future weeksinto week number 10, to cover
that and drive and maintainat least a
one-to-two-week coverage.So let's do that.So the 5,000 cases is
going to give us 13,221.And enter that in and
run that simulation.So by expediting
that 5,000 cases,we're able to drive up
our forward coveragefrom less than one week
to well over one week,to protect against that
additional promotion of a 10%off a case of wine.Based on what we've
just seen, whatare the key takeaways
from the analytics?First, we're able to deliver
real-time digital consumerinsights at the edge.We have an end-to-end
consumer-engagement processto predict and plan demand,
through the personalizationand engagement with
winery consumers.And real-time
consumer insights frommulti-channel
communications combinedwith advanced analytics
and machine learninghas enabled the winery
to better understandthe consumer experience, to
more accurately forecast demand.SAS delivers real-time
consumer analytics at the edge,that the winery
can use to improvecustomer service while
maintaining high profitmargins.Uncovering real-time
digital consumer insights,from multi-channel
communications, combinedwith advanced analytics
and machine learning,enables a winery to better
understand the consumerexperience, to more accurately
forecast demand directto the consumer-- as well
as across traditionalbrick-and-mortar channels.We just walked you through how
the SAS Solution platform trulysupports an end-to-end
consumer-engagement process,utilizing advanced analytics and
machine learning to understand,predict, and plan demand,
through the personalizationand engagement with
the winery consumers.Thank you all for
listening today.You can contact me at
the email listed here,as well as on
LinkedIn and Twitter,if you have any questions."
95,"Hi, everyone.My name is Luna.And welcome to your
tutorial on exploringSAS Enterprise Guide 8.Enterprise Guide has a lot of
capabilities and functionalityto help you explore
and analyze your data.So whether you want to access
different types of data,prepare your data, use
point-and-click taskwizards to generate SAS
code, or write your own SAScode to help you analyze
and report on your data,or even if you want to take
advantage of Enterprise Guideproject features, which we'll
talk about in just a bit,Enterprise Guide has
many tools availablethat will make working with
your data a little bit easier.So whether you are
a SAS programmerand you'd like to
write SAS programs,or you'd like Enterprise Guide
to generate the code for youby using point-and-click
task, Enterprise Guidehas got you covered.So in this tutorial, we're
going to look at both sides.We'll start by navigating
through the interface.And then we'll take a look at
the point-and-click aspect.So we'll see how we can prepare
our data using a robust querybuilder.And also how we can
generate reportsby taking a look at
the bar chart task.Then we'll take a look
at the programming sideand take a look at some of
those programming featuresthat Enterprise Guide
makes available.So whether you are a new user
to Enterprise Guide or maybeyou're just new to SAS
Enterprise Guide 8--you've used previous versions
of Enterprise Guide before,but 8 is new to you--there is something in this
tutorial for everyone.So let's go ahead
and jump right in.Before we take a look and
demo these different features,I want to get some
of that foundationdown and go a little
bit behind the scenes.So first off is, how does
SAS Enterprise Guide work?Well, keep in mind
the EnterpriseGuide is an interface to SAS.So SAS itself may be
on your local PC or itmay be on a remote server.Either way, Enterprise Guide
is going to prepare your code.Now this code may be something
that you wrote yourselfin a SAS program.Or it may have been
generated for youthrough those
point-and-click tasks.Either way, Enterprise
Guide prepares a codeand sends the code to
SAS for processing.SAS will go ahead and
execute that code.And then it's going to go ahead
and deliver the results backto Enterprise Guide
for you to view.So that's just a little
bit of the work thatgoes on behind the scenes here.Now for us, what you're
probably used to if you useEnterprise Guide before is
this concept of project.A lot of the work done
in Enterprise Guideis in this concept of a project.And a project is
really a collectionof different types
of files that youwork with in Enterprise Guide.So this could include shortcuts
to data, your SAS programsand log, task and
query, so those orderspoint-and-click
tools, any resultsthat you get from your SAS
programs and your task queriesand wizards, and maybe even
some informational notesfor documentation as well.With projects, all of these get
packaged into one single filewith the .egp extension for
Enterprise Guide project.Now if you're working with
task and wizards and queries,then an Enterprise Guide
project will be required.However, starting with
Enterprise Guide 8.1,if you are a SAS programmer
and you want to really just useEnterprise Guide to open
up different types of dataand write SAS
programs, you actuallyhave the option to do that
without using a project at all.But keep in mind that projects
do have their advantages.Remember, projects are not
just about saving all the filesin one single file.But it allows you to control
the contents, the sequencingand updating of the
items within a project.So it's a really great
organizational tool.With some of that
foundation down,let's go ahead and see
how we can take advantageof some of the features that are
available in Enterprise Guide.Let's go ahead and
get started by talkingabout how to navigate through
the SAS Enterprise Guideinterface.I have Enterprise
Guide opened up,and I'll specifically be working
with Enterprise Guide 8.2.If you want to know which
version of Enterprise Guideyou have, you'll want
to go to Help and selectAbout SAS Enterprise Guide.Next to Version, you'll
see your version numbers.So again, I have 8.2.But if you have 8.1,
it'll look reallysimilar to what I have here.I'll go ahead and click OK.And let's talk about the
layout of the interface.At the top is the main
menus and toolbar.On the right-hand
side, which takes upmost of the Application window,
is going to be the work area.And on the left-hand side,
we have a navigation area.Let's talk about the work
area and navigation areain a little bit more depth.With the work area, as you
open up tables and SAS programsand you run tasks
and you want to takea look at those
results, you'll seethem listed as individual
tabs in the work area.So this is a
tab-based interface.Now by default in the work
area, you'll see the start page.And this is just a great
way to get started.You can quickly create
new programs and projects,open up existing files, access
Enterprise Guide learningresources, or even
access to SAS community.In addition, as you open up
different types of files,you actually see them
on the start pagebecause you'll see a
list of recent items.And you can even pin items
at the start page as well.So that's a little bit
about the work area.Let's talk about
the navigation area.By default, you'll
see seven panes here.The first pane is
the Project pane.So if you're working in an
Enterprise Guide project,you'll see that
project listed hereand also the contents
of project as well.Next is the Open Items pane.And the Open Items pane will
list all the tabs or filesthat you have open
in the work area.You can use this pane to quickly
save all the files, quicklyclose out of all those files.It's really a great
organization and managementtool for everything that you
have open in the work area.Next is going to be to
Git Repositories pane.This provides access
to basic featuresto track changes or
manage version controlamong multiple users.And this is a new pane starting
with Enterprise Guide 8.2.Coming down, we have
the Servers pane.This displays a
list of servers thatare known to Enterprise Guide.We have the SAS Folders pane.This will display a list of SAS
folders that you can access.The Task pane, so all
those point-and-click tasksthat you can access.And finally, the
Prompt Manager paneto go ahead and manage all
the prompts that you have.And this will allow for
user input in SAS programsand tasks.So that's a little
bit about the layout.Let's go ahead and start
by adding in a tableto Enterprise Guide.So I'm going to go ahead
and select the Servers pane.I'm going to go ahead expand
the name of my server, whichis called local.Libraries, and from
the SAS Help Library,I'm going to locate
the Class table.Let's go ahead and double
click on the Class table.And that opens up the table in
a separate tab in the work area.Now one neat feature starting
with Enterprise Guide 8.1is the ability to freely move
around the panes and tabsthat we have.Now to do this, you can
right-click on the tab,like Class, and select Float.Or you can simply drag that
tab to wherever you'd like.You can move it around freely
within the Application window.If you have multiple monitors,
you can drag this outto a separate monitor.Or you can take advantage
of the layout guideto dock it in the bottom
half of the work area,or the right half, for example.Now to dock this back into
its original location,you can right-click
the Class taband select Dock As Tab Document.Or you can simply drag the Class
tab back to where it came from.So this is a really
great featureif you want to program and look
at your data at the same time.You can take advantage of this.Or if you just want to
look at multiple tablesat the same time.You can take advantage
of this as well.Next, let's go ahead and
open up a SAS program thatrelates to this Class table.To do this, you can go
to File, and then Open.But that same Open icon is
available on my toolbar.So I'm going to go ahead
and click on that Opena File icon, which is
just a quick shortcut.I'll go to my computer, Browse.And I'm going to open up my
Enterprise Guide 8 demo 1 SASprogram by selecting it
and then clicking Open.So here's my SAS program.And probably the
first thing you noticeis that my code is on
the left-hand side.And then my log tab is
on the right-hand side.Now these sub-tabs, just
like my tabs and the panes,can be moved around freely.You can drag them around
to wherever you'd like.But if you want a certain layout
for all your SAS programs, whatyou can do is go to View and
select Program Tab Presets.Now right now we've got that
one-to-one vertical split.But I'm going to change
this to Standard.And that way, the Code tab and
the Log tab are now together.With that, let's actually go
ahead and run this program.To run the program,
I can click on Run.Or you can use the F3
key on your keyboard.But I'm going to click on Run.The program executes.And now we have a Results
and Output Data tab.But by default, we see the
Results tab have has opened up.With the results, you'll
notice that the default outputformat is HTML.And if you'd like to zoom
in on the results here,you can go ahead and hold
down your control keyand scroll up on your mouse.That'll make it a little bit
easier to see the results.Next, let's go ahead and
look at the Output Data tab,so I'll click Output Data.I simply made a copy
of that Class table.So I Hugh I see that
represented here.And I want to show you one cool
thing that you can do here.Let's say I'd like to open
this table up in Excel.A really quick way to do
this is to right-clickthe name of the
table, which I callit class metric,
select Share, and thensend to Microsoft Excel.So this is going to go ahead
and send that data overto Microsoft Excel.And after a couple of
seconds, here we go.I have that same exact
data now over in Excel.And I could go ahead
and save this fileor make further modifications
to it from within Excel.I'm going to go ahead and
close this without saving it.But just know that that
feature is available.Let's go ahead and take
a look at the last tab,which is the Log tab.The Log tab is how we see
messages returned backfrom SAS.And at the top we
have our log summary.And this is just
a really quick wayto see any errors, warnings,
or notes that mighthave been returned from SAS.So that was a really quick
look at the interface.Now that we know how to navigate
through to SAS Enterprise Guideinterface, let's go ahead
and start talking about someof those point-and-click tasks.The first one I'd
like to show youis a querying tool
called the query builder.And the query builder allows
us to manipulate our data.In this demonstration,
I'll show youhow we can use a query builder
to subset our rows and columns,create new columns, and
filter and sort our rows.Now in order to use
the query builderor other point-and-click
tasks and wizards,a project is required.So I'm going to start by
creating a new project.I'll go to File, New,
and then select Project.Now you'll notice that as
soon as I created a project,a process flow was
automatically created.Now to help explain this, I'm
going to back up one step.I like to think of my
projects as the entire storythat I'm trying to tell.Because, remember, if you're
working in this conceptof a project, all the files
that you open and work with getsaved in that single .egp
Enterprise Guide project file.Now within projects
or within your story,you tend to have chapters.And that's what
process flows are.They help us break up
and organize all the workthat we're doing inside of
our Enterprise Guide project.In addition, as we start
adding files to our projectand we start writing
tasks and programs,you actually see those
items representedin the process flow.And you'll even see the
relationship among the itemsas well.We'll see that in
just a little a bit.So that's a little bit about
projects and process flows.Let's actually take a look
at the query builder now.I'm going to start by adding
some data to my project.So I'm going to use
the Servers pane.I'll expand Servers,
my server name,which is local, libraries.And from the SAS
Help Library, I'dlike to work with
the cars table.Before I open this up,
on a little side notehere, if I right-click
on cars, you'llnotice a default
behavior, which is bolded,is to open and add the
table to the project.Meaning, if I right-click
on cars in the Servers pane,it will physically
open up the file for meto view in the work
area, and the cars tablewill automatically be
added into my project.But that's not the
only option you have.When I right-click, I can also
choose to just open the file,meaning open the
table for me to viewbut don't add it to my project.Or I can choose Add
to Project, whichadds the file to the project
but will not open it upfor me to view by default.So again, you have
some options here.If you want to change the
default behavior, justto show you where that is.You can go to Tools, Options.And then in the
Options window, you'llselect Project
and Process Flows.And you can clear
this option thatsays, when an item is
open, automatically addto the project, meaning
that if you open up a file,it will no longer be
automatically addedto the project.You'll manually
have to add itemsyou want to to your project.I'm going to stick with
the default behavior.So I'll go ahead
and click Cancel.But I just wanted to show
you that option exists.All right back,
into Servers pane,I'll double click on cars.And again, that opens up
the table for me to viewand it adds the cars
table to my project,specifically to the
active process flow.Because I only have
one, it's goingto get added to that
single process flow.Now with this example,
what I'd like to dois calculate the
profit of sedans.So that's going to require
some data manipulation.Let's go ahead and
start the query builderby clicking on the Query
Builder button on the Data Gridtoolbar.All right, here's
the query builder.Now the first thing I like
to do is give my querya more descriptive name.That's going to be the label
you see within your projectand process flows, and also
change the output tablename as well.So for the query
name, I'll go aheadand delete the default name
and call it cars profit query.And then I'll click Change and
change the output table name.Let's call it cars_profit.And I'll go ahead
and click Save.Now on the left-hand
side, you'llsee that I have all the columns
from the cars table listed.And if I want to see
any of these columnsin my output table,
cars_profit, Ineed to include them on
this Select Data tab.And there are several
ways you can do this.First, let's say I want
to add a range of columns.So I want to add everything
from make all the way downto invoice.To do that, I can select
Make, hold down my Shift key,select Invoice, and then
drag all of those columnsonto the Select Data tab.It can also pick and
choose individual columns.So for example, I can simply
double-click on MPG city,or I can drag MPG highway
as another exampleonto the Select Data tab.You can even use your Control
key to select multiple columnsand bring them on to the
Select Data tab as well.Now I again want to
know the profit value,but I don't have a column that
contains that information.I need to calculate that myself.So to create a new
column, I'm goingto click on this Calculator icon
on the right-hand side, whichsays, add a new computed column.This is going to start the
new computed column wizard.And it's just four quick
steps to define a new column.The first step is to
choose what type of columnI want to create.This is going to involve
an arithmetic expression.So I'm going to choose Advance
Expression and click Next.In step 2, I'm going to
specify the expressionfor this new column.Now, I can directly type in the
expression in this expressionbox.Or I can choose columns,
functions, and operatorsto include in that expression.I'm going to go with
the latter method.So in order to
calculate profit, it'sgoing to be the difference
between MSRP and invoice.So I'm going to expand where
it says Selected Columns.These are all the columns
that I brought overonto the Select Data tab.And to include, let's say, MSRP,
for example, in the expressionbox, I can just
double-click MSRP.That gets added
into my expression.Then I need to subtract.So I'll click on the
subtraction operator.And then double-click
on invoice.That's it.Now this might be
a little bit small,so if you want to hold
down on your Control keyand scroll up on your
mouse, you can easilyzoom in on that expression.Now you'll notice that in front
of the name of the column,there is t1 dot.t1 is a table alias or nickname.So it's a nickname
for the cars table.It doesn't hurt to
have in this scenario,but it's also not necessary.So if you wanted to delete
that, that's totally fine.But that is it for
this expression.So I'm going to click
Next, move on to step 3.Step 3 is where I'm going
to specify some propertiesfor this new column.So for example, the column name.Instead of just leaving
it as calculation,I'm going to call it profit.And because I like to display
these values as currencyvalues, I'm going
to apply a format.So in the Format field,
I'm going to click Change.Again, it's going to
be a currency value.So I'll select the
Currency category.Let's go with the
dollar w.d format.The dollar format is going to
add dollar signs and commasto our values.And I'm going to go ahead and
increase the overall widthto 10 just to make sure
there's enough roomto include the dollar sign,
the commas, and all the valuesthat I want to display.That looks good.I'll go ahead and click OK.And that's all I
need to do in step 3.I'll click Next.Step 4 is just a quick summary.Everything looks good,
so I'll click Finish.You'll notice that the new
profit column is automaticallyadded to my Select
Data tab and isincluded as a column in
that list of columns.It's under Computed Columns now.Because it's included
in this list of columns,I can now use profit
to filter my dataor even sort my data as well.Before I move on to the other
Filter Data and Sort Data tabs,one thing I'd like to point
out about the Select Data tabis that the order that
the columns are listedis the order the columns will
appear in the new output table.So you can go ahead and drag
the columns around in the SelectData tab to change the order.Or you can take advantage of the
arrows on the right-hand side.So to show you, I'm
going to select Profit.And I'm going to click on
this Move Up arrow twice.So I can have profit
immediately after invoice.And again, you can move
these around freelywithin the Select Data tab.All right, I have all
the columns I want.Now let's filter our rows.I'm going to click on
the Filter Data tab.And remember, I'm only
interested in sedans.So in order to
create a filter, I'mgoing to go ahead and
filter on the type column.So I will drag type on
to the Filter Data tab.The new filter wizard opens up.Two very quick steps here.I want type to be equal to--and I want that
value to be sedan.Now I could type that in here.Or, an easier way, is to
click this dropdown arrow.And on the Values
tab, click Get Values.And what this does is SAS is
going to look at the input carstable and provide us with a list
of values for the type columns,so the unique values that
are in the type column.There's sedan.So I'll go ahead
and click sedan.And my filter is good to go.So I'll only see cars where
the type is equal to sedan.I'll click Next.Step 2 is, again, just another
property verification page.Everything looks good.So I'll click Finish.One last step is to go
ahead and sort our tables.So I'll click on
the Sort Data tab.And let's go ahead and
start on that new columnthat we created, profit.So this time, I'll just
double-click on profit.That gets added to
the Sort Data tab.The default sort
direction is ascending.I'm going to go ahead
and use a dropdown arrowand change that to descending.So I'll see the cars with the
highest profit listed first,and then it'll go
down from there.That is everything that I wanted
to do in this query builder.So let's go ahead and click Run.And let's take a look
at our new output table.All right, so here's
our new output table.I only see the
columns that I chose.You'll see the profit column is
the difference between the MSRPand invoice columns.And this was sorted
by the profit column.So I see those rows or those
cars that had the higher profitvalues listed first.And then it goes
down from there.And of course the
filter I appliedwhere is equal to sedan.So it looks pretty good.Now since this is
a new output table,I could take this output
table and use it as an inputto other analytical tasks to
further analyze, let's say,the profit of these sedan cars.One last thing I'd
like to show youis go ahead and click
on the Code tab.These point-and-click
tasks or querying toolsthat we use in Enterprise
Guide generate SAS codebehind the scenes.And here we can actually
see that SAS code.If you want to make
further modifications,you can always click on Modify
Tasks to go back into the taskitself.Or you can make a copy of
the code that you see hereand add in some of your own
code, if you'd like as well.So that's a little bit
about the query builderto help us manipulate our data.In this next
demonstration, I'd liketo show you a different
point-and-click task.This time, we'll see how to
create graphs, specificallybar charts using
the bar chart task.We'll see how we can really
customize the look of the barchart, how to add in code
into the task-generated code.And also how to modify
the output format.Now for this
demonstration, I'm goingto start off where we left off
in the previous demonstration,which is where we used
the query builder.In the query builder, we
created the car's profit query,and we subsetted the cars
table to create cars_profit.Now in cars_profit, I'm
only looking at sedans.And I also calculated the
profit value of each car.What I'd like to do
in this demonstrationis to create a bar chart
based off of this table.I would like the bar chart
to show the average profitvalue by the car make.And I'd also like
to have separate barcharts for each origin value.So one for Asia, one for
Europe, and then one for USA.Now to get started
in this bar chart,I'm first going to make sure
that I have the car's profittable opened up.If you closed out of
it, in the project paneyou can double-click
on cars_profit query.And that will open up that
cars_profit table for you.With the table open, I'm
going to go to the Task pane,expand the Graph category, and
then double-click on Bar Chart.This will start
the bar chart taskwith the cars_profit
table as input.Now the first thing to
do in the bar chart taskis to specify what type of
bar chart I'd like to create.I would like a
vertical bar chartwith different colored bars.So I'll select
Vertical Colored Bar.Next, I'm going to go to
Data in the Selection pane.In Data, I can verify the input
table, so that's cars_profit.And if I click on Edit, I can
actually apply a simple filterto my input table.So I already applied
a filter to my tablefrom the query builder, so I
won't need to do that here.But just know that you
have that option available.Now also in Data, I need to
assign my different columnsto different task rules.These task rules
determine how the columnwill be used in this task.If you want to learn more about
this particular task rulesor you want to learn more about
the different options thatare available in
the bar chart task,you'll want to click
on the Help buttonin the bottom right-hand
corner of the Task window.You will see this Help
button in many of the tasksthat you use in
Enterprise Guide.And it is going to
be context-sensitive.So because I clicked
on that Help buttonin the data panel of
the bar chart task,I am taking to a
documentation pagethat's specific to the data
panel of the bar chart task.So as you see here, I see
those different task rules,along with a quick description
of those different task rulesas well.If you scroll through
that Selection paneon the left-hand
side, you see allof those different
tasks that areavailable in Enterprise Guide.So definitely take
advantage of Helpto learn more about the
task that you're usingand all the other
tasks that we haveavailable in Enterprise Guide.For now, I'm going to go
ahead and close the browser,go ahead and close
the documentation,and go back into
the bar chart task.Now again, we want to
create a bar chart thatshows the average profit value
by make, with separate barcharts for each origin value.So I'm going to go ahead and
assign make to the columnto chart rule.And to do that, I can
just drag make and drop itonto that column to chart rule.Now I want the bar height to
be the average profit value.So I'm going to take profit
and drag it to the sum of rule.Finally, to have separate bar
charts for each origin value,I'm just going to show you
a different way of assigningcolumns to task rules.I'm going to select origin,
click on the plus arrow,and then select Group Charts By.So that's just another way to
assign columns to task rules.Now as a sum of rule task name
implies, or the task rule nameimplies, sum of, this is going
to go ahead and by default makethe bar height the total profit.But I want the average
profit instead.So to change that, in
the Selection pane,I'm going to Advance.So this is Advance under
the Appearance category.I'm going to change the
statistic used to calculatethe bar to average.And I'd also like to see the
average value representedin the bar chart.So I'm going to select
the checkbox thatsays specify one statistical
value to show for bars.And use the dropdown menu
to change that to average.Last thing for now
is to specify title.So I'll click on Titles
in the Selection pane.I'll clear the Use
Default Text checkbox.And I'm going to delete the
default title of bar chart.And let's go with
average profit by make.Let's go ahead
and give it a run.I didn't make too
many customizations.But let's see what we
have, and then we'llmake further customizations.So I'll click Run.Let's take a look at
our bar charts here.So you'll see I have one
bar for each car make.And that bar height represents
the average profit value.As you can see,
the vertical axisis labeled as profit
mean right now.I see my title,
average profit by make.And I have three
separate bar charts,one for origin Asia, another for
Europe, and at the very bottomI have one for USA.Now if you take a look
at my USA bar chart,you'll notice that the labels
aren't properly displayed.My make value, some of them
are a little bit too long.So they don't properly
fit in my horizontal axis.I want to go ahead and modify
this to properly displayedthe values.So to go back into the bar chart
to make further customizations,on the Task toolbar I'm going
to click on Modify Task.This will go right back
into the bar chart taskwhere we left off.So we can make those
extra customizations.Let's start with those
horizontal axis values.To make some
changes to that, I'mgoing to select Axis under
the Horizontal Axis category.Again, this is about the values.So I'll select the Values tab.And I'm going to use the
Value Rotation dropdownmenu to change that to 45.This will display those
labels at a 45-degree angle.And that'll make those values
fit in a little bit nicer.Let's go and make some changes
to the vertical axis as well.So I'll select Axis
under Vertical Axis.And remember that the default
label was profit mean.I'm going to simplify
this to just profit.While we're here,
let's go ahead and addin some reference lines.So I'll click on Reference
Lines under Vertical Axis.I'll select the Use
Reference Lines checkbox.I'll change the style to dashed.And for the color, I'll go
with a lighter gray color,specifically gray 25%.So I added in some
extra customizations.I'll go ahead and click Run.And let's see our
updated bar chart.We see our nice label on
our vertical axis, profit.We see the values of
the horizontal axisare now at a 45-degree angle.And we see the reference lines.It makes it a little bit
easier to see our values.I'll scroll down and
check the USA bar chart.And we see that those values
fit nicely now that they'reat a 45-degree angle.I'd like to make a
couple more last changes.You'll notice that in each bar
chart there's this extra title,origin equals Asia,
Europe, or USA.Now I didn't specify this title.This is added in
by default when youuse that group charts by rule.What I'd like to do instead is
to go ahead and get rid of thisextra title-- this
is called a byline--and represent the origin
value in the title directly.Now to do this, I'm going to
make some changes to the title.But I also need to make
some changes to the codethat the task generated as well.So let's go ahead and go
back into Modify Task,make a couple of
last changes here.First, let's make some
changes to the title.I'll click on Title.And I'm going to go ahead
and extend the title.So I'll say, average profit
by make in, pound sign,by val, and in
parentheses, origin.So this is interesting notation.Whenever you have
pound sign, by val,and in parentheses the
name of the group chartsby column, which in
our case is origin,that entire portion will be
replaced with the origin value.So I'll see average
profit by make in Asia,in Europe, in USA
in my title instead.Now because I'm representing
that origin value in the title,I no longer need that byline.Now to get rid of
the byline, thatis not an option that's
available in the bar charttask.But it is something
that I can do with code.So to add in just a little bit
of code to make that change,I'm going to click on
where it says Preview Code.Now by default in the code
preview for Task window,I'm really just
viewing the code that'sgenerated behind the scenes.However, if I select the Show
Custom Code Insertion Pointscheckbox, as I scroll
through the code,you'll notice sections that
say Insert Custom Code Here.If I click on that,
it allows me to typein some code in
predefined areas.So I'm going to take advantage
of that to go ahead and turnoff the byline.In the very first
insertion point,I'm going to click where it
says Insert Custom Code Here.And I'm going to type
an option statement.I'll say options, and
I'll say no byline.This means to go ahead
and turn off those bylinesand get rid of them.Now when you turn off
an option like this,it's usually a good idea to
turn it back on at the very end.So I'm going to scroll to the
very, very bottom of the code,look for the last
insertion point,click on where it says
Insert Custom Code Here,and I'll type options byline.This is just reinstating
the byline for future tasks.All right, the
code is good to go.I'm going to go ahead and
click the X to close outof this window.And that'll save the extra
code that I typed in.I like to do just a couple
more things while I'm in here.First is changing the
label of this task.Now the default label
is just the nameof the task, bar chart.But if you had multiple
bar charts in your project,it would be hard to
distinguish each of them.So I'm going to make it a
little bit more descriptive.To do this, I'm going
to click on Propertiesin the Selection
pane, click Edit.And in the Label field,
I'll delete bar chart.And I'll just type
average profit by make.Another thing that I can do here
is change the output format.The default output
format is HTML.But maybe I'd like
something else.To make that change
in the Selection panein the Properties window, I'm
going to click on Results.Again, the default is HTML.But I'm going to click on
where it says, Customize resultformat, styles, and behavior.And now I have a lot of
different options for my outputformat.What I'll do in this scenario
is, in addition to HTML,I'll also select PDF.Now next to each of
these output formats,you'll notice there's
a dropdown menu.This dropdown menu allows
me to pick a style.Now a style is going to
control the overall appearanceof the bar chart.So the coloring that's used,
the font size that's used,and more.Just to show you an example of
what these styles look like,for HTML, I'm going
to use a dropdown menuand select the Meadow style.And for PDF, I'll
use a dropdown menuand I will select
the Seaside style.OK.I'll go ahead and click OK.And those are the last
changes I'm going to make.So I'm going to go
ahead and click Run.Let's take a look at
our final results now.Here is the updated
estimate results.Now first, notice the title.I see average profit
by make in Asia.If I scroll down in
Europe and then in USA,I no longer see the byline
because I turned that off.And you'll notice that because
I applied a style, the Meadowstyle, the color scheme and
just the overall look of the barlooks a little bit different.Let's take a look at
those PDF results.In the Results tab,
there's now a tab for PDF.I can go ahead and
double-click on that.This will open up the PDF file
outside of Enterprise Guidein the default application.And now I have the same exact
result, but in a PDF file.So here's Asia.If I scroll down, I have Europe.And scrolling further is USA.And this has that Seaside
style applied to it.All right.Let me go ahead and
close out of this tab.And I'm going to
go ahead and closeaverage profit by make,
cars_profit query, the carstable.And I want to take a look
at the process flow now.I mentioned way earlier
that the process flowshows us the whole story.And we can see that here.We see that the cars
table is the inputto the cars_profit query.And the query created an output
table called cars_profit.Then we took cars_profit
into the average profitby make bar chart task.And then the bar chart
created two outputs.One was in an HTML format.And then another
in the PDF format.Now with this
process flow, there'sa couple of options
I can use here.Let's say maybe a
table is updatedand I'd like to rerun
tasks related to it.I can take advantage
of Run Options.So for example, here if I
click on Cars_Profit Query,I can click the dropdown
arrow next to Runto see my different
run options here.Run Selected Items, running
just the item that selected.Run Process Flow runs
everything in the process flow.I can do a Run To
the Selected Item,or a Run From the Selected Item.So again, if I need to update
something in the project,I don't have to run
my entire project--I can just choose to run
portions of my project instead,which is more efficient.Just to show an
example, I'm goingto click on Run Process Flow.And you can see that
the entire processflow runs in the order of
the arrow that I have here.So that's how to use
the bar chart task.I'm going to go ahead
and close this project.I'll go to File, Close Project.There's no need to
save the project,so I'll click Don't Save.I want to go into the
next demonstration.In this last
demonstration, I'd liketo switch gears and talk
about the programmingside of Enterprise Guide.I'll show you a couple
of programming features,such as formatting code,
using the data step debugger,and taking advantage
of autocomplete.Now you might
remember that if youwant to use Enterprise Guide
to just open data and writeSAS programs, projects
are not required.So I've already closed out of
the Enterprise Guide projectthat I was using in
previous demonstrations.However, you can
include programsas part of your
Enterprise Guide project.So just keep that in mind.To start off on
this demonstration,I'm going to click on the Open
a File icon on the toolbarto open up an
existing SAS program.I'll go to My Computer.And because I've already
accessed the filefrom this specific
location, I'llselect the folder in the
Recent Locations list,and double-click on
Enterprise Guide 8 demo 2.Now looking at this
program, you'llnotice that it's not spaced
in a consistent format.To quickly format this code,
on the Program Editor toolbar,I'm going to click on
the Format Code button.Now you'll notice that
before formatting the code,I actually have a
formatting error, saying,please check for an
unmatched quote on line 3.I'll click OK.And if I look at the
contents of line 3,you'll notice that I
have quote, average.And I did not close this quote.So I'll go ahead and
add in a closing quote.You'll see that some of the
color coding has now returned.And I'll go ahead and click
on the Format Code buttonone more time.And now our code
looks much better.It is much easier on the
eyes and to understandwhat's going on.So another tool I'd
like to show you hereis the data step debugger.Now I have a data
step in this program.It goes from data
all the way to run.And the data step allows
us to manipulate our data.Now to better understand
what this data step is doingand how it's processing, we
can use a data step debugger.To enable it, I'm going to
click on the Debug buttonon the Program Editor toolbar.And you'll notice that
on the left-hand side,it will highlight anything
that is part of a data step.To start the data
step debugger, youcan click on Debug icon or
anywhere in this green margin.So I'll go ahead
and click on Debug.With the data step
debugger, I canexecute data step statements
statement by statementto follow the logic of what's
going on in the data step.So the first thing is I
want to go ahead and runthe highlighted statement.This is a step statement.It's going to read in the
first row from the input table.To execute just
this line, I'm goingto click on this
button that saysStep Execution to Next Line.And on the right-hand
side, I see the first rowfrom that class table.And now I'm going to go into
this conditional processingblock.I'm looking at height value.And depending on
the height value,assigning some
value to HeightCat.Now the height for the row
that I'm looking at is 69.So 69 is greater than 65.So I'll go ahead
and execute that.Therefore, this new
column HeightCatwill have a value
of above average.If I execute this,
we'll see HeightCat--and I'll go ahead and expand
this so we can see a littleeasier--has that value of above average.Now because I've already
reached a true condition,all of my other else
ifs and else statementshad been entirely skipped.And I can actually see that
with a data step debugger.I'm going to go ahead and
step over to the next line.Now let's read in
the second row.In the second row, the
height value is 56.5.So it's not greater than 65.Move onto the next statement.It is not between 58 and 65.So it's going to come to the
catch all else statement.If I execute it, HeightCat
will now be below average.And you can continue
on in this mannerto follow the logic
of that data step.For now, I'm going to go ahead
and click on the Run buttonto go ahead and go through the
entire rest of the data step.I see debugging is now complete.So I'll ahead and close
that data step debugger.The data set debugger is
really useful if you havea logic error in your program.You'll be able to step through--
again, statement by statement--to see where exactly
that error is occurring.I'm done with the debugger.So I'm going to go ahead and
click on the Debug button justpress the debugger.Now I'm going to go ahead
and add in some extra codeto this program.And I'm going to
do this by takingadvantage of autocomplete.So I'm going to come to
the end of my programand I'm going to
come to a new line.What I'd like to do is
use a proc means stepto calculate some
summary statistics.I'm going to start
by typing P-R.And notice that the Autocomplete
window appears and ithighlights the word proc.Since I want to enter in
the highlighted keyword,I'm going to press the spacebar.That enters in proc,
along with a space.Next, I see different types
of procedures that areavailable for me to use in SAS.I want the means
procedure, so I'lltype M-E. Means is highlight.So I'll do the same thing again.Hit the spacebar.That'll add in the
highlighted wordmeans, along with the space.Next in autocomplete
I see a listof valid options for a
proc means statement.Now if you want to learn more
about one of these options,like data equals for
example, you can simplyhover over that option.You get quick information.It says, just identifies
the input SAS data step.I'd like to include this.So I'm going to go ahead and
double-click on data equals,and include that
into my statement.I'm going to go ahead and
use autocomplete to goahead and type out the
rest of the statement.So I'll type work.So I'll type a W, a period,
and go ahead and includein the class height table.I'll add in another
option, maxdec equals,with a value of 1,
and then semicolon.It's a lot easier to
type this wheneveryou're using autocomplete.I'll come to a new line.And this time, I'm going
to use a var statement.A var statement specifies
which numeric columns I'dlike to analyze.So once I type var, and
then hit the spacebar,you'll actually see the numeric
columns in my input table classheight.I'd like to analyze the weight
column, so I'll go aheadand type a W. That'll
highlight weight.And then I'll type a semicolon.This will enter in weight.And then end the statement
with that semicolon.I'll use autocomplete to go
ahead and finish the restof this proc means statement.I'll use a class statement.And I will go ahead and do
this on the HeightCat column.I'll double-click on it.And at the very end, I
will have a run statement.If you'd like, you could click
on the Format Code buttonagain to go ahead and
quickly format the code.Now I like to take advantage of
autocomplete and the integratedsyntax help.However, if you would
like to turn that off,you can quickly do that.You'll want to go to Program,
select Editor Options,and then go to the
Autocomplete tab.Here, you can clear the Enable
Autocomplete checkbox, and alsothe Enable Integrated
Syntax Help checkboxas well, if you'd like
to turn those off.You can also go to
the Indenter tabto go ahead and change
the rules that areused for formatting the code.I'm going to stick
with the defaults,but just know that you have
that option available to you.So I'll go ahead and click OK.Now I'm going to go
ahead and run this code.And I want to run just
the proc means step.So I'm going to highlight
the proc means step.And I could click
the Run button.That'll run just the
highlighted portion.Or I can use the F3
key on my keyboard.I'll click the Run button.That goes ahead and runs
that proc means step.And here we go.We have our results right here.So those are just a couple
of the programming featuresthat we have available
in Enterprise Guide.We are at the very
end of the tutorial.And I hope you've found the
information to be very helpful.I'd like leave off with a
couple of resources thatmight be of interest to you.If you're interested in
learning more about thosepoint-and-click tasks that are
available in Enterprise Guide,I would recommend taking
the SAS Enterprise Guide 1--Querying and Reporting course.If you'd like to see more of
those programming featuresthat are available, I would
suggest SAS Enterprise Guidefor Experienced SAS Programmers.We saw a quick look at the SAS
Enterprise Guide documentation.But there is also a programming
documentation, and more.So to check those out, go to
support.sas.com/documentation.Finally, there are plenty
of free SAS Enterprise Guidetutorials for you to
take advantage of.So make sure to go to
support.sas.com/tutorialto check those out.Thank you for watching.And I hope you found it helpful."
96,"Hello.This is Bahar Biller.I am a member of SAS
Center of Excellence.Today, I would like to share
with you how we at the Centerdetect supply chain
management problemsthrough the use of
simulation, machinelearning, and optimization.This work is joint
with the Directorof SAS Analytics Center
of Excellence, Jinxin Yi.And the product that
we will be using todayto performance of our task
is SAS Simulation Studio.I would like to start with our
general approach towards SASSupply Chain Analytics.First, I will be
talking about howwe describe the supply
chain flow to be modeledthrough a simulation model.Then I will be talking
about how we actuallycapture the input data and
any uncertainty associatedwith them.So I'll be referring
to the first two stepsas performing
descriptive analytics.Then we are going to be
use SAS Simulation Studio.And it will mimic
the flow of thousandsof products inventories that may
be flowing through the system.And it will produce
a large data set,which will be composed of
supply chain KPI output data.So therefore, by the end
of the third step here,we will have performed a
supply chain simulation thatprovides our KPI predictions.And then we are going to
be using this supply chainsimulation to try
different scenariosand ask many more questions.And through observations
of our results,we are going to be moving
towards an optimized supplychain design.And therefore, we will
have actually improvedfrom supply chain optimization
towards the supply chainsimulation.And this is how our
predictive analyticsand prescriptive analytics would
be connected to each other.First, what I would like
to do is to show youthe example supply chain flow.Now, we are here
going to be talkingabout how we can actually
capture a supply chain networkflow, using this
particular illustration.It is composed of four channels,
production plants, supplywarehouses, mixing
centers, and customerswhere the demand is originated.And I am using
this configurationas an example of what we can do.And I will actually be
using a simplified versionof this for the presentation.However, if you have a
different configuration,than the approach we
are presenting herecan also be customized
to our story.Now, let's just start
looking at the flow.So the demand arrives mixing
centers and supply warehouses,in our example.And then, actually, we pull the
demand from the mixing centers.When the supply warehouse
also receive demand,it also meets it and just
sends it to the customers.What feeds this system is
actually the production plantsthat produces, according to
your data production schedules.Upon completion, they push
the finished good inventoryinto the supply warehouse.Then it goes through
supply warehousesas being close to the
production plants.And mixing centers, this is
where we apply the inventorycontrol policy.And this policy determines
how often and at whatsize orders will be received
from the supplier warehousesso that actually mixing
centers can pull inventoryfrom the supply warehouse.So this is a high-level
description of the supply chainflow, which is the starting
point to our supply chainanalytics.And we incur a series of costs,
actually, through this flow.They include production
costs, transportation costs,holding inventory
at supply warehousesand mixing centers, and
also back-ordering cost.In this example, we
are going to assumethat we are going to be actually
using this demand backloggingapproach, however if there is
the step of losing the customerorder, if it is not available
and sitting in inventory,we can definitely
capture that as well.And then at the red button,
you see our key performanceindicators that we are going to
be using in this presentation.They are, therefore, the total
cost and the supply chain fillrate.We are going to be
also looking at whatis the fill rate by supply
warehouse and by mixing center.The distinguishing feature of
our show that we are using,which SAS Simulation
Studio, is that itenables us to obtain
a scalable simulation.So the logic that we are
going to be building hereis going to be independent
of the number of productionplants, warehouses, and
mixing centers, as well ascustomers and products.So therefore, the model
we are describing hereis going to be
flexible, scalable,and it will also be data-driven.And here is a simple example
that I will be using.While we can have any number
of manufacturing plants,warehouses, mixing centers, and
products in our supply chain,note here is an illustration of
having two manufacturing plantswith 20 different products.We have 10 products associated
with each manufacturing plant,two warehouses closely located
to the manufacturing plantsand the 12 mixing centers.And we also make assumptions
about where these manufacturingplants and supply warehouses
may be located at Cincinnatiand Las Vegas, in
this particular case,and where the mixing
centers are located.These locations are
important because theyare going to be helping
us determine whatthe transportation time is,
as well as transportationcost from one
location to anotheras the products are flowing
through the supply chain.Now, let's just look at, for
example, product 10 here.According to our
input data, product 10is manufactured in
plant 1 in Cincinnati.It is sent to the
supply warehouse.And when there is a
replenishment order receivedfrom the mixing centers,
which is in Oklahoma Cityand which is actual
mixing center 3,it is pushed into that
particular inventory.And then it's finally
sent to the customer,according to if we
arrive the shipment time.Now, here, you see the list
of simulation input datathat we need next.So far, we have talked
about the configuration.Now, let's look at the data
and contents one by one.Let's starts with
initial inventory.This step basically tells us
the number of units of inventorythat we will have in the very
beginning of our simulation,by location.And here, we see that on
the upper left-hand sidethat we have the most initial
inventory sitting in Pittsburghin our mixing center 2.And we can actually compare
it alternatively as well.And, again, we see that
this is where we accumulatethe most obtained inventory.And to hold a simulation, we
need more granular information.So we actually needed
the compositionof this total inventory
of 300 units by product.And this is what we are seeing.And, again, when look
at it by product,this is the location, mixing
center 2, where we carrymost of the initial inventory.After we have the information
about initial inventory,the next step is to
obtain the informationabout our production plan.So this is in an hourly basis
and what each productionline in each plant produces.And we see that production
line size in plant onemakes most of the production.And, of course, we have this
information by product as well.And here, you see
how each day, weadd to the weekly
cumulative production.We also assume uncertainty
around production time.In this very simple
example, we assumethat each production
order actuallytakes an average of four hours.And we assume a 50%
variation around it.And then when we actually send
the finished goods inventoryfrom the production plant
to supply warehouses,we assume an average of three
hours transportation time.And we also assume a
50% variation around it.So these are examples of that
type of production plan datawe need, as we are actually
describing and buildingthe first simulation model.The next step is the
customer order information.So these are the
demands that aregoing to be received at supply
warehouse and mixing centers.What we are going
to do first is showyou the results of our
deterministic demand case.And we need to define.And this is why we
have all fixed at 1,000units on a weekly basis.And we need to input
information by location.And we also need it by product.And, again, we see
that Pittsburghis where we are receiving
most of the customer demand,as well as by product.After the specification
of the customer order,we need to also specify
the transportation details.We need how long it takes to go
from one location to another.Let's take our mixing
center to give an example.So we are going to
be transporting goodswith trucks from the
supply warehouseswe have located in
Cincinnati and Las Vegas.And we see that
actually this is wherewe incur most of
the production timethat are shown here in hours.However, we also
need informationabout the details
of the trucks thatare traveling on those routes.And here, you see
the number of truckswe can have on these routes.And we also have the ability
to add more details] givenmore information about the
capacity of each truck,and the minimum capacity
each truck requires.So you can, again,
just change your trucksas individual
resources and providemore detailed information.So this is also a
great tool for youto understand the impact of
your fleet transportation, fleetdecision on your overall
supply chain performance.And finally, here, we have the
supply chain inventory controlparameters.Here, we are going
to keep it simple.If we have 20
different products thatare stored across five
different mixing centersand we need an inventory
control parameter setfor each one of
them, that determineshow often we are going
to review the inventory.And with this information, we
are going to make the orderingAnd here, we are using
the periodic inventorypolicy defined by reorder
point and order-up-to level.We are basically checking
the data inventory levelfor each product at each
location, also mixing center.And if the inventory
level, for example,for product 1 at mixing center
1 is less than 60 units,then we replace the
replenishment order.And we are going to
define, in a way,that we would like
to have a 100 unitinventory at this location.So this replenishment order
goes to the supply warehouseto pull inventory.So there are two types of
numbers on this illustrationin our example.These are the reorder points
and the order-up-to levels.If you would think of this
as an optimization problem,these levels would be
corresponding to examplesof decision variables you
would have in your system.There are two different data
sets we have left to specify.Supply chain cost
parameters is the first one.And production yield and
forecast error are the second.Here, we specify
our unit productioncost, transportation
cost, cost of placinga replenishment order, the
fixed ordering cost, the unitbackordering cost,
and the cost thatcomes form holding inventory
at supply warehousesand mixing centers.And other assumption of
60 units of your orderpoint at our mixing
center 1 for product 1and on order-up-to
level of 150 unitsand 100% production yield
and no forecast error.Here, you see our example of
the total cost of compositioninto the components
that are displayedabove And this is unit data
function of the time window.And this is going
to basically bemaking our total cost that
you would like to minimize.Therefore, so far we have
talked about a supplychain whose total cost you
would like to minimize.And, as an example,
we are focusingon how we can actually
manage the inventoryat the mixing centers.And we have two
decision vehicles,or controls, or levers
that we can play withto minimize the total cost.And those levers are the reorder
point and order-up-to level.Now, we have all
the information weneed to build our supply chain
model in SAS Simulation Studio.We have the supply chain
flow and all the inputdata in place.The next step is to
determine the randomnessin some of these data sets.Here is an example.For example, production time,
we said that average productiontime is going to be four hours.But we are going to have
50% variation around it.So on the x-axis
in the first plotyou see all the possible values
production time can take placein a production plant.And y-axis indicates
how likely itis to obtain each of
these production times.And then we have a
similar risk profilefor the transportation time.So this is the way.These are the plots we are
going to be using to capturethe randomness in the inputs.That we are going to specify the
levers we have in the system.And we are going to use them
as we design the simulationexperiments.So you can actually have
a wide variety of leversthat you can play with to move
towards a good system of supplychain design.Here, we are going to
be using all reorderpoints and order-up-to levels.And then we are going to
be execute this simulationand obtain the risk
profile of the supply chainkey performance indicators.So we are not going
to be only computinga single value about what
the supply chain flow isand what the supply
chain total cost can be.We are going to be capture
the risk in those predictions.And that is the
beauty of this tool.It allows you to
understand and beinformed about the risk your
supply chain is exposed to.So the key benefit
of this tool isthat this is scalable,
data-driven, and flexible.And we can add SAS Analytic
Center of Excellencecustomize our supply chain
simulations to your needs.And as a result, we can
help you to experimentin the virtual lab
that we create,so that you can assess the
impact of operational policyon strategic
investment decisionson your overall supply
chain performance.At the same time, you
will imbued with the powerto predict your supply chain
KPIs because the supply chainsimulation is a large supply
chain KPI data generation.Not only you would obtain
your performance measures,but you can track
any production order.You can track any inventory.You can track any customer
order in this particular tool.And furthermore, you
can quantify the riskbecause you will have
a large data set.So you can apply all your
statistical powerful toolsto understand and
quantify the risk.And once you can do
that, actually, youcan experiment with a
large number of scenariosand move towards identifying a
better design for your supplychain.And here is an
example here wherewe look at supply chain service
levels and supply warehouseservice levels, mixing
center service levels.And we also quantify the
average and standard deviationof the total cost.And this is where we have an
example of 40 order points, 106order-up-to levels.What happens actually when you
increase 40 to a larger numberor place order more often
and in larger quantities.We see that your
system improves.What if we have another
uncertainty in our system?That demand uncertainty
adds more variation.So it degrades the performance.So we can now use this tool
to find effective toolsto fight against the
uncertainty and do riskwhere exposed in the system.So as a result, we have a supply
chain simulation capabilitythat enables the
creation of scalable datathrough an inflexible
supply chain.It further integrates with
Visual SAS, Visual Data Miningand Machine Learning libraries
for you to more effectivelyactually perform scenario
analysis and optimizationof your system, so you can get
closer to real-time decisionenablement.Thank you."
97,"Hello and welcome.My name is David
Ghazaleh, and I workSAS in the development
of the SAS EmbeddedProcess for Hadoop and Spark.Today, I'll be talking
to you about howto achieve optimal
performance transferring databetween SAS and Hadoop using the
SAS Data Connector Accelerator.So let's get started.So here is this situation.You are collecting
data in Hadoop,and you are transferring data
to SAS in order to process it.And the question becomes how
to optimize the data transferbetween SAS and Hadoop.SAS Data Connector Accelerator
for Hadoop to the rescue.Here's what I want you to do.Download my paper
and actually read it.There is a lot more
in my paper than Ican cover in this presentation.Use the examples as guide,
learn more about SAS Viya,consider SAS Viya, and
experience faster datatransfer between SAS and Hadoop.Let me give you a
quick introductionabout the three major
components involvedin parallel data transfer
between Hadoop and SAS Viya.The first one is the Cloud
Analytics Services, or justCAS.CAS is a distributed
in-memory analytic serverfor complex analytics and
data management operations.It runs on the cloud
and on premise.It provides you a
fast, a scalable,and fault-tolerant platform.It is ideal for
faster processingof huge amounts of data.The second component is the
SAS Data Connector Accelerator,also known as Parallel
Data Connector.It allows parallel data
transferred between CASand your massively
parallel processingplatform such as Hadoop,
Spark, and Teradata.The third component is this
SAS Embedded Process, or simplySAS EP.The SAS EP is the core of
SAS in-database technologies,while in-database technologies
includes Scoring Accelerator,Code Accelerator, Data Step
Accelerator, Data QualityAccelerator.And while capable of
embedded processing,the SAS EP is also used
for parallel data transferbetween Hadoop and CAS.The SAS EP provides a
lightweight execution containerfor SAS code.It runs inside either a
MapReduce or a Spark taskin your Hadoop environment.All resources allocated by
the SAS embedded processare managed by YARN.Let's now put the three
major components together.So let's say you
have an environmentsimilar to this one.On the top, you have
your CAS cluster.On the bottom, you have
your Hadoop cluster.The CAS cluster is
comprised of a controlnode and multiple worker nodes.Likewise, the Hadoop
cluster is comprisedof a head node, where the
YARN resource manager runs,and multiple worker nodes,
where the YARN node manager run.In a Hadoop cluster, the EP runs
inside a MapReduce and Sparktest, just like I said.When data needs to be
transferred from the Hadoopfile system to the
CAS server, the clientsubmits a CAS load table action
to the session controller.The session controller then
starts the embedded process jobto read the input data from
Hadoop file system or a Hivetable, and evenly distribute
the data among CAS worker nodes.So this type of SAS EP job
is called CAS input mode.On the other hand,
when data needsto be transferred
from the CAS serverto the Hadoop file
system, the clientsubmits a CAS save table action
to the session controller.The session controller starts
the embedded process joband stores the data evenly
on Hadoop file system.So this type of EP job is
called CAS output mode.You can choose the
platform where the EP runs.It can run, just like I said,
as a MapReduce job or a Sparkapplication.On Viya 3.5, we introduced
a new SAS Embedded Processfor Spark Continuous Session,
or you can just call it EPCS.So the EPCS is an instantiation
of long-lived SAS EP sessionon a cluster that can
serve one CAS session.So EPCS provides a
very tight integrationbetween CAS and Spark by
processing multiple executionrequests without having to start
and stop the SAS EP every timean execution request is made.Users can provide-- where users
can improve system performanceby using the EPCS and
the SAS Data Connectorto Hadoop to perform multiple
actions within the same CASsession.Let's take a closer look
at the SAS EP on MapReduce.But before that, let's review
the basics of MapReduce.So a MapReduce job is a
collection of Map tasksand Reduce tasks
that are executedon nodes in a Hadoop cluster.The Map tasks read
splits off the put file,process them, and send
the resulting recordsto be Reduce tasks.A file we split is a fixed-size
slice of the input filethat is assigned
to one Map task.An input split is
associated with one filepath, a starting offset, and
the length of this split.The minimum and
maximum split sizeis configurable and
may not translateinto a physical block.Input splits are calculated
by a file input formatimplementation, which
is also responsiblefor the instantiation
of a record reader.For example, if you have
a job that is reading datafrom a parquet file,
the parquet input formatis responsible for
calculating the file splitsand for instantiating the
parquet record reader.But now, keep in mind
that input splitscalculation happens before
the MapReduce job is submittedfor execution on the cluster.And the record readings in
record reader instantiationhappens when the
MapReduce task is running.In a conventional MapReduce
job, each Map taskreads a record from
a single input filesplit that is assigned to it.Now, the SAS EP is a
MapReduce application.However, it does not assign
only one split to a task.It assigns many.That means the SAS EP Map
task can read multiple splitsin parallel.Another point of differentiation
is that all EP processinghappens at the
record reader level,even before the map
function is called.Well, what does that buy you?Assigning multiple fire splits
to one single task avoidsexcessive start and
stop of MapReduce tasks,reducing unnecessary overhead.This is what we
call super reader.When saving data to Hadoop
file system or a Hive table,the SAS EP job
writes multiple partsof the output file in parallel
from within the same MapReducetask.This is what we
call super writer.SAS embedded process
attempts to reschedule taskson nodes where physical blocks
of the input file can be found.For example, if file blocks
can be found on nodes 1, 2, 3,and 4, the EP schedules tasks
to run on those four nodes.But keep in mind that, depending
on resource utilization,the YARN resource manager
may decide to schedulethe tasks on different nodes.You cannot control the number of
nodes used to run the MapReducejob, but you can control the
number of tasks per node,the number of reader threads,
the number of compute threads,which are the actual threads
that are transferring databetween CAS and Hadoop, and the
number of threads writing datafor your file system.The diagram illustrates
the key componentswhen running inside
a MapReduce task.In this example, we
have two tasks per node.And let's say I define
three reader threads.That means from within one
task, I'll be able to read itthree files splits in parallel.Now, here are a
few things that Iwant you to consider
before makingany changes to the embedded
process configurationproperties.If you decrease the
number of tasks per node,you're going to be increasing
the number of inputsplits per task,
but you're goingto be reducing parallelism.So in this case, fewer
tasks means better chanceof having fewer tasks
queued for execution.Now, increasing the
number of tasks per nodedecreases the number of
office splits per task.That is going to
increase parallelism.However, more tasks
per node increasethe chance of having more
tasks queued for execution.So keep that in mind.Let's now take a quick
look at some of the Sparkarchitectural components
before we take a bigger lookat how the SAS Embedded
Process works inside Spark.A Spark application use
a user-written programthat is called a
driver program thatcontains a set of actions
and transformations appliedto the data.The driver program, along
with the Spark context,coordinates the
entire job execution.In the cluster configuration,
the driver programruns on its own container, which
is called the driver container.A Spark application consists
of one or more jobs.A Spark job is a parallel
computation consistingof one or multiple
stages, and a stageis a computational boundary
that consists of a set of tasksthat depend on each other.Tasks are executing in
response to actions.A task is a single unit of
work, is scheduled by the driverprogram to run in the
Spark executor container,and operates on a
partition of an RDD.The Spark executors are
distributed processes runningon many nodes of the cluster.They are responsible for
executing tasks and providingin-memory storage for RDD.Depending on the resources
you allocated for your Sparkapplication, tasks
can be executedconcurrently or sequentially
inside an executor.The SAS EP on Spark is a
combination of generated Scalacode, generated Spark SQL
statement, HDFS file access,generated SAS code, in a set
of specialized embedded processfunctions that know how to
operate on Spark data framesand RDDs.So SAS EP runs in
the driver containeras the main application
driver program.The SAS EP functions run in
a task inside the executor'scontainer.Each task is assigned
an RDD partition, whichmight from a Hive
table, an HDFS file,or a data that was
transmitted from a CAS workernode, which is going
to be our case in here.What can you control with
configuration options?While you can't control
the number of executors,you can control
the number of coresper executor, the
number of core per task,your number, and much more.For example, you can
control the amount of memoryyou want to allocate per
executor or per drivercontainer, or you can even ask
for how many ports per drivercontainer you want.All executors run with
the same amount of coresand the same amount of memory.The degree of
parallelism dependson the number of executors, the
number of cores per executor,and the number of
cores per task.The number of tasks
depends on your numberof partitions in the input RDD.So each partition is
processed by one task,and the number of concurrent
tasks in an executoris the number of
cores per executordivided by the number of cores
per task, as shown in here.And the total number
of concurrent tasksis the number of
executors multipliedby the number of concurrent
tasks per executor,as shown here also.So the Spark application
resource allocationmust be within the
limits imposed by YARN.For example, if YARN only allows
you four cores per container,asking more than that may
result in EP startup failuredue to resource constraints.The resource consumed by
the Spark driver containeralso takes up
resources from YARN.This should be
taken into accountwhen sizing the application
resource, so choose carefully.Let's now exercise our
resource allocation skills.So in this example, I have
a cluster with four nodes.Each node has four cores.YARN allows me for
cores per container.Since I have four
nodes with four cores,I could exhaust my resource
and allocate 16 executorscontainers, and give each
one of them four cores.However, I still need to take
into consideration that Ineed one driver container.But let's say I play
nice, and insteadof asking for 4 cores
per executive container,I will ask for three.And besides, I'll take
into considerationthat I still have to allocate
one driver container.The thing is, I
don't know in whichnode of my cluster the
driver container will land,therefore I have to reserve one
core for the driver containerone each node.Well, that will leave
me with three coresper executor container,
which is what I want.This way I can have three nodes
running three executors and onenode running a driver
container plus two executors,and I will be able to ask for 11
executive containers and my jobwill run with 33
concurrent tasks.Now, you ask me, what's
the best setting?And I'll say, well, it depends.Finding the perfect setting is
not an easy task, my friend.You have to take
into considerationthe number of nodes
you have, you haveto take the number of CAS nodes,
the number of Hadoop nodes,the maximum number of concurrent
containers allowed by YARN,the size of the data, the number
of splits or table partitions,the number of cores, and
memory available per container.Well, you already know the
data, which is a good thing,but you also need to know
your execution environmentand have a clear understanding
of MapReduce, Spark, and YARN.Thanks for watching."
98,"Hi, everyone.My name is Patrick Maher.I am a Principal
Solutions Architectin the Fraud and Security
Intelligence Practicehere at SAS.My career focuses on the
application of analyticsto answer questions
for our clients.In addition to working
with our clients at SAS,I've taught data mining and data
visualization graduate-levelcourses.I'd like to welcome you to
my virtual session titledThe Seven Most Popular
Machine LearningAlgorithms for Online Fraud
Detection and Their Use in SAS.It's a mouthful, but we'll
cover a lot of territoryhere in the next 20 minutes.This paper evolved
from my effortto learn more about machine
learning algorithms usedin fraud detection.I moved to the fraud and
security intelligence teamin 2019.So during that time, I was
trying to quickly get upto speed, and I research
both academic and industryliterature.And these are the algorithms
and their derivativesthat seem to appear most
most often in the literature.There's no magic
formula, with the cutoffbeing seven algorithms.But they do fit
within a family whereyou tend to see 7, 8, 9, 10
algorithms used most heavilyin terms of fraud detection.So many techniques have been
proposed and implementedfor fraud detection, and we
will review the most common.And the really key piece here
is not just taking those modelsand looking at them from
an academic standpoint,but actually putting
them into SASand implementing them in SAS.And in this case, all the
machine learning algorithmshave been developed in SAS
Visual Data Mining and MachineLearning.So in a majority of
settings, roles-based systemsare used when trying
to detect fraud abuse.There's currently
shift underway, though,to supplement those roles-based
systems with machine learning.According to a global
survey conductedby the association of Certified
Fraud Examiners and SAS,by 2021, about half
of organizationsanticipate employing machine
learning algorithms in additionto or supplementing their
roles-based systems.We see this as being an
increase from about a thirdof the industry, about 30%,
over the next couple years,so again a big jump
in those lookingat using these types of
techniques in fraud detection.When we consider machine
learning algorithms,they are typically
categorized in termsof supervised and
unsupervised techniques.In terms of
unsupervised techniques,you may have heard of clustering
or k-means clustering.That's one
unsupervised approach,and there was actually
an SGF paper last yearon some unsupervised
approaches related to fraud.For this paper,
this year I focusedon the bottom half of the
slide, supervised learning,so looking where we're working
to classify new data basedon historic data,
or what's typicallycalled a training data set.So we're trying to find
where this new data fitsrelative to what we've seen
from historic patterns.So the first algorithm I'll
mention is logistic regression.This family is kind of
the grandparents, I guess,of a lot of the other models,
widely used, heavily usedfor many, many years.What a logistic
regression does ismeasure the relationship
between the responseor the outcome variable,
so in this case fraud,and the independent variables,
or the input variables.Some of the strengths of
the logistic regressionare that it's widely used.So it's used quite
heavily in industry,so people are
familiar with using itand what it looks
like, the outcome.The other thing that's
really nice about it,it gives a calculating
relationshipfor each input variable with the
outcome, again, in this case,looking at fraud.One of the weaknesses, a major
weakness that a lot of peoplefind is they consider the
underlying math complicated.So if you don't have a real
strong math background,that might come off
as a complicationin using logistic regression.And when I look at
these algorithms,one way I like to think about
them is whether the model--this is the Patrick Maher
interpretation of models--whether they fit the data
or they split the data.I look at regression
models as a type of modelor an algorithm that
actually fits the data,so it will try and fit
a pattern to the data.The next model we mention,
which is decision trees,I kind of term these
that they split the dataare partition the data to
fit that model to the data.So a few strengths
about decision trees--one right off the bat,
they make no assumptionabout the distribution
of the data.So it doesn't necessarily have
to be a normal distribution.Other strengths include that
it's very straightforwardto interpret these models.They generate, basically,
if-then or Boolean-type logicthat's easy to interpret.And not only is it easy
to interpret the results,but then implement
those, for example,in a rules-based system.So this is one of the machine
learning algorithms that'svery easy to take the
results and implementa rules-based system.A couple of weaknesses
with decision trees--they can overfit the data
and other adjustments thatcan be made to mitigate that,
but they tend overfit the data.The other thing, with just a
small change or perturbationin the data, you can get a
very different-looking decisiontree.So as your data
changes over time,the decision tree might
look quite different.It might go from being a
really good performing modelto maybe a not so
good performing model,if your underlying data changes.And, related to
decision trees or what'scalled a forest,
a random forest--and I really debated
putting this one in here,as it's maybe not a
separate class because it'sbased on decision trees.But these are very
popular, so again,the goal here being
capturing what are the mostpopular models or algorithms.So strength of a random
forest, for many data sets,they produce a highly
accurate classifiers.They're really great at
classifying that historic data.Ensemble model
approach, meaning thistakes a combination of models.So an ensemble
model, typically youcan find some really
good predictive powerbased on combining the
kind of the best of models,so the random forest
combines the best of whatit finds in the decision trees.So here you see from
the little graphic--I like the graphics, They
help me understand the models.It takes the averages of
all the decision treesto create this ensemble model.However, like other
ensemble models,a weakness can be interpreting
the results of this model.So unlike the
previous slide, whereyou saw a decision tree that's
kind of one set of rules,here you're looking at
multiple sets of rulesand combining them, so that does
create a complication in termsof interpreting the data.And if you think about fraud and
the uses fraud or fraud abuse,typically there is
some explanationthat has to be done to both
internal and external clients,but also from a regulatory
standpoint to regulators,typically within the government.So that becomes a complication
with random forest,specifically, and more
generally, ensemble modelscan be a challenge or
difficult to interpret.I mentioned earlier the models
that kind of fit the data,and this is where I
picture a neural network.And I really like the
graphic on the left.Helps me remember when
neural network does.But neural networks, you
might hear them phrased deeplearning, typically
used in what'scalled deep learning scenarios.They're able to fit really
complex patterns of data.Some strengths
include the abilityto model nonlinear and
complex relationships.Another nice thing
about neural networks,there's no assumption
of independenceon the normality for
the input variables,so it could be used against
many different types of data.Weakness with neural
network-- and if you've donespent any time with them, or
even just reading about them,they can be difficult
to interpret.There's ways around that.But again, in terms
of explanationto internal and external
clients or the regulators,neural networks
can be a challenge.The next algorithm I took
a look at was a Naive Bayesclassifier, widely used, a very
popular algorithm, I'd, sayright now.The basic idea is simple, that
each input variable by itselfhas something to
say about whateverit is you're trying to predict.So in this case, we're trying
to predict or capture fraud.So again, it has some
strengths, just that as soonas you grab an
input variable, it'sstarting to kind of make
that classification.So Naive Bayes classifier,
they are widely used.They're highly interpretable,
and they give youa calculated relationship for
each predictive variable, justlike we saw with the
regression model.A big weakness, though,
is the assumptionthat all features
are independent,so all the input variables.As we know, working
with real life data,many times the input variables
are correlated with each other,so that can be a challenge in
terms of working with a NaiveBayes classifier.k-nearest neighbor, this
classifies the new casesaccording to the most similar
cases in the training set.Again, this is when I'd
say it's really taken off,become quite popular
in the last few years.It's very simple to implement.That's one of the
major attractions.It's also flexible to the
types of data that you feed it.It's called a lazy
learning method,as opposed to eager
learning methods.So it's kind of learning
as you're feeding data.I will say, as someone
who's taught a bit,this album is confused
pretty heavilywith k-means clustering.The names are similar, and both
rely on distance functions.So if I put on my
professor hat, if youwant to be a little bit mean
and trick your students,you can maybe throw
them a lot of questionsabout k-means clustering
compared to k-nearest neighbortechniques.Again, k-nearest neighbor, this
algorithm that we're looking atis a supervised
training techniquewhere we're looking at
feeding it historic data.k-means, again, is
unsupervised techniques.So that's the
biggest difference.One big downside of
k-nearest neighbor,there's a high computation
cost because we'retrying to compute the distance
of each query or each inputinstance against the
training, sample.So it can be costly
as far as computationand can take longer
to calculate.But again, with the horsepower
that's available today,many people are implementing
these as of the last few years.Another algorithm
that's really popularis support vector machines.I'd say within
the past 10 years,these have really taken off.A very popular algorithm, high
accuracy, as far as a strength.Ability to deal with
high-dimensional data, lotsof variables, and
also the abilityto generate nonlinear
decision boundaries,so a lot of strengths in terms
of what support vector machineoffers.However, as you might get
from the strengths suggests,this is a lot of work.So support vector
machines don't perform--and this is all
relative-- don't performrelatively well with large
data sets due to training time.Again, with the horsepower
available in termsof computing power nowadays,
it's not as much a factor.But it's something
to keep in mindrelative to other techniques.It would take longer
to train the model.Again, earlier I
talked about algorithmsthat either split
or fit the data.In the case of support
vector machine,this is one that really
does split the data.It finds a best hyperplane
that puts one classseparation between the classes,
in this case, one above,one below.So again, this is
one of the algorithmsthat really splits the data.So that's it in terms
of the algorithms thatwent and researched and saw
in terms of what is popular.And I thought, well,
jeez, wouldn't itbe great if we could
take all these algorithmsand implement them
somehow in SAS?And guess what.We can.There's a tool that lends itself
really well to these techniquesand more.It's SAS Visual Data Mining
and Machine Learning.SAS VDMML-- that's a mouthful--offers a lot of
different techniquesand algorithms models in
terms of what you can build.The nice thing, there
are some templates.That's how I started
this exercise.I took some predefined templates
that are offered in the productto then build out
these algorithms.And so in this case,
I started with what'scalled an advanced template
for a class target,and it gave me this kind of
a view in the user interfacewhere I'm building models.One thing I did
change or take out,I took out what it's
called the ensemble model.One, as I mentioned
earlier, ensemble modelscan be difficult to interpret.So I wanted to take that out
and make it maybe more real lifescenario where, hey, I might
not be able to explain thisto a client or to a regulator.And also what I did
was quite frankly,an ensemble model
would do betterthan a lot of these models.So I wanted to take out the one
model that had a big advantage.So started with one of
the predefined templates,and I edited it.Of course, the product gives
complete leeway where one cancreate a model flow-- this
is called a model flow--within Model Studio.So we're using VDMML
within SAS Studio--excuse me, within Model Studio--to create this flow.So before we get
to what I actuallydid and working with the
data, one thing to look at is,how well do the models or
the algorithms perform?And the really nice thing--there are a whole bunch.And I'm not here
to tell you what'sthe best way to do it because
it really depends on the data.It depends on your industry,
it depends on the scenario.Again, the really nice
thing is the SAS solutiona lot gives you all
these different options.A really typical assessment
metric that's used,though, is what's called
the area under the receiveroperating characteristic curve.It's a really
commonly used metric.It's really, in my opinion,
a quick eyeball techniqueto say, hey, how well is this
model performing, especiallywhen I'm looking at
many different modelsand I want to get a sense of
how well they're performing.There are other
assessment measurespeople look at--
misclassificationrate, false positive rate.Another one that's a
kind of a nice and morerecently been highlighted
is the false discovery rate,and it's also
available in VDMML, SOit's a nice another kind
of tool in your tool beltas you're looking
at these algorithms,assessing what is
the best performing.I will say, for this exercise,
I did look at the AUROC curve,or the Area Under the Receiver
Operating Characteristic curve,as kind of a quick assessment.But then I did look at the
misclassification rate,and I also did look at
the false discovery rate.So I kind of used these
three criteria quite heavily.Again, there are others.That's probably another
topic, another discussion.But there's the ones that
I looked at this exercise.So to demonstrate the use of
these techniques, these machinelearning algorithms, I
used a data set that'sout there that's
publicly availablecalled the PaySim data set.So it is simulating
fraudulent payment activity--fraudulent online payment
activity, to be more specific.This data set is kind
of a skinny data set.It's only 11 variables,
11 input variables,but just over 6
million observations.So it gave me a
significant chunkof data to work with and take
a look at, so again, kindof a skinny, long data set.And get to the punchline
really quickly,the decision tree ended up kind
of being the best performing--not kind of, but was the
best performing model.And it was consistent.I took a 10%-- in
this case, if youlook at the slide,
a 10% event rate,meaning that the fraud
occurred 10% of time,which would be
artificially high.But I was oversampling, is
what we call it, in this case.So against the 10%
sample, the treehad the lowest misclassification
rate and the lowestfalse discovery rate, as
well as the highest areaunder the curve.As I tested it, though,
against a 1% sample and thenthe real life sample, which
is 0.1% of fraud happeningin cases, the
treaty consistentlyperformed better than
some of the other models.I'll leave it at that.Here's kind of a quick
screenshot of decision treeresults, and you'll see
we captured 98.4% of fraudwithin a first few splits
of a decision tree.So again, really highlighting
just the one resultsset here from the
decision tree, just giventhe time to go over everything
in this presentation.And then a really nice
thing at the project level--so you can have multiple
data flows, or excuse me,multiple model flows
within a project.I only had one here
to keep it simple.But if we had multiple
model flows going on,we could get a view
of the project level.So this is tell me what's
the best performing model.It's telling me what the event
rate, how often fraud happenedin the original data set.You'll see I did this way
back in January, and I said,I hope this presentation
is going well.So hopefully, that still holds.So the nice feature here--
and this is, to me, the punchline on a graphical
user interfaceis that it allows you
some model governanceas far as it's
the modeling tool.But it's keeping
track of the metadata,the data about the
input data I'm using,the data about the
models I'm using,and the data about the models
and how well they perform.So again, it's nice in
terms of documentation.So in conclusion,
in this session,we really wanted to highlight
some of those popular machinelearning algorithms.Again, so we've taken
what we're probablya couple weeks of a
graduate-level courseand condensed it
into 20 minutes here,in terms of covering the models,
but then taking those modelsand implementing them in SAS.We used some typical
classification criteria here.In this specific case,
the decision treegave us the best performance
relative to looking at the areaunder the curve, the lowest
false discovery rate, and alsothe lowest
misclassification rate.Here are some references.I'd say so these are
great to look at,were very helpful
for me, and theymight be for you in terms of
what you may want to look at.Some are more industry specific,
some are a little more academicin nature, and some, as you see,
are offered by SAS Instituteas well.So again, some nice
references to check outif this is an area
of interest for you.So thank you again, and
I appreciate your time.And if you have any
questions on this--again, this is kind of a tip
of the iceberg presentation.There are a lot of ways
to go with these models,with these algorithms,
and with the product.You can always reach out via
email, patrick.maher@sas.com.Thank you very much."
99,"EMMA DELEHANTY: Hello, everyone.My name is Emma, and I'm
with Victoria and Austin.We are a team from University
of North Carolina Wilmington.We created a study that
modeled the potential lossvalue of coastal property in New
Hanover County, North CarolinaWilmington that's due
to the sea level rising.Think of the boom of
industry in the 19th century.There has been rapid increase
in the concentrationsof greenhouse
gases, increasing inthe average global temperature.As seen from the
bottom graph, there'sno significant change in
sea level form 1700 to 1880.Since 1880, the
sea level has risenby eight inches, three inches
in the last 25 years along,and the sea level is
predicted to continue to rise.The primary factors causing
the sea level to riseare driven by the rise in
the global temperature.Heat will cause thermal
expansion of the oceans.This is compounded by a
significant float of waterfrom melting glacier and
ice sheets from placeslike Greenland and Antarctica.This leads to several risks.The shoreline is going to eroded
from the tide reaching furtherin length, and the washing away
soil means effects of stormwill be amplified.As storm drain unable
to empty into the ocean,the seawater flow up land.Water table will rise and with
the inefficient drain system,permanent inundation
or chronic floodingwill be our new normal.Saltwater intrusion
is yet another riskleading to the contamination
of water supplies and damageto local agriculture.These risk should lead
to loss of poverty,property damage, and
increasing insurancerates for the coastal area.Our objectives are to identify
and quantify real estatewithin New Hanover County
that will experience sea waterinundation as sea levels rise,
to find the potential lossin monetary value of properties
by ZIP code and factorsbased on different
increase of sea levelchange over certain
periods of time,and to find what's the
difference in property valuepredictions with or
without the sea level rise,and finally, to create
an interactive mapto display our results.Our data source--
the sea level waspulled from the NOAA database.Property tax data was shown
from the New Hanover Country taxdepartment.The sea level projections come
from the IntergovernmentalPanel on Climate
Change, property valuefrom Zillow public database,
and last, USGS predictionsused to compare our funding.VICTORIA MULLIN: The
variables that we used--first, from the Zillow data set,
we used the Zillow Home ValueIndex, which is the median
of estimate in actual homevalues for a given geographic
area on a given day,and that is given by
ZIP code and date,specifically monthly, from April
1996 to August of last year.From the tax data set
for New Hanover County,we used the
appraised total valueof the properties, which is
an evaluation of a property'svalue based on a
given point in time,taking into account internal
and external factors.We also used parcel idea, which
is a unique identificationnumber for properties
for tax purposes,and that's our common variable
we use to merge the data.We used neighborhood and street
to estimate missing values,which I'll explain
how later on, alongwith elevation of a parcel
and the number of acres.Originally, we are
going to use flood zone,but realized predicting
future flood zones isa project in itself,
so instead weused different increments of
sea level rise given by IPCC,specifically one foot, which
is linked to the year 2060,and two feet, which
is linked to 2100.We have several assumptions for
this project, the first beingthat properties that
are close to each otherhave similar values
because the waywe estimated missing values
for appraised total valuewas by finding the median
value by neighborhoodand then by street if
that wasn't available.We also assumed that the tax
parcel data set and the Zillowdata sets are both accurate
and reliable since that'swhat we're basing our
predictions off of.Since we didn't
predict future rainfallas part of our
project, we're assumingthat the amount of rainfall
is the same in the futureas it has been in the past.And we're also assuming that
supply and demand is nothaving an impact on properties
that will not be affectedby sea level rise, meaning their
price will not increase extrabecause other homes
are being inundatedor decreasing in value.A final assumption is
that Wrightsville Beachis similar enough
to Carolina Beachthat it can be substituted
by Carolina Beachsince Wrightsville
Beach, for some reason,is missing in the Zillow
data, so we couldn't builda regression model for it.There have been
several obstaclesthroughout this project, the
first being missing valuesin variables that we needed
to make our predictions.Specifically, the
appraised total value,which is our response
variable to predict,had rising values
which we estimatedby finding the median value by
neighborhood and then by streetif the neighborhood
wasn't available.Also, there was no ZIP code
variable in the New HanoverCounty tax parcel data
set, so we used RGISto find the ZIP
code of each parcel.In order to predict the property
values with and without sealevel rise, we
need property datafor previous years in a larger
range than is available,which is just last year, 2019.We ideally would use data
from years such as 2000, 2005,'10, and '15.However, this is not
publicly available,so this is why we're using
the Zillow data because itdoes have past year's values.The only issue with
that is that it onlygives the median property
value for each ZIP code,and it's not on an
individual property level.However, this is
what is available,so we worked with what we had.We also realized that,
in the Zillow data,it does not include
the ZIP code 28480,which is Wrightsville
Beach, for some reason,and this is why we're using
Carolina Beach as a substitute,because we feel that the two
ZIP codes are similar enoughto do so.A final challenge
was, for some reason,that the Zillow data
for the ZIP code 28428does not start until July, 2005,
where all the others go backto 1996.Since 2005 is right
before the recessionwhen we originally fit a
linear model to it usingthe entire date range,
it resulted in a linewith a negative slope.To fix this, we used
a smaller date rangethat produced a model with
more accurate results.AUSTIN WILLOUGHBY: So
onto the GIS methodsused in this project.What exactly is GIS?GIS stands for Geographical
Information Systems,and it is essentially
a database managementtool for geospatial data.So what does that mean?It means we can merge databases
based on geospatial attributes.Take this image
here, for instance.We have this red flood
zone, and then wehave the cyan tax parcel data.In order to merge
them geospatially,we would use the
intersect tool in GIS,and that would effectively
cookie-cutter these two layerstogether.It would merge the
tax parcel dataalong this geospatial line
that is the flood zone data.And in the end, you
get tax parcel datathat's only within
the area you want,as well as the flood
zone data that you want.So it effectively combines
these two databases.So what were we going
to do with that?Well, initially, we wanted to
create if an elevation floodzone-based model.This was going to involve
using elevation dataand current flood zones
to predict future floodzones based off of
projected sea levels.This turned out to be
incredibly difficult,and the main reason was that
different hydrological areasact differently.So in North Carolina, we
have barrier islands here,intercoastal waters
in between the barrierislands and the mainlands, and
then rivers, lakes, et cetera.All these areas are
going to act differentlyduring a flood event.The barrier islands are
going to have a higherelevation of flooding above
sea level than the river,and that makes sense
intuitively whenyou think about the wind energy
and the wave energy comingfrom the ocean versus what
is getting into the river.So that turned out to be quite
complex, and we threw that out,and instead we went with Future
Tidal Zone Delineation model.This data was already out there.It's provided by the NOAA,
which is the National Oceanicand Atmospheric Administration.They provide sea level
data in one-foot incrementsthroughout the
whole United States,and it allows us
to see just where,geographically, the
ocean or the tidal zonewill be in these plus
increments of sea level change.So here you can see this
is present day conditions,and here is a projected
future conditionwith a certain increment
seal level rise.You can see all sorts of parcels
that are inundated by sea waterand are now in the tidal zone,
whereas here they were not,so that's going to cause a
significant amount of propertydevaluation throughout whatever
ZIP code we're working with.The only issue with the
NOAA data, or a main issue,is that they don't
tie these sea levelincrements into
time or temporally,so we had to take IPCC.That is the International
Panel on Climate Change.We took IPCC
projections to do that.And how did we do that?Well, here you have
a graph taken outof one of the IPCC reports.You can see today, in
2020, we are about 1/3of a meter above the sea
level of 1700 to 1800 range.So we've seen about 1/3 of
a meter rise since then.Now we see these
projections here.Red line is representative of
a worst case-scenario sea levelrise, and the blue
line is representativeof a best case, along with their
respective margins of error.But these are tied
in with the x-axishere that is a temporal axis.So we can tie these
in temporally.Around 2060 is when a one-foot
increment, at the worstcase-scenario, occurred and then
2100 for a two-foot incrementabove today's levels, that is.And then we can do
similar projectionsfor the best case-scenario.Now, unfortunately,
this data didn'tappear to be available
to the public, at leastthe actual data associated
with this graph, so hadto go into engineering
software in orderto get the precision that we
desired to gather the dataourselves.So we went in and drew lines
in engineering softwareto match the times and the
elevation changes up we needed.It was not optimal, but
we got the data we needed.And what did we get out of that?Well, here is an
example of a model.We've got this blue line, which
is ZIP code data from Zillow,over time.This is the projected valuation
of each ZIP code on Zillow.Now we took a linear
regression of that data,and we extrapolated that into
the future to get a model.We, next, applied this
extrapolation line to our taxparcel data point,
which isn't exactlymatching up the
Zillow data, but wethought it was more accurate.So we moved that projection
over to our data pointas the start point in 2013, and
then we had our model, at leastthe current sea level model.So if sea level were
not to increase more,this would be the
projection we would see.Now we had to apply our
depreciation in real estateto that model, and
what did that entail?Well, we assumed, based off
of what we read in literature,that every square
foot of land thatis inundated by the tidal
zone is going to be--we're going to see a 100%
depreciation, that is,they're effectively worthless.They do not have value anymore.So we applied that
to the parcels whichare inundated in the
GIS layer, and wecan come up with our
new model once wesubtract that depreciation
amount, depreciated amount.VICTORIA MULLIN: To briefly
go over our calculations,we first calculated
the estimatesfor missing values of
appraised total valueby first finding the
median value of propertiesby neighborhood and then by
street for those that hadmissing values in neighborhood.Then we used the Zillow data to
build a linear regression modelby using different
date ranges in orderto find out which
ones will producethe most accurate results, which
we define the most accurateas being as close to
the USGS predictionsas possible, and then
used that to estimatethe predictions of
home values in 2060,which is associated with
one foot of sea level rise,and 2100, which is
associated with two feet.From there, we merged
the tax parcel datawith the increments
of sea level rise datato find what parcel
would be affectedby the different levels
of sea level rise,as well as what percentage of
the parcels will be inundated.From there, we estimated
the depreciationof homes that would be affected
and made our final home valuepredictions.Now to go over our results--the plot on the left
shows our resultsfor the number of homes
at risk by ZIP codefor the years 2060 and 2100.For reference, the map in the
center is of New Hanover Countyand is there to help you
see what ZIP codes aware,and their respective colors
match those in the bar charts.I will just refer to the
ZIP codes by their colorshere, rather than the
actual ZIP code, in hopesthat it will be
easier to follow.The ZIP codes that
have the highestpredicted number
of homes at risk,the purple and brown
ones, are both coastal,which is expected.The green and light
blue ZIP codes,as you can see from the
map, are also coastal,but they don't have quite
as many homes at risk,according to our predictions.Finally, the orange ZIP code
is the only other coastal one,and it has a very low
number of homes at risk,but this ZIP code has a
significantly higher elevationthan the other ZIP codes.And we think that's the most
plausible explanation for this.Now, the chart on the right
is USGS' predictions, whichwe can compare our results to.First off, it has
many more homesat risk for the light
blue and green ZIP codes.Our results may be different
because, for the green ZIPcode, it's missing
in the Zillow data,meaning we had to use
another similar ZIPcode as a substitute.Additionally, for the
red and teal ZIP codes,the USGS did not predict
nearly as many homes at riskas our model did.Overall, our predictions
underestimatedthe number of homes
at risk comparedto the USGS predictions,
which can be noticed justby the scale of the
y-axis, which here,on the plot on the
right for USGS,it goes up to 3,000, where
our predictions on the leftgoes up to just 1,500.Here is another representation
of our predicted numberof homes at risk, in which the
larger bars represent a highernumber of homes at risk.Overall, the coastal ZIP
codes have many more homesat risk than inland ZIP
codes, with the exceptionof the southern
most ZIP code, whichhas a very low number of homes
at risk because, as I saidearlier, it has a higher
elevation than anyof the other ZIP codes.Now to go over
the value at risk,which is the difference between
the predicted value of homeswith and without sea level
rise incorporated in orderto see the property
value that would beat risk due to sea level rise--the plot on the left
shows our predictionsfor the value at risk for
each ZIP code for the years2060 and 2100.You can see that the
purple and green ZIP codeshave the most value at risk.And they're both coasts ZIP
codes, so this is expected.The light blue ZIP
code is also coastal,and it's surprising
that it does nothave nearly as
much value at riskas some of the other ZIP codes.And finally, the orange ZIP
code has the least valueat risk, which, as we said
before, even though thisis a coastal ZIP code, it has a
significantly higher elevation,and that is most likely why it
has such a low value at risk.The plot on the right
is USGS predictions,which, similar to ours,
the green ZIP codehas a large amount at risk.However, unlike ours,
the light blue ZIP codealso has a lot at
risk and may meanthat our model for this
ZIP code is not optimal.This plot also shows that the
teal and red ZIP codes alsohave a relatively high
value at risk, whichis similar to our results.Unfortunately, the
ZIP code 28449,which is the orange ZIP code
in our plot on the left,is not included in the USGS
predictions for some reasons,so we don't have a baseline
to compare that to.Similar to the number
of homes at risk,our results have underestimated
overall the total valueat risk, which
can be pointed outby the difference in
the scale of the y-axishere, which goes as high as
$2 billion in the USGS plot,where our predictions only
go up to around $600 million.However, that is where
most of the predictionsin the USGS on the right go
to, as well, with the exceptionof the green bar in 2100.EMMA DELEHANTY: We used
the animation bar chartto visualize the result of
lost value of property thatis typical and to compare
the USGS projection.Next, we created
an interactive mapto display our results
by years 2060 and 2100.As seen from the map,
when we click zipcode,there's a lot of values
displayed on the map,there's numbers of homes at
risk, and prediction mediumvalue, and the total value
below the sea level rise,and also the total
parcels in zip code,and also predict the total
value with the sea level rise.The most important, we can
see the total value and risk.Overall, our predictions
found that the coastal ZIPcodes which include 28409,
28411, 28428, and 28480have the most value at risk
and highest number of homesat risk.Compared to the
USGS predictions,our results tended
to underestimatethe number of homes at risk,
especially for the coastal ZIPcodes 28428 and 28480, which
resulted in underestimationof the total value at risk.This underestimation may be due
to the simplicity of our modelssince we only use
linear regression.We attempted other
models, such as ARIMA.However, these models grossly
overestimated future values.Moving forward, we would try
to find a better model thatcan make predictions somewhere
in between what we gotusing linear and ARIMA models.Additionally, other things
we can do in the futureare incorporate and
predict other factors,such as storm surge,
fixed floor elevation,and future precipitation.Thank you for listening
to our presentation,and we hope that you are
all well during this time."
100,"BRANDON REESE: Hello.My name is Brandon Reese.I develop network algorithms
in the operations researchdepartment SAS.In this video, I'm presenting
a tutorial, Introducing NetworkAnalytics, including
the basics about whatyou can do with
networks and SAS Viyaand hands-on
demonstrations of usingnetwork analytics to improve
your machine learning models.My goal is that by
the end of this talk,you'll become familiar
with various techniquesfor dealing with connected
data in Viya, includingsome powerful network analysis,
optimization, and machinelearning techniques.To start things off,
what is a network?We define a network as
a representation of datausing entities and connection.We'll use the term Node to
refer to an entity and a Linkto refer to a connection
between entities.We allow an arbitrary
number of attributesto be associated with links
and or nodes in a network.We also allow for
directed networks,meaning links imply a direction,
and undirected networkswhen links don't have
an associated direction.For example, if person A
and person B are friends,that's an undirected
relationship.Now if person A subleases
an apartment from person B,that's a relationship that
has directionality to it.When this is the
case, it is betterto use a directed network
to model your data.When is network
analysis appropriate?Whenever your data have
meaningful connections.This is likely to be the
case across a wide varietyof industries since the
relationships can captureabstract ideas, like
transactions and hierarchies,but can also represent
associations, like familyand co-worker relationships
as well as movements,such as routes, paths,
and trajectories.These relationships
can be modeledas static or time-dependent.And network tools, like the
ones I will demonstrate today,can provide efficient
solutions to problemsconcerning connected data.Sometimes, these are
problems you should not evenattempt to solve using
traditional relational databasejoins.It is far simpler to
use a network datamodel when dealing with data
where connections are key.So how do you go from related
tabular data to graph data?This is often a non-trivial
step with more than one rightanswer.But as a simple exercise,
consider the relational datamodel of Cora, a public
database of academic researchpapers depicted to the left.In our graph model,
depicted to the right,we represent Papers using
nodes, Citations from one paperto another using
links, and finally werepresent the
Content of each paperas attributes on each node.Simple, right?As you work with a
variety of data sets,you will inevitably need to
define increasingly complexgraph data models.But remember the basic
principle of mapping entitiesto nodes, relationships,
movements, or actions to links,and properties to attributes,
and you'll be well on your wayto constructing a
cohesive graph data model.We will revisit
this Cora data setwhen we get to the practical
demo portion of this tutorial.Here's a catalog of the many
forms of network analysisand network
optimization techniquesthat are available
through SAS Viya.There are two action sets,
Network and OptNetwork, whichhave some overlap between them.The Network action
set contains whatyou need to do many social
network analysis and machinelearning oriented tasks.The Network action
set includes actionsthat will solve
optimization problems thattake network data as inputs.In the upcoming demo,
you will see examplesof many of these actions.As a brief introduction
to machine learning,let's discuss some terminology.The terms Supervised,
Unsupervised, Semisupervised,and Reinforcement each denote
a specific sense in which youcan answer questions
about your data,and the type of
method you implementis often a consequence
of what kind of datais available to you.Methods for all four types
of machine learning problemsare available in SAS Viya.You would use supervised
or semisupervisedlearning when the
question you are answeringinvolves a target variable,
often called a Label,which is available in
your training data.The procedure is supervised
if every training observationhas a label or semisupervised
if only some trainingobservations have labels.Unsupervised
learning tasks oftenuse associations or
clustering techniquesthat don't require
any labels at all.Reinforcement learning is an
agent-based approach often usedto train models to do some
situational decision makingtask.We will see examples of
supervised machine learningin the demo portion
of this tutorialand, more specifically, I will
demonstrate machine learningclassification in
which the model istrained to make predictions
about whether an observationfalls into one
category or another.So let's take a look at
a common workflow usedto put supervised
machine learning modelsinto production.On the top track, there are
several steps needed in orderto develop a model.Sometimes these steps
involve manual work.But often, they can be
done using highly automatedprocesses, as we'll see in the
steps of the upcoming demo.Note that iteration
is often a partof building a successful model.Because insights
from a first modelbuilding experience can
be leveraged to produceimproved models in
subsequent iterations.Along the bottom are
steps taken to deploythe machine learning model.These steps must be automated.And the pipeline
that was developedcan now be used to draw
insights and make predictionsabout new and unseen data.Next, I'll show a
couple of standard typesof machine learning models,
which are implemented usingSAS Viya in the demo to follow.Neural network models
are very generaland highly effective
tools to accomplishmany machine learning tasks.Here I show a
typical arrangementof computational units
called a Feed Forward NeuralNetwork, in particular, this
is a multilayer perceptron.The act of training
a neural net includesselecting weights
and biases thatturn this generic
computational recipeinto a model that makes
effective predictionsfor your use case.Sometimes, this training
process is started from scratch.For complex problems,
though, it is far moreeffective to begin with
pretrained weights and biasesthat were learned over a
large quantity of related dataand fine-tuning them using
data examples relatedto your exact problem.This concept is known
as transfer learning.In SAS Viya there
are two action setsthat provide neural network
modeling techniques.Those are the Deep Learn action
set and the Neural Networkaction set.Decision trees are
another type of modelthat allows classification
based on usingnested partitioning
to split your dataobservations into categories.Understanding a decision tree
is fairly straightforward.And there are fairly
simple ways to trainusing any number of
heuristics, examples of whichare the Gini impurity or the
entropy-based information gaincriteria.One widely used extension
of the decision tree conceptis the ensembling
of multiple trees.This is the idea
behind the forest modelwhich contains many
trees, each trainedaccording to its own subset
of the training data.The overall prediction
consists of a voteof the individual
predictions of all the trees.The major advantage
of forest modelsis that they generalize
better to unseen data,because they are far less likely
than a single decision treeto overfit the
training decisions.The content in the
demo that followwill introduce you to the
network analytics capabilitiesof SAS Viya and go
in depth on waysyou can achieve improved
machine learning modelswhen you integrate network
analytics into your machinelearning pipelines.In the notebooks that
follow, I will demonstrateusing the Python syntax.But keep in mind that
in addition to Python,equivalent APIs exist for the
SAS language, R, Java, Lua,and REST.I will walk you through
a four-part demo whichwill include an introduction
to network machine learningclassification using neural
network models and forestmodels, hyperparameter
selection using Autotune,and model refinement
using featureimportance for selection.First, let's take a look
at the Part_0 Notebook.This will provide
an introductionto network analytics
and SAS Viyaand will demonstrate
the Python syntaxfor running network algorithms
in the network action set.To start off, connect to
Cloud Analytic Services,known hereafter as CAS.To do this, you must
import the SWOT library.This library provides
the interface for CAS.Then connect to
the host and portwhere your CAS
server is running.Next, we import pandas and
several helper functionsI've written for this tutorial.For convenience, I will
show the definitionsof the more interesting of these
functions as we encounter them.Even though we load them up all
at once from a Python script,while you follow along
with this tutorial,refer to the GitHub link
provided with this videoto review the code
at your own pace.For the first example, I'll
demonstrate Cycle Enumeration.As the name
suggests, in a cycle,we start and end at the same
node using existing linksto connect the dots.In CAS, everything
starts as a table.Note here how we
specify the networkby providing two tables.The first table lists
the nodes in our data.The second table defines links
by indicating a From node and aTo node for each link.Next we draw a picture using the
Graph Vis library for Python.So how many cycles do you count?Spoiler alert.If you read the
documentation for network,you already know the answer.There are seven.We find them by running
the Cycle actionin the Network action set.This syntax indicates that we
would like to the input tablesas a directed graph.We want all cycles
counted and returned.And we would like
the output to beplaced into two tables called
Cycle Nodes and Cycle Links.From the log, we learned
that the algorithmfound seven cycles.Here are the first few rows of
the cycle links output table.A picture is worth
a thousand words.So let's use Graph
Vis to highlightthe seven solutions found.Great.Second, I'll demonstrate
the centrality action.Centrality is a quantitative
measurement of a node'sconnectedness in a graph.Centrality has
major implicationsfor machine learning, because,
naturally, influential actors,middlemen, and hubs of
the graph often encodemeaningful information about
the connected data in question.In the following
notebooks, you willuse centrality and
other algorithmsto create features that enhance
the predictive performanceof machine learning models.Here we create another network
from a pandas data frame.Note that the Nodes
table is optional.In this example, we
only provide links.In this case, the nodes are
derived from the endpointsof the provided links.Next, we'll run two
textbook algorithmsfor measuring centrality,
closeness and degree.The results are tabulated in
the specified output table.However, centrality is more
readily displayed in a picture.In the first picture,
larger node sizerepresents higher
closeness centrality.In the second picture,
larger node sizerepresents higher
degree centrality.Now I'll show a third example.What if you wanted to
run multiple algorithmsagainst the same graph?It is not ideal to load
graph data from CAS tablesevery time.But this is, in
fact, what happenswhen you specify the graph using
the Links Equals and or NodesEquals options.A more efficient approach is
available via the Load Graphand Unload graph actions.When using Load Graph, the CAS
tables are ingested only once.And in subsequent
algorithm calls,you can point to an in-memory
graph using the Graph Equalsoption in lieu of the Links
Equals and Node Equals options.Here's what an invocation
of Load Graph looks like.We specify the Links table once.And then subsequent action
calls need only pointto the identifier of
the loaded graph object.See how we subsequently call
two different actions usingthe centrality action and
the shortest path action,each time pointing to
a variable containingthe ID of the loaded graph.Now we show a picture
of the solution computedby the shortest path algorithm.Lastly, you can clean up
your loaded graph objectsusing the Unload Graph action
to free them from memory.Of course, terminating
your sessionalso unloads any
remaining loaded graphs.Now let's take a look
at the Part_1 Notebook.This notebook will
introduce you to the datawe will be working with
and show the steps to buildneural net
classification models.The first model we train will
use existing baseline features.But in subsequent
models, we willconstruct features based
on network connectivityto improve the predictive power
of the neural network models.The task at hand is
document classification.Given the words in an academic
paper and the other papersthat the paper
cites, can we predictthe category of this paper?The purpose of this and
the following notebooksis to illustrate
the improvement wecan take by including network
features when building machinelearning models.I'd like to note
that a recent blogpost on a technique called,
Neural Structured Learning,presents a different approach
to the same problem examiningthe same data set.The network feature Engineering
Method presented here, however,achieves higher prediction
accuracy than the graphregularization approach
presented in that article.We start by loading the data.The Load Raw Data
function, implemented here,shows the syntax for
bringing the data into CASby first reading the text
files into a pandas data frameand then uploading that
data frame into a CAS table.The data set is available
for public downloadand consists of two files.The Cora.content file
includes a multi-hot encodedrepresentation of the content
of each academic paper.The first column, which we call
Node, contains the paper ID.Then there are 1,433 word
indicator flag columnsthat equal 1 if the particular
word appears in the paper or 0if the word does not appear.Finally, the last
column, Target,contains the category
that is assignedto each academic paper.The Citation Data
contains only two columns,From and To, for which each row
indicates that the From paperID has a citation
to the To paper ID.Next I'll show a
visual representationof a subset of the data using
Graph Vis to illustrate whichpapers cite which other papers.Let's try to manually predict
the category of the mysterypaper with the ID 8617.Using only the fact that
the mystery paper citesthree neural network papers
and two theory papers,what would be your best guess
to the mystery paper's category?If you guessed that it's
a neural network paper,you're correct.This is the sort
of intuition thatbacks the various network
features that we will constructfor today's tutorial.The next steps to
building our modelsare the pre-processing
of the data.In standard fashion,
we partition the datainto a training and test
data split of 80% 20%.Here is the code to do so.In order to reduce run
to run variability,we store the partition
data and load it againon subsequent runs.Three tables are
produced in result.Content Partitioned
has all the datafrom Cora.content with
a flag indicating whichpartition each row belongs to.Content Train and Content
Test contain only the trainingand test samples respectively.Next we use the eigenvalue
principle component analysismethod in the PCA action
set to prepare PCAversions of the content tables.This is a common
technique that allowsyou to keep only the components
that contain the largesteigenvalues.This means the limited number
of selected PCA variablescan comparatively represent
most of the variation observedin the original wide data.Here's the syntax for running
the eigenvalues action.Note that to maintain
disciplined use of the datawithout leaking information, we
defined the PCA transformationusing only training data.We capture the transformation
logic using DATA step code.And then we apply it
to the full data setusing the DATA Step action set.Here I choose to keep the
first 40 principal components.I found that this was
sufficient to capture mostof the variation represented
in the 1,433-word indicatorcolumns.The next step involves
preparing the Links table.In this data set,
citations represent links.We also join the category
labels to the Links data tables.This step also represents
the disciplined use of datato avoid leaks.We create two citations tables.The first, Cites Train,
will be used for training,and the second, Cites Combined,
will be used for scoring.In both cases, we join
the citations datawith only the known labels,
those belonging to the trainingdata set.The difference between the
two is that for Cites Train,we confine ourselves
to citationswhere both the
From and To papersbelong to the training set.For the scoring
table, Cites Combined,we include links to
and from the test set.However, the test data
labels are not used.They're represented as
Missing in this table.The final pre-processing
step is the generationof network features.To do this, you can make calls
to the network action set,like we demonstrated in
the previous notebook.Because I'm about to show a
number of Python functions,I will first describe
what they do.First, we initialize
a nodes table.For this problem,
the nodes tablewill be a copy of
the Content tablesince it has one node
per academic paper.Next, we'll load
the nodes and linksinto an in-memory graph
object for faster executionof subsequent action calls.After that, we will
add node featuresby running each of the four
network actions, Centrality,Community, Core Decomposition,
and Node Similarity.After each network
algorithm runs,we then merge the
new features backinto our original nodes table.Finally, the Unload Graph
action will clean upthe in-memory graph object.Here are the implementations.Note that action calls to
Load Graph, Unload Graph, NodeSimilarity, Core,
Centrality, and Community.The initial copy of
the data and mergingis done using DATA
step run code action.I will briefly
describe the featureswe are creating using the
network algorithm specified.But fore a more complete
explanation of each algorithm,please refer to the
provided documentationlinks for network.Node similarity is called with
the flag Vector Equals True.This allows us to generate
node embeddings, whichare a compact approximate
representation of the graphconnectivity as a vector space.The vector space will have
10 dimensions as requested.We will include
the learned vectorsas features in our
machine learning models.Core decomposition gives a sense
of how interconnected a givennode is with its neighbors.A higher core number
means that a nodeis connected to a larger,
more dense group of neighbors.I found that it is valuable
to look at core number,not only for the whole graph,
but also for subgraphs.For each subgraph,
one per category,we consider only
those links thatconnect to paper of that
particular category.We take a similar approach
to decrease centrality.In this function, we compute the
degree for various subgraphs.Here, we can exploit
efficient by-group processing.In the first call to
the centrality action,we group by the From
target variable.This means that we are
looking at the degree of linkscoming in from each
particular category of paper.In the second call
to centrality action,we group by the To
target variable.This allows us to get the
degree of links going outto each particular
category of paper.Remember, this
was what we did inour initial intuitive example.By looking at the In and
Out degrees by category,we create features
that tell the modelhow many others in each
category a given paper citesand how many others in each
category cite the given paper.For the last features, we run
Community Detection to organizethe nodes into communities.Think of these as highly
connected clusters.We provide, as features, each of
the summary statistics metricsabout the community to
which a node belongs.The last function defined
here, Add Network features,runs all the steps in sequence.To complete the
data preparation,we run the Add Network features
on four versions of our data.The first version is
the training table,including baseline features
which are the word indicatorvariables from the
Cora.content fileand network features based
on the training subgraph.The second version is similar,
but includes both trainingand test samples and
uses the entire networkto generate features.We extract scoring
data from this table.The third and fourth tables
are like the first two,only they use the PCA
representation insteadof the raw baseline features.We've now produced all the
variants of training and testdata that we will be using
in this and the followingnotebooks.Now the model building begins.In this notebook, we will
train neural network models.We use the same
architecture thatis used in the neural
structured learningblog post, a dense net with
two hidden layers between inputand output.We also match the tuning
parameters from the article.Note that in the
posted run, the articlereports 78.1% accuracy for
the baseline model and 79.6%accuracy after adding
graph regularization.The first model we trained
has baseline features,in other words, only the
word indicator variablesfrom the Cora.content file.In the Python functions
here, I show the syntaxfor defining neural network
architecture using the BuildModel and Add Layer actions in
the Deep Learning actions set.Next are the implementations
for training and scoringthese models using the
dlTrain and dlScore actions.Finally, to get a statistically
significant result,similar to what is presented
in literature for this dataset, I define a
bootstrapping function.In this function, we run
with many different bootstrapsubsamplings of the
test data so that wecan present the average
and standard deviationof our accuracy results.After running the training and
the scoring of the baselinemodel, we see 76.9%
single run accuracy.After the 25 bootstrap runs
with varying random seed,we get an average of 77.7%.The second model we train
has PCA features only.Think of this as a concise
representation of the baselinefeatures.After the single run, we
observe 74.4% accuracy.And after the bootstrap runs,
we get an average accuracyof 74.4%.The third model we train
has network features only.After the single run, we
observe 84.5% accuracy.And after bootstrap runs, we get
an average accuracy of 85.9%.This is significant.Because the best
published results on thisdata are around 86% accuracy.Moving on to the fourth
model, this modelwe trained with both baseline
features and network features.After the single run, we
observe 86.2% accuracy.And after the bootstrap runs,
we get an average accuracyof 86.2%.Lastly, the fifth
model we trainedhas both PCA features
and network features.After the single run, we
observe 88.2% accuracy.And after bootstrap runs, we get
an average accuracy of 88.3%.We have now
significantly surpassedthe highest published
classification accuracy resultsfor the Cora data set.Finally, we call the
Terminate functionto clean up the session.This concludes the
Part_1 Notebookon network feature enhanced
neural net model building.Onto the Part_2 Notebook.This notebook will
provide a demonstrationof building forest models to
perform the same classificationtask we accomplished with
neural nets in notebook 1.I will also give examples of how
to use the Autotune action setto select the best
hyperparametersfor these models.In this notebook, we
can skip over the dataload and pre-processing steps.They are the same as the ones
I demonstrated in the Part_1Notebook.Let's get right into
building forest modelsto make the same kind of
document classificationpredictions we made in
the previous notebook.As I mentioned in the
intro, forest modelsare ensemble models comprised
of multiple decision trees.The ensembling is based on
a concept called Bagging.In this type of ensembled model,
each decision tree submodelis trained on a
subset of the featuresin a subsampling of
the observations.No single tree is aware of all
the columns or all the rowsof the training data set.This concept of bagging
gives the forest modelthe advantage of robustness
against overfitting.In a classification problem,
the overall predictionof the forest is defined to
be either the average voteor the majority vote
of all the trees.The fact that the
model is simplyan ensemble of trees,
as shown by my cartoon,gives the forest model
a second advantage.It is more easily explained
and interpreted than, say,a deep neural network model.All that you need
to do to understandwhy a particular
prediction was madeis to follow a path from top to
bottom of each tree answeringthe question Yes or No to the
question at each juncture.One major drawback
of forest models,however, is that performance
is highly subject to selectionof appropriate hyperparameters.These hyperparameters
include the numberof trees, the maximum depth of
a tree, the bootstrap samplingfraction, and many more.Furthermore, it is not
straightforward for a nonexpert to determine
what hyperparametershe or she should use.But fear not, SAS optimization
provides the Autotune actionset that can systematically
determine these modelhyperparameters for you.We'll get to that in a moment.For now, consider the
forest model analogsof the training, scoring,
and bootstrapping methodsI presented in the
previous notebook.In this case, we use the Forest
Train action in the DecisionTree action set.But otherwise, the
training is very similar.In the Score Forest
model function,for the sake of
demonstration, I showhow to invoke the Score action
in the ASTORE action setto score your forest model.ASTORE is a format of
representing your data scoringsteps that are
universal to SAS Viya.Note that we could
just as easilyhave scored using the
Decision Tree dot Forest Scoreaction instead.We defined bootstrap runs here
in largely the same fashionas we did in notebook 1.Now we begin by training the
model with baseline features.Note that we are not specifying
specific hyperparametersfor the forest model.For lack of a better
starting point,we will run with the
default hyperparametersfor this first
round of modeling.On a single run, this
model gives 43.4% accuracy.After the 25 bootstrap runs,
we get an average accuracyof 43.4%.Next we train a forest
model with PCA features.On a single run, the
accuracy is 44.1%,After the bootstrap runs,
the accuracy is 44.6%.Also I'll mention a major
advantage of the PCA featuresmodel over the baseline
features model.The PCA model training
takes under two seconds.This requires significantly
less computation timethan training the baseline
forest model, whichtakes about 30 seconds.This time taken
to train the modelis roughly proportional
to the number of featureswe're putting into it.So any tricks we can use to
represent the information thatis in the training
set more conciselywill yield a
computational improvement.You'll see more on that
in the third notebook.Now for the forest model trained
on only network features.The single run
accuracy is now 80.8%.And the average over
25 bootstrap runsis 80.6%, a significant
improvement comparedto the accuracy achieved when
using the baseline and PCAfeature sets.This large gap is
partially explainedby the extra information
provided by the citationnetwork connectivity.But it is also largely a result
of poor hyperparameter tuning.We'll see in the next section
that the baseline and PCAfeature models are
more severely hamperedby the ill chosen
hyperparametersthan the network
feature enhanced models.Next we train the force model
using baseline and networkfeatures.The single run accuracy
is 74.2% and the averageover bootstrap runs is 73.6%.Lastly, we train
the forest modelusing PCA and network features.After we train, the single
run accuracy is 79.2%.Percent and the average over
25 bootstrap runs is 79.6%.These last two configurations
were most likelyalso hampered by poor hyper
hyperparameter tuning.With properly chosen
model hyperparameters,we should be able to train
better forest models.Fortunately, Autotune
can help us here.Even without much theoretical
expertise or experiencetuning this particular
type of model,fairly straightforward use
of the tuned forest actionin the Autotune
action set will helpus achieve much better
prediction performance.Autotune works by trying many
different tuning configurationsin a systematic way.The implementation is a thread
and machine parallel one,which makes for an especially
fast tuning process with accessto a computing cluster.By default, Autotune
will performa search of the hyperparameter
space using a genetic algorithmand will terminate
after convergence occursor a fixed number of
iterations is reached.Also, by default, Autotune
will take our training dataand internally split it
into a trained validationpartitioning.Because Autotune has some
fairly smart behavior by defaultand because the user may not
have expertise configuringthis tool, it is
usually a good ideato start off with the default
settings and customizethings from there.Here is the function
definition for autotuningwith the Tune Forest action.As you can see, it is just
as simple as the Forest Trainaction.We'll keep everything in
Autotune to the defaults,except we'll provide
our own random seedfor reproducibility.And we'll use Misclassification
Error as our objective.We choose this
objective function,because what we care
about in the end isaccuracy, which is just 1
minus misclassification error.Note that since auto
tuning can take some time,we would be wise to
save the outputs to fileso that we have our
champion models savedfor easy later reuse.We will now run Autotune
to produce forest modelsfor the PCA features only.After that, we'll do the same
for the network features onlyand for the network and
PCA features combined.For the first configuration,
PCA features only,Autotune takes just
under 2 minutes.When we score the model, we see
a single run accuracy of 68.8%.After we run the
25 bootstrap runs,we get a bootstrap
average of 68.8%.That's a hefty increase from the
44.6% we saw without AutotuneNext, we Autotune
the forest modelwith network features only.The tuning takes
about 77 seconds.On a single run, the
prediction accuracy is 85.2%.And the bootstrap average,
over 25 runs, is 84.5%.This result also
shows improvementover the 80.6% we saw
before using auto tune.Last but not least, we
train the forest modelwith PCA and network
features, which requires justunder 3 minutes to tune.For the single run, 85.2%
accuracy is measured.When averaging the bootstrap
runs, we get 84.8%.This represents a
5% higher accuracythan the 79.6% that we
measured before selectingour hyperparameters
with Autotune.So if you learn anything from
this portion of the tutorial,remember this,
forest models can bevery good predictors but
can also be tricky to tune.The usage of both network
features and Autotunehave the potential to
significantly improvemodel performance.And of each of the
model configurations,the network features
model appearsto suffer least from inadequate
hyperparameter tuning.This is likely because
properly engineered featurestend to require less
intricate decisionlogic to make good predictions.This is an important corollary
to the general conceptthat models with fewer features
tend to generalize better.Finally, we call the
Terminate functionto clean up the session.This concludes the
Part 2 Notebookon network feature enhanced
forest model buildingand hyperparameter
tuning using Autotune.Moving on to the final
Notebook, Part 3.This Notebook will focus on
feature importance methodsand using them to
construct lean models.A seasoned veteran
data scientistonce told me that in
all his experience,he had never built a model that
he could not reduce to no morethan 10 to 12 features.He went on to make the
controversial claimthat if your model is
trained on hundredsor even tens of features, you
probably still have more workto do.Well, we're not going to get
into whether that claim istrue in general.In this Notebook,
we will at leasttest this theory for our core
academic paper classificationtask.To achieve our goal of building
lean but high accuracy models,we will follow a
simple approach.First, measure
feature importanceusing three different methods.Then retain only the top
12 most important featuresaccording to each method.We will be measuring
feature importancesin our most effective
forest model,the one that combines
PCA and network features.In that model, we used 40
PCA features and 45 networkgenerated features.In this Notebook,
I'll be showingone tried and true method for
obtaining feature importance.But I will also be demonstrating
two brand new approachesas well.Once again, we will skip
over the data load and datapre-processing.They are identical to the
code we looked at step-by-stepin the Part 1 Notebook.Next, let's load the
best performing forestmodel from Part 2 to get a
closer look at the best foresthyperparameters that
auto tune found for us.We print them out here.Next, we will retrain a model
with these hyperparameters.We do this so that we can use
the built-in feature importancecalculation by setting the
flag var emp equal to true.Once again, this
is the definitionof the trained forest function.Note that this time instead
of running with defaults,we now have the hyperparameters
that were chosen by auto tune.This time, we will
traverse the branchof the code that sets
each hyperparameterto the following value.We also indicate
using the var emp flagthat we would like
the forest trainaction to also compute the Gini
importances for each feature.Now, after printing
out the top 12 featuresaccording to the Gini
split-based criterion,we see right away that
the core decompositionfeatures and the first
few principal componentswere considered important
for training this model.Also mixed in are n degree from
links in the genetic algorithmscategory and the 10th
principle component.I'm denoting this
feature importance methodas split-based
Gini importance sothat I can distinguish
it from the new method Iwould describe later.In split-based
Gini importance, weconsider the tree
split-by-split.To get a features
importance score,we add up the Gini gain
found at each of the splitswhere the model chooses
based on that feature.Without going into
too much detail,I will now describe two new ways
to quantify feature importancefor tree-based models.Both of these methods
have a patent pending.The first method called
betweenness importanceis fairly straightforward.In network theory,
betweenness centrality isa metric for how often a
node is on the shortestpath between a
pair of terminals,or endpoints when
considering the shortest pathbetween all pairs of terminals.In the betweenness
feature importance method,the betweenness centrality
of each node in the treedetermines the
contribution of that node.And an importance
score of a featureis simply the sum of
contributions for all nodeswhere the model chooses
based on that feature.The rationale behind this method
of measuring feature importanceis that decision tree and
forest models are actuallyjust networks that we can
analyze like any other network.Between this, let us measure
which nodes in a networkare important gatekeepers
to the flow of information.In this case, high
betweenness isanalogous to high importance
in the network of decisionmaking rules
represented by the tree.Now, let's look at
the top 12 featuresranked by betweenness
importance.Again, we see that the
core decomposition featuresand the first few
principal componentswere considered most important.Albeit, this time they
appear in a different order.Now, let's move on to what I
call leaf-based Gini featureimportances.This idea is different from
split-based Gini featureimportance in that
instead of findingGini gains split-by-split,
we instead calculateGini gain subtree-by-subtree.For a given subtree,
we use the differencein 2 subtree variance
to find the gain.The original subtree versus
the subtree with the root nodereplaced by a trivial node.The intuition behind this
way of measuring importanceis that if a feature is
important to the decisionlogic, its replacement
will detrimentallyaffect the Gini impurity score.Take a look at the top 12
most important featureschosen by leaf-based method.This time, core
decomposition featuresare again considered important.But only 2 PCA features
appear in the top 12.They got bumped out in
favor of additional featuresbased on degree.Just as a summary
of the top featureschosen in each of
the three methods,we find the differences
between all three top 12 sets.We find that the
leaf-based methodchooses n degree for
the rule learningand case-based categories,
whereas the other twomethods do not.Instead, the other two methods
split-based and betweennessinclude the PCA
components 1 and 10.In fact, the betweenness
and split-based methodschoose exactly the same set
of 12 most important features.We will now attempt
to build models usingonly these sets of 12 features.Remember that for both
our neural network modelapproach from the
Part 1 Notebookand our forest model approach
from the Part 2 Notebook,the best performing
models use a totalof 85 features, 40 coming from
principal component analysisand the other 45 coming from
various network algorithms.Let's see if we can
achieve similar accuracywhen we use only 12 of them.First up, we'll
try the feature setfound by both the betweenness
and the split-based Ginifeature importance methods.When training the
neural network model,we feed the deep learn action
set the same hyperparametersas before.After training, we see a
single run scoring accuracyof 88.0% on our test data.25 bootstrap runs later, we
find that the average accuracyis 88%.Moving right along, let's
train forest models first usingthe default hyperparameters
of the forest train in action.After training on
this feature set,the single run
accuracy is 79.1%.After the bootstrap runs, we see
that the 25 run average is also79.1%.Next, we'll train the
forest model but this timewith the best hyperparameters
chosen by the Autotune actionset from the Part 2 Notebook.The accuracy of
one run is 86.3%.And after the bootstrap samples,
the average accuracy is 86.4%.Now, let's repeat the analysis
with the top 12 featuresas selected by leaf-based
feature importance.For the neural network model,
we see a single run accuracyof 87.6%.And after the bootstrap runs,
we obtain an average accuracyof 87.1%.Now for the forest model trained
with default hyperparameters.The accuracy on one run
of the test data is 79.3%.After the 25 bootstrap
runs, the bootstrapaverage accuracy of 79.1%.And now, the forest model with
autotuned hyperparameters.After we train the model, the
single run accuracy is 87.1%.And after running 25 times,
the bootstrap average accuracyis 85.8%.Last of all, we call
the terminate functionto clean up the session.This concludes the Part 3
Notebook on feature importancemethods and constructing
lean models.Let's summarize the models
that we have trainedand consider both the
performance of each modelin terms of accuracy
and time requiredin training for each
variant of modelpresented in this tutorial.We train two types of model,
neural network and forestmodels, where the neural
network model basedon others' prior
work, we alreadyknew an effective
architecture to try,the multi-layer perceptron
with two hidden layers.And we already had a decent
set of hyperparametersthat gave very nice accuracy
performance out of the box,roughly 78% on our
baseline feature set.This baseline set of features
consisted of 1,433 binary flagvariables, each of
which correspondto a particular word.And the flag variable
is equal to 1if the paper contains
the word or 0if the word is not
used in the paper.The same baseline feature set
proved difficult for the forestmodel when using the algorithm
default hyperparameters.The first feature engineering
technique we triedwas principal component
analysis in orderto reduce the dimensionality
of the training dataand also to remove some of the
superfluous noise dominatedvariation from our
input feature set.We found that keeping only the
first 40 principal components,we obtained largely similar
accuracy performancewhile considerably
simplifying the models.Another beneficial side
effect of this changewas reducing the time required
to train either type of model.In fact, we see that
there is generallya direct variation between
the number of featureswe feed to a model and the time
it takes to train that model.This is true for both the neural
network and the forest models.The next step, which led
to significant improvementsin accuracy, was
creating featuresusing four particular
network analytics algorithmsin the network action set--centrality, notes similarity,
community detection,and core decomposition.This provided a major
lift in model accuracyprimarily because
together, these featuresserved to encode the structure
of network connectionsgiven by the known
paper-to-paper citations.The next two feature
sets we triedwere combinations
of the baseline,or PCA feature sets with
the network features.And this is the typical usage
pattern in most applications.We look to improve a
model by supplementingthe already available
feature set with new featuresthat we generate from
network analytics.For the forest models,
we found that wecould do better by selecting
better hyperparameters.As a systematic and
straightforward wayof selecting good
hyperparameters,I demonstrated the
auto tune actionset which produced roughly
24% lift in accuracy,or the PCA features model.And roughly 4% to
6% lift in accuracyfor each of the network
enhanced feature sets.For the last two
feature sets, wecombined a bit of innovation
with the old-fashioned adage,less is more.I demonstrated three ways of
quantifying feature importancebased on a tree-based model.And then we repeated our
training and scoring processwith only the set of 12
most important features.And upon examination
of the results,we find that there is little
to no degradation in accuracyperformance when we
restrict each of our modelsto the 12 most
important features.This is huge.Because not only does shrinking
the feature set reducecomputational costs
to train these models,it also results in models that
are less likely to overfitto our training observations
and therefore arebetter able to generalize to
new and unseen observations.One last benefit
of a lean model isthat by examining a
small number of features,we really understand better
what the model is doingand why it performs
as well as it does.I reviewed publications that
report prediction accuracyfor document classification
against the same core data set.The state of the art at the
time of this presentationwas roughly 86%.With the methods shown
in this tutorial,we're able to
fairly quickly builda forest model that
is comparable to,and neural network
models that surpassthe best known published work
to date in terms of accuracy.A key benefit of this method of
using network enhanced featuresto improve your machine learning
models is explainability.To explain where your
model features come from,you need only describe
your data as a networkand describe the
network related metric,or metrics you are using.And in practice, any
network algorithmthat produces per
node attributeslike the ones listed here
and potentially encodeuseful information
about your network data.When you have connected
data, try experimentingwith these algorithms.Each is available in the
network action set to improveyour classification models.In closing, I want to emphasize
that network techniques,Autotune, and effective
feature importance, rankingand selection techniques can be
formidable assets in each phaseof your model building process.Each piece has been demonstrated
in the code in this tutorial.Thank you for taking the
time to view this tutorial.I do hope it makes the process
of incorporating networkinto your machine
learning pipelines easier.And be on the lookout
for the latestnew features and enhancements
coming to the SAS Viyaplatform."
101,"JERRY PENDERGRASS: Hello.This is Jerry Pendergrass.I'm doing a talk today on the
CASL language specificallydesigned to interact
with SAS Viya.My name is Jerry Pendergrass.I work at SAS Institute.I'm a distinguished
software developerwho had 30 years at SAS doing
anything from Unix developmentto core development to
working on the CAS team,working on the Viya server.And now I am one of the
primary developers for the CASLprogram.So what are we really
talking about here?We've got a new language that is
specifically designed for SAS.I want to really entice
you to interact with it,understand it, and make it
part of your team, a biggerpart of your tools that
you're going to useto access the CAS server.When I did this, I
designed a programthat was embeddable into
many different locations.So currently we have three
applications of this language.The first application is a PROC.It is PROC CAS.It runs in SAS.It has become one of the
most important testing toolsthat we have today in SAS
to operate the SAS server.Almost all of our testing
is done through PROC CAS.It has been a life-saving
program that gives us so muchfunctionality.We also have the
SCCASL RUNCASL action.This allows you to take your
program that you would normallyrun in SAS but run it actually
as an action in the server.This allows you to get
closer to your data.And once you're in the
server, you actuallyhave a few extra special
things you can do.But the code is identical
from one place to the other.We actually don't know
where you're running.It looks the same no
matter which locationyou're running from.We also allow you
to put an interfaceon top of that program.That interface allows you
to build an action set.You can build your own action.You can build and deploy
whatever you want to.And anyone can use that action.They don't-- they don't need
to know anything other than,what's the interface
to that action?This action allows you to maybe
pull together multiple actionsinto one super action that
does many things for you.And this way, you don't have
to give the code to people.It's automatically available.A PROC CAS, we made it very
similar today DATA STEP.We can't say it's
identical because weneed it so much more, so
much more functionality.So it's similar.When you look at the
interface, it's similar.That's all we can say.But it is integrated into
our Output Delivery System.That makes you-- that gives you
the ability to really produceelegant reports.ODS is very, very powerful.And it is pretty much
what all the PROCsuse to display their output.But the main purpose of CASL,
the main purpose of PROC CASis to submit an action
and process the results.That's what we're going for
is to submit and process.Now, here's an example
of some CASL code.We'll see exactly what this
all means in a few minutes.But generally, we're
talking about givinga name of an action, giving
some control, optionsfor that action, and
then giving parameters.And the parameters are what
we call a list of directories.Each directory has
a list of things.One of these lists happens to be
our car table, which has a nameand a where clause.And those are what we're
going to send to our action.Now, the SCCASL RUNCASL
action is very similar.I said it is an action.So you execute the CASL code
on the server environment.It returns a prediction
area as its result.So you can just determine what
it is that you want to return.It is under your control.And you can also
provide an exit status,exactly to tell me
what actually happened.You are in full control
at that exit status.You can also provide input
arguments and macros.The server side does emulate
the SYSPUT and SYSGET operationsthat you will find on the
SAS side of the world.Now, an example of what CASL
is, you specify the actionset, and give me the code
as a character string.You specify whatever
variables you want.And that's in the
form of a dictionary.And then you specify
the macros you want.And those form of a
dictionary as well.Now, example programs of
a CASL program, here'san example of just running
a simple summary operation.I'm going to save the
results in a variable.I'm going to save the
status in a variable,and then I'm going to specify
what my table name is.Once that runs, I'm going to
check that severity status.What was it?Did it run correctly, or
did it not run correctly?If it's zero, then were good.If it's not zero, then I'm going
to set an exit status to say,I'm sorry this did not work.That exit status
is all up to you.Whatever you want
it to do, that'swhat you can put in there.Otherwise, if I get
a zero response,I'm going to send a response.And that response is
going to take this--the argument is going
to be a dictionary.I'm going to say table
equals mysum.Summary.That happens to be the
result I want to send back.And that's what I will get
back on the client side.And one of the things we want
to make sure we understandis exactly what we're
talking about here.This is a picture of our server.It's got workers.It's got a main controller
that you log into.And then you got the
session controllerwhich is created for
you and you alone.That's who we would talk to.We're talking to this
session controller.And we're giving get work to do.And that work gets
spread across the workersas we execute actions.So let's prepare some
arguments for our actions.One of the things that
CASL does very well,we want to be able
to submit an actionand give it the
parameters it needs.Those come in two forms.One form is this array.The other form is a dictionary.So let's take a look
at what that means.Well, first, we'll take a look
at the environment argumentsthat we can specify.You've seen these before.I can say result equals and
give it a variable name.But whenever my
results come back,it will go into that
variable as a dictionary.I can specify the
name by status.That, too, is a dictionary.I can specify the name of my
session because in PROC CAS,we can inherit multiple sessions
from the SAS session itself.So when you use a CAS
statement to create a session,that session can
be specified so youcan point an action to any
particular session you want to.You can then also ask for
it to be run asynchronously.That means CASL will
come back immediately.We will submit the
action for you.The action will remain--the results will remain on the
server until you ask for them.So this allows you to submit
code to multiple sessionsand then monitor them
within CASL itself.Or you may be able to
submit actions and thenleave the session and
then come back laterand retrieve your output.Now, arrays are very important.They are essentially a
part of dictionaries.If you've got a
dictionary, you mayhave a dictionary that says,
here's one of my options.But that option may actually
be an array of values.So arrays could be referenced
in one of two ways.You've got curly braces
that create arrays,and you've got brackets
which you reference itemswithin an array.Now, the arrays could
be multi-dimensionalif you'd like.We don't really care how
many dimensions you have.And they are all referenced
as indices, as 1 comma 2comma 3 comma 4 would reference
a four-dimensional array.Indexes always start at 1.We don't have 0 as an index.So that really was
a decision we madeto be compatible with really
what we thought peoplewould use this system for.Now, here's an example
of some arrays.We have some character
arrays where maybe you'respecifying a set of variables.We got make, model, type, MSRP.You may have values, integers.You may have lists within lists.Now, we got a list within
a list within a list.I can have dictionaries
within a list.So you can pretty much
exchange any of these valueswith other values that
are lists, dictionaries,doubles, strings.As you see, you can reference
or create a dictionary with--or going to write with just
using the angle bracketsequals.Now, a directory is a
little bit differentin that they have a key.So every value in the dictionary
is referenced by the key.The location in this
list doesn't matter.Its location is not guaranteed.So we generally
compact dictionaries.Arrays can be sparse.But we do allow you to
reference dictionariesthrough array
indexes, but we don'tguarantee what you will get.One of the nice
things we allow iswe can, say, do
over a dictionary,and it will traverse each
item within that dictionarywhile in the loop.So you can loop on every
item in the dictionary.Examples of dictionaries
are carstbl.name.Carstbl.name, the
dot operation, that'san indication of a directory.The dot is a special operator
that is very, very importantto CASL.If you just say CASL--carstbl.name, that says
I've got a dictionary.I must set that
value to carssashelp.I can set the where clause as
well with that designation.Or if I wanted to, I could
do it the way I did it beforeand just specify it with angle
brackets-- the curly braces.Once again, we can look
at what a table option isto our fetch option here.We're going to fetch
a table, and we'regoing to specify a variable.So we're going to
specify our sortby.And we'll see a little
bit more about howwe can deal with the
result that come from this.Now, one of the things that
we also want to do in CASL,we want to be able
to process results.If you could process
results, we cangenerate the really
nice output, wecan generate really
elegant reports.So what do you get
back from an action?You get back results.The results is a
dictionary of values.They could be integers,
they could be doubles,they could be strings,
they could be binary.It could be maybe an image.It could be an audiotape.It could be a video.So any of those are
likely to come back.The most prevalent
result you will get backis an action table.We call it a result table.The result table, essentially
what you would think itwould be in relation
to a DATA STEP.It is something that has
rows, columns, data types.It's got formats.And they come essentially
in a two-dimensional form.They've got rows from 1 to n.And then you've got
columns that have names.Within CASL, you can
reference a tableas a two-dimensional array
with the first array indexbeing the row number and
the second being the column.So you can reference an
individual item within a tableby saying row comma name of
column And if you want to say,I know what row it is, you
can put the number there.But once again, we generally
suggest you put the name justto make sure you're not wrong.Now, one of the things that we
want to look at that resultsare returned in the
form of a dictionary.We have to remember that.So if we do a similar
operation, you'regetting back a result table.That result table
isn't your result.That result table
is in a dictionary.You must pull it out
in order to use it.That's one of the most
prevalent mistakespeople make is trying to
use a dictionary as an itemsuch as a result table.Now, each client
represents these tablesin whatever form they choose.In Python, you use a data table.You use panda.In CAS, we essentially
allow everythingto be a two-dimensional array.And we allow you to see
exactly what you got back.We don't process the
information at all.We just present it to you
in the form that it came.Now, there are two
important functions herethat can operate
on result tables.One of the things
that people like to dois create their own result
tables from other result tablesor have their own data that they
want to add rows to a table.They want to be able to
create their own tables,maybe combine information from
multiple tables into one table.And then they can use
that table possiblyas an argument to a
function or to an action.Here's an example of
creating a new table.I've got four
labels, four columns.I have four data types.And then I'm going to
supply two rows of data.So once this is done, I've got
a result table of two rows.Now, I'm going to add two
more rows to that table.And then when I'm
going to print it.And all I'm going
to say is print.I don't tell it anything else.And what I get back is
this really nice output.I get Output Delivery
System output.I get a nice graphical
representation of the data.You'll see more about what ODS
can do a little bit later on.But another thing
what we do in CASLis a lot of processing
through the expression parser.The expression parser allows
you to take things and createnew things.Two of the operations
that we haveare the WHERE clause
and the COMPUTE clause.They're probably similar to
terms you would see in SQL,but the WHERE clause
will take a tableand will run an expression
over every row of that table.And it will keep whatever
rows meet that criteria.It allows you to subset a table.Now, you can do a
COMPUTE operationand compute a new column.And that's pretty cool, too.Let's see, here's an
example of a WHERE clause.So my new car will
essentially be cars whereMSRP is greater than 30,000.Or I can compute a new
column saying I've got row--I've got ratio, I've got
MSRP divided about invoice.I want it to be best 5.3.And my new column value is going
to be MSRP divided by invoice.Now, we can do more than this.We can actually
combine these two.And we can look at
something like this.It's much more about taking the
power of CASL and utilizing it.Here, we're doing
a WHERE clause.And then the dots says
do the next operation,which is a COMPUTE clause.And then they got
the angle bracket,which is a subset 1 through
5, which says give methe first five rows.And then I want you to
also subset the columnsthrough these four columns.And when you get done,
you have a table that'sfive rows and four columns.That's pretty cool.And we're going to look a little
more at the Output DeliverySystem and exactly what
it could do for us.So it really is
used by every PROC.This is how SAS output is done.It's important to
understand that it alsounderstands bygroup operations.So you'll see in the next
example I've got a PROC CAS.And I've got a GLM operation.I've got a BY clause in here.So I'm going to get bygroups.And I'm going to
print two through six.Well, I happen to know
that two through sixhappens to be the first group.So I'm going to
look at the output.And it's pretty cool.It actually displays
in a really nice form.It understands it's a bygroup.It tells me which bygroup it is.This is exactly the same
output you would get from GLM.One of the other
things we need to dois really allow
you to format data.CASL does support all
the standalone formats.We also allow you to
create dynamic formatsthrough dynamic
format variables.And we provide
the PUT operation.Let's take a look at that.So here's a PUT operation.You give it an expression,
whatever expressionyou want in a format.Next, you do a bestformat
equals best6.5.That means bestformat is now
a variable within CASL thatunderstands that he's a best6.5.I can change that to any
other thing I want to anytimeI want to.And then I can use it
in a PUT statement,and I get my expected results.So bestformat could be anything.But in this particular
case, it's best6.5.One of the things
we've learned so far,hopefully, is that CASL is a
very powerful language allowingyou access to CAS.It is the premier way
that SAS accesses CAS.The expression parser
is very powerful.That's where you
get all your power.To generate really
good reports, youneed to be able to process
your data into a formthat ODS can take.And that's going to be
taking data and putting itinto the form of a result table.But CASL itself really
has two purposes.It wants to create
your parameters,it wants to submit
your actions, and thenit also produce elegant reports.As I say, these reports
will go through ODS.And it is the same
output you wouldexpect from anywhere else.But I want you to remember
that there are three productsthat we're dealing here with.We've got PROC CAS that
is on the client side,we have RUNCASL which allows you
to run your code on the serverside, and then
DefineAction, whichallows you to take that code
and place an interface on topof it that dictates exactly what
the interface is going to beand acts just like any other
action you've ever seen.Now, I really want
you to try CAS.I want you to try PROC CAS.I want to try CASL,
RUNCASL, Define CASL Action,see how easy it could
be to really interfacewith your server.But that's just the beginning.CASL is a very powerful thing.You can read about it in the
recommended reading here.I hope you've had a good time.I hope that you've
learned something.I hope that you'll
have a good dayand that maybe you will
use CASL in the future.Thank you."
102,"Hello.Welcome to the SAS
Global Forum 2020.I'm Xilong Chen, and I'm
a senior manager at SASand the head of econometrics
and time series analysis group.Today, I will introduce to
you three powerful time seriestools for the multiple
time series analysis.They are all linked to
Nobel Prize winners--Sims, Granger, and Engle.Sims introduced the
vector autoregressionto the macroeconomics field and
Granger for the lots of efforton the study of cointegration
which addressed the vectorerror correction models.Engle proposed the ARCH
model and later extendedto the GARCH model to modeling
the volatility in the timeseries.Let's start with the
vector autoregression.We all know some
variable y dependson x you do the regression
as shown in the left plot.And we know that the y
depends on its own pastthen you use the
autoregression asshown in the right side plot.But what happens
if the y dependson its own past and the
other time series pastor and the other
time series x dependson its own past and y's past.Then you go into the world
of vector autoregression.If it's written
in the equations,it's just on the
left hand side, youput all the dependent
variables in the vectorand on the right
side all their past.And for the innovations
or the errors,now it's set on each time step,
there are multiple errors,and they are correlated
with each other.This is the vector
autoregression.Let's use the example
to show what happened.We'll model...Here is the plot of the
inflation, unemployment rate,and interest rate and plot
them together and they'requarterly data and
plot them together.It's difficult to tell what's
the relationship between them.So that's just put them in
the vector autoregression.Here, we use the procedure
called proc varmax,and you input the data.And here, we do a
little bit of a trickto exclude the eight quarters
data out so that later,we'll do the out of
sample forecasting.And then you provide ID.This is a quarterly data.And then in the model statement,
you put your dependent variablein-- inflation, unemployment,
and the interest rate.Here, we need to do the first
task for model selectionbecause we are not sure
how much past informationwe should include--one quarter, one
year, and 10 year.And we use the minimum
information criteria methodto help us to determine
how many lags of data,of past information,
we should include.And here, we put
in and then the ...As shown in this result, it
shows that the four lag, AR 4,has the minimum AICC.And then it means
that we should use pequals four in the
later for our lag.In the vector
autoregression, thereare two important concepts.One is the Granger
causality test,the other is the impulse
response function.I will talk later.And now for the
Granger causality test.It's not the similar
causality testas in the statistics
or other field.It's just talking about if x
could help the forecast of y.If it does, then we
said x Granger causes y.Under the test, it is testing
that x cannot help the forecasty.So we provided the
causal statement.You just put the y
in the group one.It's a list of variables--
maybe one, maybe a lot.And all the x variables in
the group two like here.We have three
variables, and we willtest if y1 and y3 could
help the forecasting of y2.In our example, we
have three variables,and then all combination may
lead to the 12 combination.But here, we just
show six of themand to see if one variable could
help the forecast of the other.And as shown in this result, you
could see that test 1, 2 and 5,6 are all rejected at a
5% significance level.It means that unemployment
rate and interest ratecould help the
forecast of inflation,and the inflation
and unemployment ratecould have the forecast
of the interest rate.But neither the inflation
nor the interest ratecan help the forecast of
the unemployment rate.And another concept is about
impulse response function.It's a little bit complex.It's just talking about
how the past shocks havethe effect on some variable.And in varmax, it
becomes pretty easy.You could directly plot them
out and also choose how manylags you have the interest.An like here, it
shows that when thereis shocks in the inflation
and what happens,what is the effect along time on
the other dependent variables.And let's go one more step
it's about the model selectionit's that we know that the
inflation and the interestrates cannot help the
forecasts of unemployment rate.Then if we could kick
them out of the model,that's what happens.We will use the restrict
statement to do so.After that, we could compare
it to the unrestricted vectorautoregression.We could see the restricted
version has smaller loglikelihood.It means the in-sample
fit decreases.But it does have
the better AICC.And here is the forecast
for the out-of-sample data.Now let's shift gears to the
vector error correction model.One interesting fact
is that in real world,there are lots of
nonstationary time series.But most of the models are for
the stationary time series.What's wrong with the
nonstationary time series?Let's see.These are two time
series not stationary.They seem to have
a common trend.If you do the
regression, then you'llsee how everything seems good.But you plot residuals
out, it's not that good.If you test it,
it's nonstationary.It means that we might have
something totally wrong.In fact, yeah, if we plot
the next 1,000 observations,we could see these two time
series doesn't share any trend.And in fact, the true DGP shows
that one is the random walkfrom normal distribution.The other is the
random walk of uniform.They are uncorrelated at all.Then for this
important question,Granger provided lots of
details and his researchon cointegration.It means that there is some-- we
call it long range equilibriumexists in the economic theory.We should use them to model
these nonstationary timeseries.And if we could find a linear
combination of these timeseries, nonstationary
time series,and later lead to a
stationary time series,then we should use
this information.And about how to
test it, we providein the COINTTEST option.And I don't talk about them now.And that's just to see how
to use this cointegrationInformation in the vector
error correction models.And after differentiation,
it's pretty easyto get the stationary one.I list them in the red.It's a delta y_t.And the long run relationship
as modeled in the green one,beta prime yt minus 1.And then all the
short run relationshipcould be introduced from the
A times the delta yt minus 1.So it means that in the
vector error correction model,it's pretty simple,
but it incorporatesthe both long and short run
relationship in the model.And we give an example on
the purchasing power parity,and it is just talking
about in the US,if you spend $1.50 for
bread, then in the UK,you'll spend a--oh, that's a typo--$0.80 for the--80 pounds, oh, no, 0.8
pounds, for the bread.Then for the same bread, these
two dollar values or currencyvalues should be the same.So then it means
that the exchangerate over the so-called price
index should be a constant.This is the PPP.And we put it in
logrithm, and thenthe log exchange rate
minus the log price indexshould be some constant.This is the long
run equilibrium.We plotted the log price index
and the log exchange rate here.And then this looks like
sharing some same trend.And put it in vector
error correction model.We use the cointeg statement.And as shown here--yes, and the estimated result
shows in the green boxes,we catch the long
run relationship.And in the blue
boxes, we're talkingabout we catch the
short run relationship.So in the vector error
correction model,we get both of them.And here is the
out-of-sample forecast.It's pretty good.Now let's talk about
the GARCH models.Most of the financial
times series,they have one common fact.It's that their volatility
seems time varying.And there is a fact.It's called the
volatility clustering.The big volatility follows
the big volatility,and the small volatility follows
the small volatility change.And here is the two plots of the
Japanese market and US market.You could see this one.And how to model it?Yeah, we'll talk about it later.But first thing-- let's link
the risk to the volatility.And in most of the cases
we use the volatilityto measure risk, but why?Here is a quick example.And for example, you're
working on the investment.And then-- yeah.And when you invets in the
stock, here is an example.The expected return is
a little bit positive--0.275%.And the standard
error is pretty big--1%.And if you invest on
this one, then youhave a big chance,
39% chance, to getin some negative returns.It's showing in the red area.And then it is the risk.You'll get some loss.So we quickly link the
standard deviation to the risk.And the bigger the
standard deviation,then you have bigger risk
to get some negative return.And how the correlation gets in?If you could, let's first focus
on that all these stocks haveno correlation with each other.Now if you could
find 10 such stocks,then even they have the
same expected return,same standard deviation.But your risk to get
some negative returnswould dramatically
decrease to the 19%, half.If you could find 100 such
stocks, it's just 0.3%.If 400 such stocks,
you're almostsure could get the
0.275% positive return.And if there exist some
positive correlationsbetween these stocks,
it means that youneed to find more such
stocks to make sure youhave the positive return.And if there is
negative correlation,then you just need
very few stocks.But it's still to make sure
you have that positive return.So this means the
correlation has a big impacton your risk of loss.So now we know how the risk
is linked to the volatility.We could focus on how to model
or forecast such volatility.We'll use the so-called
dynamic conditional correlationGJR-GARCH (1,1) model.This is a long name,
but in fact, it's justtalking about mainly two part.One is that for
each time series,you will use the
GJR-GARCH (1,1) on it.The GARCH (1,1) is that it
only focuses on the-- sorry--current volatility only depends
upon the previous volatilityand the previous
squared innovation.And the GJR part is
talking about news impact,and we'll talk about it later.Then the second big
part is the DCC part,the dynamic conditional
correlation.It means we do think the
conditional correlation wouldchange over time.Let's see if that's true.In the PROC VARMAX, now you
need to use the GARCH statement.You just defined DCC in the
form and the GJR in the subformand p=1, q=1 for
the GARCH (1,1).And you can even output all
these conditional volatilityin the OUTHT option.So we'll keep going on.Then here is to the
estimation result.We first talk about what
means the news impact curve.And we use the GJR
just because wedo believe that the
negative shocks in priceand positive shocks in price
should have different impacton the future volatility.In fact, as shown in
this plot, that's true.And now let's see
the other parameter.It's about the dynamic
conditional correlationversus the constant
conditional correlation.We could see DCCA and
DCCB in the red box.They're both significant.In the plot, we could
see that the correlationdoes change over time.So we choose the right model.But how to use it?Why does it matter?We use the example of in
the portfolio optimization.For the trading,
most of the time,yeah, it should be the risk
and the return trade off.And here, we use one of
the objective functions.We want to maximize the return--at the same time,
minimize the risk.And the balance,
we use the gammait's about your risk
averse attitude.And then to put them together.And then now you have
just the one objective.About the return, we
use the momentum factor.This is one of the paper.It's cited more than 12,000.So yeah, and for the volatility
part, you see the big sigma,and, yeah, we'll use what we
get from our DCC GARCH model.And put them together, and
we test it against the threebenchmark strategies.There are you only
hold the Japaneseor you only hold the US or you
just equally hold both of them.And the test period
is the 55 years.You could see our--this is the wealth curve.It means that in
1955, if you have onlythat $1, and after 55 years,
under different strategyleads you to the
different wealth value.We could see the green
line is our strategy.We balance the risk
and volatility.It's about 60% better
than the second best.And here is a table to
show the same result.We're also talking
about the Sharpe ratio.It's-- yeah.And here, we introduce the
vector autoregression, vectorerror correction models, and
the multivariate GARCH models.Later, I hope that
I have a chanceto introduce you the Bayesian
features in the PROC VARMAXor the other two more complex
models for multiple time seriesanalysis, they are state space
model and hidden Markov model.If you have any questions,
please just send me the emailor ask me now.Thank you."
103,"Hey, everybody.I want to welcome you to the
virtual SAS Global Forum 2020.We're going to talk
about a beginner's guideto using arrays
and do loops today.My name is Jennifer Waller.I work at Augusta University in
the division of biostatisticsand data science.I'm a professor there, and
have been there since 1997.And so let's get started on
using arrays and do loops.And so we're going to talk
about what an array and a doloop are, and give you some
examples to go from therein how to set one up.So what is an array?An array is just a set of
variables of the same type--specifically the same type,
all numeric or all character--that you want to perform
the same operation on.So maybe you want
to set everythingfrom missing to 0
or 0 to missing,but you want to perform
that same operation on them.You reference that array
with an array name,and that array name
is used to correspondto a set of variables.And those variables are
called elements of the array.And how do we actually access
those elements of the array?We do that using a do loop.And so what we can do--do loop references
back to the array.You can change values
on a set of variables.You can create new variables
from an existing set.And you can change
the data structure.You can do all
kinds of stuff withdo loops, not just referencing
an array, other things.And we'll hit those
at the very end.But a do loop is just you step
through the items in the array.And that's what it's used to do.So for today, we're
going to be lookingat a set of data on the Center
for Epidemiologic Depressionscale or the CES-D scale.It is a 20 item
questionnaire, and it'sused to assess depressive
symptomatology.It is scored on a
scale of zero to threeAnd the total score on
the CES-D is determinedby summing up the 20 items.One thing to note, though,
this is a depression scale,and some of the questions are
written in the positive form.So items 4, 8, 12, and 16
are written positively.So I feel happy,
rather than I feel sad.And so those items need
to be reversed before wecreate that total score.And so here's just an
example of the data.I have data out on GitHub
if you want to follow along.This used to be a
hands-on workshop.And if you wanted to go
on to GitHub and get that,you have CES-D data there
that you can play with.But you'll notice there are
20 items valued zero to three.Sometimes people don't answer an
item, so that item is missing.But the idea is that these
items- item 4, 8, 12, and 16--need to be reversed.So how do we do that?Well, with just four items,
it's not that bad, right?You can just take each
item, subtract it from 3,and there you go.There is how we would
create those reversals.And so, again, we're
performing that same operationon each of these four variables.And it's not really
a problem whenyou have just a small
number of variables.But what if you
had 100 variables?What if you had to do
this on 100 variables?That would be painful
to copy and paste codeover and over and
over and over again.Whereas with an
array and do loop,you can do it in just
a few lines of code.And so you can make your
programs more efficient,but you can also reduce the
opportunity to create a typo.And so there are several
different types of arrays,and several different
types of do loops.We're going to focus
today on indexed arraysand iterative do loops.And we're going to focus
on a non-indexed array,and what's called
a do over loop.So let's just get into
the syntax of an array.An array is referenced
in a data step--not anywhere else,
just in a data step--using the keyword array.We then give it some array name.I like to use an array name
that starts with the letter A.And that just allows me to
remember when I'm coding, hey,this is an array--just like with a format.I like to start my
formats with a letter F,so this is a format.It just keeps things
straight in my brain.You can do whatever you
want, but this array nameis just a valid SAS name.It's not a variable
name in the data set.With an indexed array, you
indicate the number of elementsin curly brackets in the array.So this n here is
just the numberof elements in the
array, or the numberof variables in the array.If your array has character
variables in them,you designate the array as
character with this dollar signhere after the indexed number.If you're creating
new variables,and you want all the
variables in the arrayto have a particular length,
you use the length option here.Again, that's optional.And then you list out
your array elements.And that's just the
list of variables,each separated by a
space, that you can do.If you have numbered variables
with a number on the end thatsequential, you can use list
format in the array statementas well.But it's just important to know
that those are just variables.They are all of
the same type, OK?That's the key.You can't mix character and
numeric variables in an array.You can have different arrays
with all numeric variables,and then another
array with character.But they have to all be
the same type of variablein the specific array.So here is an example.And this is the example
where we're goingto reverse these four items--4, 8, 12, and 16.So again, I've got
my data set here.I'm naming my array.I start with my keyword, array.I named my array areverse.It has four elements-- cesd4,
cesd8, cesd12, and cesd16.OK?I indicate that there are
four elements by enteringthat 4 in the curly bracket.The dollar sign is not
needed because these are allnumeric variables.The length is not needed
because I'm not reallydefining new variables.And I never really actually
use the length option anyways.I just let the
variable values takeon whatever type of variable.And I give it that
default numberof bits, or bytes, or whatever.But then the list
of array elementsis, again, just cesd4,
8, cesd12, and 16.So nothing too earth
shattering there, OK?You can extend this
beyond one line.Again, we just start
with the array statement,and we end with that semicolon.And as you know in
SAS, any statementbegins with some
keyword and endswith a semicolon-- same thing
with an array statement.Now a do loop, the do
loop that we want to useis called an indexed do loop.Do loops are used to specify a
set of statements or operationsthat you want to
perform as a unitduring each iteration
of the loop.So you can perform multiple
operations within a loop.You can reference multiple
arrays within a do loop.Every do loop, it
starts with the keyword,do, and has a corresponding
end statement.If you forget the end statement,
SAS is going to tell you that,and stuff will not run.So check your log.Do loops are used only to
perform several operationswithin an observation.So it's all done within that
observation in the data step.We use our procedures
to perform operationsacross our observations.We do data step stuff
to perform operationswithin an observation.So again, remember that's done
just within that observation.There are four
types of do loops.We're going to focus
on two of them today.There's the iterative do
loop or indexed do loop.And I use that with
an indexed array.And that starts with
the keyword, do.You give it an indexed
variable and an equals sign.And there's a little
more to that, that we'llget to in just a second.There is the do over loop,
which we're going to hit today.And that performs all
operations in the loopover all the elements in
the corresponding array.There is a do until, where
you perform all the operationsuntil a condition is satisfied.Or the do while, where
you perform the operationswhile a condition is satisfied.The iterative do
loop, which is whatwe're going to use
with the indexed array,executes the statements
between the doand the end repetitively
based on the valueof the indexed variable.So our syntax is we start
with that keyword, do.We use some indexed
variable name.I typically use I, J, K, L,
M, but you could use anything.You could use the
word Freddie, if youwanted to use the word Freddie.It doesn't matter, just give
it some indexed variable name.Just be aware that you're
going to be referencingthat indexed variable within
the do loop as part of,and when you're
referencing the array.You give it a start value.You use the keyword, to.You give it a stop value.And you can increment by
something other than one.The default is to
increment by one.So if you needed to go over
on just the even numbersin the array, you can do that by
starting at 2 and ending at 20,and increment by two.Or if you need to do the
odds, start at 1, go to 20,increment by--I guess, well, or by 19--and increment by two.And that'll hit just
the odd numbers.So you can do that as well.And an array is
referenced in an iterativedo loop or indexed do loop by
referencing the array name.So you use the array name as
you would a variable name.And then you specifically
tell it what valueof the array, or what
variable in the array usingthe indexed variable
that you wantto perform that operation on.So here you have your
syntax as the array name,and then that indexed
variable that youdefined in the do statement.So let's just see some code,
and see how I would do that.So the above do loop will step
through the loop four times.So I've got my array
that I indicatedbefore, array areverse.It's got four items--cesd4, 8, 12, and 16.I start my do loop.I'm going to start my indexed.I'm going to call it i,
because I don't like to type.And so anytime I can have
less typing, I take it.I start with i equals 1.And I'm going to 4.And notice my 4 matches the
number of elements in my array.I reference my array
name, areverse, here,and then indicate what item in
the array I want to referenceusing that indexed variable, i.So the i in the square
brackets matchesthe i in the do statement,
that indexed variable name.That is then equal to 3 minus
the same item in the array,areverse square bracket i.And then to make sure
that I've ended mydo loop, end it with the
end statement followedby a semicolon.And so we're going
to go through whateach iteration
looks like in thisas we step through this array.So on the first
iteration of the do loop,the indexed variable value is 1.So we're going to take the
first item in areverse, whichis the array areverse item 1.We're going to take 3
minus areverse item 1.Now, areverse item 1
corresponds to cesd4.And SAS interprets this
code as cesd4=3-cesd4.It increases the
indexed variable by one,increases i by 1.It goes back to the
beginning of the do statementbecause I have hit
the end of the codebefore the end statements.So I'm now in the second
iteration of the do loop,the second iteration.My indexed variable
value is now equal to 2.So now I'm looking
at areverse item 2equals 3 minus areverse item 2.Areverse item 2
corresponds to cesd8.And so SAS interprets that
code as cesd8=3-cesd8.We increase i by 1
again, and we go back.And we keep increasing, and we
keep stepping through the loopuntil i is greater than the
stop value of the do loop.So it would hit item
4, increase i, go backto the start of the do loop.i is now 5.And it would say,
well, 5 isn't a value.And it would exit the do loop.OK?So that is an indexed array,
and an indexed do loop.We're now going to
cover non-indexed arraysand non-indexed do loops.These are very similar.But the only thing is,
I don't have that n.I don't have the number
of items in my array.Syntax is essentially
the same, you justdon't have to tell SAS how
many items are in the array.SAS will figure it out.And so here's an
example of code.And so the only thing I've
left off is that indexed,that curly bracket
4 curly bracket.Still have the same array name.My dollar sign isn't needed
because all variablesare numeric.And then I still have the
same list of variables.So nothing is really changing.I then use a do over loop.And it's one of the more useful
do loops that I've discovered.When I use an array, and I
don't know how many variablesare in that array, using the do
over loop with that array justcauses SAS to do
everything within thatdo loop over every
variable in the array.You can use multiple
arrays within the do loop.You do have to be aware
that each array musthave the same number
of elements if you'rereferencing multiple arrays.And that if you're assigning
values from one arrayto another, that your
variables correspondin terms of the order that
they are in each array.And here's an example.So, again, you're going
to specify the array name.You're going to list
out your elements.You're going to do
over your array name,and then you're going to perform
the operations on the arrayname.So this is the syntax.So, again, you list
out your elements.You start with do over,
give it that array name.And you don't have to have an
index, which for me is greatbecause I sometimes miss
the square brackets.And then I'm wondering where
the heck stuff is doing.But here is what I
wanted to show you,that things need to be lined up.Here the issue is,
OK, if I wantedto create new variables
in reverse because I wantto maintain my original scale.So now I'm creating rcesd4,
rcesd12, rcesd16, and rcesd8.But you'll notice these
are in different ordersthan the array aold.OK?What this is going to do is
it's going to correctly assignthe reversed value for cesd4.But it's going to
incorrectly assignthe reverse value of
cesd8, to rcesd12,and the incorrect value of
reversed cesd12 to rcesd16,and the incorrect value of
cesd16 reversed to rcesd8.So things are not in
the correct order.So you have to make sure that
your variables are listedin the same order if
you're assigning the valuesof variables in the same array.So here we have the
correct order, cesd4.Reversed cesd4 is always first.8 is always second.12 is always third.And 16 is always fourth.OK?So that is basically the
gist of arrays and do loops.I have found arrays and do
loops be very, very helpful.I use them in just about
every program I create,because I typically
have multiple variablesthat I need to change a value
or do some type of an operationon.They do have many uses.You can create new variables,
perform the same operation,rename variables.You can change
the data structurefrom a short wide data set
to a long skinny data set.And I would reference
my paper for informationon how to do that.I've used arrays to simulate or
create randomization schemes.One thing to note,
you can always,always use an indexed array
and iterative do loop.You may not always be able
to get away with non-indexedand do over loops.While there less
typing sometimes,they're just not feasible in
terms of what you're doing.So I want to thank you
for joining me today.If you have any questions at all
regarding arrays or do loops,or if I wasn't clear
on something, pleasefeel free to contact me.You can contact me at
jwaller@augusta.eduAnd I look forward
to hearing from you,and have a wonderful day."
104,"ALEX LINDSAY: Hello.I'm Alex Lindsay, and I'm
joined by Mason Kopasz.We are representing the
Master's of Business Analyticsprogram at Oklahoma
State University.We are presenting our
findings on our researchof what happens after police
shootings in the United States.We would first like to thank
SAS for the opportunityto present our findings.Additionally, we would
like to thank the judgesfor taking time out
of your schedulesto listen to our presentation
and the opportunityto interact.I'm currently a captain
in the US Air Force,and I take on the role
of project manager.Aravind Dhanabal is
a full-time student,and he served as our
business analyst.And Mason Kopasz works
as a data analystin the insurance sector.Today we will go
over introduction,problem statement, methods
used, analysis, and our resultsand recommendations.Over the past 30 years,
the United Stateshas had numerous infamous police
shooting and police brutalitycases, from Rodney King
in Los Angeles in 1991to Michael Brown in
Ferguson, Missouri in 2014.The United States
has been facinga problem of police brutality.Compared to other
developed nations,the United States
dwarfs countrieslike Great Britain and Germany.In 2016, Germany had eight in
Great Britain had zero deathsdue to police shootings.Per million residents, Germany
has 0.09 police shootingdeaths, as opposed to the
United States, at 3.22.There is no lack
of ideas that aimto reduce the levels of
police shootings in America.Some states have mandated body
cameras for police officers,while some cities have
social workers accompanyon duty officers, hopefully
to defuse tense situations.The aim of the study
is to determineif there are more underlying
societal issues thatcan be causing the
use of lethal forceand the follow-on investigation
of the officers who used it.The goal is to gain insights
into the variables thatlead to a perceived justified
or unjustified police shooting,and forecast the shootings
across the United Statesfor the next 10
months in an effortto provide key insights to
those campaigning for reform.A police shooting is
perceived to be justifiedif there is no grand jury
indictment from the districtattorney.And I will pass it over
to my co-presenter, Mason.MASON KOPASZ: Thanks, Alex.The data we're using comes
from a fatalencounters.org,who has been tracking the
victims of police shootingssince the year 2000.Fatal Encounters
has volunteers whocomb through the law enforcement
websites, social media,and independent databases
to compile this dataand then input it
into the database.The database itself
contains variables,such as age of the victim,
race of the victim--such as Caucasian,
African-American, or Hispanic--gender-- male, female,
or transgender--the city, county,
state of the event--was the officer aware
of any impairmentin mental state or
intoxication of the victims,as well as the officer
disposition, whichrepresents the end
result of the events,such as indicted by
a grand jury or not.Additionally,
population data wascollected from the 2010 census.The census data was used to
normalize the police shootingdata from fatal encounters.So as you can imagine, because
of the scope of the inputsas well as the volunteer
nature of the persons inputtingthe data, many variables
are missing or collectedin a non-similar fashion.Age, race, and
officer dispositionwere among the most unreported.To prep the data, we bin
the ages into groups,with the first being
0 and 19, and the restof the ages at 10-year
intervals after that.For the analysis, the data was
partitioned with a 70/30 splitfor training and validation.And our target variable
was the dispositionof the officer, which is
whether the shooting wasdeemed justified,
unjustified, or unreported.So this brings us to the
descriptive analytics.So one of the first things
we saw in our analysiswere that males make up
a far greater proportionof the victims than females,
and that holds for all races.Then when looking
at the data by racewe see that the largest
group of victims by volumewere white males, and they
represented approximately 43%of the observations.However, using the census data
to control our population,we found that
African-Americans and Hispanicswere the most affected group,
with a much higher shootingper million than whites.And so what this shows
is that race appearsto be playing a determining
factor in police shootings andshould be considered
in our model.Next, we turn to
age, where we seethat the incidents peak
at the 20 to 29 age group,and that a large
proportion of the shootingsare occurring to
persons under 40,and they represented around
65% of the shootings.And so what this chart shows is
the proportion of race by agebands, and we see that
African-Americans and Hispanicsare making up a larger
proportion in the prime agebands of the 20 to 39 group--which is concerning,
considering they'rea lower percentage of
the overall population.Their proportions proceed to
decrease in their later agebands and are more in line
with population percentagesas a whole.So this is showing that
age is playing a factorand should possibly be
considered in our model.So finally, we looked into the
mental state of the victims,and so what the below plot
shows is whether or notpolice officers were aware
of any mental illnessesor intoxication of the victims
at the time of the shootings.So no is they were
unaware, unknownindicates it is unsure if
they were aware or not,yes means they were aware, and
the last column is yes, theywere aware of impairment.And the read portion shows
events that ended in suicide.And so what we see is
that 17% of the shootingsinvolved a person who was
mentally ill or intoxicated,and of those, 15%
ended in suicide.We also see that, where
the police were notaware of any sort of mental
issues, 12% of those instancesended in suicide as well,
which suggests perhapsthat there are
issues in determiningthe mental state during these
high-pressure incidences.And this is an example of where
social workers joining officerson call can really make a
difference in the shootingsof those that are suffering
from mental illnessesor are suicidal.So we included this map that
shows the deaths per millionfor each state's population.And while ideally we would
like to get this downto the county level, we believe
this provides the insightsinto which states have
abnormally high rates of policeshootings and could play
a role in the modeling.So the icons here are
indicating the citiesthat have implemented
the body cam initiative.And these red
arrows are pointingto these cities that
have implementeda social worker joining the
officer on a call initiative.And so I'm going to turn it
over to my colleague Alex,and he is going to go over the
predictive analytics section.ALEX LINDSAY: Thank you, Mason.Now I will dive into
our analytical models.When building our
models, we decidedto use perceived justified
and unjustified variableas our target.As a reminder, what
makes a shooting proceedas justified or unjustified
is the determinationto move to a grand
jury indictment.Additionally, the
variables receivefrom fatalencounters.org does
not include many variables thatshould be included to truly
create the best model possible.In order to create
the best model,we decided to use
misclassification rateas our determinant,
since we wantto have high accuracy in
the determining factors thatlead to a perceived justified
and unjustified shooting.We built logistic regression
decision tree and gradientboosting models with
varying features,and used misclassification
rate as the determinantfor the model performance.The decision tree
with chi-squareoutperformed all
models, recording 64.26%accuracy, while also producing
the second-highest sensitivityat 64.2%.The actual perceived
justified dispositionof the police officers account
for 46.84% of the policeshootings in our data set.On the slide, we have our
actual and predictive resultsin a confusion matrix.We found that the model
performed fairly well,with accuracy of 64.26%.Like we said earlier, we
have a precision of 64.20%and a recall of 61.43%.This results in an
F measure of 0.6278.Since perfect score
would be 1, weconclude the model
performed fairly wellwith the data we had.In our selected model, we
identified the most importantvariables, as seen here.State, county, and
department involvedare the most important
variables in determiningif a shooting will be perceived
as justified or unjustified,all with a relative
importance over 80%.Mentally impaired-- so
that's the drugs or alcoholinvolvement-- along with age
group, race, and gender lagfar behind our
location-based variables.I think that it's
important to refer backto our descriptive
statistics and the disparityin shootings per million
for the different races.When we evaluate
the after-actionof the investigations,
the race of the individualis not as important
when it comesto determining if the shooting
will be perceived as justifiedor not.On our slide, we have an
exponential smoothing forecastof police shootings
in the United Statesfrom the end of our
analysis period,November, 2019 to October 2020.We matched our forecast
to actuals from Novemberthrough February, and found
our model performed very well.And all actual values replaced
our 95% confidence interval.In the model,
there's a major dropoff in the month of August,
September, and October.This is due to a major
slowdown of shootingsin those months in 2019.We were unable to
conclude if thisis due to actions being
taken in police departmentsto focus on the easing of
the use of lethal force.We additionally studied a risk
assessment of individuals,and we found that the
high-risk demographics aregoing to be in the states of
Georgia, Nebraska, Arizona,Maryland, Louisiana, the
races of Hispanic and Latinoor African-American with
no signs of mental illnessor being drugged and in
the age groups of 20 to 39.And our low-risk
profiles are goingto be state of Ohio,
Pennsylvania, Kansas,and Oklahoma.The races are Middle
Eastern and Native American,with signs of mental illness
and the age groups of 40 to 59.In conclusion, our
study uncoveredthat African-Americans
and Hispanic and Latinosare victims of police shootings
more often than white Americansper capita.Ages 20 to 39 are the
most prone to becomingvictims of police
shootings, amassing65% of all recorded incidents.The predictive model found
the location-based variablesto be more important than
the victim's race or age,when it comes to a
perceived justifiedversus unjustified
investigation.This model could be used
in practical applicationby an oversight committee to
ensure their standardizationacross states, counties,
and police departmentsfor launching investigations
into police shooting incidents.In the future, we would like
to use this predictive modelto be used on a national
police database thatincludes limited
amounts of missing data,and includes other variables
that are not found in the FatalEncounters database.This could include
police officer profilingdata, which would
give more insightto the user of lethal force.We would also like to use the
predictive model for all policeresponses in order to
give police officers moreactionable data.Uses this could
include DUI checkpointsat high-reported areas or police
patrols in high-theft areas.Additionally, we could
use the forecast modelto expand to all
police responses.This could give advocacy
for police departmentsto gain additional funding for
local and state legislaturesto combat crime and
reportable incidents.Once again, we would
like to thank youfor your time and
SAS's hospitality.We truly appreciate
this opportunity,and look forward
to your questions."
105,"Hi.My name is Paul Dorfman.Don Henderson and I are about to
present a SAS tutorial, the SASHash Object--a Deeper and Wider Look at the
Fundamentals and Functionality.Don and I have been using
hash programming in SASsince before the advent
of the SAS hash object.We usually focus on
using the hash objectsfor solving intractable
data management problems.We have authored a
number of joint paperson hash programming in SAS.We have presented them at SUGI,
SAS Global Forum, regionaland local SAS user groups.Finally, as a team, we have
written a SAS Press bookon hash programming in SAS.As a matter of
fact, the tutorialis loosely based on
this book publishedby SAS Press in July 2018.Of course, this is
a shameless plug.But putting it
aside, the tutorialfollows the book in
a number of aspects.First, the hash nomenclature
developed for the bookis used in the tutorial.The hash object is viewed
as a memory-resident datatable supplied with
a number of methodsto enable general
data table operations.The hash object fundamentals
serve as the basisupon which the hash programming
and functionality is developed.Most data samples used
in the presentationare based on the simple data
constructed for the book, whichis a bonafide data warehouse,
loaded with the dataabout the fictitious
game called bizarro ball.As the title
implies, the tutorialconsists of two deeper
and wider parts,part 1, the fundamentals; and
part 2, the functionality.The fundamentals
and functionalityinevitably overlap,
to some degree.That is why there is a bit
of functionality in part 1,and there is going to be a
little bit of fundamentalsin part 2.A few words about
the prerequisitesfor the audience and
scope of the presentation.First the prerequisites
for this SAS DATA step.We expect that you would have an
intermediate to advanced graspof DATA step programming,
that you have internalizedthe concept of the Program
Data Vector, or PDV short,and that you would have the
basic elements of the SASlanguage, such as expressions,
control, DO loops, etcetera, internalized.The second set of prerequisites
is related to the SAS hashcash object itself.If you have some familiarity
with the SAS hash object,it is a very welcome.Those who have an intermediate
to advanced grasp of the SAShash object will benefit
from the presentationbecause after they familiarize
themselves with the material,they would be able to
rethink the existing code.For the beginning users, they
could learn some misconceptionsand pitfalls ahead
of falling into trapswhen they get to the advanced
stage of hash programming.If you are a non-user, and you
have never used this SAS hashobject before, you probably
will get interested,learn some fundamentals,
and get a decent perspectiveon the usage of this important
part of the SAS language.The scope of the
presentation is firstrelated to the environment.You may know that the
hash object can operatein three environments, the
DATA step, the DS2 procedure,and the FCMP procedure.We omit the last two
from our presentationand concentrate exclusively
on the SAS DATA step.As far as the breadth and
width of the presentationis concerned, this
tutorial is notintended as a comprehensive
treatise on the hash object.If you want that,
please read our book.Our main goal here is to
identify and remediateand/or prevent some common hash
object programming syndromes,as we call them.Broadly speaking, there
are three hash programmingsyndromes.First one is cannibalization.In general, cannibalization
is using someone else's code.In this case, it means
taking hash code samplesand maybe tweaking them
to suit one's needs.In principle, this is fine.But the problem occurs when
the learning stops at merelyadapting the cannibalized code.Well, the symptoms
of this syndromeinclude the fear
and/or inabilityto go beyond the code
template and write hash codefrom scratch or creatively.The remedy for this
syndrome is to get deeperinto the hash fundamentals,
to learn how and why the hashobject actually works.The second syndrome
is static codein the dynamic environment.The hash SAS object is
designed to be dynamic.Everything occurs
at the runtime.So its flow control
can and shouldbe governed by
runtime expressions,not by something hard coded.This syndrome manifests
itself when the hash codeis written without using
its inherent dynamic runtimecapabilities.Mostly this occurs due to
long-entrenched program habitsthat have been acquired in
the pre-hash object era,when the hash
object didn't exist.The symptoms include such things
as hard coding and/or usingmacro variables instead
of runtime expressionsor unnecessarily
crossing step boundaries.The remedy is to
review the fundamentalsand to learn how to
utilize the dynamic designof the hash object
and use it properly.The third syndrome
is the stereotypes.It's odd misconceptions
about the hash object,mostly derived
probably from hearsay.But I don't really know
where they come from.They just exist.I know that for sure.Symptom number one
goes like this.I don't need to use hash table
because my data sets aren'ttoo large.Symptom number two
is the hash objectis just another
table lookup tool.I can do my table
lookups by other means.So why do I have to waste time
to learn about hash object?The remedy for this
syndrome is to learn moreabout the true breadth and width
of hash object's functionality.Let's begin with part 1,
a Deeper and Wider Lookat the Fundamentals.First, let's talk a little
bit about the hash table'slogical image and the language.The hash object can be thought
of as a combination of tworelated components, a hash
table in memory and hash toolsused to perform operations
with the data in the table.Let's first look at the logical
image and the nomenclatureof the hash table.Mind you, that what you'll
see is not a physical pictureof the hash table as rendered
by the underlying software.Rather it's a logical
image which is the only onethat we need practically.As a matter of fact, what
we're looking at right nowis the logical image of a hash
object table, at least the wayI picture it in my mind when
I do my hash programming.It looks quite similar to
the image of a SAS dataset opened in a SAS viewer.You have columns.You have rows, and variable
names, all the hallmarksof any data table.To add to the similarity, this
image is filled with some datafrom our sample data
set, Player Candidates.However, compared to
to a SAS data set,the hash object
table has a numberof significant
distinctions with regardto its nomenclature
and data struct--Let's look at the
semantics first.The columns in the table
are called hash variables.This is necessary
to set them apartfrom the program
data vector variableswith the same names, which
are called host variables.The rows in the table
are called hash items.The entire collection
of all hash variablestogether with the attributes
is called the hash entry.The hash entry can be thought
of as the hash table metadata,in a way, similar to the
descriptor of a SAS data set.The total length of the hash
entry variables in bytesis called the entry size, or
alternatively the item size.And the entry size ultimately
determines how much RAMthe table needs in
order to store one item.As you can see
from this picture,the hash variables are divided
into two distinct groupsaccording to the
roles they play.They call it the key portion
and the data portion.We'll discuss these roles
just a tad later in a minute.Now let us look at the
hash table specificsbeyond the nomenclature.The first non-semantic
distinctionis related to the
data storage medium.Unlike a SAS data
set residing on disk,a hash table is stored
completely in memory.By the way, this is
one of the factorsthat make the hash
object operations fast.The second difference is related
to the data retention and/orvolatility.The hash object exists
only for the durationof the DATA step or runtime.Therefore the hash table
cannot proceed to be usedin subsequent SAS program steps.Nonetheless, its content can
be saved to a SAS data setand then reloaded in a DATA
step somewhere downstream.We already know that
all hash variablesare divided into groups.The key portion hash
variables are the variablesthat contain the key.The values of the key variables
are called key values.And they are used, as a whole,
to access the table randomly.Key values cannot be written
from the table to the programdata vector.They can only be loaded
from the program data vectorinto the table.The key portion can contain
only scalar variables,that is only variables of the
numeric or character data type.The data portion contains
so-called satellite variables,or non-key variables.Their values,
called data values,cannot be used for
random table access.Data values can be both loaded
from the PDV to the tableand written from the
table back to the PDV.The data portion can
contain both scalarand non-scalar
variables, for example,such variables as the variables
of type hash or hash iterator.Welcome to hash programming.The best known way
to learn somethingis to do it, make mistakes,
and learn from them.This is especially true
about SAS hash programming.The SAS hash documentation
is good and useful,but there are gaps.A number of important
concepts and functionalityaren't covered.Good examples are invariably
hard coded and static.Some assertions and examples
are factually wrong.Certain coding approaches
are questionable.By practical experimenting,
Don Henderson and Ihave learned many critical
how-tos for examplehow to use the hash object
for data aggregation, howto use a hash table as an
array for a known scalarvariables and loop through
them, how to use hash argumentsand argument tags dynamically,
and many other interesting anduseful things, all
of which are includedin our book shown earlier.So let's learn hash
programming by programming.If you have never heard
of this word right here,propaedeutics, it means an
introductory course or crashcourse into something.Since we have agreed to limit
our hash discussion to the DATAstep only, let's
begin the DATA step.We are not going to create any
output data sets at this point.So I'll just code data null.And now let's enter the
first statement from whichhash programming begins, declare
hash H. Here, declare and hashare key words.And the letter H is
simply a SAS name.It doesn't matter
what name it is,as long it is a valid SAS name.This statement is
actually much more complexthan meets the eye, and we'll
discuss its details later on.Let us just assume
for the time beingthat this statement is necessary
to create a hash objectand H actually relates to
the future hash object.In itself, this is a complete
syntactically valid statement,and we can run it.So I'll just add the run
statement to the DATA stepand run it.What we see in the log
is that the compilerhad absolutely no problem with
the syntax of this statement.And the DATA step
ran successfully.Back to the program
editor, the next thingwe need to tell SAS in order
to create a hash objectis which variables we want to
designate in its key portion.To do that, we need to call
some function that's actuallyin relation to the hash object.It's called a method.And this method is define key.In order for it to
work, we actuallyneed to point to
the letter which wechose as the hash object name--that is H-- and followed
by a period, whichis called object dot notation.Let's put a couple
of parentheses hereand include some kind
of variable name.Let's say k.This must be a literal,
enclosed in quotes.We can use single
or double quotes.It doesn't matter.Now let's designate some
variable for the data portion.Let's call this variable d.And the method we are going
to call in the same manneris now called define data.And likewise, we
just include the nameof variable d here in quotes.Now, as an inquisitive
programmer,you might wonder, what are the
values of the variables k and ddefined in this
fashion currently?You would expect
them to be missing,or should they be
something else?We'll just see.And run this step.What you see here is unrelated
to this step we just submitted.So let's just scroll down.So what do we see here?The put statement tells us
that neither the variable knor the variable d is in
the program data vectorat Therefore, when
the SAS compiler looksat these statements it doesn't
take the variables k and dand doesn't include them in
the compiler symbol table.We also see that the
step ran successfully.So does it mean
that the hash tableH has already been created
and is ready to use?Well, we can find out by
trying to do something with it.Let's recall the
code and do this.Instead of the
put all statement,let's try to find out
if a specific value of kis in the table.To do that, we can
code rc equals h check.And tell the check method
what value we are looking for.We do it by using
the argument tag key.Let's value it with one
and see what happens.Let's submit this step now
and see what the log tells us.And what we see in
the log is an error.It tells us that
method defineDonemust be called to complete
initialization of hash objectbefore line 2437, which
means that the creationof the hash object is not
complete yet at this point.We need to do something else.Namely, we had to call
the method defineDone.So let's do that.Let's add the method
defineDone to the rows here.This method doesn't
take any arguments.And now let's run it again.We get an error again, but this
time of a completely differentnature.This time the object tells
us that the key symbolk is undeclared for hash
object at line 2446.Note that the method
that told us this storyis actually defineDone.In the SAS hash
lingo, the statementundeclared key symbol means
that the symbol with this nameis not found as a variable
in the program data vector.In other words, before
the defineDone methodwas called at
runtime, the compilerhad not created the variable
k at the compile time.Therefore let's ask the compiler
to create this variable.Let's go back to
the program editorand add some statement that
would let the compiler seethe variable k, its
type, and put itin the compiler symbol table.We can do it but
a number of means.One very simple way is simply
to assign some numeric valueto variable k.So what will happen
after that, the stepwill be scanned by
the compiler, and itwill see this assignment
reference, after whichvariable k will be created
in the compiler symbol tableand in the program data vector
as a numeric variable k.Now, after we will this
step with this correction,can we expect that everything
will be hunky dory?Well, let's just run it.And now we see that even
though k is present in the PDV,the hash object
complains, or moreprecisely the defineDone
method complainsthat we have an undeclared data
simple D for the hash object.And again, it is
at line 2457, whichmeans that this is a defineDone
method that is telling us that.So the obvious
course of action isto go ahead and let the compiler
create the variable d as well.Let's do it in the same way,
using the assignment statement,let's see d equals 2.And now let's run this
step with this correction.And now we can see that
the step ran error free.Now, a little bit of semantics.The variables k
and d the compilercreated in the
compiler symbol table.And the program data
vector correspondingto the hash variables k and
d are called host variables.The process of
creating such variablesis called parameter type match.To recap, the action of
providing host variablesfor hash variables is called
parameter type matching.The assignment statement is
only one of many statementswith which parameter type
matching can be achieved.For example, we can
achieve the same effectby using a valued retain
statement instead.We can confirm it by
rerunning the step.As you see, it runs fine
with a retain statement, too.This is because
the compiler infersfrom the retained
statement the names, types,and other attributes of
the variables k and d.The retain statement is a
purely compile time instruction.Now, at the runtime,
the defineDone methodchecks if the variables k and
d defined with the hash objectare in the program data vector.If they are, it gives the
hash object a green light.If you look in the
SAS documentation,you'll see that there, parameter
type matching is alwaysdone using the length
statement, as shown here.In our view, it's not a
good programming practicefor a number of reasons.If we were on this step, we'll
see that it runs error free.However, the log is
peppered with noteswhere it says that the
variables are not initialized.It's unpleasant.So to rectify the
situation, the docssuggest that your add call
missing statement to the lengthstatement.Now if you're on
this step, you won'tsee that uninitialized notes.However, in real programming
life, parameter type matching,can be achieved automatically
without any needfor these contortions.The reason for it is twofold.First, consider the
case when the datato be loaded to the
hash object are createdin the data step itself.For example, suppose
that we are loadingthe table H from the DATA
step itself in this manner.In this case, the
assignment statementfor the variable
k and the form itused to create the variable
d define the type and length.Let's run this step.The log shows that the
table was loaded just fine.In this case, using the
length statement like thiswould be simply redundant.Moreover, it creates
a possibilityfor making an error.This is because now the types,
our length of the variablesk and d are divided into
places here and here,and it's easy to mismatch them.And the second reason for not
using the length statementfor parameter type
matching is that in 99%real-life situations, a hash
table is loaded from a file.To see what happens in
this case let's changethis program a little bit.Let's create a file.Let's call it hash.And I'll load it with the
same data we just createdin the previous DATA step.We'll simply move
the statement upward,include the output statement
here, close it with run.And now we have a
data set called hashcontaining the values for
k and d defined exactlyas we previously defined
it in this DATA step.Of course, we have to
kill the statement here.Now, to load the table
H from the file hash,we just need to
read it explicitlysomewhere here in a DO loop.By the way, LR is an
abbreviation standin for Last Record.Will it work?Let's check it out.Sure, it does work.But note, that in
this case, we didn'thave to do anything at all to
do the parameter type match.Why?Let's go back to
the program editorand look at this program.What we have here on line
22 is a set statementreferencing the file hash.What happens when the
compiler sees this statement?It accesses the descriptor
of the data set hash,extracts the variables
k and d from there,I mean their attributes.The types, the length,
format and format,whatever applies and places
all that into the Program DataVector, or this data step.Therefore after the
compiler has done its workand now comes the runtime, the
defined down method in its codelooks at the program data vector
and finds both the variable kand the variable d.And everything is all right.So in this case when we have
a file to load into a hash,and this file is referenced
explicitly in a data step,we don't have to do any
artificial parametertype matching because
the compiler seesthe descriptor of this data
set and places the variablesthat we need into the PDV.We then become
the host variablesfor our hash variables.Needless to say
that in this case,a length statement would
become completely redundant.Now, as you may
know, the hash objectis furnished with its own
input/output capabilities.So it can read
files all by itselfwithout reading
the file explicitlylike we do here in a do loop.And this is how we do it.We don't need this loop anymore.All we need to do is to
supply the name hash whichis the name of the input data
set to so-called argument tagdata set just like that.This here and nothing else than
an instruction for the SAS Hashproject to read
the data set hashand load it into the table.It looks like a much
easier way to loadthe file to a hash table.And it is.But there is one subtlety.Let's try to under-step.Whoops, again, we see the error
about the undeclared key symbolk.Why is that?One might think that
the compiler whenit sees the
reference to the dataset hash here would do the
same as it does with the SETstatement, that is,
access it's descriptorand put the variables in this
descriptor in the compilersymbol table.But this doesn't happen.Why?Because in general, the
compiler doesn't see anythingin these statements but this
index, with one exception.And we'll talk about
that exception later.So in this case, we actually
have to do some parameter typematching.But how?If you said, let's include
a length statement here.That would be wrong.Instead what we want to do
is to let the compiler seethe descriptor of
the data set hashbut not read any data from it.In order to do
that, we can simplyinclude this statement here.This way during the
compilation phase, the compilersees the reference to the data
set hash in the SET statement.And so accesses the descriptor,
and does what we need.Now, the if 0 statement
is always false.So it prevents the data step
from reading from this inputtree.Now, let's include a
stop statement hereto prevent a warning
in the log thatwould say the data step
stopped due to looping,and around this step.Now, we see that
everything is all right.So I'll conjecture about the
compiler seeing the descriptorhash here is correct.Another huge advantage of these
parameter type matching methodcompared to the length
statement is its robustness.Suppose that you didn't just
create the data set hash,so we just delete it from here.And we don't know
what center it is.So if we decided to use
the length statement,we would need the attributes
of the variables k and d.But we don't know what they are.So we have to find out.In order to do that, we need
to do something like this.For example, go to the
Explorer, find the work library.Find the data set hash.Look at this descriptor, these
attributes and see our k.Variable k is a number length 8.Variable d is a character
variable, a length to.And then from here we go
back and have to type it in.What does the length of d do?But suppose that we forget
it and say three here.In this case, we'll
get a length mismatch.But for example, if
instead of saying k-8,it will say something like this,
it would have a type mismatch.And we are going do it
this way right here.We have no such problems at all.Because the job of
getting to the descriptorand extracting the correct
variables with correctattributes is done
by the compiler.This way, we're
never going to makea typo or something like that.So this is what makes this
technique so much more robust.Now, imagine that
our input data setcontained not two
variables, k and d,but let's say 50 variables
that happens in real lifeall the time.Then what?We would have to type all
the attributes into morethan length statement.It would be almost guaranteed
to make some kind of error.I can almost hear you say,
well, you've been telling usthat the hash table is loaded.But we have seen
no evidence of it.OK, let's check it out.There are in fact several
methods of doing this.First, let's use the
all but forgotten callto the check method.Give the call to the check
method with the tech argumentkey valued as 1.Return 0, that means that the
item with the key equals 1is in the table.So let's include a PUT statement
to verify it and run this step.We see that return
code equals 0.That means that the key
value 1 is in the table.So at least this item is there.As a matter of fact, let's
check for all the keysthat are supposedly
in the table,and also check for some key
that we know is not on the tableand see what the
check method tells us.The simplest way to
do it is in a do loop.Now note, also, is to
know it for the futurethat instead of varying
the argument tagkey with a constant, I
varied it with the valueof a different variable, kv.Let's run this step
and see what happens.As expected, the key values
1, 2, 3, are in the tablebecause the returning code
from check is 0 for all them.And the key 4 is
not on the table.And correspondingly,
the check methodreturns a non-zero value.Now we know that the method
check correctly verifiesthe presence or absence of
a key value in the tablebut doesn't do anything to
the program data vector.Let's check it out.No pun intended.In order to do that,
let's first nullifythe values of the
variables k and dbefore we call the check method.And let's display the
values of k and d in the logalongside the variable rc.Now, let's run this step.So what we see here
is that whether or notthe check method found or didn't
find a key value in the table,it never changed any values
of the host variables.And they remain all null.On the other hand,
it tells us nothingabout what are the
values of d in the table.Apparently, to find out,
we need some other method.We do indeed.On this method is find.So let's replace
the check methodwith the call to
the find method.And repeat the exercise.What we see from this
picture is that whenthe find method finds
a key in the table,it grabs the value
from the data portionand replaces the value
of the host variablewith that hash value.On the other hand, when
it doesn't find the key,here the return
call is non-zero.The value of the host
variable remains the same.But now you may ask, how come
the value of the host variablek remained missing if the
find method found its key?To answer that question,
let's recall from our earlierdiscussion that the key portion
variables cannot write anythinginto the program data vector.They can accept the
values from the PDV.But they are incapable of
updating the host variables.Therefore, if we want the
host variable k to be updated,it has to be included not
only in the key portionbut also in the data
portion just like that.And now if we run the same
step, we'll get this picture.As you can see, that
now the variable kwas added to the data portion.The defined method
uses this valueto override the
corresponding host variable.One frequent mistake
made in hash programmingis using the defined
method where the checkmethod is actually called for.If you only need to find
out whether a key value isin the table, always
use the check method.It runs faster,
that's number one.And number two, it won't mess
up anything in your program datavector.On the other hand, if
you run the find methodand it finds the key, it will
override the host variables.So keep it in mind and
run and use them properly.Now we know the defined method
updates the host variablesin place in case
the key is found.But it raises the
question, is therea way to update a hash variable
in place if the key is found?That the answer is yes.And this is done using
the replace method.To see how it works, let's
call the replace method twice.First, let us call the replace
method against a key thatdoes exist in the table.This is key value 1.And try to replace the
data element 01 with 11.Next, let's try calling
the replace methodagainst the key that is not on
the table with the key value 4and try to see what happens
and what actually isin the table after this call.Let's run the step
and look at the log.What do we see?Well, when the replace method
was called against a key thatexists in the table,
where k equals 1,it did indeed replace the
existing value 01 withthe specified value 11.The items with the key values
2 and 3 remained intact.However, when the replace method
was called against the key 4,that is not in the table.What we see is the key and
the item with this key.And the new data item
were added to the table.For this reason, the replace
method can never fail.If it finds a key, it
replaces the data value.If it doesn't find the key, it
inserts any item in the table.There is another
good way to find outif a hash table is
loaded and what's in it.As we already know, the hash
object can read a sales data.Conversely, it can
write its data contentto a sales data set.This is done using
the output method.This is one of the
most useful methods.And using it is very easy.Let just go with here after the
table is loaded, each output,and here specify an
argument type data set.The same argument type
we used for input.And just supply a
size data set name.Let's say hash out.And let's run this step.As you see, the output method
has written the data setwork hash out.Notice that it has
three observations,the same as the number of
items in the hash table,but only one variable.Why is that?And which variable is that?To find out, let's
go to SAS Explorerand find this hashout data
set in the work libraryand look at its variables.We see that the only variable
in this data set is d.Now, let's open this data set.We can see that the
values of d are indeedthe values in the table
although they're notin the correct order.It raises the question, why the
variable k has not been writtento the altered data set.The answer is that,
as we already know,only the data
portion variables cancommunicate from the hash table
to the program data vector.When the output
method is running,the data from the data portion
goes through the outputthrough the host variables.And because the variable k
is not in the data portion,it's not in the output either.It means that in order
to fix the situation,the only thing we have
to do is to add the kvariable to the data portion.And now we can rerun the step.Now we can see that two
variables are in the output.Let's see where they are.We see that both variables k
and d are now in the output.And if we open the
data set, we willsee that both k and
d are in the output.Although, like I
already said, theyseem to be in a different
order from the input.The reason the output in this
case, the SAS data set hashouthas an order different
from the input,in this case, the
hash table h, isbecause the hash
object has its own ideaabout this internal order.If we want the output
data set in order,we need to tell the hash
object to all put it in order.And this is done by including
the argument that ordered.And here we can specify
whether we want itin ascending order,
or descending.In this case, it is ascending.Now, if you were
on this step, wewill see the same in the log.But now when we look
at the actual data set,we see that now it's in
the key order ascending.Note that this argument takes
specification that applies onlyto the output order.The internal order of the
hash table is not affected.And frankly, we don't care.We're only concerned
how we output the dataand which order.By far the easiest way to find
out if a hash table is loadedis to find out how
many items it contains.This number is a dynamic
attribute of a hash table.And it is returned by the hash
attribute called num_items,num items attribute is
fully dynamic in the sensethat whenever an item is added
or removed from the table,the value of num items
changes accordingly.Let's see how it is used.First, let's call the
num items attributeat this point in the program
where the hash table is notloaded yet.This is how its
syntax looks like.Note that it's different
from a method in that itdoesn't contain parentheses
after it's code.And now let's display
it's value in the log.Now, let's code the same after
every call to the add methodright here so that
we could see whathappens every time an item
is added to the table.Let's run the program.The log shows then that when
the table is empty here,the num items
attribute returns 0.That is, the table
contains 0 items.And every time an item
is added, the num itemsis automatically updated.So when the first item is
added, we have NI equals 1.Then the second item
makes it 2, and so on.Now, let's remove one
item from the tableand see what the num
items attribute will show.Let us call the
remove method here.H remove, now, we have to supply
a key value to the argument tagkey.Let's say we want to remove
the item with the key value 2.And now let's call our attribute
again and display the numberof items in the table.So let's run the step.As we can see, when the
item with key value 2was removed from the table,
the num items attributewas automatically updated to
reflect the current numberof items in the table.So it went from 3 back to 2.In a way, the num
items hash attributeis similar to the NOBS
attribute in a SAS data set.In general, the num items is an
extremely valuable attribute,especially when
the hash object isused to implement other data
structures such as stacksor queues.When you look at hash code,
you'll see a lot of thingslike this, this, this, this,
enclosed in parentheses.These elements represent either
hash arguments or hash argumenttags.This is an example of
a hash argument tag.And these are examples
of hash arguments.Reading, this says
documentation,you get an impression that hash
arguments and argument tags arealways valued with constants.Like here, there is a character
with an a, character with a k,character with a hash
out, and sometimes youhave numeric constants too.However, the truth is that none
of these arguments and argumenttag values have to be constants.This is because they're actually
designed to accept general SASexpressions.As a matter of
fact, we've alreadyseen some evidence
of it when we recalldefined method, not
valid by constant,but by a value of the variable.A constant like this
is an expression.But it is the
simplest expression.A variable is also
an expression.It's just the second
simplest after the constant.Finally, there are
general expressionsthat may contain any combination
of constants, functions,variables, and operators.Their only limitation on
general expressions assignedto arguments or argument
tags is that theymust be of correct data type.For example, any
general expressionassigned to this argument tag
must be of the character type,because this is how this
argument tag is designed.On the other hand, an expression
assigned to this argument tagmust be held a numeric type
because the corresponding keyin the hash table is
defined as numeric.To illustrate the concept,
let's assign some argumentsand argument tags,
general expressionsinstead of the constants.For example, this
expression herewill resolve to d, which
means that the hash willbe ordered descending.This expression here is
assigned byte of 107.You probably know that byte
107 in the collate and sequenceresolves to the letter k.In this expiration assigned
to the argument tag data set,we tell the hash object to take
the literal hash out, combineit with underscore and
underscore separatedby another bar.Finally, here, we assign
a numeric expressionto the numeric argument tag
key which will resolve to 3.Let's run this code and see
if all that comes togethercorrectly.And indeed, what we see is that
all the argument and argumenttags that formerly were
assigned character and numericlaterals now run fine when they
assign general expressions.For example, we can see that
this expression resolvesto hash out underscore 1.And this became the name of
the output data set assignedto the argument tag data set.The ability to assign general
expressions to hash argumentsis an extremely powerful
feature of the hash objectand will be demonstrated
in full glory in part 2.Remember that I've promised to
return to this seemingly simplestatement and take it apart.Well, I'll have to take
it apart quite literally.Because in actuality,
this is a combinationof two statements in one.First, let's reduce it
to its first component.And this will be
the same statementwithout the parenthesis.The first question is, what
does this statement actuallydeclare?To find out, let's repeat this
statement and run the step.As you can see, the
SAS log tells usplainly that h is a variable.The second thing
that the log tells usis that the data step compiler
doesn't like to see twice.But if it's a variable,
let's dig deeperand try to find out what the
value of this variable is.Let's try to use the put
statement to display the value.What the SAS log tells us now
is that the Put statement cannotbe used to display the value
of a variable of this type,apparently of object type.All right, in this case,
let's try to assignthis variable some value.Let's say, h equals 1
right around this step.Now, we get the message that a
scalar, apparently the scalarhere is this numeric value 1.It cannot be converted to the
variable whose type is typehash.That is, hash is not
a scalar variable.It's a non-scalar variable.So now we know that we
cannot assign a scalar valueto a non-scalar variable h.But if so, we must be able to
assign a non-scalar variableto h, correct?But how can we do it?Actually, there is a way.It turns out that one way to
assign a non-scalar variableto the variable h is by using
the operator called new.Now, if we run this step,
we will see no complaints.Now, what exactly does
this statement do,and what kind of value
does it assign to H?What is the role of this value?Well, it takes a good
deal of experimentingto find the answer.But we know the answer.And the answer is that
first the new operatorcreates a physical
hash object instance.Secondly, the new operator
creates a unique non-scalar IDof this instance.And finally, this ID is assigned
as a value to the variable H.Now, what is the role
of the variable H?Suppose that you want to
delete the instance thatwas just created, and
accordingly calledthe delete method.Let's run this step.It works just fine.But what is the mechanism?How does the delete method
know which instance to delete?The answer is that when
this method is calledand the reference is H, the
software accesses the PDand looks inside the variable
H. Inside the variable Hthere is an ID, an
ID of some instance.This is the instance that the
delete method actually deletes.In the scope of this
code, the only instance IDthat can be resident
in H, is the IDthat was created by
this new operator here.But what if we have two
assignment statementslike this?This works too.So which instance did the
delete method actually delete?And the answer is that it was
the instance created less.Because when this
statements ran,the ID that it created
for the new instanceoverrode the value
that was sitting in Hcreated by the first statement.Now, where did the instance
created by this statement go?The answer is, it
didn't go anywhere.It still exists in memory
and takes up physical space.The only problem is that we
cannot access it anymore.Because the ID that was created
by this statement was in H.And then this ID was
overridden by the IDcreated by the second statement.Now, what if we want to
delete the first instance wecreated and not the second?The only way to
do it is to storethe ID of the first
instance somewhere and thenrecall it from that storage
and assign to the variable Hbefore the delete
method is called.But this temporary
storage locationalso must be non-scalar.So this is the way to do it.We can create
another hash variablethat is the variable
of type hash,called T, meaning temporary.Then we create the new instance.And its ID is placed into H.And we take that
ID and assign itto the temporary
location, T. Now,we create the second instance.And now the ID of the second
instance resides in H.But we recall the value
of the first instancefrom the temporary variable
D and assign it to H.Now, when you reference
H with the delete method,it actually will reference
the ID of the first hashthat we created.And so the first
hash will be deleted.Now, this H-to-H maneuver
is just a concept.It's not really convenient.It's much better
to store hash IDsas data of another instance.This way, they can be
looped through sequentiallyand made active by being placed
into H in the PDV one-by-one.The technique of storing hash
instance IDs in a hash tableis called Hash of
Hashes, or HoH.Those are kind of a
misnomer because the modeltable contains only
cash IDs of the shooter.It doesn't contain the
physical instances.Note that instance
IDs are also referredto as instance pointers.In part 2, the functionality,
you'll see how Hash of Hashesare used in all their glory.This is the end of part
1, the fundamentals.Part 2, the
functionality follows.Thank you very much
for your attention.DON HENDERSON: Thank you,
Paul for that overviewat the fundamentals
of the hash object.As Paul said in part 2 of our
presentation of our tutorial,I'm now going to
talk about, give youa deeper and wider look
at the functionality.And specifically,
I'm going to addressthe hash object and its use as
a business intelligence tool.You know, many of the examples
we're going to see in part two,in fact, perhaps
all of them, aresimilar to or the
same as examplesin our book, Data Management
Solutions using SAS hash tableoperations of business
intelligence, Case study.So what we're going to do
here is the first thingI'm going to do is introduce
the data for the remainingexamples.This is data that we
made up that representsin some senses some
transactional type datathat we then need to turn in
to, reporting data warehouse.And so I'm going to
introduce that data.I'm then going to
talk about whatare called CRUD operations.That stands for Create,
Read, Update, Delete.And that's a standard definition
of persistent data storagewhich is something that
many database managementsystems have.And then I think
you've seen and youwill see that the
hash object alsorepresents so we can think
about the hash objectas an in-memory database.Once we get through
those, I'm goingto then go ahead and
start to present examples.As I said, most of
these are similar toor the same as
examples in our book.You know, given the
time for this tutorial,we obviously can't
cover everything.And so Paul and I decided
that the best approachwould be to pick one
particular area of BI analysis.And what we decided to do is
to focus on data aggregation.So we're going to see
a series of examplesof using the hash object
for data aggregation.The first one is going to
be using the hash objectto do simple NWAY aggregation.We're then going to
update that a bit.And we're going to provide
some table lookup facility,obviously the hash object
is good for table lookup.Although we hope
you'll walk awayfrom this tutorial
recognizing that itcan do a whole lot more.So our second example is going
to be the same basic dataaggregation.Except we're going to do some
table lookup to replace codedvalues with labels.The next example
is we're then goingto address doing some
hierarchical aggregation usingthe hash object
where one level rollsinto a higher level, which
can roll into a higher level.So that will be a
simple example of that.We're then going to
switch gears just a bitand talk about a metric
that is quite commonin a lot of industries,
account distinct.How many distinct
customers do we have?How many distinct orders
do we have, et cetera?So that's not something that
many of the standard SAS dataaggregation tools can do.Although you certainly
can do distinctcounts in SAS PROC
SQL for example.But we're going to show you
how to do that using the hashobject as we're doing our
other types of aggregates.We're then going to switch
to a couple of examplesof doing what
we're going to callmulti-way aggregates, where in
one pass through of the datawe're going to be
able to aggregateat multiple different
levels that may or may nothave any relationship
with one another.The first example is going to
be a hard-coded example wherewe're going to do, I believe
it's five different kindsof levels of data aggregation.And then we're going
to generalize thatby making it parameter
driven, making itso we have to write
a little less code.And also making the code
a bit more flexible.We are then going to take
that parameterizationto the next level once
again with data aggregation.We're going to show you how you
can loop through hash objectsand perform similar operations
on multiple hash objects.Paul and I refer to that as
the Hash of Hash facility.So we're going to
provide a simple exampleof a Hash of Hash
to just illustratethe key points about
it and how it works.And then we'll
turn to an examplewhere we're going to redo that
multi-way aggregation in a moregeneral and flexible way.And hopefully, that will
all make sense to you.So at this point,
let's go ahead and getstarted with these examples.I'd like to introduce
the data nowfor the rest of the examples
in our presentation.Paul has used some of the
data in some of his examples.But as we transition
into part 2,and we're talking more about
business intelligence typeapplications, we
needed somethingthat's more like the typical
business intelligenceenvironment in terms of
the data that's available.So the data-- and this once
again is from our book--is about a fictional
game called Bizarro Ball,which is very
similar to baseball.We chose that partly because
I'm a big baseball fan.And so I figured, I knew quite a
bit about metrics for baseball.And it also provides a
fairly rich data sourcebecause you've got lots of
players, lots of games, lotsof things going on in games.You know, every pitch,
every at bat, the result,runs being scored.And also because baseball
is very, very biginto analytics, perhaps more
so than most other sports.Although the other sports
are catching up to that.So Paul and I
would like to thankAllan Bowe for pointing
to his GitHub projecthere about Bizarro Ball.When he reviewed the book, one
of the things he asked was,he said, this is a
pretty rich data source.You know, I'm interested in it.Can I perhaps make use
of it for other things?We of course said, yes.And in exchange for that,
he packaged up a much nicermechanism to create the data.So let's go ahead
and look at that.So you can see, here's the
link to the master pagefor this project.Hopefully that
link is going to beincluded in the
text below as wellas this particular set of code.Let me try to highlight it.That's all you need to
run to create the sampledata for the book.So let's go into
the SAS Environmentand take a look at that.So here's our data.Let me go up in a level in here.So you can see that we've
created quite a bit of data.You know, we've got data
about at bats, in pitches,and runs, lots and
lots of data sets.You know, we wrote a
series of hash programsto generate this data.So we generate data
for every game,for every day over
an entire season.And every game is going to have
every pitch and every at bat.So there is quite
a bit of data here.We have actually
two data librariesthat we support in
the book, one calledBizarro and one called BW.The idea behind the
book is we described itas a case study of demonstrating
how the hash object couldbe used to solve business
intelligence problems.So we made up this
fictional leagueof teams called Bizarro Ball
and pitched to their managementthat we could take their
transactional data,convert it to a data warehouse
and then do reporting on it.So the Bizarro Library is
the transactional data.We've got some
additional informationthat are used as parameters
to generate the data.That's not really the subject
for this presentation.So let's skip right
over to the DW library.This is a star
schema data warehousethat we created to support
the analytics and the restof the book and
the sample programsthat we're going to see in the
rest of the presentation here.So let's start with
the at bats data set.Notice that we've
got surrogate keys.You know, game SK
to point to data,information about
the game, batterID to point to information
about the batter, etcetera, the state of play of
the game, what inning it is,whether we're in the
top or the bottomof the inning, et
cetera, lots and lotsof useful or
interesting informationfrom the purposes of
doing some analytics.Game SK is going to be a
link to our games data setwhich gives us information
about the teams.Home SK and away SK--SK for Surrogate Key--is going to tell us who
the teams are, for example.So I think you can see lots
of opportunities for tablelook up.Batter ID is going to be a
link to the player's data set.Since the player can
assume multiple roles,we refer to it as
player ID in this dataset, but in the at bats.And once again,
you can see we'vegot their name, lots of
other information about them.But we refer to it as batter
ID in the at bats data set.We've also got a
pitches data set whichcontains a row for each pitch.Once again notice similar game
SK to link to the game data.But here the person
who's the pitcheris going to be the
pitcher ID, we call it,which is going to link to the
player ID in that other file.So you can see we've got a
fairly rich source of data.There is another
data set we're goingto use here called leagues.This is just a simple
data set for, once again,a very generic table lockup.So once again, the
idea here is that we'regoing to use this data in
the remaining examples.You know, we're going to
focus on the data aggregationexamples from our book,
Data Aggregation and MetricCalculation.So with that, I'll conclude
this part of the presentation.And we're going to move on
next to some sample programsthat we'll review that
use the hash objectto do data aggregation.OK, now that I've given you a
basic introduction to the data,I'd like to talk about
something that is commonlyreferred to as CRUD operations.At first glance, that
seems a little silly.But CRUD is actually an acronym
that stands for Create, Read,Update, and Delete.And yes, you can search for
that on Wikipedia and see that.It's basically a
feature of what'scalled persistent
data storage, whichis what a SAS hash table is.It's persistent for the duration
of a specific data step.And many database operations
have CRUD facilities.And that's what
qualifies them in partto be considered databases.While neither Paul
nor I are goingto say that hash table has all
the features of a database,it does have these
four attributes,create, read,
update, and delete.And so many database
type functionscan be performed
using SAS hash tables.So let me cover them briefly,
the two different levelsof operations and methods
that are available to us.Paul has probably
introduced manyof the methods and functions
used to support these.So for example, at
the table level,we can create a hash object.We can delete a hash object.We can clear all the
data out of a hash objectif perhaps we want to reuse it.And there will be cases
where we want to reuse it.We can also output all of
the data currently foundin a hash table
into a SAS data set.And you'll have one
observation for each itemin the hash table.You'll have one column for
each column in the hash table.So you can see at a table level,
we've got many of the featuresof a database system,
or a data system.And when we say output
to a SAS data set,that also includes
anything that'sthe SAS data set which could
be a view into a DBMS table.So we can also use
the describe operationto extract key hash
table attributes.You know, the total size
of the hash table, that'sthe item size or
the number of itemswhich would be the number
of rows in the table.So that's at the table level.At the individual item level,
and what we mean here by itemis akin to an observation
number or a row numberin a typical SAS data set.So we can do a search to see
if a given key is in the tableand do nothing else with it.We can actually add
new items to the table.We can delete items
from the table.And we can retrieve
information from a table.Notice the difference
between retrieve and search.Search merely says, does this
thing exist in the table?Whereas retrieve is basically
saying, find this in the tableand pull it back for me.You know, it doesn't
always do a find.If you're already pointing
to an observation,you know the retrieve
is just retrievingthe current observation.We can given a key to
an item in a hash tableor based on whichever one
item we're pointing to,we can update any of the
parts of the hash table, whichare the data portion
correspondingto that particular key.Note that we can't update a
key in a database in place.That's pretty standard
that you can't update keys.And finally, we're going
to talk about what we callenumerate all and key numerate.Enumerate all is just
another way of saying,we're going to loop through
all of the items, allof the rows in the hash table.And keynumerate is basically,
we're going to say,given a key, let's
loop through allor some of the items in the
item group with that key.So enumerate all is
very similar to whata data step does each
execution of the data step.It's going to retrieve
an observation.Or if you use a DOW-loop, each
iteration through the DOW-loopis going to do that.And likewise, keynumerate
is doing the same thingif you use, for example, the
keyed access to the table.So with that, let's
start to move on.And we'll start to
cover some examples now.OK, let's take a look at our
first data aggregation example.You can see here
the program nameis going to be chapter
8 dot slashline dot SAS.Let me minimize
that again so youcan see that chapter
8 slashline dot SAS.So I'm going to go
ahead and run it sothat you can see the results.So if we run it, notice
it's creating a data set,it ran pretty quickly.It read in 287,000
observations and createdan output data set with 608
observations in the worklibrary.So let me go to work.And you can see here's
the batter slashline.A few things about it, let me
explain what these metrics are.The batter ID is obviously the
key to identifying the batter.PA stands for plate appearances.How many times did that
batter go up to home plateto perhaps take a bat?The difference between
PAs and at bats--lots of baseball folks, you
know, Bizarro wall folks,don't distinguish
between those two--is if you walk, it doesn't
count as an at bat.But it does count as
a plate appearance.One of the matrix is
we're just countinghow many hits they got.The BA stands for
batting average, whichis hits divided by at bats.On base percentage is
basically how often didthey get on base, which
is the number of walks,the number of hits divided in
this case by plate appearances.Slugging percentage
is basically whereeach hit is weighted by
how many bases it was,is a simple way
to think about it.And ops is on base
plus slugging.So you can see it's simply
the sum of these two numbers.We refer to this as a slashline.That's typical baseball
terminology, standard metricsthat have been around forever.So let's go ahead and look
at the program that did this.We're going to use the
DW, the at bats data setin the DW, Data
Warehouse library.A few notes about this program.Notice we're using
the data null.We're not specifying
the name of the outputdata set in the data step.Because once we've
created and aggregatedall of our information
into our hash table,we're just going to
output that to a data set.I'll come back to this in a bit.So we're going to
define our hash tableobject called slashline.We're going to say we want to
order it in ascending orderbased on the key.The key Is going to
be the batter ID.And as Paul has said in
the earlier sections,the key values are not
saved in the data portionof a hash object, and
thus not available to youto output unless you also save
those same fields in the dataportion.So our data portion is going
to be just the informationthat you just saw in the
batter slashline data set, PAs,that bat hits.Notice I'm using a
convention here of fieldsthat I need to include
in the hash tableto do some work
that I don't wantincluded in the output data set.I just prefix that
with an underscore.That's just a convention.And then notice
on my output dataset name there, I say drop
equal under bar colon.What a colon does in a
variable list with a prefix,it says, take all
the variables thatbegin with in this case
an underscore, regardlessof what they continue with.So that's just I found to be
a useful technique if I needto keep some variables,
some items in columnsin my hash table,
but I don't want themin my output data set.So I'm going to do
my define done methodto say my hash table object
is complete and defined.I'm going to associate a
format with those four metricsthat I've calculated, a 5.3.You know, many
metrics in baseballare basically percentages.Although some of them can be
greater than 100 or 1,000.So we don't use percent signs.So I'm going to display
the values with a 5.3.I'm now going to
use a do until loop,a so-called DOW-loop
where I'm going to readall the data in a single loop.And that's why for
example I don'tneed to define my hash
object by doing somethinglike a standard approaches
if underscore and underscoreequal 1 then do.I don't need to do that.So I'm going to
read all the data.It's really important,
and once again, Paulhas mentioned this, when you're
doing hash table operationsthat the values that are
transferred from the hash tableto the program data
vector host variables,it's important to make sure
that they are initialized sothat they don't get carried
forward from one observationto the next.You know, you want
them to be found,to correspond to the current
execution, if you will.And so you're going to see a
convention that we're regularlygoing to do a call
missing before we're goingto do the slashline dot find.So what the slashline
dot find is going to do,since we've read
in the batter ID,it's going to search to
see, does the batter IDexist in the hash table.And if it does,
it's going to returnthe data for that batter,
his cumulative data so far.So that's an important
feature to keeptrack of that you need to
put that call missing there.So the slashline dot
find is going to say,have I seen this batter before.If I have, bring in his
current data, add one.So PA, all of these
fields here that yousee other than obviously
batter ID are goingto be copied from the hash
object into the host variablesin the PDV.So we're going to increment
our plate appearances fieldPAs by one because this is
another plate appearance.Let me go ahead and go back
and look at the at bats dataagain and point out
a few columns there.The at bats data is going to
have a bunch of 01 variables.It tells me, does this
count as an at bat or not?Is this an out?Did the batter get a hit?Did the batter get on base?So these are a bunch of 01
variables that were createdin our fact table to
facilitate doing the metricsin the analysis, something to
keep in mind when you designfact tables in a star
schema for example.So we're going to increment
at bats, the at batsfield by either 0 or a
1, based on the value of,did this plate appearance
result in an at bat?Did this result in a hit?The basis field, if I go
back to the at bats fieldis going to tell me how
many bases the batter got.Bases is going to be, two is
a double, one is a single,three is a triple, four
is going to be a home run.So those are just standard,
simple baseball metrics.We need to know how many total
times we need under bar basesand under bar reach base.Those are both going
to be used to calculateeither the on base percentage
or the slugging percentage.Now, actually, we could
just end the loop right now.And once we exit the loop,
we could go back throughand iterate through
the hash objectto do the remaining divisions.But to keep this example simple,
we just did these divisionsin line instead of that loop.And they're going to be
replaced each time through.So we're going to use
the divide function.The divide function, if
you're not aware of itis a nice function in SAS.It divides the two arguments.And if the second argument
is 0 we're missing,it results in a missing value.But it doesn't give you that
annoying message in the logabout a missing value.So you can see,
our batting averageis going to be hits
divided by at bats.Our on base percentage
is going to bethe total number of
times they reach basedivided by plate appearances.Slugging is similar.But instead of just counting
as 01 for reach base,you get more weight if you
got a double, a triple,or a home run, and then the
sum of on base percentageand slugging.And then what we're
doing is we'redoing the slashline
dot replace function.If the find function found
no data for this batter,in other words, it
was the first timewe encountered this batter.All of these fields
here are goingto be set to missing by
the call missing function.And so the values
that we're goingto have are going to
be one, and dependingon these other values.So the replace function
is now going to say,if this batter exists in
the hash table already,or doesn't exist
in the hash table,add the data to the hash table.If the data does exist in
the hash table already,just replace the current values
that the find method read inwith the current value.So that way we're basically both
updating the values in place,as well as adding new batters
as they're encountered.And now, once we've
read all of the datawe do slashline.output.And notice we've got
a quoted string whichis the name of the data set.One of the nice things
about the hash objectis that you can control
at execution timethe names of output data sets.Obviously here, this
is a character literal.So it doesn't change
at execution time.We'll see some examples
later where it does.Hopefully, this first
very simple examplemakes sense to you.We're going to build
on this and augment itin our future examples.Well, when we presented these
results to the business usersat Bizarro Ball
and the IT users,they were quite pleased with
how simple the program was.But they obviously had
a concern about the factthat the table as it
exists now, the outputis not terribly useful.Because who knows
who batter 10090 is.You know, maybe he does.And maybe his mother
does or his father.But nobody else knows who it is.So we need to modify our
program to augment this outputwith the batter name.So once again, reminder,
if we go to the DW library,we've got the players data
set where we can look upusing player ID as the key,
which team they play forand what their name is.So let's go ahead
and run this program.Notice it's chapter 8
slashline with name dot SAS.If we run this, notice, if
we're creating the same data setname, we've got the same
number of observations.We've got more variables.If we go back to
the work library,you can see that batter ID
is still the first column.But now it's followed by
last name and first name.And notice that the data
is sorted in name orderby their last name and
then their first name.So let's go ahead, go back and
look at the program for that.But instead of looking at
the program standalone,I'm going to use my WinMerge
tool here, a very nice freewareutility that allows you to
compare programs and highlightthe differences.So the bright orange here
indicates that these code linesare different.The gray indicates
additional lines.You know, in something that's
not highlighted at all,the code is
relatively unchanged.So you can see the core
of our calculationsis relatively unchanged.So let's just focus on
what the differences are.You know, when we define
our slashline hash table,notice that we made the keys
last name, first name, and thenbatter ID.And since we're using the
ordered ascending attribute,that means the
data is going to beordered based on the
batter's last name, tiesbroken by first name.And we need batter ID
because that's reallythe key for the data.In terms of defined data, notice
we've rearranged a little bit.We've got basically the same
metrics from PAs through opsthere.But what we added was
these three columns.Highlight them correctly,
last name, first name.We actually don't need
tmsk for this example.But we're going to need
it for the next one.So our slashline
definition, we just added,change the keys a bit so that
we can get the data sortedby last name and
first name on outputand added some additional
fields to the data portion.But the key difference is we
need to do a table lookup.Hash tables first became
popular for just table lookup.So let's go ahead and do a table
lookup using the hash table.Our if 0 trick here, Paul
has probably mentioned that.We need to make sure that
the variables that we'regoing to use in our hash
table have been definedto the program data
vector before we declare,before we define our hash table.So by using if 0, the statement
is never going to be executed.But at compile
time, SAS is goingto look at the player's data set
and what variables are in it.And since we did a
rename here, it'sgoing to add batter
ID to the PDV.So we now can load that data.And once again,
we're going to usethe data set DW dot players.Once again, we need
to rename player IDto batter ID
because batter ID isthe field that is used down
here in our slashline dotfind lookup.So this declare statement not
only defines the hash table,but it's also going
to load the data.We're also telling it
that if by chance thereare duplicate rows
for a given player,a player can change teams for
example throughout the year,we want to replace the data
with a more current one.We're cheating a little bit
here because if we weren't goingto do this over time, we'd
have to keep track of whena player was for each team.But for this simple
demonstration,we felt it appropriate
to do this.Plus it was a good
example to point outthe duplicate replace.So we're still going to have
our players table have only onerow for each batter ID.You know, we're not going to
get an error message if thereare duplicates,
and it if there'san attempt to add another row
by using duplicate replace.The defined key is
only the batter ID.The data is the team SK, the
team name and the first name.We change our call missing.So our call missing now also has
to initialize the batter's namein the team SK so we don't
carry forward values from oneobservation to the next.And then we just
do a player's find.And what that does is,
we know by definitionbased on the
structure of the datathat the players are
always going to be found.So that's why we don't
need to do an RC equal.Players dot find is
always going to work.The only reason you ever need
to do, once again, as Paulmentioned that, an RC equal
in a hash table operationis if in the off
chance it fails,you want to capture
the return code.And if you don't do RC
equal, and it fails,or it needs to capture,
send back a return code,you're going to get an error.But we know based on how the
data was constructed that we'realways going to find
a row for that playerso that we can look up their
last name and their first name.So this was just a
very simple exampleof adding a table
look up to our exampleso that we are adding the
last name and the first nameto our output data set.So as you'd expect, when
we presented these resultsto the users, they said,
gee, this is nice, but.There is always
that but at the end.They said, you know,
the team SK field,I'm glad you put the team
information in there,but nobody knows what
the team numbers are.Can we put in the team names?So Paul and I look at
each other and smilebecause putting the team
SK in the hash tableto sort of include it
in the output data setto sort of prompt them to
think about that turned outto be a good thing.And we said, well, of course, we
can put the team name in there.And then they said,
well, while you're at it,can we also get a roll up of
these metrics, the slash linemetrics at the team level
and the league level?And we say, yeah, we can
do that, not a problem.So let's go ahead
and run that program.You can see here it's chapter
8 team and player slashline.If we run this program, notice
that we've got more lookuptables, if you will.We're using the players,
the teams, and the leagues,look up tables.If we look at the
output results,notice we've now got the data
sorted by league, then by team.And then within each team,
we've got the individual playersfor that team.So this is pretty
much what they wanted.So we're happy.And if we scroll through, we
can see we've got more teams.And we've got a
hierarchy, et cetera.So let's go ahead and look
at the program that did that.Once again, we're going to use
that WinMerge toll to identifyto do the differences
between the previous programand the current one.So once again, we
added league, team nameto the first set of keys,
before last name, first name,and batter ID.So what that does is it's
going to cause our outputtable to be sorted by
league then by team thenby the player's name.Likewise, we needed to add
to the data, the leagueand the team name
and the batter ID.Notice team SK is
no longer there.Team SK doesn't add any value
as the users pointed outbecause you want to
see the team name.We've changed our
if set because we'vegot more variables that we need
to have defined to the programdata vector.So if we go back to SAS,
let's take a quick lookat the teams in the
league's data set.The team is for each
league, which team is itand what their name is.And then the league's data set
for each league surrogate key,what's the name of the league?So once again,
those examples areyour typical, external lookup.But now it's a
multi-level lookup.So that's what we changed here.If zero, then set dw players.So we also have to bring in
the team's and league's dataset to define those variables
to the program data vector.Notice our hash on the
player's lookup tableis the same as it was before.We now have to add a hash
table so we can look upthe team name and the league.So for each team, we need to
know which league they're inand what their name is.We then have to add a hash
table for the leagues.And we're going to define
a key of the league SK,the data is just the league.And then our program is similar,
our format and our do until.But we had to do a little
rearranging of things.I'll come back to this
block of code in a bit.We have to do a
player's dot find.Once we do that, we then
need to do a slashline dotfind to see if the player has
already been added to the data.But notice what we did here
is we have a link statement.But we did the players.Once we looked up the
player ID, the player tabletold us which team
they played for.The team lookup
got the team nameand which league they're in.And then the league's
lookup gave usthe name for each league.So we replaced players dot
find by getting the player nameinformation and their team SK.Team SK lookup against
the team's hash tablegave us the team name
and the league SK.And then the league's find gave
us the league name for that.Now, notice, we've got
now a link statement.What the link
statement does, it'sa little known feature of SAS.At least a lot of people
don't know about it.The link statement basically
allows you to a branch,like a go to almost, to a
different part of the datastep, run some code,
and then return.So we're doing a
link slashline, whichis this little block
of code right here.And the return
statement here, insteadof doing a return of
control to the supervisor,returns control to the
statement right after our link.So notice our calculations
are exactly the same.We had to add a return
there to bring us back.So now what we've done,
what we have so far,what this link statement
did is when we go here,we've got a player
identified within a leagueand within a team.By blanking out the
batter ID, the last nameand the first name,
we're now gettingrid of that information.So when we do a
link to slashline,the find command is now looking
for the data in aggregatefor that team.So we're doing
hierarchical rollup.So the link does that.We then blank out the team name
and do the same thing again.And so we go in.The slashline dot
find is going to say,have we done a
league aggregate yet?And let's do the
league aggregate.And then once we're done
here, we then output the datajust as we did before.Going back to our SAS
environment and the work area,you can see that we've
got the data sortedby league, by team name.And notice that a blank of team
name, a blank of batter ID,et cetera, give us the
league level average.So by blanking out our
fields in aggregate,we're basically
able to do a formof a hierarchical aggregate.You know, this is a
subset, if you will,of an NWAY aggregate
in PROC summary.So hopefully this
has made sense.We're actually going to revisit
this whole approach of dataaggregation at multiple levels
in a couple of different waysin later examples.So this was just a
simple introductionto that first example.Well, the next issue
we're going to addressis, it's a fairly
common attribute metricthat quite often people
want to do what'scalled an account distinct.It exists in lots of industries,
how many patients do I have?How many products do I have?In Bizarro Ball, it's how many
games has a player played in?So there are a number of ways to
do an account distinct in SAS.Unfortunately, it's
not one of the metricsthat you can do in PROC
means or PROC summary.PROC SQL does have the ability
to do A, count distinct.But count distinct is typically
a challenge in SAS programs.So let's go ahead and look
at this program, chapter 8slashline with
unique count dot SAS.Let me go ahead and just run it.You can see that it ran pretty
much the same amount of timeas the previous one
using the same data.If we look at our batter
slashline data set,notice that we've now
got a games metric whichcounts how many games there are,
the distinct number of games.Remember, we're reading
the at bats dataset which has a single row
for each time a player goesto the plate in each game.So it's not a simple
count of the rows.That's what the PA's is.So we can see that Terry Allen
played in 97 games, DouglasCampbell in 138.We can see that each of
the teams of self-checkhas 180 games.Well, that's because there
are 16 teams per league.So each of those 16 teams plays
the other 15 teams 12 times,so that's 180.And then we multiply
180 by 16, and we getthe 1440 at the league level.So we've now added a count
distinct to our set of metrics,something that is
a common metricin a lot of environments, in a
lot of industries but somethingthat can be a
challenge to calculate.So let's go ahead now and look
at the code that did that.Once again, I want to
use my WinMerge toolto do a comparison of
our previous programthat did the team
in player slashlinewith our getting the slashline
with the distinct count.So there actually aren't
a whole lot of changesthat we had to make
in our program.So for example,
you can see we hadto add the field games to our
hash table for our output.But all of our other table
lookups are the same.Our players hash
object is the same.Our teams hash
object is the same.Our leagues hash
object is the same.What we've added is another
hash object called U.And the keys for
that hash objectare league, team name,
batter ID, and game SK.We don't need to define any
data for this hash table.We don't want to
return anything.All we want to do is keep track
of which league, team, batter,game combinations have we seen.Now, as Paul said before,
what's going on hereis league, team name,
batter ID, and gameSK are also in the data portion.Since I didn't do a
define data for this,so perhaps I should
have done a definedata with a single
character field,or numeric field to save
space, to save memory.But memory wasn't an issue here,
so I didn't worry about it.So once again, what the U
hash table does is it's goingto allow us to keep track
of-- we'll see that in a bit--what league, team, batter
ID, and game SK combinationshave we seen?So once again, we need to
update our call missingto add the number of games.Likewise, we do that before
each link to get data.And the game is
calculated quite simply.U.add, the U function, we're
going to say the add method,so we're going to say,
add the current league,team, player to the hash table.And if it worked, that
means it's a new combinationthat we hadn't seen before.And if it worked, the return
code is going to be a 0.So U dot add equals 0 is
going to be equal to 1,or true, whenever we're
adding a new game.And it's going to be
not true, or false whenwe've seen this game already
for either this player,this league, or this team.So that's all we had to do
to add the distinct countto our hash object.So if we go back
to our output, youcan see that we've got games.And so to reinforce what we just
did with our link statement,rows like this were added when
we were at the player levelbefore we'd blanked
out the batter ID.Rows like this we're
updated after we'dblanked out the batter ID, the
last name and the first name,but before blanking
out the team name.And this row is
added or maintained,updated when we've blanked
out all three of those fields.So once again, the key
point is, we're doinga hierarchical aggregation.And so when we do this link at
the player level, we're saying,have we seen this
game for this player?When we're doing this link to go
down and calculate the metrics,once again let me highlight
the U were checking.Have we seen this team?Because we've blanked
out the book batterID last name and first name.And at this level, it's, have
we seen this league before?So just a very simple example
of doing a distinct count.And it all works
because we couldhave an in-memory
table of what gameshave we seen before for
a given set of keys, OK.Our next issue,
our next challengeis defining multiple different
levels of aggregation.We earlier saw a
hierarchical examplewhere we could rollup
player level data to team,and then team to league.Well, there are
lots of scenarioswhere you've got
non-hierarchical aggregation.Or you've got rows that could
go into multiple differenthierarchical tables.That is actually quite common
in Bizarro Ball/baseball.Baseball is basically crazy
for what they call splits.So they want to
see the attributesfor each player for example.So if we go look at the
output, since I've alreadyrun this program,
by player we'regoing to get the metrics for
each player, for each team.So we're going to get
team level metrics.For each month, we're
going to get the metricsfor each month of the year.Who knows, maybe things are
different in the hot summermonths than they are in
the spring and the fall.By day of the week
is another exampleas is getting it by
player in months.So there's lots of
different combinations,lots of different ways that
we're going to want to do this.And once again,
the idea is we needto do multiple levels
of aggregation.This example is actually the
first of two ways to do that.This is going to be a very
brute force approach, lotsof code involved here.We're going to do this
better in the next examplevia parameterizing it.But once again, we want
to build up to thatand start pretty simply.So we're going to define
what we call our lookup hashtables here.And later on, we're going
to define our resulthash object tables,
and tables that aregoing to contain our results.So for the lockup, we
need from the game's data,we need to know what the
data is, what the month isand the day of the
week so that wecan do the month in the
day of the week aggregates.At the team level, we
need to get the team nameso that we're using the
team name instead of the SK.For the players, it's similar
to what we saw before.But notice now we've added start
date and end date to the table.Notice also that I got
rid of the duplicatethat we had before,
where we basically said,we're not going to
worry about playerschanging teams, et cetera.Well, in this example, we are.So instead of keeping track of
letting data be overwritten,we're basically
going to say, we needto use the multi-data
attribute, which says,you can have more than one row
in our hash table for a givenkey.And if we look at
our players data,you will see how that works.Our players data, notice we have
a start date and an end date.And notice observations
4 and 5, Harry Reid.Harry Reid played for team 171
from March 31 through July 25.And then he played for
a different team, 228.For purposes of what
we're looking at now,we don't really care
about the name, from 726.And we're using 9999 to
say that he's still there.It's a high value basically.So what this table represents
is typically referred to as aslowly changing dimension
table where you can keeptrack of changes over time.And in fact, in the book of
the code that creates the DWdata from the
Bizarro data, there'san example of creating
the star schemaand creating a slowly changing
dimension table to manage this.And time permitting, we'll
try to briefly cover thatat the end of the tutorial.So let's go ahead now and
look at the hash tablesthat we are creating
for our results.So the by player hash table,
notice what the keys are.It's just the
player information.And that's what we
put in the data.When we do it by team, notice
that the player informationisn't there.But instead, we put in the
team level information.But the rest of the
metrics, PAs through opsare basically the same.And they're the same
in all the tables.By month, we just define
the keys differentlyfor this hash table.By day of the week, we just
define the keys differently.For player month, we use
both the player informationand month information.So the idea is we create a hash
table for each data aggregatelevel that we want.The data portion is exactly the
same in terms of the metrics.The only difference is the keys.The data component is a
little different in termsof the descriptive variables.But it's not different
in terms of the metrics.We define our variables
to the PDV just as before.Now, there is one other
thing that I glossed over,let me go back to.And notice that
declare hash statement.We don't define anything for it.We don't tell it what data set.We don't give it any keys.We don't give it any data.The key point, and
Paul touched upon thisis that this variable
here exists in the PDV.But it's neither a numeric
nor a character variable.It's what's called a
non-scalar variable.And a non-scalar variable
is a hash object pointer.So the hash object
pointer is nothing morethan a field that points
to a memory location.It's not the object
in and of itself.So let's go ahead and look at
how we're going to use that.So we've defined all
of our hash objects.We're going to go into our loop.And we're going
to read our data.We're going to do
the call missing.We're then going to find
the game level information.We're now going to use an RC
code to do the player look up.We're going to
need the RC code sothat we can go into the loop.And basically, what
we're doing hereis we're going through a
loop to basically saying,since a player can
have multiple rows,we need to find the
row for the right date.And since the game's
data, the game's dotfind loaded the date for the
game that we're talking about,that we'll make sure to
load the correct player dataso that we can get the
right team to associatewith this particular
row of data.This is once again,
a standard techniqueto deal with what's
called slowly changingdimension tables of type
2 where our attributes canchange over time.And we're then going to
say, if by chance, weknow that this really can't
happen or shouldn't happen.If we never found a player in
the appropriate date range,we need to blank out the value.And so we're now going
to do a team's dot find.So the team's dot find is
giving us the team name.So if we scroll back
up, we did the lookupof the player's data.We did the lookup
of the game's data.We did the lookup
of the team's data.And we're now going to go
through a series of linkroutines.Our slashline calculation is
pretty much as it was before.We're blanking
out the metrics sothat they don't carry forward.And we're basically
saying, h pointeris now a copy of the by
player hash object pointer.So h pointer now points to
the by player hash table.We link to slash line.And we do the calculations.We then say h pointer
is now the valueof the by team hash pointer.So h pointer is now pointing
to the team hash table.We do the link.And we do the calculations,
the same with by month,by day of the week,
by player month.So you can see that we had to
create h pointer because wedidn't want to have to
update this code down here,our slashline.We wanted to be able to
have a common variable thatpointed to our hash table
object no matter whichhash table object it was.You know, it's a little
known feature of hash objectsthat the variable that you
use in your defined statementfor your hash object
is merely a pointer.And so this little trick here
just creates a utility variableof the right type.Note that since the
hash object is neithera numeric or a
character variable,we can't assign the value to
either a numeric or a charactervariable.We need to create something
that looks like a hash pointereven though it technically
doesn't have an associated hashtable with it.Once we exit the loop and
have read all the data,we use the output method
on each of our hash objectsto create our
output SAS data set.Hopefully that's clear.Once again, in our
next example, we'regoing to do some
parameterization.So we don't have
to quote, ""Changethe code when we want to add a
new level of data aggregation.""But once again, we
wanted to introducethis using the brute
force approach to clarifyexactly what the code is doing.In the previous
example, we createdmultiple splits by
writing out codefor each of the levels of
splits creating a separate hashtable for each one.That gave us a
lot of capability.Unfortunately, it's
not terribly flexiblebecause if you wanted to add
more levels of aggregation,for example, or remove
a level of aggregation,you'd have to edit the code.Ideally, we'd like to
have a way of doing thatwithout editing the code.So that's where
parameter files come in.So we've got a
parameter file herein our template library,
template dot chapter 8 parmfile that defines the
hash tables to create.The hash table
variable is going to bethe name of the hash
object that we'regoing to create, along with
what columns to put in it.This is a simple version.We're just assuming
everything is goingto go into the data portion.And we just add another
flag here that says,is this particular
variable a key?So the by day of
the week hash tableis going to have day
of the week as a key,by month it's going to have
month as the key, et cetera.So let's go ahead and look
at the code to do that.We're going to create
exactly the same outputas we did before.We need to do a
couple of things.First off, we need
to have a macro.You know, in a perfect world,
I define this macro in an autocall library.But for purposes
of illustration,it's best to include it
right in the program here.So this macro
create hash is goingto loop through
our parameter fileand is going to create a
hash object for each item.So notice, we're
doing a DO WHILE loopto read through
our parameter file,template dot chapter 8 parm
file that we just said.You know, before
we enter the loop,we're going to declare
the hash table orderedwith A. Eventually,
our parameter fileshould be a little
more flexible.But for purposes
of illustration,let's just assume
that this works.So we're going to read the
first row from parm file.We're going to with an up case
of reading that hash table.And we're saying, if the is
a key parameter is turned on,we're going to
generate a defined key.Notice now our
argument to find keyis no longer a
character literal.The whole point is
define key and definedata can be any character
expression we want.So it can be, for example,
the name of the column,day of the week.Likewise, we're going to add
everything as a defined data.And when we exit the loop, we're
going to do a defined done.So what the create hash
object is going to dois it's going to create
for us the hash objectcode for our data step.Let me remove some comments
here so that we can see this.The next thing we need to do
is if you remember our code,we had a series of
hash object calls.We had a series of
link statements.We had a series of
output statements.So that's what
this PROC SQL codeis going to do is we're
going to read our parm file.Notice that I'm just referencing
chapter 8 parm file here.And it's going to be, we're
going to use the cat s functionto create our macro calls.And we're going to store
that into create hash calls.We're then going to create
a macro variable, which isthe series of link statements.We are then going to create
a macro variable thatis the series of
output statements.And I'm then going to
reset the print optionso that we can see what the
values of those three macrovariables are.So let me just run
this part of the codeand the macro definition.And so you can see that our calc
hash is a series of assignmentstatements just like we saw in
the previous example followedby a link separated
by semicolon.And output create hash
calls does the same thing.Output hash in the
macro symbol tableis split across the
dictionary dot macrostable because of the length.But the value is
what we would expect.So you can see that we use SQL
to generate the needed code.So now let's go down
to the data step.So we're going to declare
our lookup tables separately.So we still need our players
table, our teams table,and games table for our
table lookup functions.We still need our
declare a hash h pointerjust as we did before.But now, instead of
having to hard codeall of those hash
table definitions,we can just reference those.The create hash calls
is calling our macro.It's generating the hash table.Our this code is basically the
same as what we saw before.And so now we're going
to use our DO WHILE loop.We're going to loop
through our at bats data.We're going to do the same
call missing, games find,players find.The code is exactly as we saw
before, except inside the loop,we're now going to
have calc hash whichis going to assign
the pointer valuelink to the slashline
code to do the calculationand then return.And once we're done, we're
going to do output hashand then stop.So if we now run
this data null step--let me highlight the code,
and let's run this code.Notice that it creates exactly
the same output tables as wesaw before.It told us how many
rows it read from,template chapter 8 parm file
for each of our hash objects.And if we go to the work
library, we can see--notice, we've got an
underscore in the namehere to separate the hash
object from the other.So we've got the same
results that we saw before.However, the
advantage, once again,of this approach
being parameterizedis if we want to add or remove
hash tables from this analysis,all we have to do is edit
our chapter 8 parm filedata set to add another
what baseball calls splits.Or if we wanted to remove
one, we could do that.So it gives us a
lot of flexibility.Hopefully that makes sense.And in the next
presentation, we'regoing to talk about yet a
different way and perhapsa better way of doing this.In the previous example,
where we used parameter filesto parameterize the
creation of hash objects,we were still generating
code for each hash object.It was a bit easier than
the hard-coded version,a bit more flexible.Ideally, however,
we'd like to havea way of looping
through objects insteadof having to generate code.We'd like to get rid
of the macro logic.We'd like to get
rid of the SQL stepsto create the code
that's necessary.In normal data step
processing, if wewant to loop through something
and perform the same operationmultiple times,
we'll just createan array of those variables.So let's see if we can do the
same thing with a hash object.So let's look at
this sample program.Let me maximize it.So we're going to create
a variable called table.We're going to define the
variables to the PDV using thisif 0.We don't need to execute
that set statement.We're then going to create
a hash object that'sgoing to load the DW at
bats data, declare hash h1.Define the key, the data done.We then create a
second hash objectfor the pitches data set.And now let's try to create an
array of those two hash objectsso we can loop through them.The fact that I'm doing
nothing more in this stepshould be a give away
that frankly, thisis not going to work.And you can see we
get an error message.And the problem
is array elementsneed to be numeric or
character variables.And as we've discussed before,
hash objects are quote,""non-scalar variables.""Sometimes they are
referred to as non-scalar.And here it says you can't
create an array of objects.So we can't use arrays.But we can use a
hash object itself.So let's go to the
program for that.You can see this is called
chapter 9 simple HoH dot SAS.If we maximize it,
we're now goingcreate a hash object that
we'll refer to as HoHfor Hash of Hashes.The key is going to be
the variable table that wesaw the previous example.Because we want to
create two hash objects.The elements of the
data portion aregoing to be the hash
object identifier itselfand the name of the table that
we're going to load into it.Of note, the key
for a hash objecthas to be quote, ""A
scalar variable,"" ie,a numeric or a character
variable availablein the data step.We then declare a
hash object pointerjust as we've seen before.This doesn't have anything
associated with it.It just defines h to be of type
non-scalar or of type object.So we then create
using the new operator,a hash object containing
the at bats data.And we then add it, HoH add
to our Hash of Hashes object.We then do the same thing.We create a second hash object
containing the DW pitches data.We then add it to our HoH table.And now we want to be able to
loop through our HoH table.This is once again
just a simple example.We're just going to write
out the number of rowsin each hash object.In order to do that,
we use an iterationobject to enumerate through
all the items in the HoH hashtable.That's what this
DO WHILE loop does.Do while there's a next item.And if we haven't entered
the loop yet, what next doesis it just reads the
first observation.So the first time
through the loop, his going to be this hash object.We're going to get the number
of rows and write it out.The second time
through the loop, his going to be this object.We're going to get the number
of rows and write it out.So if we run this code
now-- let me minimize it--you can see that we get
notes in the data stepthat the hash objects loaded
each of those respective numberof observations.And our loop wrote out those
two rows to our SAS log,thus demonstrating we can
create a hash object whoseelements are hash objects
which we can then loop through.I hope this makes sense.And in the next
section, we're goingto take advantage
of this capabilityto optimize even further the
parameterization of our dataaggregation schemes.So let's now look
at the next stepin parameterizing this multiple
levels of aggregation usingthe approach of a hash table
whose entries are hash objectsthemselves.So let's go ahead
and look at that.Once again, it's going
to be parameter driven.I've got two parameter files.Here's one of them, chapter
9 splits, and so chapter9 lookup tables.So basically what
I'm going to do hereis I'm going to make hash
objects for both my outputtables, which is going to be
the chapter 9 splits as wellas my lookup tables.So notice here's some
subtle differencesbetween these two tables.The lookup tables specifies the
name of the data set to load.That's the data set tag.So it's a designation
that it's a lookup table.That's one way I'm
going to know that.And my chapter 9
splits table looksvery similar to the chapter
8 splits table, justsome minor differences to
deal with some variable namesto do things in a slightly
more generalized fashion.So these are the
two tables that I'mgoing to use to define
all the hash objects usingthe Hash of Hash.So let me close these.And now let's look at the code.So you see the very
first thing we're doingis we're defining a HoH,
Hash of Hashes table.You know, the key is going to
be the name of the hash table.And if we look back
at the lookup tables,the hash table is
games, players,and teams, what we saw before
for the output data sets.It's basically the
description of whatthe level of aggregation is.We define an iteration
allowing us to loopthrough the Hash of Hash
table that we need that.And we also declare
a couple of variablesthat we're going to need
to iterate through things.So these are going to be items.You notice that h
and iter are goingto be items in, or elements
of the data portionof the Hash of Hash table.So the Hash of
Hash table is goingto say, what kind of
hash table that it is.Some place to store
the pointer to it,some place to store the pointer
for the iteration object.And what calc and output
does, if we look downinto our set statement,
notice we are just readingin both parameter files.And calc and output is a 01
variable that's going to tellus whether this is a table
used for calculation, 0.Or whether it is 1.Or whether it is a lookup table.So calc and output
is going to havethe value of 0 for my lookup
tables and a value of 1for my result tables.So I'm doing once
again the DO WHILE loopto read in all of the
rows from the chapter8 lookup tables, the chapter
9 splits table by hash table.If I'm at the first row
for a given hash table,I'm checking if the data
set tag is not blank.If it's not blank, I need
to create a new hash objectand use the value of the data
set tag to load the data.If there is no data
set tag, then I'mjust going to create
the hash object as new.Notice, just in case
I need multi-data,I use the multi-data
parameter for each.So this particular
block of code isbasically creating
the hash object value,giving it the value of h.I then say, just like
before, if the current rowthat I've read in, if
the variable column--so let me look at
that in splits.If day of the week,
if it's a key,I need to make it
a key, likewisethe same for the lookup tables.Then regardless, I'm going to
define every column as a data.So once again, this
reinforces the factthat the argument to
define data and define keyneed not be character laterals.They can be any character
expression we want.And furthermore,
it also illustratesthat if we were
doing a defined data,and we've got 10 columns
we want to define,we don't have to put them
all in one defined data call.We can have 10
defined data calls.And then if we have
read the last rowfor that particular hash table,
do the defined done and nowadd the hash object to
our Hash of Hash tables.So at this point, we've
got our output tablesdefined as hash objects.We've got our lookup tables
defined as hash objects.But if you remember
the earlier example,we need the name of the hash
table objects in a variable,in a pointer so that we can
reference the lookup tables,for example.So that's pretty simple.So basically we
do a find command.And we're looking
for the row in--notice we're using
the key equal.That tells us we want to search
for the HoH table for the rowwhere the key, which is the
hash table is called games.So that brings that value in.And it's the value brought
in from the hash tableis going to be h.We then declare hash games
to make a hash pointeror a non-scalar field.And we just assign h to it.We do the same thing
for players and teams.And so now we have a games hash
pointer, a player's hash table,and a team's hash
player which we can thenuse for our lookup
references in our codesso that we can
continue to code them.Just as before, we are defining
the variables to the PDVand assigning formats.And we're now going to
read through the data,go into last track.We're going to read
the at bats data.Notice, we're renaming
batter ID to player ID.So we don't have to worry
about the rename there.We do the call missing before
we do anything else to assignall the values.We find the data about what
the game is, bring that in.So once again, the game's
object, if I look at itis going to bring in the
date and the time, the month,the day of the week.Those are going to
be the variables thatare aggregation keys.We then do the lookup against
the type 2 dimension tableto make sure we get
the proper player name.We then get the team name, once
again one of the standards,so very similar to
what we've done before.And now we go through the HoH
iter which we defined up here.The iteration object for
the Hash of Hash tables.So we're now going
to loop throughso that we can loop through
all of the hash objects.So this do until loop
is looping through.We're reading one
observation at a time.And as we read each
observation, we'regoing to loop through
the hash objects thatneed to be acted upon for that.We're going to say, if we're
not a table with output results,in other words, we're
one of the lookuptables, that's what this
if statement here is doing.Then continue, which
basically skips the loop.We then do the call missing.We do the find to find the
information for the given hashtable for that row, whether
it's the player ID or the name,whatever, because
we're now pointingto the right table
pretty generically.We do the calculations.This is the same block of
code we've seen before.We're doing the replace
just as we did before.And we loop through, and
we read all of the data.Notice, we didn't have
to do any link routines.We didn't have to create the
statement label slashlineand branch to that
multiple times.It's much cleaner.We're looping through
the observations.We're looping through
the hash tables.Once we've read all
of the data and loopedthrough all the hash
tables for each row,we go through a loop to go
through the hash table objectsagain.And we basically
do the same thing.If we're a lookup table,
that's what this condition is.Continue, which means
leave the loop regardless.We're using a character
expression, whichis the name of the hash table.We're concatenating that with
our same drop equal underscorecolon trick for the utility
variables we don't need.And we create our
output data set.So it's doing basically
the same aggregationas we saw in the previous
multi-way aggregation example,except the code is a bit more
straightforward because wedon't need any macro logic.We don't need any
PROC SQL to createthe macro calls and the
link routines and the outputroutines.It can all be handled
by simple looping.So let's go ahead and run this.You can see it ran
pretty quickly.It created the same set of
output tables as we saw before.So let's go to the work
library and look at them.So there's our by
day of the week.It looks very similar
to what we saw before.Notice we don't have the
ascending or descending hereso the order isn't there.That's an additional
feature we couldhave added to the
parameter tablesbut decided in the
interest of simplicityto not worry about it.While the orders are
different, the resultsare basically the same.So I hope this made sense in
terms of the Hash of Hash.Well, we have one more
example to present thanksto having a little time
before we hit the maximum timelimit for this presentation.You may not have noticed
in the previous examplethat the number of games was
not included in the output.You know, that is
the unique count.It turns out that's a
little bit of extra work.And we wanted to not confuse
or complicate the presentationof Hash of Hash
by combining thatwith also the little
bit of extra workto deal with unique games.But we would like
to present that.So I'm going to use my
WinMerge tool again to compare.So the program on the right,
chapter 9 HoH multiple splitsis the program we
just looked at.The one on the left is
basically the enhanced versionof the program that's
giving us unique counts.So you can see there's
a few changes here.But let me scroll down.And you can see that
basically the loop whereI'm doing the processing
is not that different.So let's go back
up and review this.If you recall, the way we dealt
with getting a unique countis we created just
another hash object whichwas the set of keys
that we've already seen.So that's what we're
basically doing here.We add this U hash object,
a generic variable for that,a generic hash
object pointer, whichis going to be the
set of distinct keysfor each hash table.And we declare it
to be a hash object.So that's the two
things we need to do.The next block of
code that you seeis based on the data set tag.We have to do a
little more work.You know, we need to
create the hash objectfor the calculations.Notice that we didn't
use the multi-data here.Didn't really need to in
the first program either.So if we're doing an
output, or creatinga hash object for
the calculations,we create the hash object
for the actual resultsand the hash object for the
companion table if you will,which is just a
set of unique keys.We're creating the hash
object for the lookuptables the same way.While we don't really
need a hash objectfor the unique keys for that,
we should create one anywayas a bookkeeping matter.Notice that it's not used.Unfortunately, you can't
assign a missing valueto a hash object
pointer, otherwise wewould have done that.But that's not a capability
that exists in the current code.We are then basically
enhancing the--if it's a key, then
we're adding the keyto the hash table as a key.So in the case of, for
example, the by player ID,the player ID is a key.Well, we also have to add that
as a key to our companion hashtable, which is a
set of unique keys.And we also need
to define game SK.So since we're counting games,
so the player ID game SKis going to be the
set of unique keys.It is going to allow us to
calculate distinct gamesfor the player level.If we're at the team
level, it's goingto be team and game SK, so
a little extra code there.We also have to, in
the defined data,we have to define
end games as oneof the parameters in our output.And we have to do a define
done for the hash table that'sthe unique set of keys.We do the lookups
to get the pointersfor the games, the players
and the team's hash table.We have to add the number of
games to our call missing.That's the only change there.And just like before, we're
using the add parameter.We're saying,
whatever hash tablewe're operating
on right now, tryto add the current row to it.And if it succeeds,
that means the keysfor this particular
row don't alreadyexist in the hash table.So it's a new combination.So we're going to add 1
to the number of games.And the rest of the code
is exactly the same.So here's the program
that we're going to run.It's a program that we
basically just looked at.So let's go ahead and run it.And you can see the log is very
similar to the results before.If we go to the
work library, we cansee we're getting exactly
the same results as beforeexcept we are getting
the number of games.And each team is
playing 180 games.You know, since we have an
even distribution of howevermany weeks of the season
each day of the weekhas the same number of games,
which is 30 games, 16 teams.By month, because the
months are different sizes,notice that March is a little
shorter because we don'tgo March and October, because
in March we start later,October we end earlier.Well, we've come to the
end of our presentation.Paul and I would like to thank
you for viewing this tutorial.I'd like to point out
as Paul has mentioneda couple of times,
and I have as well,the content in this
presentation is based largelyon our book, Data Management
Solutions using SAS hash tableoperations of business
intelligence case study.And in addition to the
content that's in the book,Paul and I have posted articles
at communities.sas.com.And you can find that content
if you use the search termDMHashBook.We've also answered
a number of questionsabout the hash
object, mostly Paul,I'll have to admit at
communities.sas.com.And some of those are also
tagged with DMHashBook.And the best way to ask us
a question about anythingthat we've presented
today in this tutorial,or in fact, in
general, any questionsyou may have about
the hash object,you can post questions
at communities.sas.comand tag us using the
tags, @hashman for Paul,and/or @DonH for me.So once again, thank
you for your attentionto this presentation.We hope you enjoyed it."
106,"JOSHUA HORSTMAN: Hi,
I'm Josh Horstmanand today we're going to talk
about SAS macro variable lists.Originally this was supposed
to be a hands on workshopto be presented at the SAS
Global Forum 2020 Conferencein Washington DC.Sadly the conference
had to be canceledand so we weren't able
to meet there together.But we can still learn about
SAS macro variable liststhrough this online format.I would encourage you to
download the exercise files.There are eight
exercises, and we'llgo through them one by one as
we move through this material.So fire up your SAS
software and let's talkabout macro variable lists.Now, what do we mean by a
macro variable list, anyway?Well first of all, a
macro variable listis just a way of
using macro variables.It's not a specific language
element that exists in SAS.You won't find it in
the SAS documentation.It's just a technique that we
can make use of macro variablesto our advantage.Basically we're going
to use macro variablesto store a series
of values, and we'regoing to do it dynamically--
that is the values are goingto come from our
data, and they won'tbe hard coded into our program.They'll be determined
at runtime basedon what comes into our program.This will allow us to build
programs that are more dynamic.In other words, they adapt
and respond based on the data.Their logic is
driven by that dataas opposed to having a
bunch of things hardcoded,and so we eliminate these
hard coded data dependencieswhich means we don't have to
change our program every timeour data changes.That saves us work, and it
makes our code easier to reuseand more flexible.It also lets us include lots and
lots of ampersands in our codeso we can confuse our
co-workers and makesure they'll have to
keep us around for yearsto come in order to
maintain this code.Just kidding about
that last part.Here's a little
outline of what we'regoing to go through today.First of all we'll do
a little short reviewof the macro language.Now, I'm going to assume that
you are familiar with the macrolanguage--at least how it works
at a fundamental level.If you're completely new
to the macro language,you probably would want to
go learn a little bit moreabout that before you
proceed with this lesson,because we're going to assume
that you know some of that.So go watch some other videos.There's plenty out there on
YouTube, or on SAS's supportchannels, or get Art Carpenter's
excellent book on the macrolanguage and find
out all about it.But we'll cover the basics just
to refresh your memory in caseit's been a little while.Then we'll talk about
creating macro variable lists,and specifically
we'll talk about howto create them using
PROC SQL, and we'lltalk about how to create
them using a DATA step--two different methods
with the same outcome.And we'll talk
about why you mightwant to use one or the other
in a certain situation.And then we'll
talk about actuallyusing these macro variable
lists to do something useful.And as you can see, we'll
go through eight exercisesover the course of these topics.With that, let's dig into
our macro language review.We'll start by talking about
how macro processing works.What happens when you
submit some SAS code?Well, the first
thing that happensis the code is scanned
by the word scannerand parsed into tokens.Tokens simply means little bits
and pieces of SAS language--the various language
elements, the bitsof syntax such as
variable names,or key words, or operators,
things like that.Those are tokens.And then those tokens are sent
to the compiler for syntaxchecking to make sure that the
tokens that you've put togetherand the order in which
you put them togetherconstitute valid SAS syntax.And only once that
has been checkedis that SAS code executed.Now, when the word scanner
detects a macro triggersomething different
has to happen.Macro triggers typically
start with a percent signor an ampersand.Not every percent sign or
ampersand is a macro trigger.Sometimes those have
other meanings in SAS,but most macro language triggers
do start with those characters.So when we find
one of those macrotriggers the word scanner
routes those elementsto the macro processor.So this is a bit
of an intervention.Before SAS gets the
code to execute,the macro processor steps
in to do some stuff.Specifically it
resolves macro variablesthat you may have
referenced in your code,and it executes
macro statements.Those are things like
%IF, or %DO, or %LET.And once it finishes
with that processing,some output results from that.And that output has
to be re scanned.Whatever is produced by the
macro processor is re scanto see if there are additional
macro language triggers in itand additional
processing is needed.Because your macro code could
generate more macro code,and we're going to
see an example of thatlater in this talk.Here's a diagram that might be
helpful for explaining this.Now, this diagram came from
Art Carpenter's excellent book,Art Carpenter's Complete Guide
to the SAS Macro Language,and I think it's pretty helpful.As you can see here,
we submit our SAS codeat the top left of the diagram.And this is sort of a
flow chart representation.So we go down and say, does
it contain macro references.That's the percent and
ampersand triggers.If it does the macro
facility is invoked,macro references are resolved--so that could be references
to macro variablesor entire macros--and then macro
statements are executed.And then that results
in potentially someadditional code.Now, what comes out of there
may just be pure SAS CODE.Because after all,
the macro languageis a SAS code generator.That's what it does, right?It generates SAS code.And if that's what
it contains thenit won't have any
more macro languagereferences, so the answer
to this question will be no.And then that step--meaning the DATA step or
PROC that we just submitted--will be compiled, parsed, and
executed in the normal way.However, if that text that
came out of the macro processorhas additional macro
references in it,then this loop must repeat.And this can repeat as
many times as necessaryin order to resolve all
of the macro referencesand execute all the macro
statements until we're leftwith nothing but pure text.Now, as far as the macro
processor is concerned,everything is either a macro
language element or text.The macro processor
doesn't actuallycare whether the text it
produces is valid SAS codeor not.The macro process
will produce the text,and then if it's
not valid SAS code,SAS will complain about that
but the macro processor won't.The macro processor
will simply dowhat it's told until it has
removed all macro references.It's important to
understand wheneach of these different
events happens,because understanding the
timing of various parts of macroprocessing can help us avoid
some problems that we're goingto talk about in a few minutes.Let's talk about creating macro
variables using the %LET macrostatement.As most of you probably know
we can create a macro variableby simply saying %LET the name
of the macro variable that wewish to create, equals, and
then the value we wish to assignto that macro variable.And when this statement is
executed by the macro processorit will take that value and
store it on the symbol table.The symbol table is where
macro variables are stored.So this statement here will
create a macro variable calledoutput_path, and it will store
the value C colon backslashtemp in that macro variable.Later we can refer to
that macro variableusing the ampersand notation.So for example, here we
have a FILENAME statement--FILENAME myfile and then
in quotes where we normallywould have a complete
path and file namewe have a macro
variable reference.We have &output_path
backslash myfile.txt.When this statement is submitted
the macro processor-- well,the word scanner--
will notice that thereis a macro trigger in here.And the macro
processor will thenkick in and resolve this
macro variable reference.So the macro processor doesn't
care about filename myfile.Those just pass on through.But then it sees
&output_path and says ah,that's a macro
variable reference.I need to go check
the symbol table,find out what value
that macro variable has,and replace that reference
with that value from the symboltable.So it goes to the symbol table
and finds C colon backslashtemp, and that value is placed
into the input buffer insteadof &output_path.So now we have filename myfile
and then C colon backslashtemp backslash myfile.txt.That is what actually
gets passed on down to SASfor execution.The macro processor
stepped in, didits job of replacing the
macro variable referencewith ordinary text, and then
finding no more macro variablereferences it sent what was
left down to SAS for execution.Now there's a problem that we
have to talk about that we cancome across when
we're using %LET.And this has to
do with the timingthat we discussed a minute ago.%LET is a statement that's
executed by the macroprocessor.That means that it executes
before SAS compilesand executes the SAS code.So let's look at
this DATA step here.We have DATA _null_,
set sashelp.class.Now that's one of the built in
data sets that comes with SAS.There's a whole
library called sashelp.sashelp.class has 19 records
with information about the kidsin a grade school classroom.And one of those
kid's name is Alfred.And we've said where
name equals Alfred.So this is only going
to read in one record.So we read in the one
record where name is Alfred,and now we have %LET and we
attempt to create a macrovariable called alfred_age.%LET alfred_age equals age.age is the name of
one of the variablesin the sashelp.class data set.So we read in one
record and thenwe attempt to take the
value of the variable ageand assign it to a macro
variable called alfred_age.That should work, right?Well, this is not going to come
out the way we were hoping.Here's what happens-- when
we submit this code it goesto the macro processor first.The macro processor
sees all of this text--DATA _null_, SET sashelp.class,
blah, blah, blah, blah.It just passes that on
through until it gets to %LET.Ah, there's a macro
language trigger.%LET alfred_age equals age.And it literally assigns
the letters A-G-E--that string-- to a macro
variable called alfred_age.It doesn't see that
as a variable name,because at this point in time
the DATA step hasn't evenstarted executing yet.At this point in
time, we're still justin the macro
processing loop-- thatloop at the top of
the diagram that weshowed a few slides ago.And so it assigns
that macro variable,and then it gets to run,
and then sends all restof this code down to SAS.And all the SAS compiler sees
is data _null_ set sashelp.classwhere name equals Alfred, run.That's it.So the macro variable is
going to contain the valueA-G-E. It's not going to
contain the actual age thatcame from the data set.Because of the timing of when
%LET runs we can't actually useit to get values from our data
set into a macro variable.So how can we get values from
our data into a macro variable?Well, one way is by using
a routine called SYMPUTX.Now, in SAS routines are
sort of like functionsexcept they don't
return a value.And the way you invoke them
is using the CALL syntax.So we usually see CALL SYMPUTX.So here we have
the same DATA stepthat we had on the last slide.However, instead of a %LET
statement I have replaced thatwith CALL SYMPUTX, and then
CALL SYMPUTX has a coupleof arguments.The first argument is the
name of the macro variablethat we wish to create.And it's in quotes because
we've provided a literal string.So ""alfred_age"".If I didn't put the
quotes around it,it would think
that was a data setvariable that I was attempting
to take the value from.So the macro variable
is alfred_age,and then the value that I wish
to put into that macro variableis the value that's
in the variable age.Well as we discussed before
there's only one recordread in here, because
there's only one record thatsatisfies this WHERE statement.So the value of age that
comes in with this recordis stored in the data
set variable age,and then when CALL
SYMPUTX executes,that value is placed into the
macro variable alfred_age.The key to remember here is
that CALL SYMPUTX executesat DATA step execution time.It's not actually a
macro language statement.It's a DATA step statement.It puts a value on the
symbol table, whichis where macro
variables are stored,but it's not part of the
macro language itself.So because of that it executes
during DATA step execution,after all the macro variables
and macro references have beenresolved, and after the DATA
step has compiled and begunexecuting and data
has been read in.So at this point in time, we
do have access to our data,and we can use this method to
store data values in a macrovariable.And in this case, the value
that's read in will be 14--that's the value of age for
that record in that data set.And that's what will be stored
in the macro variable calledalfred_age.Now this brings us to
our first exercise.In exercise one we will
create a simple macro variableat execution time
using a DATA step.We will be working with
the sashelp.cars dataset, which is one of the example
data sets that comes with SAS.And our task is to
use the CALL SYMPUTXroutine to place the MSRP--that is the manufacturer's
suggested retail price--for an Acura MDX into a macro
variable called MDX_PRICE.The code is partially
written for you.There's a DATA step here with
a SET statement that reads fromsashelp.cars, and a WHERE
statement that will subset thatdata set to read and just
the one record that pertainsto the Acura M.D. x all you
have to do is add a CALL SYMPUTXstatement that will
take the MSRP--and by the way that's the
name of the variable, MSRP--take the value of
MSRP and place itinto a macro variable
called MDX_PRICE.So at this point if you would
like, you can pause the video,open up your SAS session,
load the exercise 1 filewhere you should find
this code, and then youcan replace this comment with
an actual CALL SYMPUTX statementto complete this task.I'll go ahead and do
it here on the video,and there's also a
solution file thatis included with the workshop
files that shows one way youcould complete this task.Let's open up our
SAS session, and wewill work through exercise
number 1 together.All right, I have the exercise
number 1 workshop file open.We need to replace
this comment herewith a CALL SYMPUTX statement
that will create the macrovariable that we need.So let me delete the comment
and we will type CALL SYMPUTX.And the first argument
to CALL SYMPUTXis the name of
the macro variablethat we want to create.In that case, it
will be MDX_PRICE.And then the second
argument is the valuethat we want to assign
to that macro variable.In this case, we want
to assign the valuefrom the data set
variable MSRP, whichwas read in from sashelp.cars.MSRP.All right.And you'll notice at the
bottom of the code file herethere is a little
bit of extra codethat just verifies the value
of this macro variable.Because if all we did
was run this data step,we wouldn't actually
see anything happen.We wouldn't be able to verify
that this macro variable hadbeen created.So in order to do that we
have some %PUT statements,which will write this
message to the log--""the MSRP for an Acura
MDX is"", and then itwill pull the
value of MDX_PRICE.By the way, why are
there two periods here?Well, one period is to
tell the macro processorthat this is the end of the
macro variable reference.And the second one is
just an actual periodthat will appear in the log.If I just put one
period there, then thatwould be taken as the end of
the macro variable reference,and there wouldn't be
one in the log as well.So if I wanted one in the
log, I have to put two.So let's run this code
and let's look at our log.And here we can see in our
log it did work, in fact.Our DATA step ran.One observation was
read from sashelp.cars--that was the observation
for the Acura MDX.And then the value of MSRP was
stored in the macro variablethat we wanted, and here it is.36945.Now there's a little
bonus exercise here--what happens if a %LET statement
is used instead of a CALLSYMPUTX statement?We talked about this
a little bit earlierwhen we were using the
sashelp.class example,but let's do it anyway just to
remind ourselves what happens.Let's suppose instead of CALL
SYMPUTX I said %LET MDX_PRICEequals MSRP.Recall that when this
code is submitted,the macro processor is going to
get a hold of this %LET first,before any data step
has begun executing.It will assign the letters
MSRP to the macro variableMDX_PRICE, and that will be the
value of that macro variable.Then the DATA step will run
the rest of these statements.But of course this
won't be there anymore,since it will have been
consumed by the macro processor.So if we run this code
and then check our log,you'll notice we have the
MSRP for an Acura MDX is MSRP.Clearly that's not the result
that we wanted or expected,and that is because of
this %LET statement.Let's take another look at
that macro processing diagram.The %LET executes here during
the macro processing phase.At that point in the
process the DATA stephas not yet begun executing,
nor have any data been read in.The CALL SYMPUTX statement
executes down hereduring DATA step execution,
after the macro processor hasalready finished its work and
then the DATA step has beencompiled and started executing.That's why the CALL SYMPUTX
lets us get at that value fromour data from the sashelp.cars
data set in this example,whereas the %LET does not
have access to that because ithappens too early.So it's key to understand
what happens when.Another way we can create a
macro variable at executiontime is using PROC SQL.Just like we used CALL SYMPUTX
in the DATA step to do this,we can use PROC SQL to do this.And we do that
using INTO clause.Now, the INTO clause is not a
part of the ANSI SQL standardthat exists outside of
SAS, but it's somethingthat SAS has added on
to the SQL languagein order to interact
with macro variables.The syntax is quite simple.We simply insert it into
our SELECT statement.select age-- age is
the variable that we'regoing to select, in this
case from sashelp.class.But we select it
into a macro variablethat we're going to
create called alfred_age.And we have to precede
that name with a colon--that's how the syntax works.So we select age into
alfred_age from sashelp.classwhere name equals Alfred.And as you may
recall from earlier,this only selects one record.So one value gets
selected and isstored in this macro
variable alfred_age.And it will have a
value of 14, justlike it did when we used
CALL SYMPUTX in the DATA stepearlier.For our second
exercise we're goingto create a simple macro
variable at executiontime, this time using PROC SQL.Once again we're working with
the sashelp.cars data set,but this time we'll be looking
to get the value of weightfor the Acura MDX and place it
into a macro variable calledMDX_WEIGHT.And we'll use that INTO
clause from PROC SQL.You'll see that we're provided
with some PROC SQL code.All we have to do is complete
the SELECT clause here,and store the value of weight
into a macro variable calledMDX_WEIGHT.Let's switch over
to our SAS session.I have the exercise file
for exercise 2 open here.So here is our SELECT statement.I'm going to remove this
comment and I'm goingto say select weight--that's the name of the variable
in the data set sashelp.carsthat I want to store
in my macro variable--into MDX_WEIGHT-- that's the
name of the macro variablethat I wish to create.select weight into colon
MDX_WEIGHT from sashelp.cars,and then I have my
WHERE statement thatgets the record for Acura MDX.If I run this code
and look at my logI see in the log the weight
of an Acura MDX is 4451.So that is the value that
it pulled from the data setand put it into this macro
variable, MDX_WEIGHT.You'll notice there's a
few extra blank spaces herethat we didn't ask for
that has to do with the waythat SAS converts numbers into
character values for storingthem into a macro variable.But we have a
little bonus-- whathappens if you add the keyword
TRIMMED at the end of the INTOclause?Let's try that.So that's weight into
MDX_WEIGHT TRIMMED.If we run that, it will
remove those leading spacesand we'll have just the
weight of an Acura MDX 4451.So the way we did it
first actually storesthe leading spaces as part
of the macro variable value.When we use the TRIMMED
keyword, then those basesare trimmed off before it's
stored in the macro variable.Now that we've reviewed the
basics of macro processingand we've discussed how to
create macros variables,including at runtime using the
CALL SYMPUTX routine or PROCSQL, we're ready to actually dig
into some macro variable lists.First of all there are
actually two different waysto construct macro
variable lists.These provide similar
functionality,but a slightly different
approach with each,and we'll discuss both.The two types are
sometimes referredto as a horizontal
macro variablelist and a vertical
macro variable list.A horizontal macro
variable list consistsof a list of values all stored
in a single macro variable.For example, let's say we wanted
to store these three values--these are the unique values
of the variable origin fromthe data set sashelp.cars,
the vehicle origin--Asia, Europe, and USA.Let's say we say we wanted
to store those three valuesin a single macro variable.This would be a way to represent
them as a horizontal macrovariable list--Asia, space, Europe, space, USA.Now you'll notice I'm using
a %LET statement to createthe list here, and I'm
calling it origin_list.However our goal is to create
this dynamically at runtime,not using a %LET.But just to show you how
the lists would look,I'm just representing it
here with a %LET statement.You do have to give
some thought to what youwant to put between each value.The whole point of storing
these values in a listis because we're going to
do something with this list,and we're going to need to be
able to parse it apart againlater.In order to do that,
we need to knowwhat delimiter we can use to
separate the values later.We can use the space, but if
you think your data values mightcontain a space then it might
be better to use something else.Oftentimes we'll use punctuation
marks like commas, slashes,or even the tilde which
is what I've used here.So here is a horizontal
macro variablelist with the values
separated by tilde.A vertical macro list
works differentlyin that each value is stored
in a separate macro variable,but the names of
the macro variablesare structured
such that they allhave the same prefix and then
a sequential number afterwards.So for example, if we
have these three values--Asia, Europe, and USA--and we wanted to place them in
a vertical macro variable list,then we would create a macro
variable called origin1, whichwould contain the
value Asia, origin2would contain the value
Europe, and origin3would contain the value USA.Again, I'm showing you
these with %LET statements,but that's not how we want to
create them because this ishard coded.We want these values
to come from our dataso that we aren't hard coding
these values in our program,but they're
determined at runtimebased on what happens
to be in the data set.That's where we're
going with this.So this is just
two different waysof representing the
same set of valuesin a macro variable list,
horizontal or vertical.Let's talk about
how we can actuallycreate one of these
macro variablelists at runtime using our data.First we will do
it using PROC SQL.This is similar to the syntax
that we just talked aboutin the previous exercise.However, we have
a new clause herewhich is this
SEPARATED BY language.So here we're going to
take the distinct valuesof the variable origin--that's one of the
variables on sashelp.cars--into a macro variable-- that's
what this colon means again--into colon origin_list separated
by the tilde character.That will take all of the
values that are returnedand it will string them
out in a single macrovariable separated by tildes.And you'll notice I used
the SELECT DISTINCT keyword,here because there are many
records in sashelp.cars--about 400, I think--and each of these values of
origin is repeated many times.We only want one copy of
each unique value in our listthat we're going to use
for later processing.So we say SELECT
DISTINCT origin.There are three distinct
values in there.We can catenate them
into a single macrovariable separated by tildes.And then it's also helpful
to know how many values arein that macro variable list.Because usually we're going to
do some processing with thislater, and oftentimes it's
going to be in a loop.We may loop through
this macro variable listand do something with
each item from the list.In order to do that,
it's helpful to knowhow many items
are in the list sothat we can control our loop.We could of course
count those items later,but the information is
readily accessible now,so we're going to grab it now.Anytime you run a
SQL query in PROC SQLit creates an automatic
macro variable called sqlobs,and that macro variable
contains a number representingthe number of results that
the last query returned.So we can take that
value and store itin a macro variable of our own.I just called it numorigins
here-- the number of originsthat there are.So I said %LET numorigins equals
&sqlobs I don't want to leaveit in this macro
variable, sqlobs,because that might get
overwritten later by PROC SQLif we should happen to call
it again in our program.That's why I'm storing
it in numorigins,so I know I have a separate
copy that I can use later.Now you might be wondering,
how can I do this with a %LETstatement?Didn't I just get through saying
all this stuff about how %LEThappens during macro
processing, and it's too early?Well yes, that's true.However, what we
need to understandis that when we submit SAS code
it's processed step by step.So in other words, one step--one data step or one PROC step--is submitted, run through
the macro processor,compiled, and executed, and
then we go to the next step.In this case of PROC SQL equal
it's a bit of a special case.PROC SQL automatically
executes as soonas you reach the end
of a SELECT statement.It's just the way that it works.So as soon as this semicolon at
the end of the SELECT statementis hit, then this
code is submitted,it's run past the
macro processor--of course there's no
macro language in there--it's compiled and executed,
and that query happens.And at that time this macro
variable sqlobs is created.Then the next bit of code is
run through the whole process,and that's this %LET statement.Well, %LET is caught
by the macro processor,and it goes out and checks the
symbol table for a variablecalled sqlobs, which exists
because that was created duringthe previous step.OK, so we take that value--which in this case
is going to be 3--and put it into a new macro
variable called numorigins.So what we end up with
is two macro variables--one called origin_list
that contains this string--Asia, tilde, Europe tilde, USA--and another macro
variable callednumorigins that contains a 3.For our third
exercise we're goingto create one of these
horizontal macro variablelists using PROC SQL.This time we'll use
the sashelp.class,and we're going to get the names
of all the students separatedby spaces into a macro variable
list called student_list.And we're also going to
grab the number of studentsand put it in a macro
variable called num_students.Here's the code we're given.All we need to do is complete
the SELECT statement,and then add a %LET here at
the bottom to get the numberof students.Let's flip over to
our SAS session.Here we have exercise
3 ready to go.So we need to complete
the SELECT statement.And what is it that
we need to select?We're going to select--well, we know the
variable is called name,and we only want
distinct values.Now, it just so happens
with this particular dataset there's only
one row per student.So we technically don't need
to say distinct because thereare only distinct values.However, just to
be thorough we'llsay SELECT DISTINCT name--because what if we get a
duplicate record later--SELECT DISTINCT
name into and we'regoing to create a macro
variable called student_listSEPARATED BY--and I want them
separated by a space,so I put a space in quote marks.SELECT DISTINCT name into
student_list separated by spacefrom sashelp.class.That should create my
student_list macro variable.I also want to know how
many items that I justput in that macro variable.Let's do %LET and we'll create
a new macro variable callednum_students equals and
then we will grab the valueof the existing macro
variable, sqlobs.OK.We run this code,
we check our log,and I have a little diagnostic
code here at the bottom againto show us the values of
those macro variables.So we had 19 is
the num_students,and then the student_list
macro variablecontains Alfred, space,
Alice, space, Barbara, space,and so on.And it runs through
all the students' namesall the way through William.Now we have a little
bonus here-- usea delimiter character
other than a spaceso that we could still
distinguish individual nameseven if some contain spaces.OK, let's try another character.You can pick whatever you want.Let's try a slash.I'll change that space
to a slash, run the code,and now I get an new
list of the 19 studentswhere the names are
separated by slashes.Now that we've created a
horizontal macro variablelist using PROC SQL, let's
try creating a vertical macrovariable list.The syntax is similar,
except that wename more than one macro
variable in our INTO clause.So you notice here we have
SELECT DISTINCT origininto colon origin1
and then a hyphen.Now, this hyphen
tells SAS that origin1is the beginning of an entire
range of macro variables.There was a time
in the past whereyou needed to give an
upper bound to that range.For example, you might have
said origin1 dash origin3.Or if you didn't
know how many youwere going to need
in advance, youcould say origin1 dash
origin999, for example,and SAS would just use as many
macro variables as needed.However, a few versions
ago they made itso that you can simply
say origin1 dashand it will create
as many as it needs.So if there are
three distinct valuesof origin in sashelp.cars, it
will create origin1, origin2,and origin3.We don't need to
supply the upper bound.I also included the ORDER BY
clause, which is optional.But if you want the
macro variable listto be in a certain
order, then youcan use an ORDER BY
clause to accomplish that.So in this case there'll
be alphabetical by origin.So origin1 will be Asia,
origin2 will be Europe,and origin3 will be USA.And then, again, we have a
%LET statement just like beforeto get the number of items
that we put into the list.Because again, when we
get to some applicationsof these macro
variable lists, we'regoing to most likely want
to loop through this listand we'll need to know how
many items are in the list.Here are our resulting
values from this code.We end up creating four macro
variables in this case--origin1, origin2, and origin3
with those three values,and then numorigins
containing a 3.Our fourth exercise will be
to create a vertical macrovariable list using PROC SQL.And this is going to be
just like exercise 3.We're going to get the
names of the studentsfrom sashelp.class,
but this time we'regoing to put them in separate
macro variables, student1through student19.And of course we'll also
create a macro levelcalled num_students
that tells ushow many students
are in our list.So let's slip over to
our SAS session here.Here is the code for exercise 4.We have a PROC SQL
statement, SELECT,and we need to complete
this SELECT statement.OK I want to get the
names of the students.The variable is name, but I
only want distinct values,so SELECT DISTINCT name.And where do I want
to put it into?I want to put it into
a macro variable calledstudent1 through whatever.I don't need to
give an upper bound.We'll just say student1
dash-- however many it needs--from sashelp.class.And then we will add a
%LET num_students equals--and we'll grab that
macro variable sqlobs,grab that value and
put it into a new macrovariable called num_students.Then we have some
code down here thatwill print out the values
of these macro variablesso we can verify that
this was done correctly.I didn't print all 19
just for lack of space,so we have the first
three and the last three.Let's run this code.Check our log--PROC SQL ran, and here
is this output down here.Number of students 19, and
we noticed that student1--that was this macro
variable, &student1--has a value of Alfred.&student2 has a value of
Alice, &student3 has a valueof Barbara, and so on down the
line all the way to &student19,which contains
the value William.So the 19 names of the students
were put into 19 separate macrovariables, but the
names of the variablesare structured student
1 through student 19--that's what makes this a
vertical macro variable list.We've created macro
variable listsusing PROC SQL, both
horizontal and vertical lists.Now we're going to talk about
creating those same typesof lists using the DATA step.Using the DATA step
is a little bit morecomplex than using
PROC SQL, but thereare times when it
can be advantageousbecause you have access to
the full power and flexibilityof the DATA step.First let's talk about how
to create a horizontal macrovariable list in a DATA step.I have this code split
across two slides.This is step 1.And this step is
optional, but I'massuming that we want to only
a list of distinct values.So since we don't
have the convenienceof the DISTINCT keyword
like we do in PROC SQL,we can use PROC SORT to get just
the unique values of origin.That's the variable
that we're goingto put into our macro
variable list in this case.So we do a quick PROC SORT.DATA equals sashelp.cars,
OUT equals--and I created a new data
set called unique_origins,and I only kept the
variable origin.And here's the key--I used the keyword
NODUPKEY which eliminatesduplicate key values.In this case, the key is origin.That's our BY variable.So this will create
a data set thathas just the variable origin
and just three rows with thosethree values of origin which
are Asia, Europe, and USA.Once we've done this, now we
can use a DATA step to buildour macro variable list.And we're simply going to
step through this data setand build the list
piece by piece.So we start with DATA _null_
because we're not reallyinterested in creating
an output data set here.We have a SET statement,
unique_origins--that's the data set that we
just created in PROC SORT--END equals eof.eof is just a variable
name that I made up,but the END equals option
on the SET statementwill set this
variable eof to trueonly once the last record has
been read from unique_origins.It'll be false until
that point in time.We'll need that in a little bit.This LENGTH statement
simply declaresthat a new variable called
origin list is beingcreated with a length of 200.Now, this is not
a macro variable.This is a data set
variable-- origin_list.It also appears on
this RETAIN statementbecause we're going
to build this originlist across multiple
iterations of the DATA stepas we read in values from
the unique_origins data set.So we need to be able to retain
the values across iterationsof the DATA step.That's why we have this
RETAIN statement here.Then we have the workhorse
of this DATA step--an assignment statement,
origin_list equals,and on the right hand
side of the equalswe have a call to
the CATX function.Now for those who aren't
familiar with the CATXfunction, it's simply
concatenates strings,but it inserts a delimiter.And that's the first argument
of the function delimiter--in this case, tilde.So the CATX function will
take these other arguments--origin_list and origin--it will strip them, that is
removing trailing and leadingblanks, insert this delimiter in
between, and concatenate them.Now, the first time
through the loop here--the first one through the DATA
step when we read in the firstrecord--origin_underscore
list is missing.It's blank, there's
nothing in it.origin has the first value
that we read in from the dataset, which was Asia.So when we call CATX, if
we give it a missing stringit will not in
certain delimiter.It's smart enough to not
do that, which is great.Makes it very convenient.So what we'll end up
with is CATX of missingand Asia will just be Asia.So the first time through,
origin_list equals Asia.Then we have this
if eof THEN DO.Well, eof is false
because we haven'tread the last record yet.Since eof if it's
false, we don'tdo any of this stuff
in the DO block down.Here and we just we've finished
our trip through the DATA step,and now we loop back to the
top and read another record.We read the second record
from unique_origins,which is Europe.And now origin_list equals
CATX of origin_list,which has Asia from before--and remember, it's
still there because weuse the RETAIN statement--Asia, the tilde gets
inserted as a delimiter,and Europe is the new
value we just read in.So now origin_list will
have Asia, tilde, Europe.We're still not at the end of
the data set, so this is false.We go back to the top and read
the third and final recordfrom unique_origins, which is
going to have a value of USA.So now origin_list
equals CATX of--and origin_list still has
Asia, tilde, Europe in it.And now we got another tilde as
a delimiter and then the valueUSA.So now we're going to have
Asia, tilde, Europe, tilde,USA in a DATA step variable
called origin_list.We haven't created a
macro variable yet.This is just a
data set variable.But now eof is true.We are at the end of the file.And since we're at
the end of the filewe are going to execute
these two states thatare in the DO loop--the DO block, I should say.So we have a CALL SYMPUTX--remember, CALL
SYMPUTX is what weuse to create macro variables
during a DATA step execution.We're going to create a macro
variable called origin_list,and what we're going to put
in it is the value of the DATAstep variable origin_list.That's the variable
we just created.So this is a little
confusing because thesehave the same name, but these
are two different things.We're creating a macro
variable called origin_list,and we're putting in it
the value of the DATA stepvariable called origin_list.Then we're going to create
another macro variable callednumorigins, and into it we're
going to put the value _n_._n_ is an automatic variable
that is created for usby the DATA step, and it simply
counts the number of iterationsthat the DATA step is executed,
which you could typically thinkof as a counter of which record
we're on, assuming we're justreading in one record per loop.So at this point
in time _n_ is 3,because we're on our third
trip through the implied loopof the DATA step.So the number 3 is going to
get stored in numorigins.So we've created two
macro variables here--one called origin_list which has
Asia, tilde, Europe, tilde, USAand one called numorigins
which has a 3 in it.For exercise 5 we're going
to create a horizontal macrovariable list using a DATA step.Once again we're going to deal
with the sashelp.class dataset, and we're going to take
the names of all the studentsseparated by tildes and put
them into a single macrovariable called student_list.And then we'll also store
the number of studentsin another macro variable
called num_students.And we can skip the PROC
SORT step for this exercise,because we know the
student names are unique.There's only one record per
student name in the data set,so we don't need to worry
about removing the duplicates.Here's the code that
we're going to be given.Most of the DATA step is
already put in here for you,we just have to
add a few things.So we have our SET
statement, our LENGTHand RETAIN statements,
and we havethe beginnings of this
assignment statement,student_list equals.And now we need to add some
logic to concatenate or appendthe current name to student_list
so that we can buildthis string piece by piece.And then we're
also going to needto add these two CALL SYMPUTX
statements inside this IF THENblock at the bottom.So let's go over to SAS
and we will do that.Exercise 5, all right.So here we have our assignment
statement, student_list equals.Now what I want to do is
take the existing valueof student list, which is going
to be blank the first timethrough, and concatenate the
current student name onto it.And our delimiter is
going to be tilde,so I want to put a tilde
as our delimiter that.The first argument to CATX
is always the delimiter.Then the things that
we want to concatenate.Well, we've got
student_list and we'vegot the variable
name, which comesfrom the sashelp.class
class data set.So there's our
assignment statement.When we get to the
end we're goingto have to create a
couple of macro variablesusing CALL SYMPUTX statements.So let's do that--CALL SYMPUTX.I'm going to create a macro
variable called student_list,and I'm going to put in
there the value of this--the data step variable
called student_list.I'm going to put that in there.But only that I
don't want to do thisuntil the end of the
data set because,I want to wait till
I have the whole listright before I go sticking
it in a macro variable.And then I also want to
know how many students therewere on the list.So num_students and that's
just going to be _n_.And by the way, even though
_n_ is a numeric variable,CALL SYMPUTX will automatically
convert it to character for ususing a Best12 format.So unless you need to
use a different formatyou can just let SYMPUTX
do the conversion for you.And then I have some
code here at the bottomthat will make sure these
macro variables have the valuesthat we think they should have.Let's run this code and
let's look at our log.And we have number of students
is 19, and the student list--Alfred, tilde, Alice, tilde,
Barbara, and so on all the waythrough William.So we created a horizontal
macro variable list this timeusing a DATA step, but we
got the exact same resultsthat we did a couple of
exercises ago using PROC SQL.Creating a vertical macro
variable list in the DATA stepis actually much
more straightforwardthan creating a horizontal
macro variable list was.We don't have to do
all this bookkeepingto retain information across
loops of the DATA stepbecause we're already looping
through one value at a time.And with a vertical
macro variable list,that makes it easy to put out a
single macro variable each timethrough the loop.Once again we start
with a PROC SORTin order to get only
the distinct valuesof the variable origin.In this case, those will be
the three values Asia, Europe,and USA, just like before.And then we have as
a simple DATA stephere that creates our
vertical macro variable list.Again we have a SET
statement, with the ENDequals option in effect.Then we have a CALL SYMPUTX
This first CALL SYMPUTXwill execute every single
time through the DATA step.For every record that we
read from unique originswe'll write out
one macro variable.So you'll notice
the first argumentto CALL SYMPUTX is actually
a call to the CATS function.CATS also does
string concatenation,but unlike CATX
it doesn't insertany delimiters, which is fine
because we don't need thathere.All we want to do is
take the word originand append the numeric
value representingthe current record.So _n_ will be 1 the
first time to loop,so what CATS will return is
simply the string origin1.So we'll create a
macro variable calledorigin1-- that's what
we're building here,is the name of the
macro variable.Then the second
argument to CALL SYMPUTXis the value that we want to
put into that macro variable.In this case, it's the value
of the DATA step variableorigin, which the first
time through contains Asia.So origin1 will get Asia.If eof-- that's not true.It's not true the
first time to the loop,so we go back around and
read a second record.Second record contains
the origin Europe.We CALL SYMPUTX again.This time the very the macro
variable name that we create isorigin2, because _n_
has a value of 2.And we create a macro
variable called origin2,and into that macro
variable we place the valueof the variable origin.It's Europe, so origin2
has Europe in it.eof is still not
true, so we go backfor the third and final time.We create the macro variable
origin3 and put USA in there.Now eof is true, so we execute
this final CALL SYMPUTX and wecreate the macro
variable numorigins,and we place into it a 3, which
is the current value of _n_.So at the end of
this, just like wedid when we created the
vertical macro variablelist before using
PROC SQL, we willhave four macro variables--origin1, origin2, and origin3
containing those three values,and the numorigins
containing a 3.So for our sixth
exercise we're goingto create a vertical macro
variable list using the DATAstep with the names of the
students from sashelp.class.And again, the student
names are unique so wedon't need to worry about
using PROC SORT to removethe duplicates first.We'll just go straight
to our DATA step.Here's our SET statement.We need to add two
CALL SYMPUTX statementsto create the macro variables.The first CALL SYMPUTX is
going to create the student1through studentn
macro variables,and then the second one
will only execute onceand create num_students.So let's go to our SAS
session and we will workon this exercise, number 6.OK, so we need to CALL SYMPUTX.And I want to create
a macro variable,and I want my macro variable
called student and thena number.So I'm going to concatenate the
word student and then I'm goingto use the convenient automatic
variable _n_ to get the number.And remember, the CAT
functions will automaticallyconvert numeric values
to character valuesusing the Best12 format.And CATS will also
automatically stripleading and trailing blanks
from all of its arguments.If you don't want those stripped
you use plain old CAT function.But here we're using CATS, so
it strips trailing and leadingblanks from _n_.We end up which is student1
with no spaces in it,so that's the name of
our macro variable.And then what do I
want to put in there?Well, I just want to put
the name of the student.OK, so that's my
first CALL SYMPUTX.Now when I get to the
end of the data set--I've read in the last
record from sashelp.class--I'm going to CALL
SYMPUTX one more time.This time I'm going to
create a macro variablecalled num_students.And into that macro variable
I'm going to place _n_,which is going to be 3 in this
case because there are goingto be--I'm sorry, not 3, 19.I'm still thinking of
the vehicle origins.19 students-- so _n_
is going to be 19.That will get put
into num_students.Let's run the SAS code.And we have some PUT
statements down hereto put things to the log so
we can see what happened.Here's our log, and we
can verify that yes, wegot num_students as 19.And we have variables-- student1
has Alfred, student2 has Alice,and so on just like
before with PROC SQL.So we have two
different ways to createmacros variable lists, both
horizontal and vertical,using either the DATA
step or using PROC SQL.So we've been talking a
lot about how to createmacros variable lists,
but you've probablybeen wondering all this time--what are we actually going
to do with these things?Why are these useful,
and why should Icare about how to create them?Well, now we're going to talk
about some simple applicationsof macro variable lists,
although the potentialfor these things goes
far beyond what we'regoing to talk about today.I find these things
extremely useful,and I use them almost every
day in my regular work.So first, let's talk
about how we actuallyuse a horizontal
macro variable list.How do we actually access
the individual elements?Well there are several ways,
but what I typically do is usethe %SCAN macro function.The %SCAN macro function lets
us parse the list and pluck offa specific element based on
knowing which number of elementwe want.So the first argument of
the %SCAN function is simplya string that you want to parse.In this case it's going to be
our horizontal macro variablelist, which is all stored
in one macro variable.Let's suppose it's
called origin_list.So here we have
ampersand origin_list,which is a reference
to that macro variable.The second argument to %SCAN is
the number of the word that youwish to return.Do you want the first word,
the second word, et cetera.And then the third
argument is the delimiterthat it's supposed to use.So obviously this
is going to needto match the delimiter
that you usedwhen you built the
horizontal macro variablelist in the first place.So for example, if I call
%SCAN &origin_list comma 1--I want the first word-- comma
tilde, the first word whenyou use tildes us
to define words,well origin_list has Asia,
tilde, Europe, tilde, USA.So when you use the
tilde as a delimiter,then the first
word is just Asia.So when the macro processor
sees %SCAN &origin_list 1 tilde,it's going to resolve
that to simply Asia.When it sees %SCAN
&origin_list 2 tilde,it's going to resolve
that to Europe.That's going to happen
during macro processing,meaning that all that
SAS is ever going to seeis just the word Asia, or the
word Europe, or the word USA.Typically the way we're
going to use these elementsis inside of a macro DO loop.So you see here we
have %DO i equals--i is a another macro
variable that'sbeing created to use
as a loop counter--i equals 1 to numorigins.numorigins is the
macro variable that wecreated that contains the
number of items in our list--in this case, 3.Remember, we created
that earlier sothat we would know how
many items were in the listand we wouldn't have to
go and count it later.So now we're going to use that.i goes from 1 to numorigins,
and inside our %DO macro loop wehave a %PUT.So we're just writing
some stuff to the logsso we can see how this works.%PUT item &i-- that's
our loop counter here--1, 2, 3.And then we have a %SCAN, but
you'll notice that the secondargument to %SCAN is not a hard
coded number like it was upabove.Rather it's a reference
to the macro variablei, which is our loop counter--&i.So we're going to take the
i'th item from the origin_list,which was the macro
variable list.So the first time
through the loopit's the first item, the
second time through the loopit's the second item, and so on.And what we end up with
is this in our log--item 1 Asia, item 2
Europe, item 3 USA.So we built this macro variable
list dynamically from the datawithout hard coding in any
of these values to our data.And now we've read
them back out.So for exercise
number 7 we're goingto do something with a
horizontal macro variable list.Well, first we're going
to create the list,and then we're going to use it.So the code here I've got
to split across two slides.We're going to use a new
data set we haven't playedwith yet, sashelp.stocks.And our task is to
create a separate plotin a separate file for
each stock in the data set.And we're of course
we're going to do thatwithout directly referring
to any specific values.So because we're going
to use a macro DO loop,we have to do this inside
of a macro definition.You can't do a %DO
out in open code.We need it inside of
a macro definition.So first we would
define a macro.This one's called graph stocks.And then we have a SQL step--and this part is given to
you as part of the exercise.We have a SQL step that creates
the horizontal macro variablelist by taking the distinct
values of the variable stock--and that has values like
IBM, Intel, and Microsoft.It's the name of a company.It takes the distinct
values of stock,puts them into a
macro variable calledstock_list separated by tilde.So that's our horizontal
macro variable list.And then we also
create num_stocks,which gives us the number
of items in the list.Now, your task for exercise
7 is to fix up this code hereto replace the hardcoded
values where it says IBM usingthe %SCAN function to get the
current stock from the stocklist macro variable.What we want this to do is
we want this DO loop to plotone time for every value of a
stock that was in our data set,but without us having
to specificallyrefer to those values, right?I don't want to tell SAS
first plot the IBM stock,then plot the Intel stock,
then plot the Microsoft stock,because what if later,
I get a new data setand it has a
different stock in it?I'd have to change my code,
and I don't want to do that.I want to write dynamic code
that's driven by the data.I have a DO loop here--%DO-- and inside my DO loop
you'll notice I have a PROCSGPLOT.We're not going to spend time
talking about how SGPLOT works,but that's what
creates the plot.And it's wrapped in ODS
PDF file and ODS PDF close.That opens and
closes a PDF file.So we're going to
create a separate PDFfile for each value of stock.That's why we can't simply
use a BY statement in our PROCSGPLOT.And remember that the
macro language actuallygenerates SAS code.So what's going on here behind
the scenes inside this %DO loopis all these statements--the ODS statements
and the PROC SGPLOT--are being generated once
for every value of stockfrom our data set.So let's switch over to SAS and
see if we can get this working.OK, so I need to replace the
hardcoded values of IBM withthe %SCAN function.So I'm going to call the file
by the name of the stock .PDF.So %SCAN-- now, the first
argument of %SCAN is the thingthat I want to parse.Well, that's this macro variable
that I just created called&stock_list.I have to make sure to put
an ampersand in front of itbecause I'm referring
to a macro variable.That was created up here
in the PROC SQL step.The next thing is which item
do I want from stock list?Well, I'm in a loop.And on the i'th time through
the loop I want item i.And what's my delimiter?Tilde.So let's close the
%SCAN function, .PDF.So when the macro
processor process is this,it will replace everything
from the percent signthrough the close parentheses--it will resolve that to IBM, or
Intel, or Microsoft dependingon which time through the loop.And I'm also going to copy
that little bit of code.I'm going to put it
right here as well.I can get rid of these
comments now, and this one too.OK, so now I have a DO loop.So each time I go
through the loopI'm going to open a file with
the name of the value of stockthat I pulled from my
macro variable list--that's the name
of the file .PDF.Then I do PROC SGPLOT
where stock is equal to--and again this is going to be
IBM, or Intel, or Microsoft.These values are going
to come from the data setsashelp.stocks.You'll notice in
this code now, notonce do I refer to a
specific value from the data.This is all driven by the data.Let's run this code.Here's the end of
the macro definition,and this actually
calls the macro.So we run this code.And yep, we get three
different plots.And they went to three
different PDF files.So you can see we have
IBM.PDF, we have Microsoft.PDF,and we have Intel.PDF.So we got three separate
PDF files, and we can look.Here's our SAS log.You'll notice we called PROC
SGPLOT 1, 2, 3 times writingto a different file.So Microsoft.PDF,
Intel.PDF, and so on.And if we were to look
out in our file explorerwe would see those
three PDF files.But the amazing thing about
this is if the data set changesand we add more companies
into the stocks data set,we don't have to
change the code.We just run it
again and we wouldget a separate PDF for
whatever companies happenedto be in that data set.That's what makes
this so powerful.Now let's talk about using
vertical macro variable lists.This is where the
syntax gets to bea little bit more challenging.If I want to access the
macro variable origin1--that's the first
item in my list--I just say &origin1, and that's
going to resolve to Asia.If I say &origin2 it resolves
to, Europe and &origin3resolves to USA.Now the trick is, how am I
going to use this in a loop?Because I can't say &origin&i.If I were to say that,
the macro processorwill interpret that as two
macro variable references.It will think that I'm referring
to a macro variable calledorigin, and then referring
to a macro variable called i.Well, there is no macro
variable called origin.I have macro variables
called &origin1, &origin2,&and origin3, but
not just origin.So this is going to fail.The way that we get around
this is with this syntax--a double ampersand.&&origin&i.Now, why does that work?Well, here's the way it works.When the macro processor parses,
this the first token that itsees is &&.&& always resolves to &.You just have to remember that.So the macro processor
resolves two ampersandsto one ampersand.After that it sees
the word origin--that's not a macro
keyword, that'sjust text as far as the
macro processor is concerned.So it ignores it
and passes it along.OK, so the two ampersand
became an ampersand,origin stays as origin.Now we have &i.That is a reference to the
macro variable i, and--I'm assuming we're
in a DO loop hereand i is our loop counter--the first time through the
loop, &i resolves to 1.So &&origin&i
resolves to &origin1.Remember at the
beginning of this talkwe talked about when the
macro processor finishes,its re-scans whatever it
produced to see if there aremore macro language elements.So this gets re-scanned and
there's still an ampersand.This is now interpreted
as another macro variablereference.So on the second
pass, &origin1--that's a macro
variable referenceto a macro variable
called origin1,and that resolves to Asia.So it takes two passes, but
&&origin&i resolves to Asiawhen i is 1.When i is 2, the second
time through our loop,then &&origin&i
resolves to &origin2,which resolves to Europe.So we can use it in
a DO loop like this--%DO i equals 1 TO &numorigins--that's the number of items
in the list, in this case 3.%PUT Item &i, and now
we have this &&origin&i.And this will run three times,
and the first time through thiswill resolve to Asia, the second
time Europe, and the third timeUSA.So for exercise 8 we're going
to use a vertical macro variablelist in a macro DO
loop, and this timewe're going to take the
data set sashelp.carsand we're going to split
it into separate datasets for each unique
value of origin.But of course, we're not going
to hard code those values.It's going to be
data driven, that'swhy we're doing all this.So here's the code we're given.Again, we have a macro
because we're doing %DO.We need it to be inside
a macro definition.This is our macro
definition-- split_datais the name of our macro.Here's the PROC SQL step that
creates the vertical macrovariable list.It uses the sashelp.cars
data set and putsthe distinct values
of the variable origininto a series of
macro variables--origin1, origin2, origin3.And of course, we also
create num_origins,which will have a 3 in it--the number of observations that
were returned by this query.That's the first page.We continue the
macro definition.Our task is to replace these
hardcoded values that say Asiawith a reference to our
vertical macro variable listso that this loop will work
and create a separate data setfor each origin.Let's go over to our SAS session
here and do exercise number 8.Here's our macro DO loop.We got the PROC SQL step that
creates the vertical macrovariable list.DO i equals 1 to
num_origins DATA--so we're going to
build a DATA step.Add data cars_Asia--
well actually,we don't want to call
the data set cars_Asia.We want to call it cars_
and then we want to referto the i'th item from
our macro variable list,which is going to be
origin1, origin2, origin3.So &&origin&i.cars_&&origin&i-- will resolve
the cars underscore Asiathe first time to the loop--where origin equals &&origin&i.All right, so now
we should have ado loop that will build this
DATA step, so when i is 1we'll have data cars_asia,
set sashelp.carswhere origin is Asia.When i is 2 it'll
be cars_europe,when i is 3 it'll be cars_usa.Let's run this.OK, and we see we created
cars_asia, cars_europe,and cars_usa, and if we go
over here and look at our datasets we have those three data
sets cars_asia, cars_europeand cars_usa.But we never once
in our code referredto any of those specific values.So again, if the data
set changes later,we just rerun the
code and we'll getone data set for each value
of that variable origin.Just for fun let's turn
on the imprint optionand run this again.So I want to show
you the actual SAScode that gets generated,
because remember the macrolanguage is just a typewriter
that generates SAS code for us.So let's run this part of the
code again and go to the log.And now we can
actually see, here'sthe DATA step that
was generated.DATA cars_asia set sashelp.cars
where origin equals Asia.So just the records
where origin is Asiawere put into this data set.Then we have down
here DATA cars_europe,set sashelp.cars where
origin equals Europe.And then we have one for USA.So basically we use the macro
language to generate this DATAstep repeatedly for every
single value of origin,regardless of how many values
of origin they happen to be,or what they are.To wrap up what we've
talked about here,I think hopefully you've seen
by now that macro variable listscan be extremely powerful.They help us make code that
is dynamic and data driven--code that we don't have to
constantly change and updateevery time the data changes.I don't know about
you, but I don'tlike to have to keep
changing my code.I like to write my code
once and do my future selfa favor by writing code that
won't require future changesif I can avoid it.I like to avoid hard
coding, because hard codingmakes my code more
difficult to reuseand more difficult to maintain.And I like to have
code that adaptsto changes in the data or
changes in my computingenvironment.And that's what macro
variable lists allow us to do.Hopefully you've found
something useful herethat you can use in your work.If you would like,
there is a paperthat goes along with
this hands-on workshop,and it's part of the SAS
Global Forum 2020 Conferenceproceedings.And you can find that
at sasglobalforum.com.And there are also
other papers out thereon the internet-- you might
check lexjansen.com, whichis an excellent resource
for SAS programmersbecause there are
many, many user grouppapers from over
the years out thereand you could find
other informationabout macro variable lists.You can also refer to the
book I mentioned earlier,which is by Art Carpenter--Carpenter's Complete Guide
to the Macro Language,Third Edition.He has extensive treatment
of macro variable listsin his book.And you can feel free to contact
me at this email address shownon your screen.I'd be happy to answer questions
or interact with you furtherabout this topic.Sorry we weren't able
to meet personallyat SAS Global Forum
this year, but Ido hope I will get a
chance to meet with youat a future conference.Thanks."
107,"GREG HORNE: OK, thank
you for joining usfor this presentation today.We're going to be talking
about population healthand giving a
demonstration into that.The first thing we
want to talk about,though, is just
give a little bitof example about
how we can improvepopulation health through
clinical episode analysis.And I think a good way to
start is with an example story,and I'm going to use
something from my own family.I had a family member last
year who unfortunatelywas running and had an accident
and tore cruciate ligamentin her knee.And we were able to get her into
her primary care physician veryearly the next day,
who looked and examinedand clinically diagnosed
a torn cruciate ligament,and then proceeded
to do an ultrasoundbecause a radiology image
wouldn't have been appropriateat that time, and diagnosed,
yes, indeed, therewas a torn cruciate.And it needed surgery to rectify
that as soon as possible.So we booked the surgery
for the very next day.And that was done on time
and on the correct leg, whichisn't always a given
in health care today.And she was recovered
perfectly welland sent home with
good instructions.And data shows us a
follow-up to surgeryis really important in
driving great health outcomes.And she was given physiotherapy,
follow-up clinics,and within a couple of
weeks of the operation,she was back to her
usual self, runningaround and in great shape.And that's a great story
of a health outcome.And it's exactly what we want
for all of our populationwhen they have a need
for care in that way.But the kind of crunch of this
is, or the issue with thisis, the family member
that was affected by this.And let me just
show you who it was.It was my little dog Abby.And she has recovered very well
and is still running aroundand enjoying life.But the reason I
like this story isbecause it's exactly what
health care should look like.And it was really easy to
deliver that kind of carefor a dog because there is
very limited episodes and verylimited interaction
with her as a patient.When we think about health care
for us and for our families,we have a much more
complex relationshipwith our providers,
with our payers,and with the various
processes thathave to occur through
our treatment processes.And what we're to show you today
is just helping to understandthose episodes of care and
how we drive the same outcomesfor all of us.And there's nothing
new about this.This picture, this is
Florence Nightingale,credited with being the founder
of nursing back in the 1800s.And she was also a
PhD statistician.And what she did was
she was able to showthat if you looked at the data
of why soldiers were becomingmore critically ill and dying
after being exposed to healthcare, you could create
visualizations that would justshow exactly what the cause of
death was, where it occurred,and why it occurred.And those visualizations were
presented to the leadershipof the military at the time.And she was able to show
that with better nutrition,sanitation, and overall
general hygiene,you could reduce your
mortality rate significantly.So we appreciate the idea
that the use of analyticsto drive better patient
outcomes in a populationis really nothing
new and somethingthat we could do much more
with in our current time.So why is it important to do
health analytics like thisand to really drive
patient outcomes?Well, I think we really
want to understandwhich are the populations that
are most likely to become sickand how they're going to trend.We really want to understand
where our social determinantdata fits in as well, and we're
seeing more cases now wheresocial data has become a
driver in making changes.We want to predict
negative outcomes,and we want to start to
employ strategies and changemanagement so that we can drive
a higher safety and a betteroutcome health care cost.And this is the process that
we go through to get there.We integrate our data
with our accelerators.We think about out-of-the-box
capabilities that bring rapiddeployment and great
times of value.And then we have our
end-to-end analytics.We want to visualize,
deploy, and thenstart again with a
very circular, kindof iterative process that allows
us to continuously improveour health care system.So with that, I'm
going to pass acrossto Jeremy, who will take
it forward from here.JEREMY RACINE: Thank you, Greg.So I'll just do a
quick introduction.This is Jeremy Racine.I'm a global health care
artificial intelligenceprincipal working in the SAS
Health Analytics Practice.So Greg talked a little bit
about population health,outcomes analysis,
with a specific focuson clinical episodes of care.And I think it's a great segue
into the demonstration portion.So we're going to
start off reallyat 100,000 feet, a
true population view.And using a variety of different
data sources and analytics,we've pulled together
this population healthvisualization.And we've also run this data
through our clinical episodesof care.And so as Greg mentioned,
getting a comprehensive viewof care is an essential
element of this processfor population health.And as I mentioned, so we're
running all of our datathrough these clinical
episodes of care.There are a variety of different
clinical rules, definitionsthat are used to define
these episodes of care.And by doing so, we get this
nice comprehensive view of carethat occurs before,
during, and after.Think about the,
perhaps not Greg'sdog, but perhaps yourself
or a family member,who might have a
orthopedic surgery.And there is all the
care that happensbefore the surgical event
itself and everythingthat happens after.All of that is a
clinical episode of care.Now, that care is also split
in two different type--a couple of pieces.All right, there's
the typical care,which is the care that
you should receive.And then there are a set
of complications, hopefullythings that you do not receive.These are what we call
potentially avoidablecomplications.We use those as
measures of quality.In addition to that,
we also do a varietyof different risk adjustments,
risk calculations,so that we can stratify
our population by risk.And with that, we
can take a lookat this particular
visualization right now.And we'll begin by
looking at risk.So at the top, you can see that
we've stratified our populationacross the state
of New York here,by various levels of risk.So we're looking at
critical, average, low risk.Again, those are created based
on out-of-the-box analyticsthat run behind the scenes in
that episode analysis process.And as we click on
these, we'll seethat we get some
interactions with the below.Now, in addition to just
looking at critical risk,because that's one of the
areas we want to focus in on,we want to look at where are
the highest risk patients,where can we potentially look
at areas for intervention?These are the areas
that we like to focuson the most, the
critical risk population.We've got some other
ways in which we canfilter down a little bit more.So, of course, in the states
we have different insurancepayers, and so we
can filter downon different insurance payers.You see we have ones
that are familiar to mostfolks, Medicare, Medicaid,
and our commercial insuredpopulation, which is the
majority of the population.Let's go ahead and
click on commercial.Now, we have some options,
of course, for age bracketsas well as race.We're just going to keep focused
on the commercial for a second.I want to draw attention
down to the bottom.Now, you'd seen while
we were clickingon the filters in the top that
the map on the bottom changed.And we've got a couple
of different things.So we're looking at,
on the left-hand side,you're looking at a
color gradient, whichrepresents our risk
of our population,where the stronger red
colors are higher riskparts of the population.And on the right-hand
side, we'relooking at more of
a cost perspective.On the right-hand
side, we're lookingat what we call an
average to expected cost.The expected cost is that
analytically driven costversus what the average cost is.Where we see areas
of red, it meansthat the costs are
actually higherthan what the analytics
said they should have been.Now, when you look left
and right, you can see,if we scroll over
some of these, thereare a couple areas where
risk correlates to cost.One of those areas
we'll just focus onis Niagara County right here.And the reason we
mentioned this isbecause just because a location
has a high risk populationdoesn't necessarily mean that
the costs are unavoidable.Some places just have
high-risk populations,and they happen to be
providing the care thatis necessary for that
population, whichmeans it is not necessarily
what we call potentiallyavoidable complication.But we'll focus on
Niagara because Niagarahas both a high-risk population
and, on the right, as youcan see, has a higher than
normal average to expected costratio.Now, you can see that a variety
of different scores in here,things like outcome
scores, preventive scores.These are all types
of things thatfunnel all the way to the top
for the risk stratificationprocess that we've done.And when you click
on this, we canget some additional
information very quickly.So you see, we pulled
the onion back.We just drove right down
into Niagara County,and now we're down
to a ZIP-code level.And you'll see, we've got a
variety of different filtershere.We can learn some more
about that population thatmight be driving those high
costs, those poor outcomesin this particular
county, using analytics.We can look at hospitals,
long-term care facilities,and outpatient facilities.I'm going to focus on
hospitals because theytend to be the highest costs.And you'll see on the bottom,
we've got some different colorshere, OK?So we've got a
couple of ZIP codes,and I'll explain
that in a second.The color represents
those complicationsthat we talked about.So the stronger blue gradients
represent higher amountsof complications.The bubbles represent
complications as well,but the color of the
bubble represents the cost.OK?So we're looking at deeper
blue-colored ZIP codes, whichis one of the ones we can
see right here, this 14094,with also large red bubbles.This would indicate, the
analytics are indicating,that we have a tremendous
amount of complicationsand significantly higher costs.OK.So just focusing in, and
we'll click on this oneparticular hospital
right here, you'llsee on the right-hand
side, we've gotsome additional information.We've got a readmission rate,
which is astronomically high,for this particular hospital.And we have that
expected to average costfor inpatient stays.There is a
significant variation,as can be seen there,
nearly $20,000 difference.There's something
going on, of course,at this hospital we need to
learn a little bit more about.But you also see, while
we're looking at this,two other ZIP codes.Why do we put the other
ZIP codes in here?Because you may have a
disproportionate numberof folks who are going to this
facility experiencing thesecomplications and
these high costs,but were they aware that
maybe within 15-, 20-,30-minute drive, there are
two other facilities in theseneighboring ZIP codes,
which, as you can see,have low number of complications
and significantly better costs.So you can see almost
spot-on right here.So there are alternatives.Again, we're getting a blueprint
for potential change here.Let's focus back on Niagara
Hospital here, Eastern NiagaraHospital, and learn
a little bit moreabout what's going on there.We can click on
this, and you'll seethat we get some information
about the episodes of carethat are occurring.Now, episodes of
care can be rolled upto what we call
condition classes.So they're spread out
into chronic, procedural,and acute conditions.The larger boxes here
represent higher costs.The stronger color
green representshigher complications.Chronic, which is
both the largestso the highest cost, total
cost, and the strongestgreen gradient.It's also the highest
in complications.Let's drill down into that
and take a little bit deeperlook at the conditions that
are driving those costs.Now, when we go
ahead and drill in,you'll see that
one of these boxespops out almost immediately.And that would be CHF,
congestive heart failure.Tremendous amount
of complicationsas compared to the
other conditions, 50%,and tremendous amount of costs.Now, when we select CHF, I
will say to focus over hereon the right-hand side.And you'll see,
we have on the topa trend line representing the
actual costs, the green linerepresenting the risk-adjusted
costs created by the analytics,and on the bottom, the
complications costs.And what we can see is that
they've trended up over time.And their projections,
which are automaticallycreated here from the analytics,
show a projected upward trend.Let's learn a little bit
more about the complicationsthat are driving this.Again, as a part
of the system, weidentify those complications.They help in terms of creating
a blueprint for change.You can see by taking a look
at the top complications,with the CHF cohort, at
this one hospital, that'ssepsis, pneumonia,
acute kidney failure.These are some very
serious complications,and they are the top
complications here.And we can see
how they shake outon the left at this hospital
versus the right, whichis the state average.So clearly, having
used the system,we're starting to
develop some verygranular details about what
might be driving these costs.We've got a particularly
problematic cohortat this hospital, complications
such as sepsis, pneumonia,kidney failure.Let's drive maybe
one layer deeper.And we can do that here.We'll just go over to what
we call provider cost.Let's take a look at
the providers thatare providing these services.And we're going to look
here at an intersectionof using not just
claims or clinical databut using hospital operations
data to better understandwhat has been going on.And so I've got
my providers here.These are some of my higher
cost, higher complicationproviders.And you can see one
of them just pops out,high cost represented
by the size of the box,stronger complication
represented by the green.And you can see a tremendous
amount of complications.There's something going on here.Let's draw our
attention to the bottom.Potentially avoidable costs
for this particular providerhave been trending higher.But let's look at
some operations dataand see how the
combination of these thingscan be incredibly compelling
when we look at waysin which we can intervene.Patients per hour, these
don't come out of claims.They don't come out
of clinical data.They come out of a
hospital operation system.And we can see that the patient
volume started relativelystandard, around two
patients per hourfor this particular
physician, but over time hasincreased to three.Now, remember, in
that same timeframe,complications have gone up.Sepsis has become
more of a problem.Let's take a look at this with
even a more granular lens.On the bottom, these bars
represent sepsis mortalityrate.On the top, we have the
average physician response timeto a sepsis bundle.Collectively, we can see
is, over the same timeframethat the operations
data told us that therewas a increase in
patient volume,we can see that sepsis
mortality has more than doubledon the bottom, and the
response time to sepsishas gone up nearly an hour.We're getting some very
concrete information nowabout what might be going on.So I'll pause there, for one
of the questions we want to askis, what could we do if we
applied some advanced analyticsto this problem?Could we potentially create a
new intervention, especiallyin regards to maybe identifying
potentially septic patientsat an earlier stage
in the care process?Now, to talk a little bit
more about those analyticsand how we could
create them, I'mgoing to pass over to Hiwot.HIWOT TESFAYE:
Thank you, Jeremy.My name is Hiwot Tesfaye
and I'm a data scientistin SAS's Health Care
Industry Solutions Group.What I'll be showing you
over the next few minutesis how we can use
predictive modelingas part of a clinical
intervention solution.What Jeremy showed us over
the last couple of minuteswas how we can use population
health dashboards in orderto identify potentially
avoidable complications thatare not only driving up
costs, but most importantlyare leading to poor
health outcomes.So once we're able to identify
that our congestive heartfailure patients had a leading
complication of sepsis,we can start thinking about
what kinds of advanced analyticstechniques we can potentially
use in collaborationwith other process
improvement measuresto help mitigate these issues.So in order to identify, first,
our patient cohort of interestthat we can use as part of
our model-training process,we actually turn to
SAS Cohort Builder,which is part of the
SAS health offering.Cohort Builder is
designed to takein a variety of
health care data,spanning from demographics,
data, labs, prescriptions,procedures, and
diagnoses, and much more.And we can use a GUI
interface to quickly identifypatients that have
a history of CHFand then start identifying
our target variable, whichwill be sepsis, in a
time-dependent manner.Once we have our data set
built in Cohort Builder,we can bring it in to
Model Studio, wherewe can train various
machine learning algorithms,or we can put it into SAS
Studio, where we can furtheraugment the data before we
pull it into a machine learningalgorithm.So for this
demonstration, I'm justgoing to go through Model
Studio and demonstratehow an interface like
this can dramaticallyincrease the efficiencies
of a data science teamthat might be working in
a health care setting.So now that I know
my data is ready,I can navigate over
to the Pipelines tab.You'll notice that on
the left-hand side,you have various options
available to you in orderto build out a pipeline like
you see on the right-hand side.So we have Data Mining
Preprocessing optionsthat allows us to manipulate our
variables before we feed theminto any kind of model.In the bottom, we have
Supervised Machine Learningalgorithms available
to us, anythingfrom a simple decision tree
all the way to neural networks.For Post-processing, we
have the Ensemble optionso we can ensemble
our models together.And then under
Miscellaneous, youhave various options to
further explore your data.It'll automatically build
out charts of cardinality,missing variables,
distributions,various other things for you.You have an open source
code, which is pretty nifty,which allows us to pull
in Python or our codewithin our pipeline flow.We also have SAS code and
several other options.So using these objects
on the left-hand side,we were able to build out a
pipeline like this that alwaysstarts off with the data.The data is then fed through
various data manipulationoptions, in this
case imputation.And then we're going to pull
in some open source codebefore pulling it into
a predictive model.So the nodes in purple
are all supervised machinelearning models.There's around seven of
them, almost all of whichare all SAS models, except
for this Python model.So I'm going to open this
up and just illustrateto you that in the Code
Editor, we're pullingin the scikit-learn package.And eventually, we are running
a Random Forest classifiermodel from scikit-learn.So this is just an
illustration of howwe can pull in
open source modelsand compare them
alongside SAS models.In the next and
final tab, I wantto illustrate one of
my favorite featuresthat has been included in
this release of SAS, whichis AutoML.So all I have to do is
provide SAS my data setand tell it how long to
run the AutoML generation,and it creates its
entire pipeline for me.So all of this is editable.And I can change
things as needed,try out additional models that
I didn't consider, and basicallyjust look under the hood and
see what is being created here.So just note that for every
node on the right-hand side,there's a properties panel
that allows me to change hyperparameters of each of my
models and other options that Imay have available to me.I can also perform
auto tuning, whichis an optimization
technique thatallows me to select the
best type of parametersfor a particular model for the
data set that I've provided.And then finally, in the
Pipeline Comparison tab,what we're doing
really is lookingacross all the different
pipelines that we've createdand trying to find the champion
model for the validationdata set across various
fixed statistics.So in this case, we're looking
at misclassification rate, AUCand KS statistic.It says we can see the
gradient boosting, too,from the AutoML pipeline had
the lowest misclassificationrate, one of the highest AUCs,
and the highest KS statistic,and it's been selected
as the champion model.When we're building something
like a sepsis model thatwill be used in a
clinical setting,and the end subjects that are
going to be affected by thisare real-people patients,
it's really importantto keep in mind that
interpretabilityof these models and
accuracy of these modelsis going to be of
utmost importance.So here we can see the
variable importance list.So we have diastolic
blood pressure,and the age of the patient,
a bunch of other variablesthat are feeding this
Gradient Boosting modelthat we've identified
as a champion.However, we would want to
know how these variables areimpacting the score that's
going to be given to eachof those patients that
the model is labelingas high risk for sepsis.And one way to do this
is to look for the modelinterpretability charts that
are available within SAS.So there's several that
we have available to us.When we go into the Model
Interpretability tab,we can see that we have variable
importance list, which we justsaw, as well as
partial dependenceplots, ICE plots, line plots,
and Kernal SHAP values.That allows us to understand
how each variable is affectingthe overall model
as well as how itmight be affecting an individual
observation or patient.To get a better understanding
of what each of these plotsare trying to explain, when
you click on this button,there's some natural language
generation going on hereto explain what
each of these plotsmean and makes it relevant to
the values and variables thatare actually present in this
plot, specific to your project.So now that I've
identified a modelthat I feel like has a
pretty impressive accuracyon my validation data set, the
next thing I might want to dois register this model
into Model Manager, whichallows me to track
its accuracy over timeand make sure that its
accuracy is not degrading,and if it does, to
label it for retraining.And the other option is
to publish the model.So once I publish
it, the API endpointswill be made available for
third-party applications, webapp mobile apps to make
calls to this model.Or I can publish
it and visualize itin a dashboard similar to what
we were seeing in Jeremy'ssection of the presentation.So from here, I'm going
to hand it over to Jeremyto talk a little bit
about what we mightdo with the output of
this model that labelsour patients as
at-risk or not-at-riskfor going into septic shock.JEREMY RACINE: Right.Well, thank you, Hiwot.And as Hiwot was working through
a lot of the advanced analyticsthat go on behind the
scenes, a lot of thingsthat we had talked
about, I wantedto take an opportunity to look
at what things would be likeif we were to operationalize
those analytics.We saw some of that
stuff, of course, upfront.But what Hiwot has
done is specificallyworked on a model
that would helpto predict the
likelihood of somebodypotentially being
septic in our hospital.And so if we wanted
to take that model,operationalize it, put it
in there today or tomorrow,we want to take a quick look
at what that would be like.And so, so we've got
a visualization here.And as you can see, we've
got a risk stratificationfilter at the top again.And so, again, taking
the results of that modelthat we've
operationalized, now wecan stratify our population into
being a high risk, medium risk,or low risk.So let's take a look across
our entire patient censusand see how many folks,
based on the new modelthat Hiwot created, are
in this high-risk group.We see that there
are 29 patientsin a variety of different
demographic variablesand other impactful
variables that you recallseeing in the modeling process.Those are all below.Let's take a look at those
actual patients, too.And what we'll see is the
model went a little bit furtherthan that as well.So we could take the
model and not onlylook at those that
are high risk,but also, in this
case here, I'vegot almost another
level of criticality,who are the most
highest priority basedon a variety of
different factors.And again, these all
types of things that wecan configure inside of
the modeling environment.But you can see
highlighted in redare our most critical patients.We have access to
all this information.Again, all of this is
coming in from clinical datasources but also
things that couldbe coming in from devices.We have a lot of
interoperabilitywith medical systems in general.So we'll hop out
of there, and whatwe'll do is take a look
at the last piece, whichis, we've been talking a
lot about interventions,and if we wanted to come in here
and take a look at how we couldplay a bit of a ""what if""
game with interventions,we provide our users
with that capability.And so as you can
see here, we'vegot a variety of
different interventionson the left, different
types of outcomesand how they're
impacted on the right,including costs at the top.You can see we already have
selected a few of them,but they're very simple.We just come in here and we
select which interventionwe'd like to employ.And based on a variety of
analytic models runningbehind the scenes,
again, we willcalculate the potential cost
savings as well as the impactto those various outcomes,
such as sepsis mortalityrate, average length
of stay in the ICU,readmission rates, sepsis rate,
and that physician responsetime to getting the
sepsis bundle in--bundle order in.So you get a quick
kind of overview of howwe can take those advanced
analytics that Hiwot wasworking on and how we
operationalize themvery quickly so that folks can
understand what interventionsthey might want to take and
how they can improve outcomesand potentially
better manage costs.And with that, we'll pass it
back to Hiwot for a quick wrapup.We thank you for your time.HIWOT TESFAYE:
Thank you, Jeremy.So to recap, Jeremy
showed us at the beginningof the presentation how we
could use SAS Health to organizeour health care data into
clinically relevant episodesof care.Looking at our data in this
manner, through dashboards,we can uncover opportunities
for improving health outcomesand reducing costs, with a
specific focus on potentiallyavoidable complications.We're able to understand the
demographic and social factorsthat drive outcomes as well.Because we have AI and
machine learning capabilitiesas part of this
offering, we're alsoable to train machine learning
models on our patient data setsand consider a data-driven
solution as partof our intervention process.We're also able to monitor
these models over timeand ensure that the most
accurate and reputable model isbeing included as
part of the solution.Thank you all for
your time today."
108,"JORDAN HUMES: Hello.It's so very exciting to
participate and be a partof the 2020 SAS Global Forum.I, along with my colleagues,
are happy to presentour project centered
around the reductionof ecological footprint through
the examination of panel data.We will go much deeper
into that shortly,but first, introductions--we are the SASsy Coders.Neil Belford and
Bruce Rehburg, theyare more the coder
part of our group,whereas Manasi Murde and
myself, Jordan Humes,make up the SASsy
part of the group.I will start us off providing
some background and context,what exactly we're trying
to solve for, then lead usinto approach, where my
colleague Neil will talkabout the preparation and
challenges of the data,go into analysis, model
selection, then results--at which point he'll hand back
to me to provide the insightsand recommendations.A few definitions that
will come in handyduring this presentation--ecological footprint accounting
simply measures the demand onand supply of nature.Ecological footprint
is the demand part.It is the measure of a land--of the land needed, along with
agricultural space, absorptionof carbon dioxide emissions,
and need for infrastructureto support that
population, whichis represented as the
bottom row of this slide.But capacity is the
support supply part.It is, in short, the
measure of a land's abilityto provide for and absorb waste
from a nation or population,which is the top row.And now, to take you back to
seventh grade social studies,I will define GDP in the
context of our analysis, whichis as a measure for
economic health and activityof a country.We'll be using these
definitions in measuring--in helping understand
the correlationbetween the development
of a countryand our ecological footprint.Ecological impact is
a little abstract.It may feel far removed.To make it more
relatable, insteadimagine capacity as your
income and ecological footprintas your expenses.When your expenses--
ecological footprint--is less than your
biocapacity-- your income--you would have a surplus,
or an ecological reserve,whereas if your expenses
ecological footprintexceeded biocapacity--your income-- you would have a
deficit, or ecological deficit.That's currently
what's happening here.And as you can see, it's
increasing over time.If you're anything
like me, this producesquite a bit of anxiety.But this is something more
significant than debt.After all, there's
no debtor's prison,at least not if
you pay your taxes.But this here is
about the abilityto actually sustain life--period.What are the factors
that are associatedwith ecological footprint
so that we can reduceecological footprint
of countries,the implications of which could
influence government policiesand regulation, social
awareness campaigns, researchand development, industry
best practices, and so on?Now that I have
adequately frightened you,I will hand off
to Neil to go overthe more technical
aspects of our project,starting with approach.NEIL BELFORD: Thank you, Jordan.So our approach had
three main phases.The first was data collection,
getting all of our datatogether.The next was to get the
data into a form thatcould be used for the analysis,
and then the third stepwas to perform analysis--
which, in our project,was panel regression.We had two main
sources for our data.The first was the
World Bank, for whichwe had a database of economic
indicators for nearlyevery country on earth from
the years 1960 to 2016.And our other main source was
the Global Footprint network,and this is where the ecological
footprint and biocapacityinformation that Jordan
had talked about--this is where that came from.And again, they had
the data for most yearsbetween 1960 and 2016.Both data sets did
require transformation.They needed to be pivoted,
merged, dropped, and thenfiltered.The World Bank data
in its original formhas the country name and
an indicator on the rows,and then the value for each
year was on the columns.We needed to turn
that into a formwhere each row was a
country in a year, whichis an observation
in our data set,and then each column is
an economic indicator.So we had variables
on the columnsand then observations
on the rows.We had to perform
a similar pivotingon the global footprint
data, with one extra wrinkle,in that the columns--instead of being the
year, they're actuallythe components of each measure.And they needed to be
aggregated so that, again, wehad the variable on the column,
which is the total measure.And the rows are
observations, whichis country names in years.Once both data sets
were in similar formats,we were able to merge
them using the countryname in year as key columns.That formed our master
data set, which consistedof 10,755 rows and 178 columns.Now, many of those columns did
contain a lot of missing data.We found particularly
that earlierin the data set before
1995, many much of the datawas missing for many countries
across many variables.So we chose to drop any
of the variables thathad too much
missing data, and wechose to focus on the
year 1995 to 2015,because that was
the time when we hadthe most complete information.We also chose to focus
only on countriesthat are part of
the Organizationfor Economic Co-operation
and Development, or OECD.We did that for two big reasons.One is that the data was more
complete for those countries,and the other is that they tend
to be countries that are moresimilar economically
to try to take outsome of the variance that
could be caused by varyinglevels of economic advancement.And so that gave us
our final data set,which was 1,914 rows
across 178 columns.The first thing we found when
we were looking at the datais that raw ecological
footprint is actuallyhighly correlated with
population and GDP.These things didn't come
as much of a surprise.All else equal, more
people in an area--it's going to require
more resources to sustain.And on a similar note a higher
level of economic activityis going to put a bigger
strain on the environment,all else equal.And so these three maps
are ecological footprint,population, GDP.You can see that they're
all very similar.And in fact, their correlation
coefficients are more than 0.9for all three of these.And so we chose to focus on and
derived variable for our targetcalled ecological
footprint per GDP.And the formula was to take
the footprint per capitafor each country, divide
it by the GDP per capita,and then we multiply
that by 1,000to make it a little
easier to read for humans.So it was actually ecological
footprint per $1,000 of GDP.Our data set-- it is
panel data, because itcontains cross-sectional
data across 31 countries.And it's also time
series data covering 1995to 2015, which is
a 20-year span.In our case, we chose to analyze
it using panel regression--which, in the SAS environment,
we used PROC Panel.We had four different sources,
four variables selection.One was literature
which finding outwhat other studies had
found to be significant;intuition, things
that we thoughtwere worth looking at to see
if they were significant on--to determine
ecological footprint.We also did
correlation analysis,just finding out what variables
or measures were correlatedwith our derived target--and then also
cross-sectional regression,where we took the data
from a single year,performed a normal linear
regression on that yearto see what was
significant, and thenlooking to see if that
was also significantacross the entire data set.In the end, we
chose 10 variables.We have energy use per $1,000
of GDP, which is an energyefficiency measure.We have government spending
on education, life expectancy,homicide rate, percentage
of the populationliving in urban areas,
population growthyear over year, and
then four variables thatare the percent of total
electricity production thatcomes from each
of those sources.So we have nuclear, coal,
hydroelectric, and thenall renewables
minus hydroelectric.So then the next
step was to choosewhat type of panel
aggression we'd like to run.When it was run as a
random effects model,a Hausman test was significant,
which means that unobservedvariables-- or variables that
were not part of our model--were correlated with
the observed variablesthat were part of our model.And also, our F-test
for no fixed effectswas also significant,
which meansthat there's variation
in our target variablethat can be attributed to
those variables that are notpart of our model.So in the end, we chose to
use a two-way fixed effectsmodel, which means that we
fixed the variation across time.And we also fixed or
controlled for variationthat could be attributed to
those unobserved variables thatare outside of the model.And what does that mean?So that means that all of
this variables that we foundto be significant are
significant after controllingfor variation over time and
controlling for variablesthat are outside of our model.This is a list of the
coefficients for our panelregression.The most important things here
are which ones are significantand the sign on each one.You'll see that the
top six were allsignificant at the 0.05 level.However, the
electricity productionvariables-- only nuclear was
significant at the 0.1 1 level.The others were not.And it helps to think
of these in groups.So first was what we
call population dynamics.This was your
percent of populationliving in urban areas and
your population growth.Both of these were
significant evenafter controlling
for total population.And our next group we called
societal well-being, educationspending, life
expectancy, homicide rate.And these five variables,
while being significant,we thought were a little bit
more difficult to recommendas places to look as
far as policies go,because they are difficult
to affect-- particularlythe population numbers.And the other issue is
that panel regressionsays nothing about causation.So for example, we can't
say that a higher lifeexpectancy causes a lower
ecological footprint.It could very well
be the opposite,where a lower ecological
footprint causes the higherlife expectancy.And so these five were
a little less actionablethan the other variables
that were significant.And so of those
energy efficiencywe thought was a
good place to focusbecause it was significant.And then energy
production-- nuclear energy,again, what's significant.It was significantly related
to a lower ecological footprintper GDP.We did find it interesting
that coal, hydroelectric,and renewable energy production,
or electricity production--none of those were significant.Particularly the renewable
energy we found interesting,and we thought--we had two ideas on
why that might be.One is that it is such a small
percentage of total electricityproduction that it could just be
it's such a small area that itcan't have much of an effect
on the overall ecologicalfootprint.And the other thought we had
is that ecological footprintaccounting takes into account
more than just emissions,and it could be that those
renewable energy sources haveeffects besides emissions
that could be increasingthe ecological footprint.And then now Jordan will
go over our recommendationsand finish our presentation.JORDAN HUMES: Thanks, Neil.As Neil mentioned,
the OECD countrieswere our primary focus,
represented here.As countries around the world
continue to industrialize,their ecological
demands are increasing.The US, for example--its ecological footprint is five
times that of its biocapacity.Industrialized countries
have the largestecological footprint.As countries develop and
industrialize we predict,as mentioned, that global
ecological footprint will go upto the point that we are
consuming three earths' worthof resources.So nations must monitor
their footprint and workon decreasing their
ecological impact.With that said,
future studies shouldtake consideration of the costs
associated with nuclear energy,from the building
of infrastructureto disposal of waste--also analysis into government
policies and legislation,social awareness campaigns,
industry practicesthat can contribute
to the reductionof ecological footprint.We provided the
data, the analysis,and the recommendations
for reductionto the global footprint.We consume more than the
planet can provide as a whole,and our ecological
impact is increasing.It's up to us and
our governmentsto focus efforts on
energy efficienciesand better sources of
energy production--as our analysis
suggests, nuclear energyand overcome the
stigma associatedwith the use of nuclear energy.Nations around the world
would be contributingto a global reduction in
humanity's ecological impact,benefitting all generations,
present and future.I would like to leave you
all with this quote from JaneGoodall that embodies
our findings.""You cannot get through a single
day without having an impacton the world around you.What you do makes a
difference, and youhave to decide what kind of
difference you want to make.""On behalf of our
group, SASsy Coders,thank you for your time and the
opportunity to share our work."
109,"Welcome to this super demo.I am Vincent Rejany,
principal programmer at SAS.And today we have Nancy Rausch.We are going to give
you a quick look at oneof our upcoming
product, scheduledfor the end of this year, SAS
information catalog on SAS via.Before starting the
demo, let's startwith few introductory
informationfor setting the scene.A movement towards service
involves a need for userto get a complete and insightful
view of organization dataassets, including the
context of user, originand from the dictionaries
to metadata management,there is now a third
generation of solutions, dataand information
catalogs, which aimat leveraging what is known
through classify data analysis,automation and collaboration.The difficulties
of data managementhave intensified
at a steady paceover the past several years.The organization is
struggling to get and maximizethe value from its data.And the following are
three main reasonsfor this that explain why data
catalogs have been emerging.The first one is
data proliferation.Your organization has
never managed so much data.And more data that is spread
over multiple locations.Regulatory pressure.Your organization is
now heavily scrutinizedby industry, state, and national
regulations such as GDPR, CCPA,PIPA, PPR, KVK and so on.They are asking for
transparency and accountability.And the last one,
data democratization.Your data consumers are
requesting more and more data,but at the same time, they want
to know where it comes fromand how reliable it is.They ask for the end
of tribal knowledge,and for the advent
of data democracy.These three drivers explain
why the data cataloghave become so popular versus
the former metadata managementapproach.End users are no longer
able to spend more timelooking for relevant, adequate,
up-to-date, qualitative andreliable data that
is spent analyzing.Data catalogs are key
insights service strategiesby being the entry point
for next valuable actionswith data.On the other hand, by
identifying sensitive databefore it's applied
to business analytics,data catalogs reduce the
impact of potential breacheswhile meeting all industry
and government regulation.The data catalog
more specificallyis a repository of metadata,
centralizing informationon data sources,
schemas, tables, columnsand their extending information.Typically we do access
data, profile informationand do some discovering
classificationso as to feed a catalog.SAS information
catalog is going to gobeyond this objective of
only cataloging data assets,by also proposing to search
over business terms, businessrules, reports, models--any kind of metadata that will
be stored inside via platform.But now it's time to
switch the demo with Nancy.NANCY RAUSCH: So this is
the information catalog.Data is indexed into
the metadata server.And then you can search for it
when you come into the catalog.So there's a search bar up here.You can enter simple
searches, such as justshow me everything
that's in there.And it will return a lot of
the data that's in there.You can also choose to
search using wild cards.So if I want to see all the
tables that begin with a,I have the ability to do that.Notice this score.The score shows you how
well it matched your query.So you also have the ability
to do natural language queries.And here's where it
gets interesting,in terms of being able to match.So there's a natural
language pipeline builtin to this intelligent query.So I can do things like
tables with auto emission.Search for that.And now I'm going to get
back a ranked search thatshows you all the tables that
have content in them relatedto auto emissions.There's a graph that
shows you detailsabout-- or a summarized view
of the data that you retrieved.And you can see
in this word cloudthat there were many fields
inside these tables thathad data related
to auto emissions,including what the most
common words were found.There is also a usage
chart that shows youhow the data is
being used over time.And a value score that shows
you how good of a qualitythe data is that was returned.So as you go down, you can
see that the score goes down.These are tables that
have less and lessrelated to auto emissions.If I then opened one of these
data sets, which I can do,I get more details.So here I see that
actual data set.I get to see more
details about it.And now I'm starting to
drill into profiling.So I get a score
about the completenessof all of the data.I can see how the value
score has changed over time.I also see history.There's more details about how
it was used, who's using it,and who to talk to or is
responsible for this table.There are tags.This table has been identified,
classified, the content in it,and there are tags about
what was found inside of it.There's also a lot of
information about wherethe table was located.If it's in CAS or not.If it's physically located
somewhere, on a serversomewhere.If it's in the cloud or not.Some things we're exploring is
whether it's been watermarked.Watermarked is a technique for
ensuring overall good quality.And if it's been
vetted and approved.So there's that capability.There's also the ability
to drill in and see detailsabout each of the columns,
including profile results.And there's a sample data tab.And what this shows
you is sample datafor what's in this table.Above here you can see
how many unique valueswere found in the data.And you can drill in and
see details of that sample.This table has 98 columns,
150 rows in its sample size.There's also the
ability to collaboratewith other people on this data.And there are actions that
you'll be able to take.These actions aren't
implemented yet,but as you go into
the data, you'llhave the ability to open it.And if you go back
in search results,you can see some of those
actions if I pick that table.You'll be able to bring it in
to prepare it for analysis.Bring it in to
individual analytics.Potentially into model
manager and to other areasto work with it.So it's really a superset of
what you can do in the SAS dataexplorer.Over on the left here
are other featuresavailable to you in the catalog.This data is populated
into a search index.That's why it's so fast.That configuration
is done over here.There's a way to set up
those configurations.File someone-- here's another
example of an MLP search.So return tables that have
vacation information in them.And you can see, yep, that
there are various tables in hererelated to vacation
information, includingsome surprising things.There's some related to vacation
that are potentially justodd-named tables.So that's the
beauty of search, isit can find that
content for you.The approved
process is somethingthat administrators can do.They can mark a table as
one that's useful for--or fits a purpose
for being used.Recommended.Some of them have not
been approved yet.And some of them have
been flagged for research.So this is a simple way of
vetting the data to ensurethat it's high quality.VINCENT REJANY: Thank you,
Nancy, for this great demo.For my last line, I would
like to introduce youto the ODPI Egeria project.Egeria is the world's first
open-source metadata standard.It aims at delivering
a framework includingopen APIs, even forms and
types, integration logic,so organization can share
data management and governanceacross the entire enterprise
without reformattingor restricting the data to
a single format, platformor vendor.SAS information catalog will
be fully compliant with ODPIEgeria by the end
of 2020, allowingyou to exchange typical
SAS metadata with platformssuch as Apache Atlas
or IBM data catalogs.Thank you very much
for your attention.I hope you enjoyed this demo.And if you have any
kind of questions,you can feel free to contact
me on LinkedIn or Twitteror directly email me.Thank you very much."
110,"Hello and welcome to SAS
super demo on Banking on Bots.A little about me before we
get into the presentation.I'm Joan McGowan.And I'm a Global Banking
Industry Principal at SAS.I've worked in the
industry for closeto 18 years in various
strategic roles.And I've had the
privilege of beingable to study our
industry close up,both from the
perspective of the bankand also its
surrounding ecosystem,including technology providers.My focus has been
on the intersectionof technology and
business and morespecifically what
the impact of newerenabling techniques
and technologiesis on the banking industry.I've authored and
published over 30 reports.And I've written numerous
articles on this space.From my overall
experience, the key reasonwhy we continue to stumble in
the implementation of newerenabling technologies
such as AI or chat botsis not the fault
of the technology--I'm assuming it's
built right here--but rather the lack of
preparation and governanceundertaken by banks.However, I do see a
sea change as thereis a desire to move more rapidly
to a digital first model.And now we can also learn
from those that went first.It's going to be a
much straighter pathto digitalization from here in.My presentation today provides
a quick and dirty overviewof how to build a bot
that enhances the customerexperience.So what does SAS mean
by banking on bots?What's a bot?It's the ability to offer
virtual banking, referredto as bots, chat bots,
virtual assistants,or conversational banking.In this presentation
and demo, we'regoing to look at
the role of languagein creating a fluent,
nuanced, and intelligent bot.This is a critical fundamental
requirement for success.We all recognize the
frustration and irritationof talking to a
two-dimensional bot.I know that I will hang up,
or I will close the browserif I get frustrated.It's a turn-off.And it can kill
customer service.So banking on bots means banking
on natural language processing.NLP is a branch of AI.It is a technique used
to understand, interpret,and emulate human language.It draws from many disciplines.And that includes
computer science,computational linguistics,
machine learning,and deep learning in
the contextual analysisof unstructured data--contextual being the
most important word here.Interpreting and emulating
unstructured text--unstructured text is the largest
human generated data source.You can see the staggering
numbers in this slide.And it grows exponentially
by the minute.We increasingly converse through
text, instant messages, email,and social media.Conversations range from
objectively stated factsto our thoughts,
our own perceptions,our own perspectives.And sometimes, we're texting
utter nonsense, as well ascomments on good or bad
experience, et cetera.Each of the data sources
that are shown in the slidehold an enormous volume
of unstructured text.Human language, unlike
structured information,does not fit neatly
in rows and columns.Think about trying to fit a text
message conversation into rowsand columns.The word or value just
doesn't fit into a box,nor would those box perfectly
align to some distinct value.Anything you would write with
a pen or type out on a keyboardshould be considered free
form or unstructured text.Without natural
language processing,that information, as shown
in the examples in the slide,would hold no meaning
to the machine.And you cannot build a bot
without the interpretationof unstructured language.Now, this is where it's
going to get complicatedas we look at the different
natural language processingapproaches.At SAS, we believe
the best practiceis to analyze unstructured text
data with a combination of NLP,machine learning,
and human input.Human expertise provides
the guidance for accurateanalysis and machine learning,
then helps that analysis scalewith ease.When using NLP to
analyze data, the machinemust first split the
unstructured text datainto useful,
comprehensible units.And this is known
as tokenization.If input has been
spoken, the machinemust convert that
speech into textfirst before further processing.From there, NLP performs
further parsing, applyinglinguistic analysis
to extract featuresfrom the text such as root
words along with variationof the words, sentence
boundaries, partsof the speech, noun groups,
syntactic structures,and a lot more.In addition to machine
learning, the applicationof deep learning through
recurrent neural networkmodels, specifically
long short-term memoryand gated recurrent
units, can beapplied for time series
forecasting, text generation,speech to text, and ultimately,
text to speech, hence the bot.Human subject
matter expertise canbe added in the form
of linguistic rulesto improve model accuracy.Machine learning can help
reduce the human modeling--human model building
efforts by leveragingsemisupervised learning to
automate the tagging data.And that's based on human
input to training that data.All of this processing to
identify the meaningful piecesand relationships
in human languagecan be performed using machine
learning-based models or modelsbased upon linguistic rules.So hybrid models that use both
approaches we believe generallyperform the best.Again, hybrid models are the
best practice approach for NLP.So what I have just said may
have sounded complicated.And you probably would
be to your advantageto go through this slide
in particular more slowly.But the outcome of
what I've just saidshould be simple to understand.The bot should be simple
and nuanced and fluent.Let's take a look at a short
demo that includes voiceat the beginning.And that's there for you
as an example of whatit would be like with voice.And then it switches to text.SPEAKER: Can I get a loan?JOAN MCGOWAN: Short and
sweet, but the value here,the value of using
NLP correctly,is in the simplicity
of the language,the nuanced responses.And within a minute with the
use of advanced analyticsand artificial
intelligence, the botwas able to approve the
person applying for the loanand move them to the next
stage within a minuteand in real time.And as you can see from
the Forrester quote,wowing with a good
experience is a winner.Get it wrong and you
do lose the customer.Our industry has only
just begun to scratchthe surface with regards to
the potential of chat bots.At the foundation
of all of theseadvances is the ability to
collect insights and applyadvanced analytics to the
benefit of the customer,of the consumer.So if you have further
questions on the use of NLPand the AI technologies
used to create a bot,please do reach out
to me, Joan McGowan,or my colleague, Katie Tedrow.Thank you."
111,"Ah, there you are.This is Michael A.
Raithel from Westat,and welcome to my SAS Global
Forum 2020 presentation.Let's go ahead and get started.Well, the presentation
is A Programto Compare Two SAS
Format Catalogs.Now, SAS programmers
are sometimesfaced with the
task of determiningthe differences between
two SAS format catalogs.Maybe you received an
updated format catalogfrom a collaborating
organization,or maybe a colleague of yours
updated the format catalogto reflect changes in
the underlying data.But no matter which
case it is, how do youdetermine the differences
between the old format catalogand a new format catalog?Well, the tried and
true way of doing ithas always been to go ahead
and output each format catalogusing the FMTLIB
option of PROC CATALOGand then manually
comparing them.Well, that's fine if you
have very, very small formatcatalogs and it's relatively
easy to compare them.But the whole
system breaks down,it becomes error
prone and cumbersome,when you have very, very
large format catalogswith many, many entries and
many, many different catalogswithin them.So the thing is, you need
a better way of doing it.And that's why I created the
Compare Two SAS Format CatalogsUtility.It's a SAS macro program.It compares the entries
of two catalogs.We have BASE, which
is the catalogto use as the basis
for comparison.And we have DATA, the catalog
compared to the BASE catalog.And when we execute
this macro, it'sgoing to look for mismatches
between the two formatcatalogs, mismatches
in format name,START, the beginning value, END,
the ending value, and LABEL.The program can produce up
to four mismatch reports--catalog entries that
are found in the BASEbut are not found in
COMP, catalog entriesfound in COMP that are not found
in BASE, catalog entries thatare found in BASE and COMP
with mismatched values for END,and finally, catalog entries
found in BASE and COMPwith mismatched
values for LABEL.And then, the greatest
of all possibilities,and hopefully this
is where we end up--it could possibly
produced a reportwith no differences
found between entriesin the BASE and COMP catalog.The Compare Two SAS Format
Catalogs Utility macro programrequires five parameters
to be specified.You need to specify
BASELIB, and that'sthe directory holding
the format catalog thatwill be used as the
basis for the comparison.So it's going to be the
full path to that directory.BASECAT is the catalog
you want to useas the base for comparison.COMPLIB is the full path to the
directory holding the formatcatalog that you
want to compare.And of course, COMPCAT is going
to be the name of the catalogthat you want to compare
to the base catalog.And finally, REPORTS
is the directorywhere you want your
reports to be written to.This is what the invocation of
the macro utility looks like.And you can see that we have
specified one, two, three,four, five parameters.BASELIB.The base catalog resides in C
colon backslash TEMP MYCATS.And the name of the base
catalog is chipsfmt.The compare library, COMPLIB, is
C colon backslash TEMP MYCATS.And the name of the
catalog that we'regoing to compare to
ships format is chipsfm2.And then, finally, we want
to have any reports thatare generated from this written
to the Raithel Programs PROCFORMAT Reports directory.So what we're going to go over
in the next 10 to 15 minutesis the following.We're going to go through
my program step by step.I like to break my programs
into individual stepsto do a specific function.So step 1 will be
LIBNAME allocations.Step 2, dump the contents
of the BASE SAS catalog.Step 3, dump the contents
of the COMP SAS catalog.Step 4, match merge
BASE and COMP.Step 5, create report
data sets and macro flagsto identify empty data sets.And finally, step 6, create
reports of the mismatches.I'll have some
concluding remarks,and we'll go from there.So step one is the
LIBNAME allocations.First what we do is we
have a LIBNAME for BASELIB.And we're going to specify
the directory holdingthe SAS format library
you want to useas the base for comparison.And of course, a
LIBNAME for COMPLIB,where we specify the directory
holding this SAS format libraryyou want to compare.Very, very straightforward.We do this kind of
thing in our sleep.And it looks like this.Here you can see step 1.And we have the first
LIBNAME statement BASELINE.And then, don't forget, we're
passing in the full pathdirectory as macro
variable &BASELIB,and that'll resolve right there.Similarly, LIBNAME COMPLIB.And COMPLIB is
the macro variablewhich will resolve to the full
path of the comparison marker.Next, step 2, dump the
contents of the BASE catalog.And what we're going
to do in this stepis we're going to execute PROC
FORMAT with the BASE formatcatalog as the input.And we're going to specify
the CNTLOUT= option to createan output SAS data set.And when we do that, there
are many, many variablesthat get popped out of that.But we're only going
to see the following--FMTNAME, START, AND, and LABEL.And after we do that, or
while we're doing that,we're going to change
the name of ENDto BaseEnd since we're
dealing with the base catalog.And we're going to
change the name of LABELto BaseLabel since we're, again,
dealing with the base catalog.And after we do that,
we're going to go aheadand we're going to sort the
data set by FMTNAME and STARTfor the upcoming match
merge in step number 4.This is what the
code looks like.Very, very
straightforward again.We have our proc format
library=baseline.&BASECAT.And that's going to resolve to
the name of the base catalog.And we're using cntlout.We're going to create a SAS
data set called FormatBase.And we're only keeping four
variables-- fmtname, start,end, and label.And we're renaming end to
BaseEnd and label to BaseLabel.After we do that, very
straightforward procsort to sort it
into merging order.Now that we've done that
for the BASE catalog,let's do the same thing
for the COMP catalog.We execute PROC FORMAT with the
COMP format catalog as input.And we're again specifying
the CNTLOUT= option to createan output SAS data set.And we're only going to say
the following variables--FMTNAME, START, END.And on this one, we're
naming END to CompEndsince it's the COMP data set.We're renaming LABEL CompLabel.And we're sorting it
for the future merge.The code is very, very similar
to what we did for BASE,and it looks like this.The proc format
COMPLIB.&COMPCAT,which resolves to the name of
the comparison catalog thatwe've passed into the macro.The same type of thing
where we're only keepingfmtname, start, end, you label.And then we're renaming end to
CompEnd and label to CompLabel.Doing our proc sort
for the upcoming merge.Well, we've been talking
about the upcoming merge,and it is upon us.What we're going
to do in step 4 iswe're going to do a match merge.We're going to merge the
FormatBase and FormatComp SASdata sets by MFTNAME and START.And when we do this,
we're going to create upto three possible SAS
data sets, from 1 to 3.We're going to create an output
data take called BOTHCATS.And that's going to contain
catalog entries thatmatch on FMTNAME and START.We're going to
create BASECATONLY,and that's catalog
entries that are onlyfound in the BASE catalog,
not in the COMP catalog.We're going to drop the
variables Compend and CompLabelsince we won't really need them.They would be missing
values anyway.And then we're going
to create possiblya third one called COMPCATONLY.Those are catalog entries only
found in the COMP catalog.And we're going to drop
BaseEnd and BaseLabelbecause they would
just be missing valuesfor that output data set.This is what the very, very
simple SAS code looks like.We're creating data bothcats,
or possibly basecatsoonly,dropping the CompEnd and
CompLabel, or comcatonly,dropping BaseEnd and BaseLabel.So we're creating possibly
these three data sets.And we're merging the
FormatBase, in=a, FormatComp,in=b, by fmtname and start.Now, if a and b meaning,
that we match completelyon fmtname and start, then we're
going to write the entries outinto both cats.If it's a and not b,
meaning that it was onlyin the BASE catalog and
not in the COMP catalog,we're going to write the entry
out to the basecatonly SAS dataset.And if it's b and not a,
meaning that it was onlyin the comparison catalog
and not in the BASE catalog,we're going to write
it out to the compcat,to the comparison data set.Here's an example.We're going to take a
look at three examplesof the files created by this.So the first one is bothcats.And you can see here that
on FMTNAME and START,we had nine entries that
were matched one-for-oneon FMTNAME and START between
the BASE catalog and the COMPcatalog.You can see FMTNAME, START,
the Base ending value, the Baselabel, the Comp ending
value and the CompLabel.So those were direct matches.Ah, but look at this.We ended up with
two entries theywere in the BASE catalog that
were not in the COMP catalog.So the FMTNAME for
them was binary.And first one had a START value
of 0 and an end value of 0and a BaseLabel of 0.And similarly, START value of
1 and 1 and a BaseLabel of 1.So those were only in the
BASE but not in the COMP.Here we have, in
the COMP catalog,these are entries that were
in the comparison catalogbut not in the BASE catalog.And you can see there
are three of them.For the AGECAT format, we had--and a START value of
17 was one of them.And then we had a MIKEBDAY--two of those.One, the START was the
OTHER, and the other one,the START was 0427.So these are mismatches
between the two catalogs.Now that we have done that,
we've done our match merging,we're in a good position
to go ahead and createa couple more report data sets.And then, finally,
we're going to createsome flags to
identify the reportdata sets that are empty.So we create two data tests
from the BothCats SAS data set.And let me remind you,
the BothCats SAS data setis the data set that
was created whenour observations from both
data sets matched on--they matched on the FMTNAME
and on the START value.So the first of the two we're
going to create is EndMismatch.So those are observations where
the values BaseEnd and CompEndare not equal.So they matched on
FMTNAME and START,but the END value was not equal.Similarly, we're going
to create one data setcalled LabelMismatch.And those are observations
where the values have BaseLabeland CompLabel are not equal.Here's the code to do that.Data EndMismatch.set bothcats where
equals BaseEnd is notequal to CompEnd.Notice that we popped
into EndMismatch.Similarly, data
LabelMismatch set bothcatswhere BaseLabel is not
equal to CompLabel.So now we've done most
of the heavy lifting.What we're going
to do now is we'regoing to create a series
of macro variables.And these macro
variables are goingto govern the reports
that we eventually print.You don't want to end up
trying to print a report outon an empty data set.And what this is
going to do is it'sgoing to determine if we
have any mismatches in anyof the mismatched data
sets that we've created.So we're going to create macro
variable flags BASECATONLY,COMPCATONLY, ENDMISMATCH,
and LABELMISMATCH.And BASECATONLY
is going to be setto 1 if there were observations
that exist in the BASECATONLYdata set.COMPCATONLY set to 1 if
there are observationsin the COMPCATONLY data set.In ENDMISMATCH, observations
exist in the ENDMISMATCH dataset.And LABELMISMATCH is set
to 1 if observations existin the LABELMISMATCH data set.Here's the code to do this.This is probably
the most, I'll putthis in quotes, ""complicated""
example that we have,but it's really not that bad.We have a data null
step, and we'rereading in sashelp.vtable,
where library equals WORK.And in the WORK library, we
know that we have 1, 2, 3,4 SAS data sets, BASECATONLY
COMPCATONLY, ENDMISMATCH,and LABELMISMATCH.And what we're doing
is we're readingin the metadata for them from
the vtable on the SAS table.And when it's BASECATONLY,
if the number of observationsis greater than 0, then
we're using call symputto set the BASECATONLY
macro variable to 1.Otherwise, we're
going to set it to 0.Same thing for COMPCATONLY.If the number of observations
in the COMPCATONLY SAS data setare greater than 0, we're
going to set the COMPCAT macrovariable to 1.Otherwise, it's
going to remain at 0.We do the same for
ENDMISMATCH and LABELMISMATCH,setting those macro
variables to 1if we have any mismatches
in those various data sets.Well, we've done all, all,
all of the heavy lifting.And now it's time for us to
reap the benefits of whatwe have done and create
reports of the mismatches.So we're going to execute a
series of %IF, %THEN, %DO,%END macro statements to create
the appropriate report whena macro flag variable
has been set to 1,which we did in
the previous tab.Reports are written to a
Microsoft Word document,but you could just
as easily write itto another ODS destination,
such as Excel or PDFor, if you're still
playing around with RTF,you could do it that way also.And what we're going to do is
we have five possible reports.Catalog entries found in BASE--and we're going to have the
name of the BASE catalog--but not found in COMP,
name of the COMP catalog.Catalog entries found in
COMP but not found in BASE.Catalog entries
that were found inBASE and COMP with
mismatched values for END.And catalog entries that
are in BASE and COMPwith mismatched
values for LABEL.And then, finally, another
possibility, of course,is no differences found between
entries in the BASE catalogand the COMP catalog.This is what the first
report looks like.So we have %IF
BASECATONLY equals 1,that macro variable step to
1, then we're going to doeverything between the
%DO due and the %END.We're using simple ods
WORD file equals &REPORTS.That's going to resolve to
the full path of the REPORTSdirectory that you passed
into the macro way, way,way earlier.And it's going to create a file
that has a very, very long filename.It starts with Catalog.Catalog Entries Found in BASE.And then we're going to
have in the middle of thatfile name which is going to
result to the name of the BASEcatalog But Not Found in COMP.And that's going to resolve
to the name of the COMPcatalog, docx.Very, very long file name,
but very, very descriptive.A very simple Proc print.We're Proc printing from
basecatonly var startbaseLABEL.And it should be
base sum base name.And we're doing it by
fmtname, id fmtname.And then we have a
title statement--two title statements.And then, in our footnote,
we're saying BASE Catalog is,and we're putting the name of
the BASE catalog in and thenthe BASELIB, which is going to
be the full path of the librarywhere the BASE catalog is found.And same thing with a COMP
Catalog, the name of the COMPCatalog, and a COMP library.So then, that way,
in the footnotesyou can know exactly
what directoryand what catalogs
you were compare.This is what an example
about looks like.So we have START catalog
entries found in BASEthat are not in the COMP
Catalog when attemptingto match on FMTNAME and START.You can see the START
value, the BaseLabel,and the different values.Similarly, we have-- we're doing
the same thing for COMPCATONLY.And these were ones that were
found in the comparison catalogbut not in BASE.The code is exactly the same,
except it's aimed at COMP.And here you can see the
example of the output.We do the same thing in
creating catalog entrieswhere the mismatched one end.And in this example,
the output isboth of them match
on FMTNAME and START,the BaseEnd said high,
but the CompEnd said 16.Definitely got to
look into that.And then we have
the same examplefor where there was
a mismatch on label.Here's an example of that.And you can see that there
were three entries that wereall that mismatched on LABEL.For the FMTNAME AGECAT
with a START of 15,BaseLabel was Older Teen, while
the CompLabel was Medium Teen.Similarly, for CHIPTYPE of
2, BaseLabel was Lay's whilethe CompLabel was Michael's.And for a CHIPTYPE
with a START of 3,we had Kettle Chips versus
just simply Kettle, somethingthat we will definitely
have to look into.Is that good or bad?Well, who knows?And then, finally we
have code for if therewere no mismatches at all.So if BASECATONLY equals 0,
if macro variable flag was 0,and COMPCATONLY was
0 and ENDMISMATCHwas 0 and LABELMISMATCH
was 0, thenthere were really no differences
found between the two catalogs.And so in that case, we create
a very, very simple report.And it's going to be a report
that says No Differences FoundBetween Entries in the
BASE and COMP Catalogs.And what we're
going to do is we'regoing to go ahead and print
out all of the entriesfrom both catalogs.And that's what that report
is going to look like.And in that way, it serves
this like an audit trail.Not only are we stating that
there were no differences,but we're reflecting back what
we found in both catalogs.So in this presentation, I
introduced the Compare Two SASFormat Catalogs Utility program.And this can be used to compare
to use SAS format catalogsand find differences between
fmtname, start, end, and label.And we create
reports that you canuse to research and document
the differences between the twocatalogs.This is a programmatic
solution that'sway, way, way better than the
previous solutions that we had,which were just simply
printing them out,two different copies,
putting them on our desk,and looking at them side by
side to determine whether or notwe had differences.So definitely a
better way to go.Well obviously, in
this type of format,there is no way that I
can really take questions.It just doesn't work.But I would love to have
your questions and comments.So go ahead and send them
to me at my email address,michaelraithel@westat.com.That's
michaelraithet@westat.com.And you'll see it also in the
paper in the SAS Global Forumproceedings.Well, that's it for
this presentation.I really do appreciate
you joining me today,and I look forward to
getting your comments.And best of luck to you in
all of your SAS endeavors.Thank you."
112,"KIRK LAFLER: It
provides an end to endtype of process and programming
application developmenttechniques.I'm an educator,
and also author.I've written several books.One, in particular, is my PROC
SQL Beyond the Basics UsingSAS.It's currently in
its third edition.And it came out,
the third edition,in 2019, just about a year ago.We'll begin this webinar.I want to show you
a couple of datasets that we're going to
be using in the examplesthat I'll show.Originally, this
webinar was designedas a hands-on workshop.But we've converted it over
to a 20 minute presentationwith examples.There's also a paper that
goes along with this webinar,feel free to download that.There's also code examples that
will help you better understandand try out some
of the techniquesthat I show in this webinar.The first table or data
set that I'll be showing,and we'll be using
in our code examples,is found in the SASHELP library.And it's a cars data set.Many of you are probably
familiar with this data set.It's a fun data set consisting
of 428 observations or recordsin 15 variables.Because I want to show
you various aspectsof the hash objects
themselves, we'realso going to be creating
another data set or table.And it's the colors
data set, and it'sgoing to be stored
in our work library.It's going to consist
of six observationsand it's three variables,
giving us the abilityto show how we can
use hash objectsfor merge or join operations.So beginning with
this webinar, we'regoing to look at a
brief understandingabout hash objects.Then we'll look at some of
the syntax, basic hash objectssyntax.And then we'll go into the
exercises in the examples.Let's begin with
understanding hash objects,and I purposely kept
this very simple.There's a very good book on this
subject by Paul Dorfman and DonHenderson, which I encourage
you to look at and to inspect.Because they have a
lot of great contentthat goes well beyond what I'm
covering in this introduction.But to begin, hash objects,
as they're defined,is basically like
a data structure.It contains an array
of items that maps keysto their associated values.It's implemented as a
DATA step construct.So we're going to use the DATA
step to implement our hashobjects.Hash objects are currently
not available in the PROCs.And at the end of the
process, the hash objectis automatically removed.How does a hash object work?Well, the contents are
essentially read into memoryfrom any table.They're read into memory once.SAS, as needed,
can then repeatedlyaccess the contents
of the memory.Let's assume-- and this holds
true in most situations--that memory-based operations,
which are nanosecond speed,are typically faster than
disk-based operations, whichare traditionally
millisecond speed.Consequently, hash
objects give usthe ability to utilize memory.Ultimately, the users may
experience faster operations.So if we have a table, our cars
table in the SASHELP library.And we want to work
with the colors table,we can bring this together.And we can process the data
in memory, which hopefullywill be a much faster
user experience than if itwere disk-based.Now let's turn our attention
to basic hash object syntax.Now keep this as
simple as possible.As I mentioned, this is just an
introduction to the hash objectitself.The hash object is used by
calling various methods.There is right around
26 known methods.The basic syntax looks
something like this.The name of the hash
table, which is typicallyuser-assigned, a
physical dot, followedby one of the desired
methods by its name,and then any specifications
that are passed to the method.Akin to a function, for example.You're passing information
into the method itself.A couple examples.These are not complete examples.But a couple examples of
the methods themselves.We have a method that we
are going to define our key,and the hash key is the name
of the particular table itself.And then we also have a
find method, which gives usthe ability to find certain
things based upon the key,referencing the hash
key that we define.And we're going
to see these codeexamples in greater context.I just wanted to give you
a little bit of an exampleof what a method looks like.And as I mentioned,
there's numerous methods.I listed them alphabetically
on the next three slides.The methods that we're going
to look at in the code examplesare highlighted in yellow.And that's not to say the
ones that aren't highlightedare not important, it's
just that I'm limitedto a 20 minute time slot.So I purposely chose
certain methodsto show how to make
certain things work.So we're going to
look at the add methodto define data, define
done, define key, and find.We're also going to look
at the output method.So I mentioned these are
in alphabetical order.And there's the
remainder of the methods.So there's a lot of
different methodsthat a user has
available to them,if they use, and explore,
and apply the hash objects'techniques.Now let's take a look at
the exercises and examples.The first example, has nothing
really to do with hash objects.It's just that along with
the SASHELP.CARS data set,we want to have
a second data setto be able to do merge
and join processes.So in this first
example, we're goingto create a car's
color data set.It's going to
contain, essentially,three variables,
the type of car,or automobile, or
vehicle, the exteriorcolor, and the interior color.I purposely made this data
set structure very simple.So it consists of
three variables.We have six different
observations, hybrid SUV,sedan, sports, truck, and wagon.Now, we're creating this
data set into DATA step.We have a variety of different
ways to create data sets.But I purposely just
chose a DATA stepsince we're dealing with
the hash objects thatis implemented in a DATA step.So that way you can see how we
can treat data sets as well.So in this case, we're creating
a data set called WORK.COLORS.Obviously, you can
store this colors dataset in a permanent
location, whether it'sin SAS User or some
user-assigned libref.But I just wanted
to just illustratehow to create the
data set, store itin a temporary data set.And then at the
end of the session,unless you save it permanently,
this colors data setwould automatically be removed.So once we submit
this code, we'regoing to have essentially a
rectangular structure knownas a data set stored in the
work library called colorswith the three variables
and six observations.If we want to further
explore the results,we can issue, for example, PROC
PRINT, or PROC SQL, or PROCREPORT, or whatever
PROC that wereaccustomed to using to look
at detailed information,just to verify and validate that
our data was read in properly.So PROC PRINT or PROC SQL.Since this is not a PROC
PRINT or PROC SQL webinar,I provide the code here.But the end result
is, we want to beable to verify that our data
was read in properly usingour input statement, et cetera.And we see that we do indeed
have six observations and threevariables.Now we can turn our attention
to the hash search or lookupprocess.A search or table
lookup operationis a common task
performed by hash objects.The results of the hash
search or find operationproduces a results set
of a single observationthat matches a user-specified
search criteria.Now, obviously, you
can make it so that itcan select multiple matches.But the example I'm
going to show next,I'm going to show
you how we can justselect first occurrence that is
found in our search or look up.Now the methods that we'll
be showing in the data stepconstruct, uses the
following methods.DefineKey, DefineData,
DefineDone, and Find.Now, obviously, we
can use other methods.But just to keep this
as simple as possible,the bare bones minimum,
these are the methodsI'm going to illustrate.Now, we're going to
see the actual code.The code looks
something like this.It's a DATA step implementation.So we have our data statement.And one of the things I want
to show you, at compile time,we have the ability
to essentially selectthe metadata from
whatever data setor table that we want
to process again.In this first example you see,
if 0 then set SASHELP.CARS.We're collecting the metadata.It's ultimately going
to be used to populatethe hash object in memory.The next thing
we're going to do iswe want to look at the
USA manufactured vehicles.Origin = 'USA'.And then just the
first iterationthrough the data set loop,
not for all observations,but just the first iteration.If _n_ = 1, first
iteration through the loop,we want to declare
our hash objects.What do we declare it?We are naming, essentially,
assigning a namethat hash object in memory.In our example here,
it's called HCars.And it refers to the
data set SASHELP.CARS.Then our goal will
be to define our key.In this case, we're going to
use the define key method--origin being the
key in this case.We're going to
define what data we'dlike to display when it
finds a match or look up.In this case, the
defined data or variablesthemselves are the variable
type, make, model, and MSRP.And then just to
complete the scenario,we specified the
defined done method.You might notice as well, once
we named our hash object HCars,we refer to that
object in memorywith the name we assigned.HCars.DefineKey,
HCars.DefineData,HCars.DefineDone.But we're not quite done yet.We have to now tell SAS how
to essentially perform define.If HCars.find-- we haven't
specified any argumentsin this--= 0 then output.So if there's no errors or
define something, output it.Where outputting it to?The data set that we're
creating, called hash search.And we just happen to be keeping
five variables, origin, type,make, model, and MSRP.And then we specify the
stop, because we're done.We only want to collect the
first occurrence of a match,and that being USA.Once again, we issue a
PROC PRINT or a PROC SQLselect to display the results
of whatever we found is a match.And we see that we have the USA,
type SUV, Buick is the make,Rainier is the model, and
then the associated MSRP.So, indeed, this hash object
selected the first occurrencethat matched the Origin = 'USA'.And I might say that this can
be very fast because we'redealing--once we load the
data into memory,the hash object takes
over from that pointwith its specified code
that we write, producingthe results that you saw here.The second process
that I'd like to showis how we can perform a hash
match-merge or join process.Essentially, what we're showing
here with the Venn diagram,highlighted by the AB in
teal is the intersect.So in order to emulate a match
or choosing a merge processin the DATA step, or in SQL
JOIN with the intersect,we can construct and
emulate a hash status stepconstruct using the
following methods.DefineKey, DefineData,
DefineDone, and Find.And again, we can use
other methods as well.But I'm keeping this to the
bare minimum at this point.Looking at that same
code that we saw earlier,very similar to the first
approach that we saw,where we did a search or a find.In this example, we're
going to create a dataset called has_match_merge.And once again, collect
the metadata from the dataset we want to
collect metadata from.The metadata is going to be
very helpful in providinginformation to the
hash object in memory.And we're selecting the
metadata from the WORK.COLORSdata set we created
in the first example.Just for processing the first
iteration through this dataset, we're going to
declare our hash objectand assign a name to
it called HColors.The data set it's going
to refer to, again,is in the work
library dot colors.We're then going
to define the key.In this example
it's called TYPE.Once we do that, we
can define our data.We only have three
variables in this data set.So that's, essentially,
what I'm doingis selecting the type, exterior,
color, and interior color.Notice, we have two variables.They're enclosed
in quotes, and thencommas separating each one.And then we're done.We call our method to find done.Now we're going to read
the SASHELP.CARS data set.That's going to be a
disk-based operation.We're going to keep the
variables origin, type, make,model, and MSRP.And then we're going to say,
by calling the find method.If HColors, the hash object
in memory, using the key TYPEis = 0 then output.So in other words, if it finds
a match, if there's no errors,select that and send
it to the data setthat we're creating on the DATA
statement, hash_match_merge.We can once again use
PROC PRINT or PROCSQL select to display
the results of a hashobject itself.Essentially what was found
with the match-merge or joinprocess.And this is what we have.We have a data set that
contains the matchesthat we desired
based upon the keys.The next application I want to
illustrate using hash objectsis called a hash sort.Sorting is a very common task
performed by all SAS users.Using hash programming
techniques,SAS users have an alternative
to using the sort procedure.Essentially what we're
doing is emulatinga sort with a hash
DATA step constructusing the following methods.DefineKey, DefineData,
DefineDone, the add method,and the output method.Now even though in the previous
examples, we used output.That was the output statement.Now we're going to
turn our attentionto using the output method.So we're going to add
data to the hash object.And then internally,
we're going to havethe hash object sort the data.And the first example is
going to be an ascending sort.And then we're going
to physically outputthe matches in sorted order to
our results set or data set.So in this example, we're
going to have data_null_.Because now we're going to
utilize the output method, notthe output statement.So we're going to
assign the nameto the coput data set
in our output methodwhen we call that.So again, we're going to collect
the metadata from the cars dataset.We're just going to
sort in ascendingorder the cars data set.The first iteration
to the DATA step,we're going to
define, essentially,define the hash object.Assign a name to it, HSort.In ascending order, notice the
(ordered:'a'), that's declaringthe direction of the sort.In this case, an ascending sort.Then we're going
to define our key.In this case, our key consists
of three variables, make,model, and MSRP.You define your
keys accordingly,whatever keys are
necessary to differentiateone record from another.And we're going to define our
data, the variables origin,type, make, model, and MSRP.Then we tell the hash
object that we're done.Once again, we're going to
perform a disk-based readon our SASHELP.CARS.We're going to use the parameter
option end=eof, end of file,to be able to entrust SAS on
what to do when it reachesthe end of file.We're going to add the data
with the key to the hash object.And then at the end of the file,
declare it with the end=eofon the set statement.We're then going to use
the output method to writethe results to a data
set called 'Hash_Sorted'.This can be in the work library
or in a permanent library.If it is a permanent
library, youwill want to specify
a two level name.In this case, since we're
defining a single level name,it's always work dot and
then the name of the dataset we assigned.Again, just to view
the results to verifythat the sorted data is what
we intended, and what wehopped for.We can use PROC PRINT
or PROC SQL select.And see that our data's
been sorted or arrangedin ascending order using the
key variables that we specified.Finally, I want to
show you that we're notjust able to sort
in ascending order.We can also perform
descending hashes.We can also define
descending hash sortsusing the same techniques we
saw in the previous examples.We're going to
collect the metadata.If 0 then set SASHELP.CARS.First iteration
to the DATA step,we're going to declare our hash.In this case, we'll
call it HSort again.But the ordered parameter
here or argumentis D for descending, not A.Then we're going
to define our key.Again, the key that
I'm constructingconsists of three variables,
make, model, and MSRP.Defining our data, origin,
type, make, model, and MSRP.We tell it we're done.We read the data.Disk-based operation.Assigning end=eof, end of file.Add the data with the
key to the hash object.At some point at
the end of the file,after it's read in
all the records,we're then going to write
the contents using the outputmethod to the data set
called Work.Hash_Sorted.To verify the results
are correct, again,we use PROC PRINT
or PROC SQL select.So in conclusion, I
wanted to just show youhow a better understanding
of what an introductionand understanding
of hash object.Some of the basic
hash object syntax.And then some simple
exercises and examples.I might add that the
code and the paperare available on SAS Community.So they'll go along
with this webinar.If you'd like to read that, I
would encourage you to do so.And I also have many
other referencesthat you can read
as well, to get upto speed and to enhance
your knowledge about howto use hash object.A little self-marketing here.My third edition of my PROC
SQL Beyond the Basics Using SASis available at the SAS
bookstore, amazon.com,and other online
bookstores everywhere.And the third edition consists
of some additional information,chapters on fuzzy matching
and data-driven programmingtechniques.I'd like to thank you for
attending this webinar.Should you have
questions, pleasefeel free to reach
out to me at any time.I love to receive questions.I don't have all the answers,
but I will do my bestto at least reply to you.And if I don't have
an answer, I mightbe able to give
you some resourcesto check into as well.I want to thank SAS
Institute and the SAS Globalcommunity of users
in the conferencefor making this webinar
available to all users.So thank you again, and be safe.And I look forward to seeing
you and or hearing from youat a future conference.Thank you again."
113,"DOUGLAS LIMING: Hello, and
welcome to the SAS global forum2020.My name is Doug Liming.I'm from Stratacent.And today, I'm
going to be talkingto you about how to take
SAS to the cloud with Azure.I am currently head of
sales for Stratacent,we're a Systems Integrator.Before joining Stratacent, I was
previously at SAS for 23 years.I did everything from R&D
to enterprise architectureto playing with
containers in the cloudand things of that
nature as well.I'm glad you could join
me today and look forwardto talking to you.Before we begin, I kind of
want to start with the targetstate in mind, i.e.,
just bear with me,and I'll go back and talk
about what a container isand what that means.But to begin with, why
would you want to do this?So let me jump out
of the slide here.As you can see here, I'm
working in SAS Studio.So SAS Studio is
our program editor.And let's say I was working
on doing some data analysis.Now, as you can see right
here, what I'm currently doingis currently working on
my desktop or my local SASinstance.As you can see here, I've
done a proc HP logistic.And this logistic took
roughly 15 minutesto run, which isn't
really a big deal,but I was only running
on a 5 gig dataset that I just generated
to do my analysisNow, in this case, what if
I needed the flexibility,or if I was doing
something really big,and it says that it's
taking 15 minutes,it took me an extremely more
amount of time to do that?Now, by running SAS in
a container in Azure,we have the capability
to take the SAS instance,push it up to a more
scalable environment,as in from my Azure
environment, and run it there.So let's say I
decide to do that.So I will go out to Azure, and
I will spin them up an instancein which to run it on.And for today's
demoing, I'm goingto run this on an
eight-core instance,compared to my two-core instance
on my desktop or my laptop.So now that I've
spun up the machine,I can then pull in the
container and run it there.Now, let me switch over to
the terminal for my container.Now here I am in
my Azure instance,my eight-core instance.The first thing I
would need to dois pull the container
into the CentOS.And we'll talk about this
a little bit more later,but this is done with
a docker pull command.So we can do a simple
docker pull command,and what this will
do is, this willgo out to the Docker repository
and then pull the image in.Now, once it's
pulled the image in,I'll be able to do a
docker images here.And there I can see I
have my image there.And then I can do a
docker run command -it,and then this means
run it interactively,and then I have to do a dash
port to deal with my ports.As we know, SAS studio
usually runs on port 38080.And then run the
container sas94, enter.Now, that right there was
literally the amount of timeit took up to start a
brand new instance on SASon a new machine.And it's pretty
impressive right now.Since I'm running SAS
Studio out of a container,now what I can do is go
back to my web browser,and then from my
web browser, thenable to connect to this
new SAS instance runningon my new eight-core
machine, versus previously,I was on my laptop or my
two-core desktop machine.So now I'm back on my
laptop or my desktop.I have my web browser.This is where I was currently
doing my work with my--remember, we had 15 minutes
to run roughly real time.Here's where I spun
up in Azure, whereI spun up my machine instance.What I need to do is, I need
to connect to that SAS Studioversion that's running
on that instance.So I come here, and
I'll get my IP addressthat I have shared out, open a
brand-new tab, paste that in.And if you recall, remember the
port number, 38080, hit Enter,and then hopefully SAS
Studio will come up for us.Now that we have
SAS Studio up, Ican enter my username
and password ID.Now that we've logged
into SAS Studioon the new eight-core
instance, I'm just going to,for the sake of
this demo, go back.Let me get my data.So we'll go ahead and
bring the data overto two different SAS
Studio sessions here,not to confuse you.The one that I'm currently
right here working inis the new eight-core instance.So we'll run that guy.And then as that's
running, that'sgenerating my 5-gig data.I will then run the proc
h logistic on afterwards.And we can go back and
discuss exactly whatit was that was happening.So while that's running, let's
go back and say exactly--when I mentioned I was
running a container,so what happened was,
I'm running my laptop.I took my SAS container,
and I moved it upto that eight-core Azure
instance in the cloud.But Exactly is a container?A container-- my
definition is just enoughof the underlying system or
system tools, system libraries,to run a single application
or a single process.So the idea here
on the screen isthat if I have a SAS container
sitting on a host machineor server someplace--in the first example,
it was runningon literally a
container on my laptop,versus a container running on
my Azure eight-core system.The nice thing, the reason
that they're so portable and somuch smaller, is because
versus a virtual machine,I have all of the
system librariesand all of the OS
and everything that'sneeded for the entire
machine, versus just what Ineed for a single application.I often get asked, what are the
differences between a containerand a virtual machine?And it's that
underlying premise.Also, with a virtual
machine, you actuallywill consume part of
the underlying machineOS, meaning as in the picture
at the bottom of the screen,if you have to create
a virtual machine,you literally carve
off, say, eight cores.You carve off so much memory.And then that is dedicated to
that one virtual machine there.Versus a container
starts up, it literallywill see all of
the cores and allof the memory of the
underlying machine,and it's a lot more
effective sharingthat host of all
those applications,because Docker manages sharing
the host, the underlying hostmachine as well.They're a lot more
lightweight, clearly,because they're a lot smaller
than, say, a virtual machine.That makes them
extremely portable,where I did that docker
pool on the Azure machine,I was able to [AUDIO OUT]
Azure machine.And then run it.That also means, as with this
particular demonstration,that it makes it
extremely more scalable.More scalable means, literally
in this example that was--[AUDIO OUT]--my laptop, and I'm running
it on an eight-core machineup in the cloud in the Azure.So a lot of these advantages
as to why a Docker container isreally nice to use, and why
have SAS in a container itself--well, the biggest
reason is, asidefrom my example of
going to scalability,is it also gives
you the capabilityto take the analytics
to the data,meaning if I have a lot of
my data in Microsoft Azure,I can run my
container out there,where the data
lives in the cloud.As with the example
with my laptop,I can continue to run it on
premise, as we're showing here.And the portability
and run anywhereare huge advantages ofb running
with a container, as well,of course, as what we're
talking about todaywith the scalability, including
efficient use of resourcesas well, too.So that before we talk about
how did I get this actual SAScontainer to begin
with, it's actuallya really straightforward
process that's easy to use.Let me switch over
to my command lineand show you how that
was done as well.Now let me show
you real quick whatit takes to build the
actual SAS container.There are one major file
when I do a docker build,what actually creates
the container itself.It looks to a file called
Dockerfile, which we have here.And this Dockerfile is what is
used to create the container.So we start with the base
image on the first lineof, we built from a Red
Hat 7 type of CentOS image.In order for the SAS math, we
do need some high-level mathlibraries as well.And then I went ahead and
added a user manually.You could always use your LDAP
or single sign-on applicationthrough SAS Studio.And then we add
these home binaries,which come from your
SAS 9.4 binary installs.Then there are--[AUDIO OUT]--those 38080 port of SAS.And you can easily change
the SAS Studio configurationto reflect a different port
number if your organizationdoes not allow 38080.And that's it.And then from there, we would
do a docker build command,and from the docker
build command,then we would have
our SAS container.Now let me switch back
to the web browser.Now we're back in
our web browser,and let's go back
to my SAS instancerunning in Azure, which
is this guy right here.So I just created my data.And so I created
my 5-gig data set.Here's the results.Here's the output.Here's the log.Let's see how long it took
to create this on this guy.It took around 50 seconds
to actually create it.Now, much just the same code.I cut and paste the code over
from my laptop local sessionthat I had running on my laptop,
the same 8 proc HP logistic.Now, it's important-- and why
am I using a regular proc HPlogistic versus a
regular proc logistic?And it's important.By having this HP
in front of here,I don't change the rest
of my logistic code.The HP is High
Performance logistic,an it's multi-threaded.So then I'm taking
advantage of allof the cores on the machine.One thread to actually
run the logistic,and then by using HP
logistics, it was using two.Now, the advantage I have of the
portability of the containersand the scalability
of the containers--if I take the new
logistic and put iton that eight-core
machine, then Iwill be leveraging more cores to
do the math and to do the work.Therefore, I am
scaling out my problemand therefore getting
more use of the cores.When I was running my
container on my local desktop,what is the best way
or most efficient wayof moving this highly portable
scalable container around?So the easy way to do
that is to simply dowhat's called a Docker push.And what I'm doing is,
if I have my containerin the bottom-left
purple box here,I push it up into
the Docker registry.Now, a Docker registry
is just a file storeof all the different
pre-stored containers.It's much like-- think of it
as like an FTP site for files.It's much very
similar as an FTP siteis to files, as
Docker containers isto a Docker container registry.Now, once I've pushed
it up, I push it upto any kind of registry.You can have a private registry.You can have a public
registry, AWS, Azure, GCP.They all have registries,
including Docker themselves.They let you host
different containerswith different registries.Now, once I've pushed
it up into the registry,I can then go over into
my new Azure instance,and then I can do a Docker
pull from the Docker registryonto the new Azure machine.Once I have it on
the Azure machine,and the Docker container
is actually there,then I can simply run it.And that is how it's easy
to move containers aroundto a new scalable environment.In this case in
particular, if you recall,I went from my two-core
laptop machine.I pushed it up into the cloud,
into a container registry.In the container
registry, I thenpushed it onto my
Azure instance,my eight-core Azure instance.And now, in a matter of
instants, a matter of seconds,I now have a new highly scalable
eight-core instance to run on.Therefore, if we take
the example of my proc HPlogistic just now running--for example, it was running
on two cores on my machine.When I did these slides,
it was 60 minutes.The one you saw live
was around 15 minutes.Now, hopefully, if
I'm running on Azure--let's actually go back to
my SAS Studio instance--and it is finished.And then on my
eight-core box, I'mdown to a total runtime of
2 minutes and 47 seconds.So now we can clearly
see how similar it wasto when I created the slide--2 minutes, 58 seconds.So I'm running a little
bit faster today.The point being is,
that I'm clearlygoing from roughly 15
minutes down to a hairunder 3 minutes, which
is significant savings,over half the percent
of times saved.So what the containers
are enablinghere is giving me
the flexibilityand scalability to run
my containers anywhereand everywhere that
I need to run them,as well, as the same time,
it's having the flexibilityto scale out my SAS Analytics
depending on the workloadthat I have.So I hope that you
found that useful today.Please feel free to reach out
if you have any other questionsor comments."
114,"Hi, I'm Jess Mayo.I'm a senior associate test
engineer in the data managementdivision at SAS.And this is Bringing
It All TogetherUsing Fast Data
Preparation to IntegrateAll Your Data Sources.When data is
integrated in one spaceand combined together
into one larger data set,we can answer questions
that could never be answeredby each data set individually.This provides a more
holistic picture,and allows you to gain
more valuable insightsinto your data.But what if you have data
stored in multiple differentlocations, multiple
different servers,and multiple
different file types?Accessing and
standardizing that datacan be incredibly challenging.No matter where
your data is stored,you should be able to access
it and easily integrate itinto your analytics lifecycle.With access to over 60 different
types of data and counting,SAS breaks down those data
silos and facilitates easy dataintegration across all your data
sources, from delimited filesand spreadsheets, Hadoop
MySQL, images and social media,on site or in the cloud, SAS
Data Explorer and SAS DataStudio guide you through
accessing the dataand combining the
data sets togetherwithout you ever having to move
or store your data elsewhere.In this demo, I'll walk
through a real world exampleof how we can access data
from different sourcesand combine the
data sets together.We'll look at three
different data setsto view data on customers
from a specific bank.These are the bank's
most loyal customers,ones that have a bank account,
a checking or savings, a homeequity loan and a credit card.If we can learn more about these
customers and their habits,we can advertise to
people similar to themand gain more business.To learn more about these
customers and their habits,I need to bring
the data together.Let's start by taking a
look inside data explorer.I can get here by
selecting applicationsswitcher on the left and
choosing manage data.First, I need to import my
first piece of data, whichis a text file on my machine.Select the Import tab,
local file, and thisopens up a Windows Explorer.I'll select bank
customers.txt to add thisto the import queue.From here, I can change the
table name and target locationif I want to, and select
to replace the file.While it's importing,
if I wanted to,I could have selected to save
as an in-memory table only.Notice that the table
successfully importedwith the green message
strip and is ready for use.From here, I'll choose
Actions, prepare data,to open this up in
fast data studio.I can drag and drop
a Join transformationfrom the Transform panel on the
left to the planned propertypanel on the right.This automatically opens
up the joint dialogue.First I'll start by
selecting the pencilicon to select table 2.This now opens up a
choose data dialogue,which looks exactly like SAS
Data Explorer applicationthat we were just
in, because it is.This is SAS Data
Explorer as a dialogue.In here, I can import
the next table, whichcontains home equity loan data.Back on the Import tab,
I can select folders.This dialog shows files
stored in the file service.Inside my folder, HMEQ, I have
Home Equity Loans Excel file.Click OK, and this file gets
added to the import queue.Again, I can choose to
change the target tablename and location if
I want to and replacethe file that already
exists in that location.While it's importing,
notice there'sa couple of feature
specific to Excel file,such as specifying a
specific worksheet in a fileif there more than one, and
limiting the range of importedrows and columns.Click OK once the table
successfully imports.I want to take a
moment to highlightsome other features of
the join tables dialogbefore we add our last table.Customer ID in table 1 matched
with table 2 automatically.If I had other columns
I want to join on,perhaps first name and last
name, I can click the plus iconand add that join condition.I can also specify the type
of join with this icon.Whether I want a full,
left, right or inner join.I'm going to stick
with inner join,because I only want customers
that exist in all data sets.Click the double table icon to
add a third table to the join.Again, click the pencil icon
to open the choose data dialog.From here, I want to go
to the data sources tab.My last table
contains informationon customers with credit
cards and is stored in Oracle.So I'll access it through
an Oracle cards library.To create a new
connection to a database,click the connection icon
and enter the connectioninformation.Change the type from
file system to database,and fill in the important
information, such as username,password, path and schema.To finish creating this
connection, click Save.I already had the
set up, so I'm goingto cancel out of this
dialogue and drillinto the cache
shared default serverand find my Oracle library.Notice the connection
informationappears on the right when I
drill into the Oracle library.Scroll down to find the
credit card customers table.Right click and select Load to
load this table into memory.This table has
persisted in Oracle,but now I've loaded
it into memoryso that I can access
it and use it Viya.I didn't have to physically
move this table anywhere.The credit card customers'
table is loaded into memorywhen the icon changes to include
a lightning bolt. Click OK.Customer ID column
in table 1 matchedto a similarly named
column in table 3,so our join conditions are set.By unchecking the select
all columns check box,two more tabs in the join
tables dialog become enabled.The first tab will
allow me to selectonly the columns
in the input tablesthat I want to show up
in the output table.If you scroll down, any
column from a subsequent tablewith the same name as a
column in a previous tablewill have underscore
one appended.So we can easily filter
out those columnsthat are duplicates.For now, I'll accept the
default, all columns.The renamed columns
tab allows meto specify output column
names, labels if I want them,and formats.I'll rename the address column
to address but proper case,and bank account NO
to bank ACCT number.Now I'll click OK on
the join tables dialog.Notice when I did that,
a message strip appearedabove the table, indicating
that the session table is notcurrent to the plan.I need to run the
Join transformationso that the table gets
updated with thosejoin conditions
that I specified.The Join transformation has
run and the session tableis now updated with a
bunch of new columnscoming from the credit
card and home equity table.This new table contains
only the rows of customersof all three types of accounts.Let's see how we can start
to use this data to gainmore valuable insights.Click Save As to save the
plan and output table.I'll save the plan in my
folder and change the nameto indicate that this is
the bank customers all plan.I'll also change the
target table name,so that we know this is the
all customer's output table.If I wanted to, I could
change the physical fileformat stored on disc, but
I'll stick with sashdat.And I'll click Save.Once the plan saves, I
can reopen in the futureand make modifications.Once the table saves,
I can seamlesslymove to the next stage of
the analytics lifecycle.I can simply select the overflow
icon, actions, save table,explore and visualize, to bring
up the Save table and VisualAnalytics.I can start to explore
my data and seewhat insights I can
gather from this muchlarger combined table.I'll drag and drop a bar
chart onto the report canvas,change the direction
to vertical,and start assigning
some data roles.I'll analyze loyalty
level, which comesfrom the credit card table.Measure with household income,
coming from the banking table.Requested loan amount and
value of current propertyfrom the home equity table.And lattice by gender, again
from the banking table.These three data sets combined
together in one space,let us explore way more
patterns and relationshipsthan we can ever explore from
just one of these tables alone.And there we have it.In less than 10 minutes
we are able to access datafrom three different sources
in SAS data explorer,integrate them together into
one data set using Fast dataStudio, and begin to
explore and gathervaluable insights into the data
using SAS Visual Analytics.I hope this demo
showed you how easy itis to use SAS data preparation
to integrate all your datasources.Thank you for watching."
115,"LINDA JORDAN: Welcome to the SAS
Visual Analytics data discoverysession.We're going to spend a little
time taking a look at usingSAS Visual Analytics to
understand our data better,to look for those
trends in our data.We'll also touch briefly on
reporting, since probably, whenyou find those
trends in your data,you're going to want
to report on them.So let's get started
with a quick introductionto SAS Visual Analytics.SAS Visual Analytics
is a web-based product.That means all we need is a
URL to hit our Visual Analyticsserver with some
logging credentials,and we are good to go.Visual Analytics promotes
large volumes of data.It is designed to use big data.And so we've all heard
a lot about big datain the last couple years,
but what do we do with it,and what does it really mean?Well, these are very large.There's not a specific
number related to it,but they're data files
that sometimes are justtoo large to manage--too many observations,
too many columns, too manyfields for us to handle
in a typical environment.So we have a
cloud-based environmentthat allows us to load these
data files and do some datadiscovery with them, and that's
our goal for this session.SAS Visual Analytics
allows for you to accessmassive amounts of data.We can import data, and once
we have the data into the Viyaplatform, then we can go
about exploring the data,finding those data trends
that we're looking for,and then creating some
reports, and ultimately,sharing this information.The data will be stored
on a server called CAS.This is our Cloud
Analytics Services.It is very high speed.It is scalable, so if
your organization maybebought a smaller
Viya environment,you can always add to it.It's resilient and
it's fault-tolerant,so if things happen--and things do happen--so if a node goes down
or I have some sortof break in communication
between my nodes,there is backup.The data is always
backed up, so restassure that your data will be
there when you need it in CAS.Viya comes with two basic
architecture structures.We have single machine, which
is just what it sounds like.It's one machine.And then we have
multi-machine, and again,this sounds like what it is--more than one machine.Typically on Viya, you'll have
a multi-machine deployment.You'll have a head
node and then you'llhave other nodes that
communicate with each other.The head node's in charge, and
it equally distributes the dataamong the worker nodes.And this is what we were talking
about with the node-to-nodecommunication fault tolerance.So if one of those
nodes goes down,the other nodes know that,
and they pick up the pace,and the head node
grabs the backup data.So all of this happens
very seamlessly.When we log into SAS Viya, we're
going to go to the SAS drive,and the SAS drive is kind of
the starting place for us.This is where we can
grab our reports,maybe make some changes to
how we want to store or shareour information.But Visual Analytics
is where we'regoing to spend the
majority of our time.We're going to use Visual
Analytics to do the datadiscovery piece that
we're interested in.We're also going to use it to
generate some quick reports.If the data needs to
be cleaned at all--so maybe you have
too many columns,or maybe you have a
lot of missing valuesin a particular column, or maybe
you don't have all the clothesthat you need--you can use SAS Data Studio
to prep or clean your data.It is a separate
application, though.So I can use SAS Data Studio
to prep or clean my data,and then I can load that
clean data into CAS,and Visual Analytics
can access it.The Environment Manager,
and the Theme Designer,and the Graph Builder we're not
really going to focus much on.Those are generally not
often used by the analyst,and that's kind of the world
we're taking for this session.So you can see, SAS
Visual Analyticshas a variety of
different users.We, for this session, are
going to primarily takethe analyst role.And we'll be using
SAS Visual Analytics.I'm sure some folks out there
might be data scientists,and while you'll use
SAS Visual Analytics,you'll probably spend more time
in Visual Statistics or VisualData Mining and
Machine Learning.And then we have our
SAS administrators.Again, analysts might
do a little bit of this.And their primary role would
be to prepare the data.And we'll use SAS
Data Studio for that.We then have our administrator.This class isn't really
designed for administrators,unless they take the role of
analyst or data scientist.And in some
organizations, they do.The administrator's
primary role isto make sure the
environment is healthy.So they can use the
Environment Managerto look at all of those
nodes that I talkedabout previously,
to assign rolesor capabilities to the users.And then we have our
information consumer,and you can see they have access
to Visual Analytics as well.But the only thing that
information consumer'sgoing to have access to is
the viewing of the reports.They will not be able
to edit the reports.Actually, they could maybe
edit some pieces of it,and we'll talk about
that a little bit later.OK, we are going to jump in to
the demonstration that gives usa quick little
overview of SAS Drive.This is SAS Drive.So I've logged in, and
this is where I land,and this allows for us to
organize our information.We can organize our reports.We can reach all
of the applicationsthat we have access to.So in the upper
left-hand corner,there's three
horizontal lines, and Ihave access to all of
those applications thatshow here in that dropdown.The centerpiece here--
the upper center piece--is called our quick access area.So if I had some
item that maybe Iwas going to use on a
fairly frequent basis,I might want to drop it
into the quick access area.So over on the far right-hand
side, you'll see my login.I logged in as Eric today.And under Eric, I have
three little dots,and those give me
the menu optionsfor this particular application.It also shows that the quick
access area has been expanded.So when the arrows
are pointing up,then we have this
real estate available.If you decide that
there's not reallyanything you want to put
in the quick access area,then you can minimize it,
and you have more areafor the center canvas.So as we look
through this, you cansee I have some SAS
videos in my folder.So if I go to my shared
content in the Public folder,I have a couple global
forum reports that I woulduse while I was demonstrating.If we go back and open up--if I select the little
plus, it actually looksat my information and
makes a suggestionas to what would be good
in my quick access areabased upon what I've used.Well, I've just logged
in, so it doesn't actuallyhave any recommendations for me.All right, well, we're
going to move on,taking a look at some of
the data visualizations thatwill help us understand
our data better.During this session,
we're going to take a lookat a variety of
different visualizationsthat will help us
understand our data better.So the goal of this is to
do some data discovery.Maybe we've been told there's
certain issues in maybeparticular locations
or regions, and wewant to do a little
geographic analysis.Maybe someone has lots and lots
and lots of distinct valuesand they want to have an
easier way to look at them.Maybe you need to
just understandthe shape of the data.Do I have a lot of missing data?Do I have a lot of outliers?So let's take a look at
some visualizations thatmight help us explore
our data little bit more.So first, we have to get
that the data into CAS.And again, CAS is our server.It provides us
access to the data.And we have a couple
of different toolsthat we could use to
get data into CAS.So Environment
Manager would be a waythat a SAS administrator
would load data.Typically, end users
or analysts are notgoing to be loading data
through Environment Manager.We may use the import function
in SAS Visual Analytics,and that allows us
to load the data.And we can make some
report-level changesto the data, but
no changes that aremade in SAS Visual Analytics
would be actually storedwith the table.They would be changes that
would be stored with the report.If I want to make changes
that allow me to edit the datasource, then I would need
to use one of the toolsin the lower portion
of the slide--so anything like SAS
Data Studio, SAS Studio--if you're SAS programmer,
SAS Enterprise Guide.If open-source, then you can use
open-source to make some datachanges.And then you want to store
that data in an area thatis accessible to your analyst.So maybe you have a
public folder, somethingthat other people can get to.In Visual Analytics, we
can easily import our data.So from within Visual
Analytics, whenwe go to open a data source,
we have the option to import.Once we select
Import, then we havethe option of what type
of import do we want.Is it local?Is it server-based?Is it social media data?So once we've picked the
type, whether it's an Excelspreadsheet, or
it's a text file,or maybe it's a SAS
data set, once we makeour selection as to
what we want to import,we'll then be presented
with a little wizard thatwill walk us through it.Once the data gets
into Visual Analytics,then it's broken down or
categorized into two types.It is either categorical
data or measure data.So it's character data
and numeric data--just those two types.Note that, in the
character or category side,we see date times.So Visual Analytics
uses date timesas a categorical
or character datawhen it's generating reports.Now, if we need to use
the date time as a numberto do a calculation,
we can do that.We just need to add in
one more little stepso that Visual Analytics
knows to use it as a number.Within Visual
Analytics, we can modifylots of the properties
of the data items.We can create new data items.We can add a variety
of different filters.We can add filters on the data.We can add filters on
an object in the report.Our goal here is
to prep our dataand then use that to discover
some of those patternsor trends, explore
relationships.We want to have a better
understanding of our data.In Visual Analytics, there is a
visualization called automatedexplanation, and automated
explanation will gointo a column and basically run
as many algorithms on it thatit can't.So it gives me this very
high-level summary informationabout the column I selected.So here the column that was
selected was days to deliver,and then it gives me the
statistics-- the descriptivestatistics regarding
days to deliver.And then it looks
at the relationshipbetween days to deliver and
other columns in the data.It also will group values
based upon the average valuefor the days to deliver.So it goes in and
it runs, like Isaid, a bunch of
different algorithms,and then it
determines what wouldbe the most important data item
to explain or understand daysto deliver.So what other data
item in my tableis closely related
to days to deliverand can provide some
explanation on days to deliver?And that would be
order type here.So I'm going to jump in,
and let's do a little demo.OK, so we're going
to take a quick lookat the automatic explanation.Here, I've just opened
an empty report.The reason that I've
opened this and I justdidn't start with a new report
is because the data has alreadybeen assigned to this.That way, I don't have
to pull in my data.Again, for the demo, I'm
going to stop my videoso that it doesn't hide the
upper right-hand corner'soptions.First, I'm going to minimize
this Start From a PageTemplate--which is kind of a nice thing.It'll give you some
sample templatesif you're not sure the best
way to set up a report.But we're going to take a look
at this automated explanation.So let me go into the
data, and then I'mgoing to right-click
on Threat Level.And let's go to Edit.So basically, what
this did is ittook a column called
conservation statusand created a new data item.And all of these are values
for conservation status.And they all are basically
not threatened species.So this data that
we're looking at--it's the threat level for
the species becoming extinct.So then we have ones
that are threatened,and they are endangered,
proposed endangered, proposed,threatened, and then we
have our extinct, whichis only those that are extinct.So we've decided to create
this new value threatlevel, this new column--new data item threat level.And it has possible--five different values.So I'm going to go ahead
and hit Cancel, because thisis exactly what we wanted.We've got these three
plus missing possibilityand species of concern is
also a conservation status.So that is my threat level.And we leave the
missing in therebecause that is
species that have notbeen identified as a threat.They don't have a status
yet, so they're notthreatened or extinct.And then the other value,
that species of concern,would be species that might
need a proactive protection,but we don't have enough
information at this pointto categorize it.I would like to know which
of my other data itemscan help explain
the characteristicsand the contributing factors
for the threatened species.So I'm just going to
go to the threat level,I'm going to right-click, and
I'm going to select Explain.And I want to explain
on the current page,because I don't have
anything on this page.So threat level is
my response variable,and pretty much, the other data
becomes underlying factors.And let me click away from that.So you can see that the category
is the other data item thatcan explain threat level.So let's go ahead and filter
out some missing values,because I think they're
skewing this output.I'm going to pop over to the
filters on the right-hand side.And you need to have
your object selected.Select New Filter.I'm going to filter
on threat level,and I want to clear
the missing values.And now you can see it
recalculates and gives mea little bit
different information.So now the order is going
to be most closely related.And because this is a
categorical data value,I can actually pick
the level of the eventthat I want to look at.So over in the top right-hand--
right now we're lookingat the extent--extinct-- sorry.And we could say,
hey, maybe we wantto look at those species
that are threatenedand what factors are mostly
correlated to threatenedspecies.So as we look through this,
we can also expand it.When we expand or maximize,
we get additional information.I'm going to scroll
this up a little bit.So it just gives me an overview.So what did I pick?I picked threat level.It removed or modified
5 out of those 13.It used a decision tree.And basically that's
what it's doing is here,it ran seven
decision trees to seewhich of those
underlying factorsmaybe played a part in
understanding the threatenedthreat level.The threatened threat
level-- that's a lot.And then we can look at
the screening results.So it tells you which of those
columns were either ignored--yeah, they only
had ignored ones.And then we have the
relative importance tab.And the higher or closer to the
number one, the more importantthat underlying factor is.We can see order is
the closest, so thatwould be the additional
data item that bestexplains the threat level.And again, we're looking at
the threat level of threatened.So let's minimize this back up.So here, the threat
level has a 12.48% chanceof being threatened.So it's the second most
common threat level.But we can pick other
ones, if we're interested,to see what part they play.So you can see how much
park name played a part.Let's go look at category.So you can see the relationship
between the threat leveland then that second
underlying factor for category.So each bar is split to show
that portion of the level thathas the threatened species,
which is the orange bars.And then the portion that's
non-threatened is the gray.And we can see that the vascular
plant is the most threatened.And then, when we come down
and look at the groups,it tells us that
certain vascular plantswhere in Hawaii have an 82%
chance of being threatened.Again, they're using
decision trees to run this.Here is order.If we're interested, we
can derive a data item--a new data item out of this.Let's go back to category.And we're going
to pick this high,and let's derive a group item.Oh, and then it's going to
put it in our data pane.And it did it twice
because I clicked it twice,so let me go remove it.There we go.All right, so here's
my derived data item.And then this should
have only two values--whether the species is
in or out of that group.So it's either in or
out of the category.So let's take a look
at the species thathave a high chance
of being threatened.We're going to go and
pick the park name.Let's clear this.Pick the park name,
and common names,and let's add those
to the new page--to a new page.It's just going to
give me a listing.And then this is
all of them, but I'mgoing to use that new derived
data item to filter outonly those that are threatened.So I'm going to
select the new filter,I'm going to use it
on this threat level,and then I'm going
to clear the outsand clear the missings as well.So now I get a listing of
all of those common namesfor the items, and
they should allbe in that-- in the
threatened threat level.OK, so automated
explanation is a great toolto learn a little bit
more about your data.And we are going to continue
with our data explorationby taking a look at
some charts and graphs.OK, so let's continue
in our data discoveryand use some charts and
graphs to understand moreabout our data.So we have a variety of
different data sources or dataitems in our data
source that we can useto learn more about the data.So again, maybe we want to do
some sort of location analysis.We want to see where our
customers are located,or maybe you're doing some
sort of factor analysis.You want to know where all
the plants are located.The previous example
was park data, so maybeI want to know where all of my
national parks are located--to lots of different options
for continent or data locationanalysis.We also want to
focus on profits,so we're going to be interested
in which product line isreturning the greatest profits.And we have some
other categorical dataitems that we're going
to focus on as well.One of the really nice
things about Visual Analyticsis the automatic chart.So in the previous example,
I selected two columnsand dragged those
over to a new page.It created a list table.So Visual Analytics looks at
the data that you've selected,and when you drag it over,
it picks the best fit.Here, we have a
categorical data item,and it happens to
be a geography item.So when we drag it over, it's
going to build a map for us.The second example is
a handful of measures.When you select
only measures, itlooks at the cardinality of the
data, which is the uniqueness.So do I have a lot
of unique values?If so, it's probably going
to generate a heat map.Otherwise, it's going to
generate a correlation matrix,which is what we see here.The listing is the
detailed level data.So we just saw a listing.I dragged over my park
and specie information,and it listed them out.Every single row in the
data that met that--well, I added a filter, but
every single row of the datawould get returned
before you filter it.A crosstab is summarizing
that information.So maybe I want to
summarize my data upby location, or product
type, or gender,so you want to have a
categorical data item thatdrives the summary information.So this is like a by column.So I'm summing this up
by the continent name.When we're looking at the
measures in the data--so the numeric data--often, we want to look at
the spread of the data.So a histogram does a great
job for a single measure.So this shows me the spread
of my numeric value quantity,and you can see the majority
of the data is in this threeor under.And then the further
out, our tailgets smaller and
smaller and smaller.On the right-hand side,
we have a box plot.This is another
great way to lookat the variability of the data.It also provides us
with the extreme values.This particular data
had a lot of outliers--so many that it couldn't use
a bubble to display them.It needed to use the
gray boxes, and that'sjust because there
were so many outliers.It couldn't just use bubbles.They would have all been
laying on top of each other,so it used the shading
scheme to do this.Bar charts are a great
way to explain your data.So here we have a variety
of different bar charts.We can have vertical bar charts.We can have horizontal.Bar charts we can have
stacked bar charts.And we can use it for a variety
of different information.So nominal values--
nominal valuesmean there's no
specific order to them.Later on, we'll take a look
at ordinal values, which meansthe data does have an order.Something like
small, medium, largewould be considered ordinal.Then we have our
time series data.That obviously needs
a date time value.We can rank our data,
and we'll take a lookat that in a little bit.And then we can also
do stacked bar chartsthat normalize to 100% so
we get parts of a whole.So let's jump into demo and take
a look at using our auto chart.OK, so you should be
looking at my SAS Drive.And we're going to go ahead
and start a new report.So let's go to the new--oops-- not there-- up here--and pick the Explore
and Visualize.Again, I'm going
to stop the video.We're going to start
with some data.So we're just basically
creating a new report.So the first thing we need
to do is go get some data,and I'm looking for
my products table.So we might have to drill in
to this data, and then let's--there we go.PRODUCTS_CLEAN is the data
source I'm looking for.It's available,
but not currentlyin memory, so over in
the far right-hand side,I'm going to click
the little lightningbolt to load that into memory.And as soon as my OK
is done in the bottom,then I know it's ready.That was quick.We'll select OK.So now we have our
products data source--and lots of columns.Quickly scroll through those.So you can see I have
some xy if I wantedto do some geography stuff--lots of categorical data.And we don't really know
anything about this data,so we're going to take
a quick look at it.But let's just talk about
the interface for a minute.On the left-hand side,
we have the data iconwhere we can go grab our data.Then these are all
the possible objectswe could use in our report.We also have some Suggest.And Suggest is a really cool
attribute of Visual Analytics.When I select Suggest,
it goes into the dataand it picks basically
some suggestionsthat might benefit you
to display your data.The outline basically
is just an outline.It shows me everything that's
in my report right now.It would be empty
because I don'thave anything in the report.If we look all the way
over to the right-hand sidewe see Options, and this
has to do with the objects.So right now these are
sort of generic options.Roles also have to
do with an object.We can set up actions.We can do some display rules--so maybe set some
perimeter parameterson how to color values,
add filters, and add ranks.And we'll take a look at all
those throughout the class.So I would like to
create a list table.I'm going to go to
my objects, and I'mgoing to take the list table
and drop it into the center.And then I would like to
add all of my columns,so I'm going to over to
the Add in the roles,and I'm just going
to select them all.Hold down my Shift
key, and select OK.This will take a
second because thereare a fair amount of columns.So the listing just shows
us all of the observations.It's going to stop--let's see if I go to my...you can override the
system data limit.I think it's 40,000, but it
definitely will exceed it.So it's basically giving
you a little sampleof all of the observations.So I'm going to
scroll through this.It's sorted by
city name, so we'llsee all the missing
observations first.And I just switch the sort.Again, this is a lot of data.OK, so there we can see our
cities in descending sort orderso that we don't see all
those missings first.All right, so I want
to focus on profit,but as I scroll through this, I
don't see that I have a profit.So that seems like something
we're going to need to create.I do have cost and I
do have retail price,so let's maybe start
looking with those.Let's go to the
data icon, and we'lllook at the cost and retail
price by the order type.So I'm going to
select order type,and then I'm going
to select cost,and then I'm going to
select retail price.And then I'm just
going to right-clickon any one of these
gray areas, and we'regoing to add it to a new page.So I'm on page 2,
and now I'm lookingat the cost versus the retail
price for each order type.So we have three order types--you can buy stuff
in stores, you canorder items to be
at the catalog,you could order things
on the internet.So let me give this a name.We'll go over into objects.You need to make sure that the
object that you're working withhas been selected, so there is
a little black-- faint black boxaround my object.And I'm just going to give this
a better name than Bar OrderType--so Cost and Retail
Price by Order Type.And then I think we'll turn
off the automatic title.So we'll select No Title--gives us a little
bit more space.You may have noticed the
bars just all shifted up.So if we go and
look at the roles,now that we have an
object, you should seea variety of different roles.When I showed that
to you previously,there wasn't anything
in there because Ididn't have an object,
and each objecthas its own set of roles.So here you can see our
category is order type.We have two measures.You can add many more.We could group it.Lattice columns means that
they're individual charts thatsort of lay next to each
other sharing an axis,and then the rows are
just the other direction.We can add data to values.So this is my data tip.When I hover, it
tells me that thisis the cost and retail
price for retail sale.So maybe there
was another factorthat I would just like to
know, but I don't want it to--we'll select Frequency--I don't want it to modify
or impact the actual graph.So here, nothing's changed,
except frequency is nowadded on to the data tip.So again, this is
your data tip value.We can also animate it.We'll see animation
a little bit later.And then you can
have hidden roles.So maybe you would like
a value to basically dothe opposite of
the data tip-- isI don't want that
data item to show up,but I do want it to
impact the graph.So we can talk about those
in a little bit as well.OK, so let's go and take a look
at creating some new data itemsand applying filters.So creating data items--and you can see there's a
little dash line going downon this slide--the items to the left can be
created in Visual Analyticsor using the data prep tool--so SAS Data Studio.Or if you know SAS code, you
can use Enterprise Guide--SAS Studio-- not Data
Studio, but SAS Studio,if you know how to write code.We can also create them
within Visual Analytics.The other ones on
the right-hand sidecan only be created
in Visual Analytics,so I can't create a geography
data item in Enterprise Guide.So these are items that need to
be created in Visual Analytics.So let's just take a
quick look through these.We have calculated data items,
and that would just speed,line by line, every
observation comes inand a calculation is performed.So you can see the salary
comes in, it gets an increase,and there's a new
salary written out.But sometimes what
I want to do isdo this calculation
on a group, and thatwould be an aggregated measure.So here I'm going to sum
up all the US salaries,and then I'm going to sum
up all of the UK salaries,and then I can do a
calculation to lookat what percent of my
salaries fall to the USand what percent fall to the UK.So that would be an
aggregated measure,because I'm grouping
it first and then I'mdoing the calculation.We can also create
custom categories.We have a point-and-click
tool within Visual Analyticsto do it.So earlier, when I showed threat
level, and we went into Edit,and we looked at the different
categories for threat level,those new categories
had been createdusing a custom category.I could also use code.I could do some boolean
logic to create it.They produce the same results.So whether you want
the point and clickor you're going
to write some codewith the new
calculated data item--either way, it's going to
produce the same output.Now, filtering data-- we've got
a couple of different options.So filtering the data as
the report designer meansthat I'm going to
filter maybe on input,so I might want to bring the--filter the data
when it comes in.If you do a data source filter,
that will filter the dataand update those
distinctive counts.So when we looked
at the Data pane,all of the categorical values
had a number after them,and that is the number
of distinct counts.So if I add a data
source filter,those numbers will be updated.I can also do them
in the right pane,and we saw that earlier, as
well as I removed some missingvalues.And that would be
a basic filter.Now, I could also do one
on the right pane thatwould be considered
an advanced filter,and that might be a filter
that uses some comparisonlogic or some
expression-based filter.That would be
considered advanced.We can also do
post-aggregate filters,meaning I group my
data, I sum it up,and then I add the filter.So the filter would be
on the grouped data.And these are all options when
you are the report designer.When I'm the report viewer,
meaning just the informationconsumer, I have different level
of filters that are available,and we'll talk about
those a little bit later.So we can do some location
analysis by using our geo map.We can do coordinates.We can do region coordinates.These are both useful when
I have an even distributionof my data.So because the
region coordinatesare plotting at
a central point--so it's picking one spot--so here, all of these
are very clustered in,but it's a center
point of each state.Because it's presenting
information for a region,you want to have the average
value for that region.If we have very dense
data, we might wantto switch to a contour map.And again, we can do just
region maps if we have--we can do unique regions if we
have polygons or shapefiles.So within Visual
Analytics, you'reprovided with a handful of
generic name or code lookups.So country names are codes.State/Providence
names are codes--US state abbreviations or
names, and then US postal codes.They all come with.They're all predefined.If I have my longitude
and latitude,then I can do custom
geography data items--so what is the longitude and
latitude of every university--and then I could plot that.If I have those shapefiles,
if I have a polygon provider,I can then load those shapefiles
and use that geographic dataprovider to do unique
regions or somethingthat needs to be outside
the scope of these defaultlocations.So maybe you want to
do school districts--anything that would have
its own unique area,you would need a shapefile for.Hierarchies are a great
way to nest your data.So we're going to start at
the top level, the valuewith the least number
of distinct values,and work our way down to
the most detailed levelof the data.So in the end, month will
be rolled into quarterand quarter will be
rolled into year.So I may have four
years of data,so I'd have four year values.When I drill in, I would
then see the quarterfor each one of those years.And if I drill again
to the next level,I will see the months of
the quarters for that year.This is a great way to
display a lot of data.So you could have massive
amounts of data storedin a hierarchy, and
the viewer is onlygoing to see the data
that they've selected,so they're only going to
see the data that they'vedrilled into and expanded.All right, so let's take a look
how we can create some new dataitems.So we're going to create
some new data items.I'm just going to add a new page
and start with a fresh slatehere.And I would like to create
a new geography data itembased upon the continent name.So I'm going to go to the data
icon on the left-hand side,and then I see the
continent name.Oops-- that's
supplier continent.Let's do the right one.There we go.And I've got this
little dropdown.So I'm going to select this
to edit the properties--tells me that it's
continent name.That's correct.I want to modify the
classification, though,to geography.So continent name stays.We could edit-- modify
this if we want,but let's drop down
to the geography data.We're not going to use
any of the lookup codes.We're actually going
to do a custom.So we're going to select
longitude and latitude in data.And so in my data, when I drop
down, I have an xy for latitudeand an xy for longitude.So let's match them up--xy continent lat and
then xy continent long.And it shows me 100%
of my data was mapped.So I should have five
geography data items,because I had five
different continent names.And you can see from this
tiny little screenshotthat it's mapping
it into the center.So here's Africa, and it maps
it into the center of Africa.I'm going to select OK.So let's go ahead and create
another geography data itemwith country.So here's my country.Again, drop down
to edit properties.Switch this to geography.And then we're going
to keep the name.It is using one of the
lookups, and it happensby using the very first lookup.Here are some of the
other lookups, though.So if this was those two-letter
ISO codes or states--but it happens to be the
country or region names.And I am 100% mapped.This is a winning strategy.I'm going to select OK.Within this, I'd also like
to create one more, whichis my US state names.So I'll go to State
Name, and I'll drop down,and we'll switch
this to Category--I mean to Geography.I'm going to actually
change the name of this one,though, and call
this US State Names.Oops-- and we are going to use
a geographic name or lookup,but we're going to use
our US state names.And you can see only 19%
of my data got mapped here.And it shows me just a
little bit of the listingof the unmapped values.So we have this geographic
data from all over the world,but we also have just
these US state names,and so I'm going to need
to add a filter justto focus on the United States.But we'll leave it as is now.And actually, I guess they
don't want me to add US.I'm going to try to follow
with the demo script,so let's leave it as
is, and I'll select OK.And then finally, we're going
to grab the postal code--do the same-- drop down,
Geography, postal code,and then locate
those US ZIP codes.And we see the same thing.Only a portion of
our data gets mapped,and that is because
we're looking at datafrom all over the world.And we're going to fix that,
but let's just select OK.So let's take a look at
creating a hierarchy.We're going to go to our new
data item, select Hierarchy,and then we're going
to call this location,and we're going to drag
over those values--so continent at the top.Then country-- within
country, we want states,and within states,
we want postal codes.So I'll select OK.And then let's deploy
this in a geo map.So I'll go to my objects,
and I scroll down,and there's actually a whole
category for geographic.And I'm going to create
the coordinate map.So it builds that
map, but you can seeI don't have any data assigned.So over on the right-hand
side, I can assign my data.I could also pick
it from here too.So you can drag and drop them.When I do that, you see that
location gets populated.So these are all of my--they're not that large,
but you can see my bubbles.And then, just to take a look
at the geography hierarchy,as I drill into
Europe, take a lookin the upper left-hand corner.We have trailing-- these are
sometimes called breadcrumbs--and then that lets you
drill through this.And you can always switch.There we go-- switch
to North America.And you can see this is my
point about it putting itin the center.So when I drill into
this, those statesget-- the dot gets
put in the center,and this is why we would
want to be representing, say,an average value for Nevada.And we can go back
to the top levelby just selecting the All.So let's add some more data
to this because that's justlooking at the frequency.So we're going to
add in the discount,see what size discount we're
giving for each location.And we need another
one-- retail price.So this discount
is a little odd.And I'm going to go
take a look at this,because that seems like
a weird thing to me.So when I see something
like that, I'm like usually,I need to go investigate.So we're going to take a
look at the properties,and I see that the
aggregation is the sum.So what this is doing is
it's taking every averageand summing them up together,
meaning, if I had 10 storesand one store was doing
a 5%, another storewas doing a 10%,
and then another 5%,it would be adding them up, so
it'd be at 20% at this point.And that doesn't
really make sense.I probably want an
average discount,so I'm going to change
this to average.And then I probably
want to rename it too.And let's double-check what's
going on with retail priceas well.And that is doing
the sum as well.So this is the
total retail pricein millions for the entire area.Again, I think average
would be better.So average retail price tell us
a little bit better of a story.So let's go back.We can see the largest discounts
are in Asia and Africa.And you'll also see
later down the roadthat they have the
lowest profits--so a little bit of
a problem there.Well, let's take a
look at North America.And we'll double-click
this, and then Ihave my Canada and
the United States.So we can drill into
the United Statesand then throughout
the United States.And I'm just going to zoom in
a little just using my mouse.Zoom in.There we go.So it looks like South Dakota
is giving a really big discount,but it has a really low average.In fact, it might be the
lowest average retail price.So places that are trying
to move products maybearen't bringing in as
much money because theyhave to get a big
discount to sell it.So let's take a look at Texas.I'm going to double-click
Texas, and these--and now let me zoom again--these are all of the
ZIP codes for Texas.And maybe I'm interested
in Austin, Texas.So if I knew Texas
really well, Imight be able to expand and
drill in the Austin area.But in the upper
left-hand corner,we have a pin that allows
us to search for value.So I'm going to go
ahead and do that.I'm going to select
the little pin,and then I'll just start typing
Austin, and it should pop up.And there's Austin, Texas.So it drops pin
for Austin, Texas.Maybe I would like to look at
the 50 miles around Austin,so there is an arrow that
gives me more options.One of them is the
geographic selection.So I'm going to do
that and I'm goingto look at not five
miles around Austin,but I want to look at
50 miles around Austin.There we go.And let's draw the selection.So now this shows me all of the
ZIP codes in the 50-mile radiusaround Austin, Texas.We could even go
in and filter this.So if we wanted to
do a new filter--now it's built a filter, and
over in the right-hand side,if I go to the
filters and I hover,these are all the ZIP codes in
a 50-mile radius from Austin,Texas--pretty cool.All right, we have a
little bit more to cover,so let's pop back to our slides.OK, so let's take a look at
our lesson section in the datavisualizations by investigating
some data analysisvisualizations.So we have some graphs
that are a little moreanalytic in nature.The bubble plot allows
for three measures.So we have our x, our y,
and then the size, or the z.In general, we want to also
have a categorical valueto summarize this information.Otherwise, the
bubble plot is goingto attempt to display a bubble
for every single observationin your data source, which
is probably not a good idea.The train map is also
a really great wayto display categorical
data with two measures.So here we can see the color
is the number of orders.So it looks like clothes with
the most number of orders.And then the size
is based on profits.So outdoors generated a higher
profit, but not as many orders.And we'll take a
look at using these.We can also use a
correlation matrix.This is going to allow me
to use up to 60 measures,and it will look at
the relationshipsbetween the measures.And it gives you
additional informationin the analysis
section so you can takea look at those relationships.We also can use a scatter plot.Scatter plots are only
going to be usefulwhen you have two measures.We have a heat
map, and a heat mapis great when you have
high cardinality measures.So cardinality
means that there'sa lot of distinct values.So something like ID would
have high cardinality.Maybe retail price could
have high cardinality--if it's actually looking at
every single retail price, notthe sum of the retail prices.Here we're looking at total
revenue and unit cost--lots of distinct values, so
it's going to use the heatmap to display that information.We can also use a
category in here as well.We can add fit lines to
scatter plots and heat maps,and by default, it will plot a
linear fit, which would be niceif all of our data was linear.So as unit cost rises,
so does total revenue.Unfortunately, not
all data fits that,so you do have the
option to use best fit,and best fit will then select
the most appropriate modeland display that fit line.You can see a little
of our options here.If we have time data--so time series plots are great.We can also just
use a line chartto look at data over time.So a line chart also is
great for ordinal data.I mentioned earlier
that we would takea look at some ordinal data.So this is customer age group.There is a specific
order in this data,so that is why we
call it ordinal data.We have the youngest
to the oldest--versus the nominal
data, which meantthere's no relationship
between the categorical values.So something like
states would be nominal.We can also do forecasting.So forecasting is pretty cool.It takes a look at
the historical data,and based on that historical
data, it builds a model.It takes that model
and applies itto the data in both the
historical observationsand the predicted observations.So when it applies the model
back to the historical data,it's called a hindcast.And so on this
slide, you can seewe have a few outliers
in the hindcast,but generally, it's pretty
close for the predicted model.As we look to the
actual predicted values,it's a pretty tight
band, and this bandis your confidence interval.It looks like maybe
about three months out,that band starts
to get much larger,and this means it's
less sensitive.The blue shaded area's
the confidence interval,which is saying that it predicts
a future value, say, in Mayof 2014 would fall
anywhere-- and you can hoverand it'll tell you the upper
point and the lower point.So the wider the band, the
less sensitive or precisethe forecast is.Let's take a look at using
some of these visualizations.So we're just going
to add a new page.Click the little
plus next to page 3.Let's take a look at some
of our measured data.I'm going to go to my data icon.And we want cost
to be average cost,so let me clear that selection
and drop down next to cost.Sorry about that--
clicked outside.Here we go.And let's change it
from sum to average.So instead of total, we're
going to have it b average cost.And let's do the
same for quantity.Change the name here so it's
better a representation.OK, and we're going to start
with a correlation matrix.I'm going to select average
cost, average discount,average quantity, and
average retail price,and I'm going to just drag
those over and release them.And because they're
measures, it'sgoing to go ahead and
do a correlation matrix.And if I hover, the darker
the bluer or the brighterthe blue is a
stronger relationship,and it tells me that the
correlation is 0.779.So as average cost increases,
so does retail price.If I want to learn more
about this relationship,I can expand this
and take a lookat how it compares to
all the other ones.So I would like to do a little
further analysis of this,so I'm going to select it, and
I'm going to do a right-click,and I'm going to select a
new object from selection,and I'm going to
look at a heat map.So it's hard to
see the heat map.I'm going to drag it
to the right-hand side.There we go.So when I look at this
heat map, I immediatelyam noting that I have this one
very, very, very dark blue baror bright blue bar.So it has a really
high frequency.It's got 265,186 observations
fall in this 1,250 to 3,750range, and a retail price of
negative 100 to positive 100.So I'm not sure that's
great for my business.I have a losing value.Also, let's change the fit line.This does not look linear to me.I'm going to right-click,
select Fit Line,and switch it to best fit.And it should update.It should update.That is not-- I'm going
to do it myself then.Oh, I think we want to switch.Sorry, guys.Let's switch.Let's go to the roles, and
I want to switch these.There we go.Now we're looking at it.So here's our cubic fit
because we want to see how--when average retail
price increases,what happens with average cost.And we can see that
goes up as well.All right, sorry about that.Well, that brings us to the end
of the data discovery piece,and we're going to quickly
demonstrate some reportingoptions.So the focus of our session
was really on data discovery,but without introducing
reporting in Visual Analytics,you would be missing a
lot, so let's take a lookat creating a simple
report and thenadding some options that can
make your report interactive.So in designing
your reports, youwant to keep in
mind a few things.You want it to be
easy to navigate,so add things like
descriptions and instructions.You want it to be attractive,
so be careful of the colors.Multiple pages is great.You do not want to
overcrowd a page.And then you want
to use a varietyof the different actions and
tasks that are available sothat your report is versatile.A lot of times, people
overdo it with a report,so keep in mind that
your report should beappropriate for the audience.Not everybody will
understand the datalike you do as a designer.Also, you want it to
tell a single story.If you feel like you're going
off into tangents or sidebars,you might want to create a
second report or a hiddenpage--which we'll talk about--to showcase that
additional information.And then you want to keep
it visually appealing.I know there's lots of
different objects available,but if you can tell the story
with the most basic object--maybe a bar graph--then that would be the
best object to use,even though we have a lot of
really fun visualizations.So let's just take
a look at this ideahere about graphs for reporting.I have a word cloud and
a donut or pie chart.Word clouds are very
popular and appealing.People love donut
charts, or pie charts.There's a little bit of
a question here as to,is this really the best
way to display the data?So here, Madrid and London
are very, very close in size,and I would be hard pressed
to pick which one had more--pretty sure this
is frequencies--but pick which one
had more values.And then we have a similar
situation over hereon the order type.Which is larger-- catalog
sale or internet sale?We can always expand the
objects to look at the table,but we really want our consumers
to be able to understandthe story at a quick glance.And sometimes, even if they
are a very popular object,they may not be the best
object for our data.So it's very difficult to
compare the relative sizein slices of the pie charts.We have lots of
dual axes options,and these are
fantastic when you havemeasures with different ranges.So we have percentages,
and then wehave millions
totals values here.Here we have a
count of employees,and then the facility
efficiency rate--inches for the rainfall,
temperature in Fahrenheit.So you can see lots
of different optionsto display the data when
you have different measures.So let's take a
quick look at thisand create a simple report.OK, so we're going to
start a new report.I kind of like to start and stop
on my home or SAS Drive page,so I'll go to my dropdown and
select Explorer and Visualize.And this was that
last report, but Iwant to create a new report.So over in the far right-hand
corner, I will select New.So now I need to
go grab my data.Again, I'm going to use
that products table--so products clean-- and
select OK to add that data in.Because I'm generating a report,
I may want custom titles,so I'm going to go ahead and
add that as an option on my ID.So in the far right-hand
corner, when I select the E,I'm logged in as Eric, and I'm
going to select my settings.Within the settings, I
would like to go to General.And as I scroll through
this, I get to the titles,and I would like to set this
to custom title so that--because I'm in
the report side, Iwant to just make this a
little more user-friendly.I want to make it easier for
my consumer to understand.So we made some changes
back to the datain the previous report.Those changes do not get
carried over unless I save themas a data view, which we didn't.So I'm going to just make a
couple quick changes here.I'm going to change the
aggregation of retail priceto average, and then I also
want to update the name--Average Retail Price.And I'm going to start
with a pie chart.So we'll go to the objects,
and I'll locate my pie chartand drag that over.I'm going to go ahead and
add in the order type.So I'll expand the add
and select Order Type.So right now this
is the frequency.So if I hover, the
majority of my datacomes from the retail
sales, and it shows methat there were 715,970
observations for retail sales.Well, instead of
the frequency, I'dactually like to look at
that average retail price,so I'm going to select the
frequency under the Measureto replace it, and now I'll
select Average Retail Price.So now I have these
three sections,which are kind of hard to tell
apart, but if we expand this,we can then see that those
values are quite close.The retail sale-- while it
had the most observations,it has the lowest average price.So the purple is a bit lower
than the internet or catalog.Let's make a couple
more changes to this.And over here in the untitled
section, I can add in my own.Actually, let's
replace untitled.OK, so Average Retail
Price for Order Typeis going to be my new title.So as I mentioned,
pie charts are notthe best visualization for
values that are close together,so let's change this.In the upper right-hand corner,
I have these three dots--upper right-hand corner of
the object, I have three dots.And Change Pie to
is one of those,and it recommends a bar chart.So now it creates
the bar chart, and Ican tell this is much
clearer that retail price isless than catalog
sale, and catalog saleis less than internet.But that was very hard
to tell in the pie chart.I'm going to go ahead and
just name the object itself,so let's go here to our--and we're just going to
call this Order Type.This is just the object name.We still have the
title in there.And then we can clear the
axes, because we don't--we know what that is.We know that that's order type.So let's clear that axis's label
and clear that axis's label--just gives us a little bit more
real estate for the report.OK, so that's the beginning.We are just going to do
a simple report to start,and then we're going to talk
about making this reportinteractive.So let's pop over
to our next section.So we just created a very
basic report with a bar chart.We're going to talk about things
that we can add to that reportto make it interactive.One of the new things in
version 8.5 of Visual Analyticsis viewer customization.So I mentioned earlier that the
information consumer can onlyview the report, and then
I said, well, maybe theycan make some changes.So as the report designer, you
get to decide, at what leveldo you want your viewer to
be able to make changes?So the default is
comprehensive edits,and comprehensive edits
means that it could alterthe report in a meaningful way.So I could change
an object type.So currently, we
have the bar chart.If I come in and I have
comprehensive edits,I could make a line chart.I can make it--turn it back to that pie chart.So you want to be careful
about this default.Simple edits lets
them make changes,but not impact the original
intent of the report.And then data edits
has the most impact,because they could
even change datafor certain objects
in the report.So by default, they can
change objects, and again,change the intent of the report.And you get to set this up
and decide what you wantyour viewers to be able to do.So typically, my report
will have multiple pages.You want each page
to tell a storyor focus on a single idea.Limit the number of
objects on any given page.Limit the number of pages.If it looks like you're
having seven, eight pages,you might want to think
about having a second reportthat you could
link to the reportso that the initial report
is not so overwhelming.We can also use hidden pages.Hidden pages are pages that
only show up by a prompt,meaning, if I make a
selection in the bar chart,then my hidden page will load.Hidden pages are fantastic to
provide additional information.We can also filter data from the
viewer's perspective, meaningI can set this up so that my
viewer can make selectionsat the report level,
at the page level.We can set up interactions
between objectsso they could filter them.We can set up links
so that one pageopens another page or
a selection will open--take you to a URL.So we've got lots of
different interactive options.When we set up these
prompts, we havesome choices of the
different types of prompts.We can do a list control, which
allows multiple selections.We can use a range slider.That's good for
numbers and/or dates.Button bars are
great when you havejust a few distinct values.This is a toggle, meaning
I make the selection--it filters everything
based on that selection.If I click this selection
again, it clears it.Dropdown list-- so if I have
a lot of distinct values,then I might want this
smart-type dropdown list.So I start to type, and it
shows me all the possible valueswith those characters.And then we could
just have a dropdown,and this is useful when
you have a moderate number.What does moderate mean?It's really subjective.You could have a dropdown
list with 10 items.You could have a dropdown
list with 50 items.It's whatever you
think is reasonable.I think 50 is a bit much.I would probably not have a
dropdown list with more than 20or 30 items.But again, that's subjective.Page prompts and report
prompts allow your viewerto make selections.If the prompt is at
the report level--so it's above the page--then it will impact
everything in the report.This report only has one page,
but if it had multiple pages,those selections--Europe and plush-- would
be limiting or filteringall the objects on
all of the pages.So this also has some embedded
objects that are filtering,but because these report
objects filter everything,they also filter the
other control objects.So you can see that we're only
getting European countries.And for the product,
we're only seeing plushes.We've added in this
additional range sliderto limit our customer
satisfaction range.And then we can make a
selection on a country,and we can make a selection
on a plush object.All right, so
let's add in some--a demo and demonstrate
this to you.OK, so I want to change the
viewer customization level,or at least take a look at that.On my options, if
I expand general--then I scroll through this--Viewer Customization
is towards the bottom.And then you can see
the default levelis these comprehensive
edits, and thatmeans that they
can change reports,which might make a change in
the story that you're telling.And we've actually decided
to give them data edit.And that is the most
powerful, and it allowsthem to even change data--so lots of capabilities here.All right, let's
start a new page.And this applies to the whole
report, that customization.And because we are on the report
side, we're going to name this.Earlier, we were
just adding pages.So page 2 is going to be named
Supplier Analysis and page 1is going to be named--there we go--Order Analysis.OK, so let's go to our
Supplier Analysis page.I'm going to go ahead and
add a dual axes bar chartto our Supplier Analysis page.So I'm going to go ahead
and add order month.Click on the Categories, select
Add, and we have Order Month.12 months-- that seems right.And then we want to look
at the average retail priceand the average amount ordered--average quantity ordered.And it looks like, maybe
starting here in April--but look at these months.They're not in order.So we want to sort that.I'm going to right-click,
and I'm going to select Sort,and I want to in
ascending order.All right, now let's look at
this average retail price.Looks like the fall
we're doing OK.I'm surprised that we're not
making more money in December.And the first quarter, we
seem to be doing pretty good.And then in the
summer, not so great--we're not making as much money.But people are ordering
a lot of stuff.All right, so let's
continue with creatingour interactive report.And I want to add a
bubble plot to this.I'm going to go to my objects,
and select Bubble Plot,and just drop that to the left.Then we need to go
assign some data.So we want to create our
x-axis with the cost, and thenour average retail price.And for the size,
we're going to usethe average quantity ordered.And so down in the
far right-hand corner,I have this i, and
it's basically sayingtoo many values were returned.So it's just too much data.I want to look at this
by city, so if I goand I add in city
name, it's stillgoing to try to create
over 10,000 bubbles.I think there's like
10,500 different cities.And so it'll spin here.It might even return a
super crowded bubble plot,but usually it just
returns an error.So give that a sec.And so it has attempted to plot
all of those bubbles, whichis kind of crazy.I want to just look
at the top 10 cities,so I'm going to go
over to my ranks.And I should have
done this first,but I want to show you what
happens, because it's notalways obvious to
us when we're firstplaying with the data
what would happen.And so I like to demo it.I know it takes a little bit and
you got to sit there and wait,but you're only going
to do this once,and you're going to
realize, oh, it'strying to plot a bubble for
every single distinct city.And if I go to the data--let me scroll up for a seconds--and I hover, it tells me there
are 10,507 distinct cities.So now it's tried to
display 10,507 bubbles.So let's put a rank on this.Over on the rank side,
in the Ranks tab,I'm going to select
New Rank, and I'mgoing to go grab City Name.And I want to see
the top 10, but Iwould like to see the top
10 by product ordered--or quantity ordered.So let's give this a second.And change this to
average quantity ordered.And it's going to update.I also would like to look
at this over the year,so I'm going to go to my roles,
and scroll down, and selectAnimation.And we want to look
at this over the year.So now I have this
play button, and Ican see how the
sales has trackedover the year for each city.Let me pause this.Maybe I'm interested in London.That is my red bubble.And I'll select play.Now I can see how all the
other bubbles have interactedwith London over the last year.All right, let's continue
making this interactive.I'm going to go to my
Order Analysis tab,and I want to go ahead
and add in a page prompt.So let's drop down
to expand our area--Expand Page Controls.So we could do report
controls or page controls.We're going to do a
page control here,which opens up this
little bit of real estateat the top where it says drop
a data item or a control itemto create a page prompt.But if I wanted it
at the report level,I would have to go over
to the options in the farright-hand corner, and then I
could expand report controls.Or I could just
expand everything,if I was interested, but I'm
happy with this page prompt,so I'm going to go and
drop some data on it.And we can just drop our--whoops-- try this one more time.You want to drag this
until it says auto-control.So down here, it's
creating an auto-chart,but up here it's
doing auto-control.And so now, when I
click on Internet Sale,I'm only going to see
my internet sales.And then, when I
click, it clears it.All right, so this is a nice
quick interactive report,and I hope that there are
some takeaways from thisthat enhance your
Visual Analytics usage.And thanks."
116,"ATRIN ASSA: Hello and welcome
to another SAS Global Forum 2020virtual session.Today, we're going to
talk about bringingmore power to our report
viewers with SAS VisualAnalytics on SAS Viya.I'm Atrin Assa.And I'm going to be
sharing the cool new thingsthat your report consumers can
achieve with our new viewercustomization's capability.So let's go ahead
and get started.Now, if you've ever had to
build a reporter dashboardfor your organization,
you've probablyrun into a situation
like this in the past.You spend lots and lots of time.And you work with lots
of different people.And you build this
beautiful report.You build this
beautiful dashboard.You bring together all kinds
of data and information.You perfectly craft
each individual chart.You make sure it looks gorgeous.You make sure that it's useful.You make sure that it's usable.You go to
sas.com/beautifulreports.And you learn all the principles
about making a great report.And then you publish the report.And you get some initial
feedback about this report,and then it starts happening.The people start calling you up
or emailing you or chatting youon Skype or Teams or Slack.And they start to
say things like, hey,so on page 3 over here, you
used a bar chart to displaysome of this information.And I don't think the bar chart
is the right way to do this.Our team could
really use, insteadof this bar chart,
something like a pie chart.So can you go ahead
and make that changeof making that bar chart
a pie chart for us?And you kind of think
about it for a second.And you deliberately
decided on a bar chart,but you figure, all right.Well, maybe I can make that
change to a pie chart for them.And then about 30
seconds later, youget a call from another
team who's lookingat page 5 of this report here.And on that page, you've got
a geomap with a bubble ploton it.And that team,
they approach you.And they basically
say, well, you'vegot this geomap over here.That looks pretty good,
but I really don't like it.Instead of being
able to see a geomap,I just want to have a
simple view of this.So can you make that
geomap into a bar chart?And this kind of repeats itself.So people will come
back to you and askyou questions, for
example, about can youput some labels on this chart?Can you have this display--this table over here display
the sum of values for you?And so this creation of yours
that you spent so much time onto get right ends up
not serving the needsof all the different kinds
of people with valid concernsand valid needs that
need to be handled.So how do you
approach this problem?So how do you
handle this problem?Well, historically, what you
would end up doing 9 times outof 10 is that you would hear out
everyone's concerns and tweaksand changes that they would want
to make to this report of yoursor to this dashboard
that you've built.And then you'd
basically go throughand create half a dozen
different versionsof this report that
have basically 90%the same content but then
tweaks for each individual teamso that each
individual team can getthe perspective that they need
from that report or dashboard.And this, of course,
becomes cumbersome.It's difficult to maintain.And of course, when
you're set with eachof your half a dozen different
versions of the report,there's going to be more
feedback and comments and morerequests that you're going
to have to continually makeadjustments for.So your phone will
never stop ringing.And your inbox will
always be filledwith these kinds of requests.So the source of this
problem is that typicallyin a traditional way of
building a report or dashboardis that you are
predefining pathsfor your users to go through as
they interact with this report.And that means that
every path that your userwants to traverse,
you're going to haveto think of ahead of time,
figure out ahead of time,design and build and test
and make sure that itis working ahead of time.And this can slow you down.This can mean that you
have to build a very, verycomplicated report
in order to haveit meet the needs
of various users,or you're going to
have to basically builda bunch of different
kinds of reportsto better meet the different
needs of different users.And that can become a
maintenance problem.So what we've done with SAS
Visual Analytics is basicallysay, why can't you have
your defined paths,but also get a
little bit of freedomto explore and make that
available to your end user.So if you have the defined
path, you build the reportthat you want your
users to have,but then you give
them the flexibilityof deciding for themselves
where they want to go,what they want to do with
that report, all of a sudden,you can build a
fairly simple report.And then your end
users can customizethat report for themselves
and get the insights that theywant out of it for themselves.And that's what Viewer
Customizations is all about.It's about letting you
create those defined pathsbut then also providing
the viewers, the consumersof that report, the
freedom to exploreand the freedom to make that
report their own so that theycan get those insights, they
can make those small changes,without having to call you up,
without having to complicatethe overall report.Viewer Customizations is a
simple and governed way for youto handle conflicting requests
from your organization.It lets you build a report
that then confers some controlto the consumers so that they
have some freedom in makingit their own.And it's really simple to do.As you build the report, you
can set the viewer customizationlevels to any one of
three different levels.So at the bottom, you
have simple edits.At the medium level, you
have comprehensive edits.And if you really want to
have your consumers havesome freedom, some real
freedom available to them,you also have the
option to take itall the way up to data edits.So let's drill down into each
of these different levelsand see what they get you.So the first of these
levels is simple edits.And this is what you've
been used to with SAS VisualAnalytics in the past.So these are what
your viewers aregoing to be comfortable with.This is what you know and love.So you build the report.When you set it to
simple edits, youintend to basically
preserve whatyou intended to build as you
built that original report.So none of the changes
that the consumers can makewill fundamentally alter
or corrupt your intentwith the report.So users can do
things like sort.They can toggle legends.And they can toggle
whether or notvalue labels are showing on
various graphs and charts.So this is the
safest option if youwant to kind of continue to
have a very locked-down report.And you want to have
those defined paths.Then simple edits
is the way to go.But if you dial things up
to comprehensive edits,you gain one really, really
powerful capability--comprehensive edits.And this is actually
when you createa new report in SAS
Visual Analytics,this is the default setting now.When you set things to
comprehensive edits,you basically are allowing
the report consumerto start to wiggle
around and maybealter some of your
intentions, someof your original
intentions for the report.This basically gives
your users the abilityto change chart types.So for example, they might
want to change a bar to a pie,or they might want to change a
pie to a list table or a geomapto a list table.So comprehensive edits
allows them to do that.Next, there's data edits.So this is the case where you
want to give a lot of freedomto your end customers.You want your viewers
to be able to modifythe data with the
existing data that you'vebaked into the report.So for example, you
want your consumersto be able to change data
assignments, for example.So maybe you want
to let them swap outone measure on a bar chart
with another measure on the barchart.That way, they don't have
to call you up and askyou to do it or create
a new chart for themto be able to do that.They can just do that with the
report that you've given them.They might be able to
alter some of the filters.They might be able to
change those ranks.They might want to
make adjustmentsto the display rules that
you have added to the report.This allows those users to make
these changes for themselvesand not have to
call you up and askyou to make a special version
of the report just for that.So it does really open
things up for the viewers.And it kind of gets
them out of your hairand lets you just craft a much
more open-ended experiencefor them.So those are the three
different levels.So let's go ahead and look
at the Viewer Customizationsfeature in action.So I'm a report author.I've built my report here.And I'm ready to share
it with the world.But before I do that, let's
go ahead and go to our optionspane on the right.And let's go ahead and make sure
our report is selected here.And down below, you can see that
we have a viewer customizationcapability level setting here.So by default, of
course, as I mentioned,our capability level is
set to comprehensive edits.So this basically lets our
users make all the thingsthat they get in simple edits
in terms of changes and edits.But it also gives
them the abilityto do things like
change object types.So if they don't like,
for example, a pie chart,they can switch it to
a bar for themselves,or they can switch it to a list
table, or any number of chartsthat might be available
to them based on the data.So again, it frees
you up from havingto create multiple
versions of the report.And each user can customize
a report for themselveson the fly.So let's take a look
at how that actuallyworks We're going to
go ahead and switchto viewing this report.And now, I'm looking at
this report like a viewer.I can flip through the pages.I can see all the
different visualizationsI've added to my report here.And well, it looks like
I've got a bar chart here.And this bar chart's nice.But maybe instead of looking
at average transactionsdescending, I prefer to
look at it ascending.So I can go ahead
and right click.And I can make a
change to the sort.So instead of descending,
we'll go to ascending.And there you go.That's already making me as an
individual user much happier.So I've got all the
capabilities I'm used to before.But as you can see, I've
got some additional optionsas well.So for example, I can
change the grouping style.So right now, this
bar is stacked.I can switch it to clustered.And again, this is all
just happening for me.So it's not affecting
anyone else.This is just happening for me.And if I like that, I
can leave this report.I can come back to
it a week later,and it just becomes preserved
as part of my user state.If at any time I want to go back
to the original report statethat the report author
had created for us,then I can restore the
default report state.And that's going to take us back
to exactly what I had designedthis report with down to taking
us back to the first page.So it looks like it
exactly like it was before.But let's say I
don't like this bar.Let's say, I'd rather
see this as a list table.Again, I can right click
right on the chart.And I can say down
below change bar chart.I can change it to a list table.And I can do all
kinds of cool thingsto make this more useful for
me without having to go backto the report author
or an admin and askingthem to make these changes
for me, which wouldaffect lots and lots of people.So for example, if I
want to show a cellgraph on my average
transaction call,I can just go ahead and add it.And now, I have a bar chart
that's available to me.And I can see the
change and the variationin the average transaction
amount for my customersa lot more easily.So there you go.I just made some changes to this
using the comprehensive edits.And again, I didn't
affect anyone else.If I leave, and I
come back to this,I can pick up where I left off.And I can even save a copy of
this report just for myself.So I can save a copy of it.It's going to preserve
all these changes.And it's going to be
saved to my folder.I can open it up and go to it.And I can continue to make
changes to this as well.So I can change the sort.I can freeze all
columns to the left.I can make all kinds of
changes, change chart types.Maybe let's go ahead and turn
this box plot into maybe a barchart as a recommendation.I can see that information
that way for me as well.So that's comprehensive edits.Let's go ahead and
take a look at whatyou can do with data edits.So now, I'm back to editing
this report as an author.And you can see again my
report as I created itand as it's in
its default state.I'm going to go ahead back
to my viewer customizationson the right hand side
in the Options menu.And I'm going to dial this
up from comprehensive editsto data edits.I'm going to save my report.And I'm going to go back in
this case to a fresh view reporthere so we can start
from scratch again.So now, let's look
at some other thingsthat data edits lets viewers do.So the first thing I might
want to do is as a viewer,I might look at this
information and say, well,age group and write-off
risk are interesting to lookat on this box plot.But what if we could
swap out age group?Instead of age group,
let's look at our customersbased on their mobile use.And now, the box
plot will changeto show me my
customers' mobile useand what their write-off
risk is, in this case.So again, I can make some
of these data changeson the fly based
on the data that'savailable to this report.And I can then, again,
without having to go backto that report author
explore a completely new paththat perhaps the author
didn't originally intendfor me to explore but wanted
me to be able to if I wanted.And that's exactly
what I did here.Again, we're going to go ahead
and select our list table here.And you can see in our list
table if I go into my filters,we've got some filters
set on this list table.So the report author created
some filters and filtered outsome of the write-off
risk values.Well, maybe I want to look at
all the write-off risk values.Well, with data edits,
the report authorhas told me, yes, go for it.If you need to
make those changes,go ahead and make them.And I'm going to
go ahead make them.And now, you can
see my table showsme the full gamut of write-off
risk for my customers.So again, this is
a really great wayto help open up the report
without compromising thingslike security but still
allowing your users to workwithin the context
of this reportand get additional insights,
make new discoveries,and not have to be dependent
on you to provide themwith these customizations
and these changesthat they need to
be making decisionsand to be making a difference
in the organizations.So that's Viewer Customizations.With Viewer Customizations
and SAS Visual Analytics,you can take the guesswork
out of building your reportsand dashboards.You can't make everyone
happy with one report.But you can give
your users the optionto customize what
they see in the reportso that at least they can be
pleased with their own customversion of the report.And that's what Viewer
Customizations is all about.Viewer Customizations
helps your teams developa much richer body of insights.It just opens things up.It decentralizes
and democratizesthe process of
developing insights.And it's got
governance built in.You control how much control
that your end viewers have.So with Viewer Customizations,
anyone in the organizationhas self-service power
to go beyond the reportthat they see.And they are now allowed to
find and explore new insightsto consume and
share and hopefullymove your organization forward.Thank you so much.If you are interested
in learning moreor have any questions,
feel free to contact me.And I will be back in touch.Thank you so much for watching."
117,"TYLER WENDELL: This session will
cover the presentation entitledCognitive Data Quality--Get Smart About Your Data.My name is Tyler Wendell.And I have been with SAS for
around three and 1/2 years.And I am currently
a software developerin R&D. I just wanted to say
thank you for taking the timeto watch this video
and find out whatare some of the innovative
things we have been doingat SAS.With that, let's dive in.Cognitive Data
Quality is a projectaimed at infusing SAS Data
Quality with the latestand greatest natural language
processing techniques.Before I show you
a demonstrationof what it can do,
I'm going to show youwhere data quality is now.Suppose you have a table that
you've loaded into SAS Viya.Now, as you know, most tables
are not as nicely namedas you see right here.So let's pretend you
had to run profile on itto see what kind of
content was in there.When you run a profile,
part of the informationyou're going to get back
as an ID analysis score.What that does is
it tells you whatit thinks that information is.Based on this one you
see in the red circle,it thinks that your contact
column is individual names.These tags can be
used to automatevarious transformations with the
automated suggestions engine.This is nice because
you could potentiallystandardize and transform
a large number of tablesautomatically.This tag was made possible
by some of the functionalityprovided by SAS Data Quality.More specifically,
identification analysis.Identification analysis
takes in stringsand essentially just
puts a label on them.Some other Data
Quality functionalityincludes gender analysis,
locale guessing, parsing,data extraction,
standardization, and casing.The heart of all of this
functionality in the DataQuality engine is called the
QKB, the Quality KnowledgeBase.The QKB is a conglomeration
of vocabularies, grammars,regular expressions, and
other deterministic rules,all strung together.To start supporting a new
locale or a new data type,it takes a substantial
amount of time and effort.And this is where Cognitive
Data Quality comes in.To keep up with
demand, we startedlooking at modern approaches
in natural language processing.We could take some of the
handwritten deterministic rulesin the QKB and replace them
with models and other machinelearning techniques.So this will start
to look like this.All the functionality that
we used to provide for youjust driven by models.I'm going to show you
a small example of someof the identification
analysis and gender analysisthat I showed you earlier, only
running through our CDQ models.Parts of this functionality
are still in development.What I can demo is an
interactive command line toolthat we built for
testing our models.The command line tool loads
one of our models into CAS.The strings we
interactively feed itare then pushed into CAS.Through the model.And then the resulting
classificationis printed on the screen.When this product
is released, youshould expect it to have
one of the SAS interfacesthat you are familiar with.To start the tool, I must
first load the model into CAS.All right, so now, it
is prompting me to typein a string that
it can classify.This demo model will
classify each stringas one of six
different identities.You will see that
it will print outthe likelihood it thinks it
is one of those identities.A little example,
some generic bank.You can see the
six identities hereand the likelihood it thinks
it is one of those identities.It is very confident
it is an organization,and that is correct.So let's try a person.It got it right.You could also classify it with
a variety of different formats.So let's try a few other
different variationsof that name.So middle initial,
actual middle name.And let's abbreviate
a little bit more.As you can see here,
it can classifya variety of different formats.And that should be the case
for any these identities.What if Kathy wanted to open
up her own organization?Woods, LLC.Yep.Classifies it as
an organization.All right, let's try a few of
these other identities then.Let's see what happens when
we give it a phone number.Plain format.We could change the
format a little bitto a variety of
different formats.We could also try a
phone number with lettersinstead of numbers.Sometimes you see those
in advertisements.Yep.And it can still get it right.Finally, let's try an address.OK, a simple address.And if we take the city,
state, zip off the end,we get delivery address.Now, if we abbreviate,
and let's change the 100to some apartment number.Yep, still address.Let's try to trick it by
jumbling up that a little bitand seeing if we
can still get it.So it's still,
probably if a humanis reading that, apt A. If
we take the apartment A off,we're back to an
individual, Dr. Carmen.The next thing I
want to show you guysis a gender analysis model.So let's go ahead and
load it up into CAS.A gender analysis
model should beable to guess the gender
of a name given to it.The first name I want to
try is Antonio Alvarez.And it got it right.Awesome.Let's try a different
male name, and then we'llswitch it up a little bit.Where the opportunity
to confuse the modeland have the model
get it wrong isgoing to be in situations
where a person might have twofirst names one of each gender.For example, Susan John, first
name Susan, last name John.Obviously, John is very
common male first name,but it can also be a last name.So let's see how it handles it.That's great.Our model figured it out.Let's switch it again.What this means is that
the model has picked upon positional information.And not only is it learning
which names are male namesand which names
are female names,but the position of the names
matter to determine the gender.Let's continue to challenge the
model and try to confuse it.But let's keep the same
names in their position,but change the format so that
it is actually a female name.And look at that.I think that's so cool.Figured it out.It's not only figuring out
positions, names, but alsoformats.I just want to try a couple more
things to confuse the model.First one, let's try a prefix.Well, let's make it weird.Let's go old school Johnny
Cash, a boy named Sue.Let's pretend we actually
do have a man named Susan.So we'd probably address
this man as Mr. Susan.So let's try this.All right.Well, it did pick up
that Mr. Susan is a man.That is a good.The last thing I want to try
is a gender neutral title,prefix Dr. Susan.Dr. Susan John should
probably come outas a female, unless I
has some bias in it.So let's check our
model for that.I am so glad that the
model guessed correctly.This will conclude
the demonstration.Thank you so much for
spending your time with me.Wish we could have done this in
person, hopefully, next year.But while you're
on the SAS website,please go and check out other
demonstrations and other videosto see how we're innovating.Thanks again."
118,"Hello.This is Margaret Crevar.I work in the SAS
R&D. And I help outwith customers who have
performance issues,and in doing so we learn
lots of informationabout how to run SAS
on various hardware.One of the trends we're seeing
right now with SAS usersis their desire to move
their SAS applications,whether it's 9.4 applications or
new SAS Viya 3 applications, upto the public cloud.There are many
reasons in doing that.But one of the things we've
learned in helping customersmove there so far is that
you need to really understandwhat you're doing
and what you'rehoping to achieve in order
for this movement of your SASapplications to the public
cloud to be successful.So that is the
reason for my paper,""Important Performance
Considerations WhenMoving SAS to a Public Cloud.""I'm going to start off
with a disclaimer here.As you can imagine, things are
happening on a monthly basis.Sometimes even on a weekly
basis with public cloudcoming out with new instance
types, new storage types,new whatever types,
new enhancements,all kinds of information.So the information that
is in my published paperis based upon what was available
at the time the paper waspublished in March of 2020.And the presentation
I'm talking to youabout today is based
upon when I'm actuallyrecording this presentation.Next week, there might be
some additional informationto be said.And when we do learn
additional information,we do post this in the SAS
community for administrators.But we're happy to say that
the public clouds are listeningto the needs of SAS
applications and areenhancing their instance
types and storageto meet the users' needs.So let's get started.In order for you to be
successful with your movingof your application from on
prem up into the public cloudis you really need to have a
very, very good understandingof what your existing
SAS application is doing.If you have any SLAs,
Service Level Agreements,that have to be met,
all of that informationneeds to be known to help you
decide on the instance typeand network connectivity,
the storage typesthat you would choose when
moving your SAS application upto the public cloud.You also need to determine
what the success criteria isfor you to move an
existing SAS application upto the public cloud, or if
you're going to be sending upa brand new application.We have a lot of
existing SAS 9 userslooking to start
using Viya, SAS Viya,and they're doing
that by starting it upin the public cloud to
understand the needs.So just some kind of success
criteria needs to be known,because that will help
in the decisions of whatinstance types to use.Another thing to mention in
order for you to be successfulis you have to
understand everythingneeds to be close to one another
in the same general location.That includes your source data.That includes third-party tools.For instance, authentication
tools and your SAS client.If you have this
information spreadall over the continental
United States,or even across different
continents themselves,then you're going
to have to-- youwill experience a
lag in performance,because you'll be using a
WAN versus a LAN connection.And it just takes longer
for the movement of data.So let's get into some
general informationabout what's going on.Whether you're a SAS 9 user
with your compute tier.And if you're running
SAS Grid, therewill be multiple
compute tiers that arehaving to do heavy analytics.Your requirement of a shared
fault system for storagefor these compute tier nodes
for SAS Grid and your mid-tierand metadata servers
or you are a Viya userwith a CAS controller
and worker nodes, whichwould be in a SAS 9 perspective
similar to the compute tier,MicroServices, and
Postgress, and thenalso your SAS
programming runtime node.You need to understand what
the activity is going to be,how much work is going
to be on them, whatIO throughputs you're
going to need whenyou go to move to the cloud.And then we have to look
at the information that'savailable to you from the
actual cloud providersto see what you can meet with
a different instance type.One thing for sure that
you're going to want to dois in the compute
tier for the SAS 9and the CAS controller
workers for the SAS Viya,you're going to want those all
to be the same family processorset for Intel processors.They all need to be Skylight.They all need to be Broadwell.In Amazon AWS, it's
very easy to do that,because each of their
instance familiesare only dedicated to
a single Intel chipset.However, with Azure from
Microsoft and Google,you could have a family series
that could be either Broadwellor Skylight.So you'll have to work
with that cloud providerto ensure that all
of those componentsare the same processor speed.It just works best because
in doing certain thingson a Skylight, it can run twice
as fast than on a Broadwell.So you would not in a
SAS Viya infrastructurewant to have some of the
workers finishing up in halfthe time of the other workers.It's just not a
good thing to do.So those are some
things to thinkabout when you're doing that.Also, when you go to--one thing that I will
mention is in my paper thatis published on the SAS
Global Forum website,I go into a lot of detail
about the different instancetypes for the Amazon
AWS, for MS Azure,for Google, and for
Oracle Cloud offerings.And I'll talk about
the pros and consfor each of the types of
the bullets you see up herefor SAS 9 and SAS Viya.I will go into which
instant type is good for SASand which is not,
and then give youthe reasons why they're good.But some of the things I want
to mention to you right nowis the fact that when
you go to stand upyour instances in
your storage, oneof the things you need to
make sure that it is happeningis that you're in a region.Everybody's going
to want to haveall these machines in a region.But within a region for most
of the major public cloudproviders, they have the area
zones, availability zones,area zones, AZs.You're going to want to be
within that, because youcould be-- the region could
be East for, say, Amazon.And then they have
four availability zoneswithin Amazon.So you're going to want to have
the areas then to be the same.That way it might be
Virginia, for instance.So you have east and Virginia.But even in Virginia, there
are a couple of citiesso you're going
to also want themto be in the same placement
group, placement proximitygroup, however the verbiage
is for the public cloud vendorthat you're working on.So you're going to
want to make surethat you're all there together.That means all of
your componentsare close to one another.They're on a LAN.They're not having to
go through firewalls.They're not having to go
through multiple network routersand things like that.You'll be able to get the
best performance possible.So that's one of the
things that you need to do.When you go and you
work with Amazon,for instance, for standing
up the same things,they also have what they refer
to as a clustered placementgroup, which means
all of the componentsthat you're working on will
be on a low network latency,high network throughput.So communication is going to be
very, very fast between those.From an Azure
perspective, you'regoing to want to make sure
when you set up the network,the network connectivity is
in the same Azure placementproximity group.Same kind of thing
as with Amazon.You just need to make sure
that things are as closely aspossible when you're setting
all of this information up.So there's a lot
more in my paper thatgoes into more details,
but for sake of time,I'm going to go ahead and
move on to some other topicshere that you need
to pay attention to.Storage.Storage is one of
the biggest thingsthat you have to
pay attention to.And you're probably
going to end upspending a lot more
money than whatyou had wanted to with storage.Because in order to
get the recommended IOthroughput that's SAS has for
both SAS 9.4 and SAS Viya,you're going to have to
spin up a lot of disks.You're going to have to
spin up a lot of capacityto be able to meet
the IO throughput.Again, my paper goes
into a lot of detailabout how to go
about doing that.But some key things
that I want to mentionto you is if you are looking
at Amazon, what you haveto pay attention to when you're
choosing your instance typeis the network connectivity.So in some instance
types for Amazon,you're going to see it says up
to x gigabits of IO throughput.That does not you
can get 100% of that.You might be able to if nothing
else is sharing the hardware.But for the most
part, you're notgoing to get 100%
of the 10 gigabit.So that's going to
be a problem there.So that's why we tell
you to go to the instancetype in Amazon that has
a dedicated gigabit NICcard in it.That way you are guaranteed to
always have that IO throughput.For SAS on a processor
that has 16 physical cores,you're going to want
to have that gigabit bearound a 25-gigabit
NIC card in thereto be able to achieve the
100 megabytes per secondper physical core.And that reminds me.I forgot to mention
this already.Be very careful when you are
looking at the informationon the public cloud.They refer to vCPUs.You need to make sure
that you understandwhether the vCPUs
are physical CPUsor does it hyperthread
is turned on.Most of the time it is
hyperthread is turned on.So from a SAS perspective,
you take that number.Divide by 2, because we
don't run on the hyperthread.We only run on the physical
cores on that Intel system.And then that'll be
the number of coresthat you will be licensed
for from a SAS perspective.And that's where a majority
of the SAS work will happen.It's on the physical core.So for instance, in Amazon
there's an i3.8xlarge.It has 16 cores
in it, but it hasa single 10-gigabit
dedicated network card.You're not going to get
100 megabytes per secondper physical core on that one.You're only going
to get about 48.But Amazon has recently
released the i3en that now hasa 25-gigabit NIC card in it.And you're going to
get that doubled,so 2 and 1/2 times
what you had before.And you will be able to
make that 100 megabytesper second IO throughput.If you're looking at going
to MS Azure, Microsoft Azure,you need to go
into their little--on their website where they
talk about the instance typesand scroll all the
way over to the right.Right before you
get to the columnthat talks about network
availability and network speed,there's a column there that
says maximum IO throughput.Microsoft has gone
in there, and whenthey set up their virtual
machines in the VMware--in the VM software, the
virtualization softwarethat they used, they had
put a hard cap on therethat says the maximum IO
throughput you can achieve.So yes, these machines have
enough network capacityto give me 100 megabytes per
second per physical core,but for instance,
with the E32sv3 there,they have 16 physical
cores in it as well,but they have capped it at 768
megabytes per second maximum.And that equates to
only 48 megabytesper second IO throughput.So that's something
you need to look into.And if you really do need
100 megabytes per secondper physical core,
then you're goingto have to look at doing
something else within Azureto be able to achieve that.So that talks about getting
outside of the machine,outside of your instance
to physical storage thatwill persist upon a reboot.We also need to talk about
the temporary storage that'sin there.In Amazonland, you have--their new instances have
NVMEs inside of them.That makes for very
fast storage for youto be able to get at your--for the temporary space.However, in Azureland,
they only have a single--maybe a very small SSD
drive, which is not as fast.It doesn't have the same disk
capacity that Amazon has.But anyway, you just
need to look into that.So because there
is not enough spaceon the internal
ephemeral drive, you'regoing to have to have your
temporary data externalto the instant type.So it's going to go against
that-- through that same pipethat our permanent
storage is goingthrough that could not meet
the 100 megabytes per second.Now you have two
file systems needingto go through the same pipe.So it's going to mean
a lot less speed.Again, I can answer more
of these questions one-offwith you.I'm sorry for having to go
through them so quickly here.But meeting the time
constraints of the presentation,I need to go through
a couple more things.And then I'll be available
to answer some questions.There also is going to be a lot
of detail in the actual paperitself for you to be
able to understand.I mentioned if you were
using SAS grid a shared filesystem is a requirement.So what do you have available
to you up in the public cloud?You have DDN's Lustre
shared file system.It's available everywhere
through the marketplacesassociated with
the public cloud.IBM Spectrum Scale,
formerly known as GPFS,is available to the
marketplace in Amazon.Amazon has a special
usage of DDN's Lustre.They call it Amazon's
FSx for Lustre.Again, it's only
available on Amazon.It's very fast.And it's very price performant.So it's a really good buy.We've got quite a few
large telecommunicationcustomers using it,
and they're very happy.Amazon also has their
Elastic File System, EFS,storage available to you.You have to be
careful with that,because it does have a maximum
IO throughput for each filesystem.So you just need to go in.And in my paper, I
mentioned what that is.I also mentioned a
couple of other caveatsaround this shared
file system for youto be able to understand.Azure has their NetApp
File system available, ANF.It is based upon--it runs on a NetApp, per
the title, and it uses NFS.So that is another
shared file system.There are several
limitations for usingNFS in that there's not
design for doing heavy writesto that file system.So you have to be
careful with it.We do have some customers
that are successfullyleasing NFS as their
shared file system,but they're more what I'll
consider smaller customersthan what the
typical customer is.So if you've got
large volumes of data,if you have large
amounts of writes thathave to happen to the
shared file system,I would not recommend using NFS.Some other topics to talk
about, I alluded to thisat the very beginning.Make sure all of
your components thatare part of what's
going to be your staffinfrastructure in a public
cloud are close to one another.I've helped multiple customers
that have moved their staffapplication up to the
cloud, and they'reunhappy with performance of
staff in the cloud comparedto SAS on prem.When we start peeling away
the layers of the onionto be able to figure
out what's going on,we realize either their
authentication tool is stillon prem, but everything
else is in the cloud,staff is in the cloud, but
their source data is on prem,or their source data is
in a different regionwithin the cloud.All kinds of things
like that do it.So you have to be very
conscious of that,because when you start
going through a WANto be able to get data into
where your SAS is running,your megabytes
per second go downinto the 25 or less
megabytes per second.So that just means it's
going to take a long timeto move your data over.And if you're moving
your data over a lot,then that's going
to be a problem.High availability
is a discussionthat you need to have with your
technical architect for SASwhen it comes to doing
that up in the cloud.You may have to
rethink what's actuallyneeded when you get up there.The clouds have a 99.5% uptime.So they're going to be able to
spin up a new instance for youvery quickly.And if you set up your
instance correctlyand you have it backed up
to an AMI or some other kindof device, again, you'll be able
to spin the new hardware up,attach the AMI, and be
up and going in minutes.So those are some new things
to think about of that.So just some other
things that needto be thought about-- different
ways of doing high availabilityin the cloud, because
there are offeringsfor keeping their hardware
up and running at all times.So in conclusion, what
I'd like to talk--to remind you of is you need to
understand a complete workloadand what to expect
of the workloadwhen you move it
up into the cloud,so that then you can decide
and you can create thingsthat the infrastructure
is correct to beable to meet the workload.You need to understand
the limitations of the IOthroughput.When you get up there,
especially to external storage,and see how that maps
into your expected SLAs,and if you'll even be
able to achieve it.Within the public cloud,
they have the abilityfor taking one of the 16
physical core instancesand cut the number of
physical cores in half.It's called constrained
within the Azure.They have a policy to do that.Now, from a SAS perspective,
you would license onlyon the constrained
cores that you're using,but from an Amazon, or
actually, that was Microsoft.From an Azure perspective,
you would stilllicense for all 16
cores, because that'swhat the machine is.So it's going to cost you
a little bit more to do it,but you're able to
set up somethingthat can meet your SLA from
an IO throughput perspective.You need to understand
about the factthat any time you
access anythingfrom outside your LAN
that you're running oninside the public cloud
it's going to takea long time to get at it.Another one is that instances'
storage types are alwayschanging up in the public cloud.And like what I mentioned before
with the high availability,you just make a copy
of what you're running.You spin down your instance.You spin up the new instance.For example, for the Amazon
where they went from an i3to an i3en, and you attach
your ghost of the previous one.And all of a sudden
now you're able to takeadvantage of the
bigger NIC cards,the extra NVMEs inside that.It's very, very easy,
very, very simple to do.So one thing I want to
mention is this paperthat is on the
Global Forum websiteis it gets updated once a
year in time for Global Forum.But there's also a website
that we have of support.sas.comthat we are putting
additional papers on,especially ones that are
specifically just for how--what you need to do to make
running SAS in MS Azuresuccessful.And that website is
support.sas.com/kb/62/239.html.So that is something
to mention therethat you'll be able to look at
and get additional information.So I want to thank
you for listening.My contact information is
here if you have any questionsafter reviewing the paper
that's on the SAS Global Forumwebsite."
119,"JIM KUELL: Hello, everyone, and
welcome to Virtual SAS GlobalForum 2020.My name is Jim
Kuell, and today I'mgoing to be talking about
diagnosing the most common SASViya performance problems.I am a software performance
engineer in the SAS PerformanceLab, in the Compute Services
R&D. I work very closelywith SAS Technical Support, our
customers and hardware partnerson a daily basis.I wear many hats in my job,
but everything that I dorevolves around trying
to maximize performancefor customers, both
internally and externally.Today I'm going to be talking
through our methodologyfor diagnosing
performance problems,which consists of
two main parts--gathering information and then
analyzing the information.Then I'm going to talk
a bit about the mostcommon causes of
performance problemsthat we see at customer sites.A quick note about
this presentation,whenever I say Viya
throughout the presentation,I'm going to be
referencing Viya 3.x.There's a lot of content to
cover in the next 20 minutes,so apologies in advance
for talking quickly.So in the performance
lab, we understandthat diagnosing performance
problems can be extremelydifficult and time-consuming.We've spent many years doing
so, and throughout that time,we have developed a
methodology, and we'verefined it quite a bit.That helps us really
get to the bottom--get to the root cause of
these performance issuesas quickly as possible.We use it daily, both internally
and externally with customers.And we use it quite a bit to
help with SAS Technical Supporttracks, when our customers
are having performance issues.Our performance process
does service multiple causesof performance
issues quite often.It's very important
that wheneveryou are trying to
fix these issues,you only make one
change at a time.And I'm going to kind of
harp on this a little bitthroughout the presentation.So the initial
set of informationthat we're going to
gather in this process,we're going to use that as
our baseline measurements.And that's what we're going to
use to compare future changesand results against
to see if we'reimproving our performance.So after each and
every change is made,you're going to want to
go back and follow itwith another round of
testing and informationgathering and analysis.And then you compare
that to see if you'reheaded in the right
direction or if you'veachieved the performance
that you're looking for.It's also very important
that you document everythingthat you change along the way.The last thing
you want is to seea couple of potential causes
of a performance issue,and then you make a handful
of changes all at once.And then you retest, and you
see that your performanceis actually much worse
than it was initially.So you try to revert
all those changes,and you end up not being
able to remember everythingthat you did change.You don't want
that to happen, somake sure you document
everything along the way.So gathering information, it's
obviously the most vital stepin the process.Here at SAS, we
love data, and wetry to gather as much
data as possible.It's extremely important to
understand all the aspectsof the environment and the
specific performance problemat hand.We gather a bunch of
information, and we use that.And we craft it
to create a storythat we use to
eventually lead us backto the root cause of an issue.It's very likely that
no one piece of infois going to lead us to the
root cause of an issue.So what we do is we take it,
and we correlate it all togetherto create that story.Once we have the information
and we are looking at it,we usually start by looking
for telltale signs of the mostcommon causes of
performance problems,because 9 out of 10 times,
that's going to be the causes.So as we work with
those, we tend to know--we see little flags of what
is in the information thatleads us to that diagnosis.So I'll talk about that a little
bit more in a couple of slides.Also, the situations
at customer sites, someare more complex
than others, so somemay require additional
information gathering.But what I highlight in
the next couple of slidesis going to be enough to get
to the bottom of most of these.The information that
we gather is alsogoing to be very similar
with SAS Viya as to whatwe gathered with SAS 9.But how we analyze it is going
to be extremely different.With SAS Viya, between all the
microservices, your applicationlayers, your physical and
virtual infrastructures,your file systems, there
are many more moving parts.So our analysis process
has to stay very fluid.The first and most important
thing that we gatheris the problem definition.Finding the origin
of a problem isimpossible without first
clearly understandingwhat the problem is.We need to make sure
that the definition isn'tjust what the user's perception
of what's happening is,but what's actually
occurring on the systems.A lot of times, we'll have tech
support tracks that are opened,where a customer says, my
system is running slow,or it's not running as
fast as I want it to.And that really doesn't tell us
anything about what's going on.So we really need to know what
is happening on the system.Is there a single table
that all the usersare trying to operate on?Or are all the users
trying to load and processseparate tables?Is there a job
that's running slowernow than it was previously?When does the issue happen?And how often does it happen?This is all vital to getting to
the bottom of what's going on.And in that, we need to know
what the ideal outcome is,what level of performance is
going to make the user happy.Are there any SLAs that
need to be accounted for?Next is the
application definition.Different applications
have varying impactson the performance
of the system.So we put together a full
mapping of all the applicationsthat interact with the
problematic systems, not justthe SAS Viya applications.In this, you need to make
sure that you identifythe specific SAS Viya
applications thatare having the problems because
that really changes how we goabout looking at
all the information,whether it be Visual
Analytics or visual statisticsor something else.Also, what interface
is being usedto start the SAS jobs in
question and run the SAS jobs?Are they being run via batch or
via SAS Studio, or EnterpriseGuide?Next, we work to create a full
infrastructure definition.Many performance problems
can be traced backto issues within
the infrastructure,whether they're hardware-
or software-based.And that includes the server
and network information,the operating
system-level tunings,and your I/O subsystems.There are tons of parts that
make up the infrastructure,and any one of
those can ultimatelybe the culprit of a
performance problem.So it's really important
that we understandevery aspect of the system.We've written a tool that's
called the RHEL GatherInformation script.I'll discuss this more
in a couple slides.But this helps give us a more
detailed look at the operatingsystem level info.And it's really a
great complementto the information that's
provided by the customer.So a couple of things that we
look for-- in the three bulletshere--for the server and
network information,we typically look at the make
and the model of the system,and what the instance type
is, if it's in the cloud.Are there any virtualization
software that's being used?We look at the CPUs
and the memory,and the network connection,
the speed and type of that.For the operating
system, we typicallydive into the operating
system versionand the tunings quite a bit.We also look at the
file systems thatare being used for your
permanent SASDATA files,as well as the
temporary SASDATA files,which is going to be your SAS
Work and your CAS Disk Cache.And also, the source data
location and type, thathas a big factor as
well, whether it'sSASDATA files or an
external databaseor Hadoop, or something else.And also, your I/O
subsystem information,we look at all the
cards in the ports,and any connections being used,
and the bandwidth of those.SAS logs are typically where
we begin our investigation.We need logs from the jobs that
are servicing the performanceissues.Those are vital to the process.These contain metrics
and session optionsthat really help us narrow
down the cause of an issue.If it's possible to also get SAS
logs from when the jobs are notexperiencing the
performance issues,then that's just an added bonus.We also want logs from
the SAS applications thatare experiencing
performance issues as well.And tech support can
work with customersto give them specific
instructions on howto turn on debug logging
for these applications.And they can
actually also providea few other helpful settings
for the SAS job logs as well.We usually can't tell what the
bottleneck is from the SAS logalone.However, it does give us
a much better indicationof what the issue is,
as well as the timeframe for when it occurred.So we can then take
that informationand we overlay it with the
other tools and informationthat we've gathered.And that really helps us
narrow down what to look forand when exactly to look for it.There's a ton of moving parts
in SAS Viya, like I was saying.So analyzing the logs
is much more complexthan it was with SAS 9.With SAS 9, we would always see
a couple red flags in the logs,and we typically know exactly
what to look for after that.This, like I say,
it's much more fluid.It's much more complex,
so we ask the customersto collect those logs and
submit them to SAS Tech Support.And then my team will
work with tech supportto analyze those and ultimately
get to the bottom of the issuesat hand.Next couple slides are going
to be a couple of toolsthat we like to gather,
and tech supportlikes to gather as
well, the first of whichis the RHEL Gather Information
script, which I briefly touchedon a couple of slides ago.This is a standalone script.It was written by us here
at SAS, and it's free.It does not require
a SAS installation.We ask that this is run on
all problematic systems.What it does is it
gathers and packagesrelevant OS level information.So we can then take that output,
and we can look through itand look at how different
things are set up,how different things are tuned.We can see if our best
practices are beingfollowed, and stuff like that.And it really helps us create
that infrastructure definition.Another tool is called
the SAS Viya Perf Tool.This is another standalone
tool that was written by us.What this does is it tests
the available throughputof the network between the CAS
Controller and the CAS Workernodes.And it also tests the throughput
of the SASDATA and CAS DiskCache file systems.This is in the final
stage of development,so it's not quite released yet.But we will post on the SAS
Community's Administration pagewhenever it's released,
so keep an eye outthere for when that is finished.Next is IBM's nmon script.This is my team's hardware
monitor of choice.It's free, and you can
download it from the web.What it does is it runs
a bunch of existingtools behind the scenes, and
it compiles all that datainto a flat text file.We then use what's
called the nmon analyzerto process all that data.And that turns it into a
graphical, tab-separated Exceldocument, which makes it
much easier to look at.This needs to be running
on the problematic systemthe entire time that the SAS
job in question is running.And it's really
important to haveboth of these running
from the same time periodso we can correlate
those results.Sometimes customers will
send us an nmon and a SAS logfrom two separate time
periods on the same system,and that does not
help us at all.Another really, really helpful
tool is called gridmon.This is included with SAS Viya.This is a CAS administration
and monitoring utility.And what it does is it lets us
really dive into the activityon the CAS nodes.And at deployment, we can
see what is happening.And you can stream it live, or
you can record it to a file.So we ask customers to
record it to a file,and they can send out to us.And we can pull that up
and play it back and lookat what exactly is happening
on all those nodes at the timethat the performance
problem is happening.And the final two tools are
tkgridperf and sas-peek.These are both also
included with SAS Viya.Tkgridperf, we use it to
really isolate network issuesbetween the CAS server nodes.And sas-peek is a biometric
collection utility.It's awesome.And that runs on
all the machinesin the Viya deployment.So I'm going to go
through some of the mostcommon causes of our
performance problemsthat we see at customer sites.Like I said earlier,
the infrastructureis almost always the cause
of performance problemswith SAS Viya.It's made up of tons of
different parts and pieces,and any one of
those can ultimatelybe the cause of the problem.So while every situation
varies in complexity,and it may be something
outside of this list,this list of stuff is going
to count for the vast majorityof issues.The most common and one of
the most easily correctableis an insufficient
network bandwidth.There's a large amount
of communicationbetween the different
SAS Viya nodes.And there's also quite often a
large amount of data transferbetween these nodes as well.Because of that, we
strongly recommenda minimum of a 10
gigabit nic forthe internode communication.Next is the physical location.It's extremely important that
every component and resourceof the SAS Viya infrastructure
is physically located as closetogether as possible.Now this includes everything
from your source datafiles to the storage to
systems to your authentication.And you always want those
located on the same subnet.I've seen customers put source
data in one public cloud,their compute resources
in another public cloud,and their authentication
back on premises.And that just really
kills their performance.We've seen serious
performance hitsanytime a LAN comes into play.Next is going to be your
bandwidth to external storage.SAS Viya has very heavy
I/O throughput needs.And we've unfortunately
seen it asunaccounted for in quite
a few architecturesthat we've reviewed.Our minimum recommendation
is 100 megabytesper second per physical core
for permanent SASDATA filesand 150 megabytes per
second per physical corefor temporary SASDATA files.Also, when using network
attached storage,we've seen a lot
of success come outof using a separate
nic for the storagethan what is used for the
inter-node communication.Next is memory.I'm sure you noticed that
Viya is an in-memory analyticsengine, and it uses
memory map files.So in order for it
to operate optimally,it needs to stay within the
bounds of virtual memory.In order to make
that happen, youneed to make sure that the CAS
Controller and Workers haveenough memory for
that to happen.So as a general starting
point, we typicallyrecommend that memory
is two times the sizeof total incoming data.Again, this is just
a starting point,so you need to make sure
you monitor the systemsand are ready to
expand it from hereif that ends up
not being enough.The amount of memory needed can
vary greatly from site to site,so it's really important that
you do a detailed workloadanalysis before setting
up your infrastructure,and to make sure that
your memory capacityplanning is done correctly.Next is the CAS Disk Cache.This is an incredibly
efficient file system.It serves as both an
extension of memory maps,as well as a backing store
for your replicated data.It's extremely fast when
it's set up correctly.But you need to
keep in mind, it'sstill not going to be as the
processing speed of in-memory.CAS Disk Cache can only
perform as fast as the devicesthat it resides on.So again, it's really
important that you have enoughRAM to keep the
processing in-memory.While CAS Disk Cache is
also going to be used,when you have enough RAM,
the amount that's usedis going to depend on
the specific applicationand what is being done
by that application.The default location of
CAS Disk Cache is /temp.And we find that that is quite
often the cause of performanceissues with this.We usually recommend to change
this location for two reasons.One, if /temp fills up, then
the operating system can't runcorrectly, and everything
on that system stops,which is bad.And two, /temp is typically
not very performant,so we like it to be moved to
a file system that can meetour minimum recommendation of
150 megabytes per second perphysical core.I get a lot of questions about
how big CAS Disk Cache shouldbe.And we usually recommend
starting with about oneand a half to two times
the size of memory.Same as with memory,
it's really importantthat you monitor your
systems and you'reprepared to expand the
size of CAS Disk Cacheif you find that
more space is needed.And lastly is virtualization.This is an extremely
common practiceand can actually
be very performantif it's set up correctly.The main thing to
remember is SASdoes not perform well when
it's thinly provisioned,especially when it's
in a virtualized serverfarm with shared everything.SAS really likes
dedicated resources.This is extremely
important to rememberif you're moving to a
cloud-based environment.A lot of times, you can end
up using shared resourcesand not even know it.So it's extremely important that
you have dedicated resources.And with virtualized
environments,we found a lot of the
performance issues with thesecould be traced back to
the underlying hardware notbeing set up and
tuned for performance,and that really needs to be.Another quick note to keep
in mind in virtualizationis you want to prevent
NUMA from coming into play.And we usually see this happen
when the virtual CPUs spanmultiple physical sockets.So what now?Well, once you've identified
the cause or the causesof the slowdowns in
your environment,you need to determine
what you wantto change to correct these
issues or improve them.Like I said earlier,
it's really importantthat you only make
one change at a time.Each change should be followed
by an additional roundof testing and information
gathering and analysis.So if you have multiple
potential causes of issuesin your environment,
you're goingto want to make several
iterations until your desiredperformance is
eventually achieved.Now once those changes
have been implemented,it's extremely,
extremely importantto continuously monitor
the environment.Data sizes, application
usage, your number of users,number of jobs, these
all change over time,often increasing by quite a lot.So proactively monitoring
the usage patternsallows you to be prepared
when additional resourcesor upgrades are needed.And the majority of the
time, it will end uppreventing you having to go
through this entire processin the first place.Thank you all for listening.My paper on this topic is
available on the SAS GlobalForum website, and my contact
information is listed here,if anyone has any questions."
120,"Welcome to this breakout session
for the SAS Global Forum 2020virtual event, entitled ""Hybrid
Marketing, Campaign Management,and Analytics' Last Mile Using
SAS Customer Intelligence 360.""My name is Suneel Grover.And I hope you enjoy this
presentation and demo.So let's begin.The marketing industry has never
had greater access to the datathan it does today.The more we know the
customer, the morewe understand the essence
of customer experience.The more we understand
customer experience,the more we can
shape it, develop it,and better serve the customer.However, data alone does
not drive your marketingorganization.Decisions do.And with all the recent hype
regarding the potential of AI,a successful
cross-channel campaignis propelled by a personalized,
data-driven approach that isinjected with machine learning.According to insights
from the Futurum Researchand SAS Institute's global
research study entitled""Experience 2030, the Future
of Customer Experience,"" whichinvolved more than
4,000 consumers,executives, marketers, and
technology professionalsto understand what defines
customer experience todayand how it's evolving
through the year 2030,a key topic surfaced--
digital trust.To summarize, many of the
products and services consumersuse on a regular basis have
a strong digital footprintthat provides brands with
a wealth of user data.And while brands are looking
to leverage data and technologyto improve their
understanding of consumers,increase efficiencies
in operations,and drive more consistent and
relevant consumer engagements,consumers have
concerns, particularlyregarding data and the
ability of brands to notinfringe on personal privacy.The report specifically
states, ""Trustis a key element in the
overall customer experience.And brands that cannot provide
a high level of trust cannotprovide a high level of
customer experience.""This is a challenge
for brands as theywork to balance the richness
of the customer experiencethat they can offer as a result
of the user data they collectwith the ever-present
risk of a data breach,and the risk they know they
face if their source of datais turned off.The question that
immediately surfacesis, what can brands
do to build trustwhile staying data-driven?How brands handle and
protect consumer dataprivacy is more than
a compliance issue.It's a competitive
differentiator.There is a file
balance to strikebetween the desire for
privacy and the desirefor personalization.How do we make customers
feel known and welcomedwhile also preserving
their privacy?Wait for it, because
here it comes.The recommendation for every
brand in 2020 to pursue--the bolstering of their
customer experience with AI.Problem solved, right?However, the question
I'd like to askis, is all AI built the same?And the answer to that is no.A better question to address
is, what do we actuallywant AI to achieve?And how does that intersect
with the customer experienceswe want to deliver?Inherently, it will vary
from brand to brand.Ultimately, brands are trying
to get customers to an end stateor conversion event,
like a product purchaseor signing up for a contract.Here are a few
items to consider.There is streaming
data availableabout your customer's
and prospect's every day,hour, minute, and second.In addition, customers
who trust your brandexpect at every
interaction that you knowwhat has happened in the past.So we can use that,
along with our potentialtargeting actions and
anticipated financial impact,to make the right decision.That's easy, right?The hype behind AI
is primarily focusedon augmenting decisions,
process, natural languageprocessing, and computer vision.The result is a set
of trends made upof the following features.One-- algorithms producing
better analytics and accuracy.Two-- automation
and machine learningaligned with greater
productivity.Three-- embedded
analytics making AImore impactful and consumable.And four-- human-like interfaces
creating approachability.What happens when AI becomes
useful for your brand?It can effectively be renamed
from artificial intelligenceto analytical integration,
into any internal processor external customer experience
that your organizationfacilitates.But there are barriers
to AI adoption,such as available talent,
stakeholder buy-in, and the endsolutions and data strategy.Transforming hype
into reality for AImust focus on data,
discovery, and deployment.Taking action enabled
by AI-enhanced decisionscompletes the enviable last
mile of embedding analyticsinto personalization strategies
using experimentationand testing, recommendation
systems, next bestactions, attribution,
segmentation,and journey optimization.One of the biggest
challenges thatis happening right now
in the marketing industryis the clashing of two worlds--the traditions of
database marketingintersecting with modern
digital marketing.Many organizations
have, understandably,pushed their digital
strategies separatelyin a silo from their
traditional database marketing.That strategy worked
for a long while,especially whilst digital
marketing was less prevalent.However, there is
mounting evidencethat suggests that
customers respond betterwhen their journey is
managed cohesively.The opportunity lies in
successfully tying experiencesseamlessly together to
drive multi-channel,personalized marketing.The concept of hybrid marketing
evolved when digital marketingmatured and brands recognized
these two disciplines canno longer operate in silos.However, customers
really don't carehow complicated and convoluted
the marketing technologyecosystem is.The frustration
with the industryoverall has led to personal
privacy legislation,such as GDPR and
CCPA, which placesmore responsibility on brands to
protect their customers' data.So, how can marketers
interact with their customersand prospects in a respectful,
yet personal manner?Hybrid marketing
combines the capabilitiesof direct and digital
marketing in a single platform.Organizational silos break down,
enabling analysts and channelmanagers to leverage
online and offline datato improve both customer
journeys and the efficacyof their efforts.So how does hybrid
marketing work?There are two important
considerationswhen a technology
vendor requiresa brand to onboard their
managed customer dataand intellectual property
into a cloud solution.One-- data duplication.And two-- synchronization.Unless your brand is moving
all of their operational andmarketing systems to
a single location,this will always be a challenge.A hybrid marketing platform
approach can provide a secureWebSocket-based connection
between your chosen datamanagement environment and a
software-as-a-service MarketingCloud.An API gateway of
agents can providea framework that
allows integrationwith any external system.Thus, rather than requiring
the movement and duplicationof any of your brand's managed
customer data to a marketingcloud, the hybrid
marketing designallows you to map a customer
identity from your datarepository to an anonymous
hashed identity in a marketingcloud.For clarity, your managed
environment can be on premises,hosted by another company,
or stored in a cloud system.It's your choice.And you control your data.Let's transition now into
SAS Customer Intelligence360 and review a
segmentation examplefrom multi-touchpoint targeting.A brand has a large
volume of customersin their on-premises database,
including customer Suneeland Jerry.They are active
customers who haveopted into email
communications and haveinteracted with the brand
within the last 30 days.Lastly, they've been scored
by a machine learning modelwith respect to
unsupervised segmentation.As with most customer
relationship managementdatabases, or CRM databases,
countless attributesare available, ranging from
demographics, psychographics,householding, transactions,
analytical scores,third-party data
pens, and so on.Historically, what's missing
in these environmentsis the available of structured
online user behavior,which has typically
been in a silowithin an external
cloud platform.Not anymore.SAS Customer Intelligence 360
offers a comprehensive datamodel capturing
and contextualizingcustomer behavior on
websites, mobile apps,and other brand-owned
properties,as well as customer interactions
across A/B tests, remarketing,personalization, and
other targeted effortsmeasured by impressions
and conversions.The Unified Data Model,
or UDM, is a complete setof structured tables
that are beingmade available by
a stream or APIfor analytics, online-offline
data integration, campaignmanagement, and other use cases.Returning back to SAS
Customer Intelligence360 and our
segmentation example,a key value proposition
that I want to call outis that there are no personal
identifiers for either Suneelor Jerry.Instead, they are mapped
by anonymous IDs--ABC123 for Suneel
and XYZ789 for Jerry.Let's take a step
back and reviewhow I started the design of
this segmentation example.If a marketer wants
to target a segment,the platform can query
information residing in a brandCRM database and leverage
cloud-stored data capturedfrom own digital properties,
such as your websiteor mobile app.This process is similar to
traditional campaign managementor marketing
automation processes.The difference is that instead
of requiring your organizationto duplicate all your
first-party offlinedata in the vendor's
cloud solution,the individuals who meet your
use cases' targeting criteriaare simply mapped to a segment
in SAS Customer Intelligence360.No Suneel, no Jerry, no
other personal informationabout them--just ABC123 and XYZ789, along
with other customer-linkedanonymous identities
that are nowassociated with your
defined segment.Let's take a closer
look at the influenceof machine learning in the
campaign management process.The last step of
the segment mappingis applying
algorithmic clusteringfor unsupervised segmentation.But how were these
clusters created?Within SAS, analysts can
perform artisanal modelingusing no code, low code, and
high code user interfaces.Given that customer
behavior varies over time,there isn't one algorithm
that rules them all.You can choose from a
variety of techniquesfor both supervised and
unsupervised approaches.Let's walk through an
example of visual k-meansclustering that transparently
showcases how the results arederived and made available for
use SAS Customer Intelligence360.The first step is to
select an analysisobject, which is available
over here on the objects menu.As I scroll down, you
will see where I selectedthe clustering analysis object.From there, we need to
assign roles or attributesto those roles, customize
model properties if desired,and execute the model in
order to provide results.In this example, the
following attributeswere fed into the analysis--age, tenure in years, and
number of interactionsacross mobile app, website, and
email within the last 30 days.We want to identify
supervised synergies thathelp explain segment behavior
across these online and offlinesignals.The results include two
interactive visualizationsthat are worth highlighting.Displayed presently,
the cluster diagram--or sometimes referred
to as the matrix--displays a two-dimensional
visualization of each clusteronto a specified
number of effect pairs.These projections are useful for
spotting cluster similaritiesand differences
within the plots.Each cluster is
assigned a unique color.Although each cluster
is unique in n space,the two-dimensional
projections overlap.It is important to note that
each observation or customerbelongs to only one cluster.Interacting with
the diagram allowsus to improve our understanding
of any plotted effect pair.For example, the interpretation
of the relationshipbetween customer age and the
number of website interactionshighlights the fact that
four unique segments existand that two clusters,
representing two different agegroups, display moderate to
lower levels of web engagement.We can place our pointer
over any centroidand obtain displays of
each cluster's mean valuesfor that specific effect pair.But what about the data
story of each cluster?Let's switch to the
parallel coordinatesplot that enables us to
accelerate the understandingof each cluster's trends.The plot displays
data as lines thatare moving through
categories and bin measures.The thickness of a line
indicates the relative numberof observations within that bin.But wait a minute.Does this kind of
look like spaghetti?How is this even helpful?We can interactively
restrict the active linesto one or more bins in order
to focus on only the datathat interests us.In addition, we
can adjust the plotto explore the data based
on cluster membership,a specified range for one
or more variables, or both.For example, a single
click with the visual plotallows us to focus on
a specific cluster.The interpretation, or
story, is that this groupskews to a younger
age, moderate tenure,and low-to-medium
engagement levelsacross web, mobile,
and email touch points.Another click allows us
to focus on cluster three.The interpretation
here is a groupthat is aged 35 to 55 years,
with high values of tenureand digital
touchpoint engagement.The interactivity doesn't
have to end there.We can select the vertical
bar that represents the agedistribution and further
filter on customers40 years of age or greater.Let's pivot and
assume we're feelingconfident by the results
of our clustering analysis.Before transitioning to sharing
the results with marketingteams who manage and deliver
campaigns across touchpoints,let's walk through a
supervised segmentation usecase that combines
automated machine learningand natural language
explanationsof the segmentation results.The objective is to derive
actionable audiences whohave higher propensity to
meet our brand's conversiongoal, which is a
revenue-driving event.The first step is to right
click the attribute in our datathat represents the
conversion event of interest,and select Explain.This feature within SAS is
called Automated Explanation,and it quickly determines
the most importantunderlying factors or predictors
for the conversion outcomeby analyzing all the rows and
columns of the input data.At the top of the page, a
natural language-generatedoverview of Goal
Site Conversion Indis displayed, including both
the percentage of observationsthat converted and comparative
details to other valuesrepresented within this
attribute's distribution.Below the overview text
section, color-coded factorsare displayed on a
horizontal bar chart,visualizing the
relative importanceof each attribute's influence
of Goal Site Conversion Ind.The most important
predictors appear on the top,and the predictors are displayed
in descending order by weight.The weights are determined by
one-level algorithmic decisiontrees.Placing our pointer over
each bar interactivelygenerates a pop-up display of
the relative important score,standardized between
1 and 0 to simplifyinterpretation, higher values
equating to more influence.Now that we know which factors
influence Goal Site ConversionInd, how can we determine
which data valuesof these important factors lead
to higher or lower conversionpropensities?When we select a data item
like Goal Site Conversion Indto be analyzed, the
automated analysis resultsdisplay a list of
attractive segments.These homogeneous groups receive
natural language descriptionsthat are based on recipes
of influential factorsthat contain specific values.In this example, multi-level
as opposed to single level C4.5decision trees are executed to
identify each of the segmentsthat are performing at
high conversion rates.And we can review the
explanations to assessthe audience profiles.Because we selected the factor
traffic source origination,all influential values related
this attribute are contextuallyhighlighted.Not only can we see the
impact of the selected factor,but we also can see how
other factors interactwith it to identify
high value segments.Moving on, we want to explore
the bivariate relationshipsof factors with Goal Site
Conversion Ind in more detail,to assist in both interpretation
and presentation storytelling.The last visualization appears
on the right side of the reportand highlights the interaction
effect of a selected factorwith Goal Site Conversion Ind.We can review the natural
language explanationsof the visualization
and interactively selectother influential factors.After selecting the factor
customer intelligence interestindicator, other
visualizations appearto help us improve our
understanding of the influenceit has on Goal Site
Conversion Ind and the impacton identifying
attractive segments.A powerful feature
in SAS is the abilityto increase analysis
sophistication.There are two
additional exampleswe highlight in detail within
the supporting white paper,but regretfully,
did not have timeto cover in this
video presentation.The first-- switching
the underlying algorithmof the segmentation analysis
based on numerous options.And then the second--leveraging model autotuning
or hyperparameterization.I do genuinely hope you'll
check those examples out.So how can we turn our
insights into actions?Without a clear path,
there is no finish lineto deriving business value.To conclude this
process, let's focuson the results of the clustering
analysis from earlier.Specifically, let's cover
targeting attractive segmentswithin SAS Customer
Intelligence 360,as well as providing
transparencyfor how campaign management
processes can easilyabsorb algorithmic-defined
segments and take action.Now is the time to share the
prescription with marketingteams who manage
content deliveryacross consumer touchpoints.The audience segments
from the analysiscould be used for direct,
email, web, mobile,or other interaction types.To begin the
transition, we simplyneed to score customer records
with cluster segment tags.Rest assured, there are
different approachesother than using
a table to makinganalytically scored
data availablewithin and outside of SAS.What options do analysts have
to help their marketing teams?One-- the ability to target
segments in batch or real-time.And two-- access to the
segmentation scoring enginevia SAS, Python, or REST, for
APIs available through SASAnalytic Services.With the segmentation
analysis tagsnow available for
the marketer, let'srevisit the segment map
leveraging the clusteringscores that we
looked at earlier.To create the final step
of the segmentation map,a split note allows
us to leveragethe analytical prescription.For the marketer, it's
just as easy to leverageanalytical scoring as it
is any other piece of dataavailable in the system.After the segments
are defined, wecan schedule segments to run
once or on a recurring basis.If your brand is using a
third-party such as a mailhouse, print shop, or
email services provider,for execution and delivery, we
can choose the proper exporttemplate and send the output
files with names, addresses,and other personalized
informationdirectly from the
SAS environment.None of your
brands' managed dataever enters a SAS Customer
Intelligence 360 cloud.Likewise, contacts and responses
can be captured and collectedwithout requiring a
pass-through in the cloud.To absorb the clustering
enhanced segmentationinto the export
process, we simplyneed to add the segments
to the targeting criteria.Alternatively, SAS
Customer Intelligence 360doesn't rely on third-party
email service providers,sometimes known as ESPs.And it can actually support
email touchpoint deliveryitself.The benefit is that contact
and response trackingis immediately updated
in the platform,as opposed to a 24-
or 48-hour batch feedthat your third-party
partner sends back.And downstream
personalization canbenefit when customers
interact with your brandacross other touchpoints, such
as web, mobile, call center,and so on.Once SAS Customer Intelligence
360 is used to deliver email,a file is uploaded to the cloud
that contains addresses alongwith any personalization
parameters,such as customer name,
required by the email template.Once the email is sent,
the file is automaticallydeleted so that no
personal data persists.From a targeting
perspective, it'simportant to recognize
that a defined segment isavailable across any
supported touchpointor task within Customer
Intelligence 360.As in the direct marketing task
example discussed previously,the cluster segments are
available for the email taskin just a few clicks.Let's discuss this
value propositionin the context of
customer journeys.This example depicts a
coordinated series of web,mobile, and email
tasks that are designedto meet the goals of
a marketing campaign.Within Customer
Intelligence 360,customer journeys are
referred to as activity maps,which use tasks and events.It charts the customer
journeys between tasks,such as sending a message
through a channel,and the conditions, such as
the primary goal and evaluationperiods.For example, you
might use a web taskto present an offer
on your website.After allocating the
creatives for the interaction,the hybrid marketing
value propositionsurfaces again under targeting.This information is
available at the user'sfingertips-- demographics from
your managed data environmentoutside of SAS Customer
Intelligence 360's cloud,as well as analytical
segmentation scores, webdimensions, and behavior
stored inside the SAS CustomerIntelligence 360 cloud.Coming back to the
customer journey,we can use a mobile or email
task to communicate a messageto all users who received an
impression of the web taskoffer within the
last week but didn'tmeet the macro conversion
goal, as an example.Allow me to close with
a peek into the designprinciples of SAS
Customer Intelligence 360.There are two distinct themes.One is associated with
analytical, logical, andfact-oriented thinking,
while the other isassociated with creative,
intuitive, and visual thinking.This translates to
the following factors.The first theme links
tightly with the authoringand deploying of predictive
and machine learning models.The second facilitates
the orchestration layerof marketing activity
across planning, creative,and operational functions.Together, the intent is to
better understand and managecustomer activity, regardless
of channel, in alignmentwith the brand's
goals and objectives.At the end of the day, both
the analytically-mindedand the creative-minded
need to be in lockstepwith one another.This is how SAS Customer
Intelligence 360 works.Within this viewpoint,
AI has the powerto transform the
world around us.And analytics is at the heart
of delivering on this promise.SAS embeds advanced AI
capabilities into the platformto support initiatives
from start to finish.Against a backdrop
of acceleratingmedia and technology complexity,
evolving privacy regulations,and increasing
consumer expectations,hybrid marketing provides a
simple but powerful approachto address today's
MarTech ecosystem.You can turn
insights into actionby embedding AI within
your marketing processto complete analytics' last
mile and transform hypeinto reality.And with that, thank you.Get in touch.And I genuinely hope you
enjoy the other breakoutsessions for the users' program
of the SAS Global Forum 2020Experience."
121,"hello everyone and welcome to another
shall I say episode of SAS Global Forum2020 virtual Edition I'm here that my
name is hunter glans I'm here to deliverone of the hands-on workshops virtually
to you and so let's go ahead and getstarted the title of this hands-on
workshop is using Jupiter to boost yourdata science workflow again I'm hunter
glans I'm a statistics and data sciencefaculty at Cal Poly in San Luis Obispo
California and here's a little bit aboutmyself so at Cal Poly it's predominantly
undergraduates and I am extremelyfortunate and excited to be teaching
statistical computing in SAS R andPython at various levels to our
undergraduates including kind of moreadvanced data science and machine
learning type material I especiallyenjoy working with these kinds of
conferences and events hackathon eventsconnecting students to these and getting
them involved in orce all sorts of waysso with that let's go ahead and get into
things all right so the aims and themesof this talk are reproducibility and
integration of SAS with open-sourcetools like Jupiter the the integration
that SAS has with open source tools hasonly grown in recent years and Jupiter
is just one of many tools that that SAScan really work with but I'm really
excited to talk to you about Jupiterbecause it's an extremely flexible and
robust tool that helps in a lot ofdifferent ways for a lot of different
things all right so a little bit ofcontext in history right historically or
at least one kind of I grew up both as astudent and in my kind of early
professional career collaboration onprogress projects was hopefully
organized but possibly fragmented rightwhen I was at least going through school
right for a particular class this is anexample of a class I took an
undergraduate called stat 330 this isactually my
my class in in SAS you know I might havehad a lot of homework files SAS files
and things like that things like thatfor a particular homework but it was a
statistics program and so I was oftenbeing asked to perform things with data
you know compile them into answers toquestions or something like a report and
stuff like that and so while I did writeyou know plenty of SAS code in possibly
different scripts and things like thatit was almost always the case that I had
to kind of put everything together intoa final assignment form right and so
this was never kind of completely it itwasn't ever just the code it was putting
everything together as well so againkind of organized hopefully into
different you know pieces you got maybecode in one folder pictures graphs plots
those kinds of files image files in aseparate folder possibly you know
however you organize things but they'reall kind of separate and you had to kind
of you know combine them yourselfafterwards which was again okay right
but suboptimal in my experience thestory is similar between academia and
industry so you might have some data aspart of your your project or your your
goals your team is working on some kindof problem that has to deal with data
and so you know oftentimes there's acleaning a wrangling a managing data
phase of that pipeline and then eitherafter at some point there was a
summarization and visualization phasewhere you kind of explored that data
maybe an EDA type of phase and thendepending on what you were doing or what
your goals and what were your questionsto answer you are analyzing and possibly
modeling and then delivering thoseinsights those conclusions and things
like that in some kind of you knowsynthesis or report right similar to the
homework assignment that I justdescribed but in a mine you know much
more kind of sophisticated complex orwhole way right in kind of a big project
and so this comic on the right is kindof meant to describe how at least I've
often felt right this is this is a lotbut it was something that I kind of had
to get used to and I'm sure many of youhave had to get used to this at certain
points in your your career experienceand it can be kind of frustrating
because there's a certain level ofredundancy here right
you kind of know the story as you'reyou're going through the data as you're
building it in everything like that andso these these steps are often
overlapping right and be nice if therewasn't so much assembly required at the
end right that these things could allkind of happen in the same place so
again historically there was kind of afragmented collection of files code in
one place images in another data andanother and then maybe narrative or you
know text so you accompany your datastory in a separate file completely
right and so because of thiscommunication readability and
reproducibility kind of suffer becausethe more times you have to do something
the more places you have to to work onsomething the more chances and
opportunities there are for error andthings like that right and that's not
good so there's this unnecessarily largedistance between your data which is
maybe usually the starting point and thestory that you're going to tell with it
to either your supervisors collaboratorsteammates you know bosses whoever it may
be what kind of solutions are there outthere for this right because this may
have been you know something that you'restill doing something you historically
done and you may think well yeah I usedto do things that way but it's gotten
better right and I agree that it has soI des or other and other editors have
made something like this a lot easier todo some of these tools help facilitate
the process right so in my experience Iguess Emacs notebook + + vim or VI tend
to be tools that make the coding processeasier and better which is really good
because being able to share and documentand collaborate on code in a nice easy
smooth way is important but it's it'stools like SAS studio R studio and as
we'll see maybe Jupiter that are goingto make this whole process a lot easier
but in general these these tools heremake computing a lot easier and then
there are other tools for making areport writing easier and these were
kind of historically disparate right SAShas had a couple different ways to do
this so you know proc report proctabulate or kind of or maybe a half step
or step depending on your use of them inthat direction there's the report
writing interface for SAS which issomething that I don't have a lot of
experience with but I was blown awaywhen I saw it you can do a lot of really
cool stuff with the report writinginterface and that's all within SAS
which is really kind of nice our studioand are marked down those kinds of
things are a really nice tool forfacilitating report writing lay tech if
you're you know not a fan of MicrosoftWord and things like that can make
report writing a lot easier but theseare kind of tools that are independent
of or at least the the functionality forreport writing and these are kind of
independent of the ideas and tools thatmake computing easier right so even
though there's some shared tools herethe things that they help facilitate can
be a little bit separate so perhapsunfortunately until recently nothing out
there really addressed the completeneeds of a tool that kind of brought
everything together into a singlevehicle or or application right I want
something that again on the wholeintegrates coding and storytelling to a
point of like minimizing if notcompletely eliminating that distance
between the data and my story right soSAS 2d in our studio have kind of built
in documentation so there's some reallynice things there color coding
formatting that make computing reallyeasy or a lot easier our markdown the
report writing interface help with thisdocumenting and maybe adding our text
and contextualizing and things like thatto our code writing and analysis processwith latech is actually more built out
than you might think and so that can bekind of nice but again the idea is we
want a tool that really puts coding anddocumentation as far as like you know
writing text summarizing the analysisand narrating this data story in the
same place right I don't want to have toleave and change applications export
image files and do kind of you knowbreaking up of things and then
recombining them in another place soagain the capabilities of the ideal tool
are kind of what I just described I wantall of the things that are made easy by
those computing tools that I des andthings like that I want all of the
things that or as many of the things Ican get right of the report writing
tools so you know all the tools that youcan kind of imagine getting a Microsoft
word like bold italics kind of easyincorporation of images all that kind of
stuff right I want to streamline thisprocess of exploratory data analysis
data science research all this kind ofstuff and as a bonus it'd be really nice
if this didn't come in the form of somemassive tool right or package so
everything is kind of working togetherhere all right hopefully kind of
demonstrated by this colorful image onthe right and so cue the Jupiter project
this is at least a few years old now butI think it's still in some places just
gaining traction but if you haven'theard of it this is an extremely cool
and good useful tool to again boost yourdata science workflow and what's really
nice is it's compatible with over 40software languages so this isn't just
something to use with sass I hadactually kind of started with Python
originally but now supports over 40different software languages which is
really awesome all rightso that's
come forward that's what you we've beenbuilding up to but you may be a little
bit stuck on the previous slide thinkingabout those those cogs and that colorful
picture and going wait that those cogsdon't actually work right those don't
turn if you actually look at it thatimage is pretty static those wheels
wouldn't turn that doesn't really workif you look back at it so if you want a
good chuckle maybe go back and checkthat out but the eye let's go ahead and
move into talking more about Jupiteralright so getting started with Jupiter
notebooks how how does this work for youthis is a tool that is compatible with
over 40 different software languages butit is something that you install kind of
alongside most if not almost all ofthose software languages so if you're
we're gonna if you want to work withJupiter with SAS then you will need to
have kind of SAS installed as well asJupiter but Jupiter is a kind of
extremely lightweight at leastrelatively speaking and so that
shouldn't really be a problem if you areinterested in Jupiter with Python you're
probably thinking this is already kindof old news to some extent Jupiter
actually comes with the Anacondadistribution of Python by default now so
if you're going to if you want to gocheck things out just kind of play
around Jupiter notebooks it's all freeto use right now
Jupiter is free to use and so you cancheck that out just by downloading
anaconda if you want if you really wantto check out Jupiter with SAS though
that's also free to do and I'm going toshow you how to do this with SAS
university edition so the top image hereis the one that we're gonna pursue if
you want to get started with Jupiterkind of right away or you've been
working with it in SAS University dishand you're ready to kind of make the
leap then the SAS kernel for use withJupiter notebooks is available on github
so you should search for kind of thisrepository within github and follow the
follow the directions there for settingup SAS with Jupiter notebooks on your
own kind of machine servers whatevermaybe it
your home or organization or whatever inmy experience the support of this kind
of project on sasses side has beenphenomenal
they're very communicative and awesomein helping people set this up so I
definitely encourage you to check thisout if that's what you're interested in
but let's go ahead and check out sassUniversity edition so if you haven't
checked out sass university editionalready I'll encourage you to do that so
this is kind of the freely availableversion of sass that everybody can kind
of set up and use on their own personalmachine and we're gonna actually gonna
its accessible and usable through theweb browser so that's what I'm going to
jump to now but this kind of assumesthat you have
satis University Edition set up and canaccess that so when you start up sass
University Edition and go to the kind ofpage that it prompts you to go to once
you've started up your virtual machinethis is what you're going to be looking
at and this actually comes this is howsass universities edition is set up for
you kind of by default which is reallyawesome and by awesome I mean what's
staring you kind of biggest in the facehere is welcome right sass University -
and Information Center this is kind ofthe homepage SAS studio is the interface
that is kind of you know the main mainline here and that's that's kind of the
primary interface I think that mostpeople are used to working with Assassin
University Edition in SAS studio isgreat if you haven't worked with
University Edition this is a greatinterface to use and everything but
again for at least a couple of years nowJupiter notebooks and what's now called
Jupiter lab is also accessible to use asa different interface than SAS studio so
if you click start Jupiter lab hereagain there's no extra installation or
anything like that needed it kind ofcomes with SAS University Edition if you
click thatyou'll be brought to a page like this
okay what we're looking at is the kindof Jupiter lab interface and in the kind
of biggest portion of the screen here isthe launcher and in SAS University
Edition these are kind of the defaultand I think only options I may be wrong
on that point but these are at least thedefault options so we can start a
notebook that uses Python a jupiternotebook that uses python or a jupiter
notebook that uses SAS which is whatwe're going to do or we can actually
just start up a text file and if youkind of pursue working with jupiter on
your own machine for your own uses andthings like that you'll find that you
can use jupiter for lots and lots ofthings it can be again a way to interact
and work with text files create your ownstuff like that you can work with lots
of different programming or computinglanguages and then on the left here in
the left kind of panel what we'relooking at is just a kind of a
differently looking file explorer' okayso these are folders you know on my
machine that I can kind of navigatearound and things like that when you
setup sass University edition it has youset up a shared folder between your the
virtual machine that says UniversityEdition is going to be accessed through
and the actual like you're actually youractual computer so that assess
University Edition can actually accesswrite data files as scripts all that
kind of stuff and so that you can accessall the code and cool work that you do
you know once you're done with it insideSAS University edition so these folders
that you're seeing here on the right arewithin that shared folder of mine when
you open up SAS University Edition andJupiter lab here you're not going to see
these folders right these are on mymachine I've created these and so you'll
you'll see a little bit differentsomething a little bit different but you
can navigate around the folders here asyou would kind of on your own machine
okay all of the materials the slides andthe Jupiter notebooks you're about to
see are all available on a githubrepository publicly available that I
will show you at the end of thispresentation okay
so you'll be able to access everythingthere so let's go ahead and go into the
lessons folder instead of if I wanted tostart a new fresh notebook
Jupiter notebook I would click one ofthese buttons here okay
let's just do that actually so you cansee so if I click sass under the
notebook kind of area here it's going toopen a new notebook in the location that
I'm in within my kind of folderstructure here on the left and you can
see there's it's it's hard to tell thatanything actually happened but you can
see a new kind of file is open it saysuntitled IP Y and B at the top and we
have this kind of gray looking cell hereat the top that I can click into if I
want and things like that and again Iwant to point out that it says SAS in
the top right the whatever it says inthe top right is kind of the kernel
that's being used as to the softwarelanguage that we've kind of chosen to
use under the hood of this notebook okayJupiter notebooks are basically made up
of cells and you can have as many cellsas you want these cells can be one of a
couple different types the primary twotypes are the only that we're gonna talk
about today in this video you can havecode cells or you can have markdown
cells and with a cell kind of selectedyou can choose from this drop-down what
kind of cell you want that to be so ifyou want to be a code cell then this is
a cell in which you would write SAS codeif you wanted it to be a markdown cell
you select markdown you can see that thelook of it changes a little bit and now
when you type in it it's just going tobe text instead of typing anything here
yet I'm just going to walk through kindof the tools and skeleton of the
notebook and then we'll go over to thesepre-built notebooks that I have in the
github repository that you'll be able toaccess as well there are keyboard
shortcuts for every everything that youcan think of doing inside the jupiter
notebook but you can also kind of doeverything through the menus
here at the top so we have you know fileedit view run all kinds of stuff up here
which is really nice so it's it's prettyaccessible and stuff like that the main
thing that I think we we're going towant to do though is add more cells
usually Jupiter notebooks are acomposition of lots and lots of
different cells some code and somemarkdown I like to use some of the
keyboard shortcuts because they'rethey're much quicker to use if you're a
VI or vim user then you'll be I thinkvery happy to find out or discover that
the Jupiter notebook supports a lot ofthe same or a few of the same than VI or
vim controls that you're used to so withnothing selected here or I'm sorry with
the the top kind of only sell hereselected as indicated kind of by the
blue bar there if I hit the B key itcreates a new cell below b4 below the
cell that was selected if I hit enterwith that key selected or that cell
selected I go to I go into kind of anedit or insert mode and so now you can
see that I have the freedom to kind oftype into this cell notice that this
cell was created with kind of a defaulttype of code right so this is a code
cell so I could type a data step youknow my data step X is one run proc
print data equals my data step run sothis looks like you know some SAS code
that you would normally see if you wantto run a code cell so execute the code
in that cell you can have you know comeup to the the run if you want but you
can see that there's actually keyboardshortcut for that as well which is
shift-enterso again with the code this this cells
selected if I hit shift enterright SAS connection establish some kind
of you know behind-the-scenesinformation there but then directly
below it below this code cell I'm seeingthe output of that code cell right which
is really nice so I'm seeing the resultsthe output of my code in line below
where that code was typed and run and soagain if I hit there's all kinds of
shortcuts here so if you want tonavigate between the cells again you can
use kind of VI or vim controls otherwiseyou can use the arrow keys you can use
the mouse to click around in cells andwe can generate as many cells as we want
so I just hit be like 10 more timesright to generate all these cells if I'm
in this cell here and I hit a a4 abovegenerates a cell above that particular
code so okay again we can mix and matchit doesn't matter kind of what order
they're in I can make this a markdowncell right so hashtag in markdown
language is kind of a header right andnum1 hash is a header of level level one
so this is you know my sass code demothat looks a little bit weird but if I
render the cell which is the samekeyboard keyboard shortcut so shipped
enter that markdown actually getsrendered right now kind of on the fly
and looks really good right that looks alot better than hash my sass code demo
so it's actually being rendered as alevel 1 header right and I can't see
that if I wanted to get back to that Icould select that cell and hit enter or
I could just double click so double leftclicking the cell also brings me back
into kind of an edit mode here so that Ican make changes and all that kind of
stuffalright that was a lot of information
really quick about like introducing youto a new notebook I'm gonna step over to
this typesetting with Jupiter notebookthat is kind of pre-populated in the
collection of materials that you haveaccess to so I'm double clacking
double-clicking that - to open that youcan see it's appearing here in kind of
our viewer kind of main window here justnext to the previous untitled file that
we had opened originally and you seethere's lots of text in here right so
what I wanted to talk first with youabout with without really getting into
any code yetbesides that example I just showed you
is the typesetting right so all of thesecells in a Jupiter notebook have the
option to be markdown cells alright soyou could just use this as a typesetting
tool if you wanted to write markdown isa pretty simple straightforward language
if you don't know markdown or youhaven't heard of markdown all you have
to do is kind of Google search markdowncheat sheet or markdown tutorials and
things like that and you can do there'ssome really nice kind of references out
there for markdown language but whatyou're seeing is kind of a rendered
Jupiter notebook right but these are allmarkdown cells so there's no code here
and again double clicking just lets ussee what's under the hood so if I double
click the cell you can see it's anotherlevel one header typesetting with
Jupiter notebooks and then it just lookslike kind of regular text but when I hit
shift enter it actually renders thatcell okay again one of the biggest
features that all kind of repeat lotsand lots of times is that Jupiter
notebooks allow you to do data scienceand by that I mean include both rendered
markdown text and live code along withits resulting output in the same file or
location right this is that one kind ofunifying tool that I was saying we want
okay so before we get into programmingagain let's first explore the
typesetting a bit sowe have lots of cells here there's a few
there's a few more bits of informationin this cell I'll let you read through
this on your own for explain thecontrols editing and all that kind of
stuff and if you actually double clickthese you can actually see what I did to
actually kind of type them right orformat them so you can tell here that it
says you know ctrl + Enter and the ctrl+ Enter seem to be in bold right so how
do we do bold in markdown becausethere's no kind of B key right or
control B necessarily if you're aMicrosoft Word user so you can see and
you would see this on a markdown cheatsheet that to do bold we put double
asterisks on the outside of the thingthat we want to be in bold ok these
headers are a little bit smaller thanthe top header and you can see two
hashes means level 2 header right soit's a little bit smaller than the
overall header as I scroll down we cansee lots of other things and hopefully
it's starting to feel a little coolright if not a lot cool that you can do
such nice things in such a niceeasy-to-use interface right I really
like the feel of Jupiter notebooks andbeing able to do this typing and
rendering for individual cells as we goright and you're actually not forced to
do that so let's just say all of thesecells you know I had my 1 2 3 or 4 cells
here all kind of an edit mode right Iwas actually like writing and building
this notebook out for presentation toyou great and I hadn't hadn't done
anything with it yet I can actually goup to run here
and I can run just the selected cellthat's what I've been doing so far right
that's shift-enter but I can actuallyjust run all the cells or I could run
all the cells above the selected cellright so that will render every cell
above whichever cell I had selected sothis one didn't get run but everything
above it did okayand so I I really like this this jiff of
Shia LeBouf right hopefully this feels alittle bit like magic I know it does to
me still but I'm pretty easily amazedone of the other things you'll see if
you look at that markdown cheat sheet ishow to include images or gifs or even
videos I think inside of markdown andthat's the kind of code if you will or
syntax is is not all that difficult sothis is just a link to that particular
jiffand so I didn't have to download that
file or anything it's just a link tothat jiff which is really nice and
straightforward so bulleted lists bolditalics strike throughs all of that kind
of stuff including links right are allkind of very straightforward it's
straightforward to do so there'sactually a link to a markdown cheat
sheet here so I told you that you couldgo ahead and Google that but there's
actually a link right here that I'llI'll point you to if we were in person
at SAS Global Forum then I would haveencouraged you to check out the first
exercise notebook and try some of thisstuff out and I would have been able to
kind of walk around and help you outexercise 1 is just basically doing your
own version of the type SOI typesettingnotebook so start a new notebook create
a numbered list of your top 5 favouriteSAS procedures for each of these include
a link to the online documentation forit then in separate cells maybe create a
header to the name of that procedure andan image that you think best represents
that procedure and then finally createat a
that contains three rows two columnswith information on other talks right or
videos that you wanted to see at SASGlobal Forum whoops 2020 since I'm not
there I won't be able to kind of helpyou out with this one-on-one like I
would have liked to but this will stillbe included in the github repository
materials and so I would still encourageyou to give this shot just to play
around with it and try it out that's alittle bit about typesetting not not too
complicatedwhat about code right this is all about
how to work with Jupiter with SAS sowhere does programming come in all right
so my first SAS stupider notebook thisis the programming with SAS notebook
file that's also included in the lessonsfolder of our repository and here you're
gonna see that I've got some SAS code todemonstrate write this is fantastic can
this be real so an introduction tobasics s I am a statistician by training
and so of course I have a soft spot forFisher's iris dataset if you're not
familiar with this dataset there's alink here to a description of it this is
actually such a popular dataset thatit's one of the ones included in the SAS
help and in many other kind of languagesby default in there installation and so
as we saw with that very first exampleif I run this cellthat took a few seconds but if I run
that cell right it's proc SG plot I'mjust creating a scatter plot of my data
the iris data sepal length petal lengthwith the points colored by species right
and again what's super cool and amazingis that this graph is appearing in line
after the code here all right so it'skind of I could type some introduction
and some narrative text about what I'mabout to do or what I'm doing some code
to do that thing and then the output isgonna be is gonna appear right here
right so this is extremely easy tofollow it's kind of the epitome of
reproducibility right because I canactually step through someone's analysis
their whole thought process and datastory and hit run as I go to see what's
going on right so this was an example ofa code cell notice that we get the nice
color coding that you're maybe used towith SAS editors it may not be the
colors that you're used to but we getthe nice color coding that we'd like but
that's that's really nice alright soI've got another kind of set of code
here some basic summaries of the irisdata set right to my knowledge we can do
almost anything you can think of in SASbut we're not limited to comments as our
only means of documenting and annotatingour code right so what's awesome is that
we can kind of mix and match and we'veinterweave our code and our text so some
code here a graphic maybe an explanationor some more text a new section some
more text and now write some more outputand all of this is very very
customizable so this is kind of thesimplest possible example or one of the
simplest possible examples I could giveyou pretends the first 10 rows of the
data set and then proc means right butif you wanted to you know limit the
output or do whatever you normally do ingreat sophisticated ways in SAS those
are all kind of still at your disposalhere within the jupiter notebook if
you're used to SAS though right you'reprobably wondering where the log went in
many ways it's nice that our notebookhas remained clean and only included our
text code and output but the log is apretty critical part of SAS were right
being able to debug and see what's goingon and all that kind of stuff below are
two ways you can access the loginformation and here you go so a
it may have looked like these appearedquite suddenly I think I had a small
typo in my notebooks but they're here inthe video now and it's a small Google
search a way to take a look at the lockso there's % show log and there's % show
full log so there's our two differentways to view the log one I think is just
the most recent log and one is kind ofthe full log history so either way all
you would do is execute say one of theseso let's just go ahead and look at show
log all right so I don't haven't run anycode recently I guess besides the
previous cell and I've run some otherstuff since then but you can see the log
is going to appear in the cell rightbelow it alright so maybe just to expand
a little bit it wasn't mentioned thetypesetting notebook but we also have
access to math notation via latex typesyntax so if you know latex or you're
familiar with that or you have a desireto include math syntax inside your
notebooks and data stories you actuallycan do that alright so you may run some
regression analysis here's our proc regon the iris dataset you may have seen
proc reg output before right and if Iwanted to incorporate that information
or kind of document that right so myslope and my intercept here into you
know a formula you know formula or mathsyntax notation I can do that here and
again users of latex will recognize thisbut otherwise you can look up manuals or
tutorials cheat sheets on latex there'sone here a two a nice two-page cheat
sheet on latex and text but it's not toobad to incorporate that kind of math
syntax into your markdown cells so to dolay tech this is a markdown sale not a
code sell and you're just doing that andrendering it with the same hotkeys as
beforeall right
usually proc contents is a good firststep in any data exploration or analysis
but it was until after plotting that weran proc contents it was until after
plotting that we ran proc contents rightwe can kind of move things actually I
think we did the reverse we actually ranfrock contents first before we ran proc
regression but we plotted the databefore we ran proc contents right so
proc contents wasn't the first thing wedid even though it's usually a good
first step what's cool about Jupiternotebooks these days because I don't
think this is always the case is that wecan actually drag and drop cells they're
really really easy to kind of movearound so if I wanted to say kind of
acknowledge that yeah proc contentprobably should have been one of the
first things I did if not the firstthing I did I can click drag this cell
up above that plotting right so I kindof dragged it a little bit too far but
you can see it was kind of easy enoughto just grab that entire code cell and
move it up above so it's a little biteasier than say selecting all of the
code in a particular code cell copyingit pasting it all that kind of stuff so
this is meant to be just kind of a anintroduction in the exercises folder
there's a second notebook that asks youto actually run some sass code on one of
the data sets in the sass help libraryso if you're not familiar with the sass
help library there's a bunch of datasets that kind of come with a sass that
you can checked outand so this notebook just asked you to
run some sass procedures and things likethat on a data set of your choosing from
the sass help library okay so in summaryJupiter is a web application and abling
creation of documents that contain livecode equations visualizations and
explanatory text it's that singlevehicle that I was that I'm I've been
yearning for and I hope a lot of youbeen kind of yearning for write for a
long time I will I will acknowledge thatthis isn't a replacement for everything
right if you do a lot of computing or alot of coding and sass not all of our
SAS code is telling a data story rightwe may be doing a lot of heavy lifting
computing modeling whatever it may bewith our SAS code data wrangling we may
be writing a lot of code that does notreally contain or include a full data
story right this the stupider notebookinterface an environment is really I
think best for telling a data story andif you're writing SAS code for a certain
you know a single piece of the pipelinethen I'm not sure Jupiter is something
you know that is kind of meant toaugment or replace that right or or
improve that so this isn't thiskind of you know Mendte to replace or
augment everything but it is useful in aton of situations I think this again
extremely streamlines the the whole datascience workflow and communication and
reproducibility in a team that's workingon data analysis data wrangling all that
kind of stuff again over 40 languagesare supported including SAS Python and
are kind of the three of the three ofthe biggest kind of statistical
computing languages we can have codewithin the notebook that can produce
rich output such as images videos latexand JavaScript so I kind of gave you the
tip of the tip of the iceberg as far aswhat your code can create but you can
actually do a lot of cool thingsincluding interactive widgets that can
be used to manipulate and visualize datain real time so your your Jupiter
notebook can contain kind of niceinteractivity which is really awesomeit's extremely easy to share these so
you might have seen that file extensiondot I py and B and thought okay this is
kind of weird it's not a SAS fileyou know what is this and I'll go back
actually to my notebook here so you cansee this is kind of originally short for
ipython notebook which was the originalname of the project years and years ago
but it's kind of moved beyond that thesenotebook files
you know we're maybe originally kind ofweird and different but these are
extremely shareable now so it's notuncommon for collaborators teammates all
that kind of stuff to just sharenotebook files amongst each other but I
also want to point out that if you go tofile/save notebook as I know that's not
what I wantexport notebook as sorry so files export
notebook as there are actually a lot ofthings that you can save your Jupiter
notebook as so you can export it to PDFyou can export it to markdown but I
think one of the coolest options here isHTML and executable script
right so if you were to render orexecute all of the cells in your
notebook then it's already all been runit's all kind of done maybe your outputs
there and all your text is rendered atthat point right it's kind of a nice
deliverable HTML or PDF and so it's kindof a finished product at that point so
being able to export straight to thatkind of finished product file you know
file type is really nice if acollaborator or teammate really just
wanted all of your code but it did youknow they didn't want all the narrative
text and stuff like that you might thinkyeah okay I could understand that like
you don't need to keep rereading all ofthat stuff or navigating through my
notebook to see it all if you exportyour notebook to an executable script
it's just gonna be the dot SAS file thathas the code from every one of your code
cells and that's it so I really likethat option all right
again these notebook files have justblown up in popularity and accessibility
these are actually kind of renderableand you're able to interact with these
on github binder is a great kind of reaprepository for jupiter notebooks and
things like that so I would check thoseout again I can't emphasize enough I
think the ease of use here right so frompersonal use to jupiter hub for
organizationsI think jupiter notebooks make
statistical computing easier to do andshare than ever before the need to
minimize thought to execution frictionis probably the single biggest
productivity requirement I think incorporate America and so Jupiter
notebooks really let us take a giantleap to achieving this when it comes to
data science and statistical computingso to really sum it all up it's a single
vehicle for all that stuff that used tobe all totally separate their dynamic in
the sense that you can run these cellskind of independently or in collections
view the results in line all that kindof stuff and you still get the bonuses
of at perks of your kind of reportwriting facility or a coding editor okay
so that concludes my presentation myemail address Twitter handle are here if
you ever want to reach out ask questionsor connect with me about any related
kind of topics all of the materialsslides and Jupiter notebooks themselves
are accessible at this github repositorywhich is publicly available so get all
calm that's me H glands and then this isthe name of the repository so all of
that is available to you whenever you'dlike all right thank you very much
see you aroundyou"
122,"Hi, my name is Steve Sloan
and I work for Accenture.I'll be talking about
the different waysthat we can make our programs
run faster and use less space.Got 20 of them and my email
address is at the end.If you can think of any
others, please reach out to me.And also if you
have any questionsor you want some
more information,please feel free
to reach out to me.I am a graduate of
Brandeis University.I have master's degrees
from Northern Illinoisand Stern Business School.And I have a certificate
of financial analysisfrom Stevens Institute.I'm a data science senior
principal at Accenture.And I lead multifunctional
projects thereon a cross-industry
basis using data scienceand artificial intelligence.I've presented a number
of SAS events, over 20regional and global forums.And I've been published
in professional journals.And I really appreciate
being able to workwith the nice people at SAS
Institute, who are alwaysvery helpful and cooperative.In fact, on some of my papers
we've collaborated together.Now what where we're
looking at hereis that it can take a long
time for our programs to run.That's inconvenient.It can frustrate our users.It can retard our development.Also the data being
used by the programscan occupy a lot of
space and that can makethem take even longer to run.First because they're
crunching through more space.And also because sometimes
you run out of spaceand you have to start over.So anything we can do
to save time and spacewill make our lives
easier and enableus to serve our clients better.Now we're often in a hurry.It's not like we're
starting from a blank slatewhere we use all the
traditional methodsof structured programming
and being careful.And sometimes we've got
someone else's code.And they say, can you just
make a simple change to it?Which is never as
simple as they say.Although we benefit in reverse.Sometimes things
that look complicatedcan tend to be simple.But I want to focus
more on the other side.We also run into issues where
we have very tight deadlines.So we don't always
have a lot of timeto really improve the program.We just want to get it to run.Now there's four different--I'd classify these
efficiency programsin four different ways.One thing we can do is we
can reduce the observationsand variables.It's the rows and columns.We can reduce the
number of timesthat the data is processed.We can look at
sorting efficienciesto make it sort faster.A lot of times those PROC
SORTs take a very long timeif we're not careful.And we can also reduce
the size of the variables.A lot of times we can
squeeze more variablesinto our existing
storage facility.We don't have to ask for
new storage facilities.We don't have to
run things in piecesif we can reduce their size.So to reduce the number of
observations and variables,there are a few
things we can do.We can use DROP, KEEP, DROP=,
KEEP= to make sure that onlythe needed variables are used.DROP and KEEP are commands
you can use in your program.You don't want to use both,
because it will keep everythingthat you don't drop.It'll drop everything
you don't keep.What I try to do again to make
the programs easier to readis, whichever list is shorter,
that's the one I'll use.If I'm keeping
more than dropping,I use a drop, and vise versa.Now DROP= and KEEP= you can
put right in your data setstatements.In your data statement and
in your set statements.Your OUT= statements.And there you can
really mix and match.You can have a DROP= to bring
things into your data set.And then you can put that
on your set statement.Or a KEEP=.And then when you're
leaving the data step,on your data statement
there are some variablesyou might have needed
for your calculations,but you won't need
on the output.You can use the DROP= or KEEP=.You can use the subsetting
IF or the WHERE statements.Make sure that only needed
observations are used.And again you can
use WHERE= and DROP=,KEEP= on the output
data from your PROCs.One thing I do a lot is
on the PROCs summary,I almost always drop the
underscore type underscore.Quite often the _FREQ_.And these can be very helpful.So it saves you the
trouble of going backin and running another data step
to do your DROP or your KEEP.Especially on a PROC SORT.Again, you can reduce the amount
of space you need and timeright up front while
using a DROP= or KEEP=.And then finally, you can use
the _NULL_, the DATA_NULL_.If you don't need
an output data set,that can be useful if
you're just setting upmacro variables for future use.SIM, PUT things like that.And just use the DATA_NULL_ and
you're not taking up any spacewith this SAS data set.Now you can reduce
the number of timesthat the data is processed.And where you would do that
is, you can use the REMNAME=in the set with the
MERGE statements.Again, sometimes you're
merging data sets,or you're setting data sets
in a concatenation format.And when you're
doing that, you oftenhave enough variables that
mean exactly the same thing,but they have different names.Now what I used to do before
I realized I could justRENAME in the SET or the MERGE,
was create another data step.Create a copy or take
the existing dataand RENAME the variable there,
and perhaps drop the old value.You can do it right in
your SET or your MERGE.So if you're merging by AB and
it's AC in your other data set,you can use the RENAME= right
after the data set in you SETor your MERGE statement.You can use the PROC FORMAT with
the CNTLIN or use IF-THEN-ELSEor CASE constructs.And this can be very valuable
because now you're nothaving to go through a MERGE.Especially the PROC FORMAT.The CNTLIN is very useful
on the PROC FORMAT.Because you're
defining a format.And then you can use
the PUT statement.So you don't have to do a SORT.You don't have to do a MERGE.You can just do
the PUT statementright in your data step.That'll save you
quite a bit of time.Again and the IF-THEN-ELSE
or the CASE constructs,you can just use to
manipulate the data.So you don't have to reprocess
it through multiple data steps.PROC APPEND instead of SET you
can use to concatenate the SASdata sets.And then it's only processing
the one being appended.It's not processing the
one you've identifiedas the base data set.So if you have a data set
with 20,000 observationsyou want to append 2,000
observations to it.And they have the same
variable names or you've doneyour RENAME=.At that point, if
you use the SET,then you're processing
22,000 observations.But if you use the
PROC APPEND, you'reonly processing the 2,000 that
are in the smaller data set.You can use the CLASS statements
with PROC SUMMARY or PROC MEANSwith unsorted data.And again I want
to alert you here.If the data is already
sorted, then it'sfaster to use the BY
statement with the PROCSUMMARY or the PROC MEANS.Because it's only going to
process the individual segmentsidentified by the BY statements.It's only going to
process those subsets.On the other hand, if
the data is not sorted,then you can use
the CLASS statement.And you won't have to sort it.So the hierarchy
here is the fastestis the PROC SUMMARY or the PROC
MEANS with the BY statementfor previously sorted data.The PROC SUMMARY or the PROC
MEANS with the CLASS statementsfor unsorted data.And then using the
SUMMARY or MEANSis the slowest with
the BY statement.And you can take the most--if you're running IF-THEN-ELSE
or CASE constructs,put the most commonly
occurring situations first.Because what will happen
is once it finds it,then it will stop going
through that decision tree.So if you puts the most commonly
occurring situations first,it's only going to go
through the first conditionon your IF-THEN-ELSE
or your CASE construct.Now there's some
sorting efficienciesthat are very helpful.You can use the TAGSORT
with the large data sets.With PROC SORT.And it will use less space,
but it might take longer.What will happen
with the TAGSORTis it'll take the variables
that you are sorting byand it will temporarily separate
them and put pointers back.And so it will take
those variablesand it'll just sort a small
data set containing onlythose variables.So it'll use a lot
less space because itwon't be pulling your entire
data set into the buffer.However it could take longer.It depends on how
big your data set is,and how much time you're saving.Because you are going
after the data twice.First you're going after the
data to pull your BY variables.And then you're going
back and getting the data.However, the efficiency
you gain from sortinga much smaller number of
variables will, in many cases,more than make up for
the fact that you'regoing after it twice.And something I've got on
my to do list for the futureis to see if I can
get more specificand actually get an algorithm
for when to use TAGSORT.And when TAGSORT will
make it take longer.But if your main
constraint is space,TAGSORT will definitely
save you space.Now you don't have to run a PROC
SORT to get a sorted data set.Your data set may
already be sorted.And if you know
that it's sorted,then you can use
the BY statementswith your other PROCs.And you remember, in
the previous slide,I talked about using
the BY statementswith the CLASS and the MEANS.There you're saving the
trouble of the sort,but you're getting the
benefit of the factthat you're PROC is processing
smaller subsets of the datainstead of the entire
data set all at once.And finally, there's a
PRESORTED option in PROC SORT.And PRESORTED will tell you if
the data set has been sorted.That's a short job.And if you're not
sure, PRESORTEDmight ask might add a
little extra time if youknow it's sorted, but it'll
save you from some sorts.Now you can also reduce
the size of the variables.Reducing the size
of the variables,there's a number of
different ways to do it.You can use the LENGTH
command to definethe lengths of both character
and numeric variables.What will happen
quite often, you'llbe bringing a data
set in from Oracleor from an Excel
spreadsheet or CSV file.And what you'll find is that
SAS will default 255 characters.And that will almost always be
much more than what you need.So if you need to
save space and youknow what the maximum length
is, use the LENGTH commandand just define the
length of the variable.Now if a numeric variable
is always an integer,you can use the table
at the end of the deckto define the length.I'm going to go
to that table now.On this table, you
have the lengthin bytes and the
maximum absolute value.So if you know that your
variable is always an integer--And this is important
because it doesn't alwayswork if any of them
aren't integers.Then you can use that
table to define the length.Because SAS will default to 8
bytes on a numeric variable.And as I mentioned
earlier, you wantto calculate the maximum
length of charactervariables, especially those
from Oracle or from Excel.Now there are times when an
integer numeric variable wouldbe better off as a
character variable.And again, looking
at this chart,you can see that the smallest
length for a numeric variableis 3 bytes.Now if your numeric variable is
only 2 bytes or is only 1 byte,then you'll save space
as a character variable.And you can check and
make sure that it'sbetween minus 9 and 99.Remember the minus sign takes up
space as a character variable.So even though you can go up
to 99 on the positive side,you can only go to minus
9 on the negative side.And I find this quite useful.Quite often I'll be
working with data that'sbasically yes no or zero one.So I only need 1 byte.And it comes in is 8 bytes.I can reduce it to 3 bytes
If I keep it numeric.But I can reduce it to
1 byte if I switch itto a character variable.And then also, you can take your
all-integer character variablesand switch them to numeric if
they're longer than 3 bytes.And again, looking
back at this chartfor the numeric variables.You can see that you can
get up into the millionsor the billions as you start
to get lengths and bytesof greater than three.So for instance on
that example, if youhave something that
goes up to a millionand it's a character variable,
you're taking up 7 bytes.If it has commas,
you're taking up 9.But as a numeric variable,
it would only take up 4.So again there are
tests you can use.You can use the INT function.And you can test and see
if they're all numeric.If you want to check
if they're all integer,you can use the INT function.Which will truncate and check
and see if they come outthe same after that truncation.And then once you know
they're all integer,you can reduce the integer size.And also you can
use the functionto check for missing values if
you're using an INPUT functionon the character variables.Now here's some
other efficienciesthat can be very valuable.You can use REUSE=YES and
basically that'll just reusespace in your system.Sometimes SAS will
not reuse spacethat isn't needed anymore.On your Windows or
on your UNIX boxes.And so REUSE=YES will free up
more space for your programto use.So that after you've
deleted files,you get some of that space back.Or after you've
reprocessed them.Now you can use COMPRESS=YES
or COMPRESS=BINARY.COMPRESS=YES gets rid of
leading and trailing blanks.COMPRESS=BINARY doesn't always
respect the byte boundariesand just crunches the
data all together.I've gotten great, great
reductions in space usingCOMPRESS=BINARY.It could make the job
run longer, however.Because when you're
using these compression,SAS will decompress
when it's processing.Again, that could be a future
research to get an algorithmabout testing specifically when
COMPRESS=YES or COMPRESS=BINARYwhen the compression,
which save space,will make the job run longer.Again, especially with
data sets you're notgoing to use very much, you can
compress them and it really,really saves you a lot of space.Sometimes with COMPRESS=BINARY
I've been able to cut my spaceusage by 90% or more.Now when the program
finishes, SASdeletes all of the
work data sets.All the data sets with
the work live name.The worked-out data sets.But you can use PROC DELETE
and you can delete themwhen you no longer need them.And again that frees up space.Especially when
combined with REUSE=YES.And when you're extracting from
external databases like Oracle,do as much work as possible
in the external database.If you do it through
the connect and youwork in the external
database, then you'retaking advantage of a
lot of the efficienciesthat are in Oracle
or in Teradata.And you're doing a lot of
work there based on the factthat it was stored in Oracle.You take advantage
of the efficiencies.And so you're only
bringing back to SASthe rows and columns that you
need after the work is done.On the other hand, if
you do with the LIBNAME,it brings everything into SAS
and then works on it there.And so you've got
the migration issueand you've got the
time for the migration.And also the time to
work on it in SAS.Now it could require
more processingonce it's in SAS because
again, everything has a caveat.The one advantage of using
the LIBNAME doing it in SASis that you have your SAS
functions available to you.If there are SAS
functions that are notavailable in the
external database,then you will need to at least
work partially on it in SAS.Again a good mix is
to do as much as youcan in the external database,
bring the rest over to SAS,and then finish up with any
SAS functions that you need.And a lot of this
is a judgment call.And a lot of this is
using your experience.Knowing how much you can
do in the external databaseversus how much you
need SAS functions.On all these things,
there's always a caveat.Here's is the chart that
we showed about the lengthof the numeric variables.My contact information is
stephen.b.sloan@Accenture.comPlease feel to reach
out to me at any time.I'd be happy to review this.I'm always learning.I'm always getting
good suggestions.So if you have
tricks, techniques,tips for efficiency,
please send them on.Perhaps I can incorporate
them and give you creditthe next time I give
this presentation.Thank you very much for
taking the time to attend."
123,"Hi, this presentation is called
using the R Interface in SASto Call R Functions
and Transfer Data.I'm the presenter, Bruce Gilsen,
Federal Reserve Board, and hereis my bio, which I'll
leave for a moment.Introduction-- well,
first of all, thisis a 50 minute presentation
that we've compressed downto about a 20 minute
presentation for this format,so I would urge you, if
you want more detail,to go to the paper that's
going to be publishedin the conference proceedings.It will follow the
same narrative flowas this presentation,
and I'll have--you'll see some things I'm
going to omit that you can findin the paper for more details.Here we go.This interface is for
Linux and Windows SAS usersstarting in SAS 9.3.You can call R functions
from within PROC IML,get the results
back, and you cantransfer data between SAS and
R, either between a SAS dataset or an R data frame,
or possibly an IML matrixand non-matrix if
you are so inclined.And you'll be able to copy R
data into the R interface whichcan do R functions.You'll be able to save R
data from this interfaceto files with
standard R functions,and you'll be able to
call R from within SAS,not the other way around.You're a SAS user inside of
SAS, calling this wonderful Rinterface.Now, who are the
potential users?Well, the first group
is PROC IML users,and I know there might be
a few of you out there,but that's not the focus
of this presentation.The focus is for
people who are notIML users, which is most of us.But you've got an IML
license at your site,and in order to do what's
in this presentation,you must have PROC IML
license at your site.That's really important.But if you do, even
if you basicallyknow zero about PROC IML,
you can use PROC IML just asa wrapper to transmit
data between SAS and Rand call R functions and
get the results back.Again, I'm going to focus on
those of us who are not IMLusers, but have an IML license.Why would I do this?Well, we live in a
world where there's SAS,there's R, and
other languages, Youmay want to use R data as
input to a SAS program.You may want to
export SAS resultsto R for further processing.I think perhaps one of the
biggest usages is as follows.Because R is open source,
what often happensis there's an
econometrics journalarticle or statistical article.And some new wonderful
technique gets put out thereand gets talked about.But if that happens
on Friday, by Mondaysomebody somewhere will
have an R routine to do it.Now it might not be robust.It might not work all the time.Might not always get
the right answer.But caveat emptor, it's
out there, it's available.Now, SAS on the other
hand is a mature language.For them to introduce a
new econometric techniqueis going to take some time.They're going to have to develop
it, test it, do QA, integrateit into SAS.So on and so forth.That's going to take a while.And you want to use
this new technique now.But you're a SAS user.What do you do?Well you continue using
SAS, but you reach outto R using this interface
to use this new technique.And you go about your work.I think that's going to
be a major reason whypeople use this.Now what are your
system requirements?Most of us, I assume,
are running SAS 9.4.But you have to have SAS 9.3
or later on Linux or Windows.You have to have
SAS/IML software.How do you know if you
have SAS/IML software?You can do proc setinit;run;
and see if you have it or notin your list of products.You also have to have R.Now how to configure.There's a little
bit configurationwork on Windows and Linux.They're a little bit
different on each.But we don't have time.That's going to be in the paper.I'll leave that to you.OK here is the informal syntax.Now I should point
out that we'regoing to make this
simpler, so don'tworry if this looks scary.It's going to be much simpler.We bookend it with PROC IML
to start and end PROC IML.You have the option for those
people who are IML usersto do IML statements.There are two functions
that copy data to to R,call ExportDataSetToR,
which copiesa data set to a R data frame.And call ExportMatrixToR, which
copies an IML matrix to R.And two functions that are
absolutely the reverse,call ImportDataSetFromR,
which copies an R dataframe to a SAS data set.And then there's call
ImportMatrixFromR, which copiesan R matrix back to IML.And you'll notice by the way
that those are not insidewhat's called a submit block.You'll see that we have
submit/R and R statementsand then ENDRSUBMIT.That sequence is
called a submit block.That's where you place
the R statements.Every thing between
submit/r and endsubmitis going to be submitted
to this R interface.You'll notice that the
functions that transfer the datatake place outside
of the submit blocks.And you can have one
or more blocks of code,of submit blocks.You can have one or
more sets of statementsthat copy data back and forth.And one or more sets,
blocks of code to do IML.Now, since most of
us are not IML users,let's strip this down.Cross off all the things
that are for IML users.See that we've done that.Now we will look at something
much simpler and easierfor the rest of us.All we're doing now is we have
a PROC IML and a quit statementto bookend.We've can optionally call
ExportDataSetToR to copySAS data sets to R data frames.Or call ImportDataSetFromR to
copy R data frames to SAS datasets as much as we want.And then we have one
or more submit blocks.This is really simple now.And you see that even
though you're notan IML user, if you
have IML, you'rejust using it as a wrapper.OK now a little bit of detail.And then we're going
to do some examples.OK at any moment in
time in R there'sa current working directory.And by default it's the
temporary SAS WORK library.You can check the
location of the workwith the proc options
statement as shown.Anything that you wish to have
persist after the session,you don't want to put
in the WORK library.Because that will go away at
the end of the SAS session.So you could in R and
in this interface,modify the working
directory with setwdas shown on this slide.And that change will
persist across submit blocksfor as long as the
IML session is going.OK more generally, let's
briefly mention persistence.This R environment that we're
creating in submit blocks,you can have multiple
submit blocks.But it saves everything that's
in there, any data framesyou created or whatever
are going to stick arounduntil the IML session ends.And if you create plots,
they'll stick arounduntil you explicitly close
them or the IML session ends.But we mentioned in
the previous slidethe current working
directory is preserved.Now if you want to end IML
but keep some of the R datathat you got in the R
session, you can use--in SAS, outside the submit block
but inside of the IML session,you can use ImportDataSetFromR
which we talked about before.To copy an R data frame
to a SAS data set.Remember that inside this submit
block we're just doing R work.So we could use one or
more standard R statementsto copy one or more R objects
to a more permanent location.With saveRDS, save,
or save.image.And these are
standard R commandsthat we can use like any
other standard R commandinside of this interface.Now, here's a gotcha with
saving an entire workspace.But we don't have
time to talk about it.There's a workaround,
It's in the paper.OK simple file path limitation.Actually if you are
in Windows, we'reused to using backslashes
for file paths.That will not work.I guess it won't work in R. It
won't work in this interface.So instead you can
use a forward slash.Or you can use a
double backslash.Now remember if you think
about it, when you'reusing a URL for a web page,
you're using the forward slashif you're in Windows.So it just doesn't
seem so strangethat using the forward slash
would work here as well.OK a little bit about
data transfer details.Missing values, dates,
times, and datetimesand variable names.OK, missing values.As you know, in SAS
there is the dotand there's a dot A - dot Z.
And there is a dot underscore.And all of those are converted
to the R missing value NA.And then when we
go from R to SAS,the R missing value NA is
converted to dot in SAS.Now the dates and
times and datetimes.Now as you may know, a SAS
date is the number of daysbefore and after
January 1, 1960.It's an integer value, it's just
this numeric variable in SAS.Well zero is January 1 1960.One is January 2, 1960.Two is January 3, 1960.And so on and so forth.And SAS time values
are just the numberof seconds since
midnight on a given day.And SAS datetime values are
just the number of secondsbefore or after January 1, 1960.Now when we employ these in
SAS, strictly speaking itis not required that we
associate formats with them.And there are specific formats
associated with dates, times,and datetimes.As a practical matter,
we end up havingto use them because if we
don't, for example as shownon the slide, you can say PUT
DATE1 and there's no format,it's just going to print,
for May 8, 2018, 21,312.So inevitably, for
practical display purposes,we associate formats like DATE9.or my personal favorite--YYMMDDN8.There's many, many others
for each of these things.Now when we get over
to this interfaceit becomes essential
for correct processingto associate formats with
dates, times, and datetimes.And the reason is that when
we copy SAS data into R,the interface is going
to look at the formatsto figure out what the thing is.And in R, classes are going to
represent dates and datetimes.So when the transfer
takes place,when we copy SAS data
sets to our data frames,if it sees a date
format it's goingto give them a class of date.And if it sees a time or
datetime format associatedwith the variable
it's going to put itinto the class of POSIXct
and the pseudo-class POSIXt.(I'm not sure what that means).One thing to note is that
there is no real conceptof time values in
R. So what theydo there is they
just take the timevalue into a datetime value
with a start date of January 1,1960.But the key is you have to have
formats on those date, time,or datetime variables in
order to get the interfaceto understand when you copy.When you go the other
direction, R back to SAS.When you copy an
R data frame, itwill assign a format
based on the class in R.If the class is
date, it'll put DATE9as the format of the
SAS date variablewhen it's copied back to SAS.If it's got POSIXt, it
will make it a DATETIME19.Otherwise no format.And let's talk about
variable names.Any variable that's valid in
SAS, is valid in R. However,R variables could have a
period which SAS won't like.Or the variable names could
be longer than 32 characters,and SAS is unable
to handle that.So when
ImportDataSetFromR copiesa data frame back to SAS,
it's going to do two things.It's going to convert
periods to underscores.That's very easy.And then it's going to
truncate any names longerthan 32 characters
down to 32 characters.At that point, it's
going to check and seeif there's any duplicate
names caused by this.And if so, there's an
algorithm that it goes throughto create unique names that
I don't have time to discuss.But it will make unique names.OK we've given all
the background.Now let's look at
some simple examples.And first of all,
the first example,the first one here, is
just a proof of concept.We're not going to
transfer any data.We're just going to do
the same work in SASand R just to show
you how it would work.So on the left-hand
side in SAS we'regoing to do a little data
step to make a data set.And then we're going to do a
proc means to get the mean.On the right-hand
side, we're goingto do the same thing
with this interface.We're going to use PROC
IML and quit as bookends.We're going to do a submit
block that starts with submit/r.We're going to create
an R object calledX. We're going to
take the mean of thatand we're going to
print the result.And then we're going
to use endsubmit to endour submit block.And that's all
we're going to do.No data transferring
or anything.Just basic stuff.And you can see here that
happily both proc meansand the mean function
in R, that we'vegotten through this
interface are goingto return the same value, 22.OK example 2, just to show
more of the nuances of datatransfer.And show the things we
talked about earlier.We're going to make
a SAS data set.We're going to copy it to R with
the ExportDataSetToR routine.We're going to look at missing
values, dates, and funny names.And then we're going to do
various functions insideof this interface to show
what our data looks like.And then we're going
to copy it back to SASwith ImportDataSetFromR
and look at those as well.And I do need to point out
one little sort of cheatthat we're doing here.In real life, when we
use this interface,all the output
that we generate isgoing to go continuously
into either the output windowinteractively.Or the output file.Here what we're going to do,
just for educational purposesto help see what
we're doing, is we'regoing to interleave
the output as it getsgenerated by the statements.That again, I'll show you again
at the end of this examplesreally quickly what
the true output looklike I want you to see what the
implications are for the thingsthat we do.OK here's our SAS data set.We're just going to do a
little data step here and makea SAS data set with
a few variablesand a few observations.Note that the last column,
date1 is a SAS datewith the format of yymmddn8.Note that in our
last observation,that tax is a missing value.OK now we start PROC IML.We copy data set TWO from
SAS to R data frame IMLTWOwith callExportDataSetToR.we start a submit block.What we're going to
do is we're goingto create a new column in
this data frame called IMLTWO,net income is the
name of the column.And if we do the names function
we can see that we've added it.There's now four columns
in the data frame.If we do a print, we can
see that the date variableis in fact understood
to be a date.You can see that
we've added a column.You can also see that the
missing value of dot in SAShas become the missing
value of NA in R.And here we just print
one of the columns.We take the class of
one of the columns.You can use the str function to
get information on the columns.These are all things you can do.Remember we're just doing
standard R processing.We're just displaying
information.And then we're going to
use the summary functionto get even more information.And presumably then we would
do some actual real work.Let's exit from
the submit block.Then we're going to do
call InputDataSetFromRto copy the R data frame
IMLTWO back to a SAS data setcalled FROMILMTWO Now
we can do a proc print.Sorry the columns on this are
less than perfectly lined up.But that's OK.Should still be visible.So here in our proc print you
can notice a couple of things.First of all, the date
column is, in fact,going to be a SAS date in
SAS with the datetime formatas we talked about.You'll notice that the
NA missing values in Rare dot in SAS.That word net_ in the top right
I think got scrolled over.But it should have
been net_income.Remember that we had a variable
in R called net.income,dot's not a valid SAS name.So it replaced it with an
underscore and its callednet_income.So you can kind of see the
mechanics of data transfer.And here would be the
actual non interleavedoutput you would see in
the output window or file.OK now on to example 3.This is more like what we
would actually do in real life.Now I'm just going
to demonstratethis with Ordinary Least
Squares regression,which is in SAS as well.But you could replace
that here with somethingthat's in R. Some statistical
procedure that's not in SAS.And it would work exactly
as shown in this example.We're going to copy SAS
data to an R data frame.Run a regression
in R. Then we'regoing to return the results.OK so here is a data step that's
going to create a SAS dataset called Class with
about 20 observationsand a few variables.Now just to illustrate we'll
first do proc reg in SASand I won't show you the output.It's pretty basic.Nothing too exciting.Then we're going
to start PROC IML.We're going to copy
this data set calledClass using ExportDataSetToR
to an R data frame.Call it Class1.And we're going to
start a submit block.Then we're going
to use the functionat the bottom of the screen
to do Ordinary Least Squaresregression.That's standard R code
there, nothing special.Then the next thing
we're going to dois we're going to extract from
the results of our regressionsome values.The parameter
estimates, the predictedvalues, and the residuals.And then we're
going to print them.So you can see that having
done the regression in R,we can in R in this
interface extractvalues that are the results.Then what we're going to
do is end our submit block.And then the interesting
part is we'regoing to copy the parameter
estimates from the objectParamEst in R into a
SAS data set called pe.And then we're going
to end PROC IML.Now why did we do this?Because we wanted to run
this new exotic regressiontechnique in R and get
some results back to SAS sowe could do some
further processing.This is exactly what the
way I think people arelikely to use this interface.Or one way.So now, we can see here
if we do proc print,we can see what this pe data
set, which has the parameterestimates looks like.And then in SAS we'll continue
with our regular analysisand just be SAS users again.And you can see here that
what I've done in this exampleis I'm using the parameter
estimates that I calculatedin R and brought back to
compute predicted valuesfor some height.And this is again, the type
of approach people would use.And we go ahead and
run our data step.And we're going to
generate some new results.And then we do a proc print
and you can see the results.So again, just to
review, this isI think kind of the
essence of the whole thing.You're in SAS.Sent our data set
over to R. We didsome statistical procedures.We pulled out some results
from the procedures.We sent those
results back to SAS.And then went about
our business in SAS.Really pretty nice
and straightforward.And here's the actual raw
output you would actually see.Now Example 4 we're gonna skip.This is a more complex one
where we do things like call Rpackages and do R graphics.I'm going to skip that.It's in the paper in detail.Next one, source file.It's very possible
that you're goingto have R programs that are
sitting in files that you justwant to run.If you're a SAS user, you're
familiar probably with the ideathat you do %include.And you bring in
SAS code from a fileand it's as if you had key
stroked it right there,and we run it.The source function in R
does the exact same thing.And you can use it right
here in this interfaceto bring in R code
from a file and run it.And I've got an example
there of how to do it.And this is how you can use
programs that you've writtenand run them within
this interface.OK last example.This is going to show you the
mechanics of copying and savingfiles.Let's just say that
if first you work in Rand you do bunch of things.I will go fast here in the
interest of time saving.But you're going to work in R
and create a working directory.Create some objects, save them.And then, so you do that
or somebody else does that.And then we're
going to go into SASand we're going to
work with these files.So let's start in R.
We're going to usesetwd to set current
working directory.We're going to create two
objects, object1 and object2.Then we're going to create a
data frame containing those twoobjects.Then we're going to
save object1 usingsaveRDS, which is a standard
R function for savinga single object.Then just to illustrate
the difference.we're going to modify
object1 by changingthe values in some cases.And then we're going to
use the save functionto save both object1
and object2 to a file.And then we're going to
use save.image to savethe entire workspace.The save, saveRDS and
save.image functionsare the standard R
functions to save objectsor the entire workspace.My R expert friend
reminds me to always saythat save.image should
be used carefullybecause if the
workspace is very, verylarge that could create a
big problem at some point.So we've done this in R.
We're not even in SAS yet.Now we go into SAS.We start a submit block.We change the current
working directorywith setwd to be the
one we used in R. Nowwe can start retrieving
objects that we created in R.Now you'll notice
that above eachof these blocks of
statements, I'vegot a comment with a
little block of code thatshows how I generated them
in R on the previous slide.So you can kind of
see how we got them.So first I'm going to use
readRDS to bring back object1.That's the mirror
image of saveRDS.And you can see I print it out.Then I'm going to
use the load functionto get back the two objects that
I saved with the save function.That's the mirror image there.And I'm going to print them out.And you'll notice that
the values of object1reflect the change I made to
them in R between when I didsaveRDS and when I did save.And then last but
not least, we'regoing to use the load
function to bring backthe entire workspace.Which is again a mirror
image of save.image.And now I've got
everything back in my SASenvironment, everything that
I had when I was working in R.And then I could use them.I can use a library
function just like in R.Because I need this library
to get the mutate function.I could add a column to a data
frame with the mutate function.Print that out.And then, once again, I'm now
done in the SAS interface to R.But before I leave,
I might want to,or somebody else might
want to look at this data.I can once again
use save and saveRDSand save the information
here out to filesso that it'll be available
to me or somebody else in R.And I'm going to do saveRDS,
save, and save.image.And then I'm going to end
my submit block and quit.Again, this is just an example
of copy and saving files.And here's the actual
output that youwould see in real life.OK conclusion.Well this R interface, it's
for Linux and Windows SASstarting in SAS 9.3.You're sitting in
SAS, you're notsitting in R. It lets you if
you have a PROC IML license,you can very seamlessly run
R stuff, call R functions,get results back, transfer
data back and forth.The important point is also
that while a few of youmay be PROC IML users,
you don't have to be.As long as you have PROC
IML licensed at your site,you can use PROC IML just as
a wrapper to transfer data,to call R functions,and that's really what this
presentation was really about.It's this interface.You have IML, you're
not an IML user.So thank you.Here is my contact information.And feel free to contact
me with questions.Thank you very
much for your time."
124,"JASON SECOSKY: Folks, I've
had a few calamities at home,my rotting back steps
finally collapsed.Our dripping shower
turned into a drizzle,and our bathroom
sink pretty muchstopped draining altogether.If that wasn't
enough, my dad callsand says he's
driving through townand will be by
tomorrow for a visit.I don't know what
your relationship islike with your
dad, but my dad, Iwould be embarrassed for him to
see me and my broken down houseand that I don't know how
to fix these problems.I needed a simple and fast way
to fix each of these issues.At the hardware store,
to fix the rotten steps,the associates sold
me special screws thatdidn't require pre-drilling.For the dripping shower, he
sold me washers and springs,and for the clogged drain, I
bought a barbed plastic stickto fish out any clog.All of these were simple
and fast solutions.When I arrived home
with my supplies,I completed the work
just before my dadpulled into the driveway.Well, I didn't get all
of the planks down,but I was proud of
my accomplishments,simple and fast.I'm Jason Secosky.I'm a senior manager in
research and development at SAS.I'm coming to you
with simple waysto make your data steps
run faster in SAS Viya.SAS Viya comes with
two compute engines--traditional SAS and
Cloud Analytics Services,also known as CAS.In SAS, DATA Step runs
in one thread or core.With smaller data,
it runs great.With big data, some data
steps can take hours to run.CAS is composed of
one or more machines,and DATA Step runs in
all available cores.By splitting the input
data among the coresand running the data step
in parallel in each core,your data steps complete faster.Today, we'll show how it's
simple to code data steps thatrun faster in CAS, in
particular, programs with a BYstatement.We'll also look at handling
row order differenceswithin BY-Groups between SAS
and CAS and merging tables.Let's get started
with an example.Say we have this table
of animal weights.We want to write a program to
discover the minimum weightfor each type of animal.One way to do this is to
sort the data with proc sort,then to write a data step
to discover the smallestweight in each group.To do this in CAS,
we'll first needto load the data into CAS
using a new libname engine.The way it is loaded onto
the multiple machines,we cannot maintain the order
of rows from the original data.Then, we automatically
group the rows by animal,and we can write a data step to
run in parallel on each groupto discover the minimum weights.If there is one slide you
remember from this talk,this is it.Performance is improved by
splitting data across machinesand processing in parallel.So if you want to take a
screenshot, this is your slide.Here's the SAS program
that we use to do this,then we'll convert
it to run in CAS.I'm using a libname
statement to create a librefto where my data is stored.Proc sort, I'm going to
sort the animals data setby the type of animal,
and then my data step,DATA Step's going to use
first and last dot processing.And so we need to
have a BY statement,and on the first row
of a group, we'regoing to set the minimum
weight to missing.Then, we're going to
compute the minimum weight,and then on the last row of the
group, we will output a row.And we get these results
which are the minimum weightsfor each kind of animal.So how do we tell data step
to run this program in CASinstead of SAS?Well, first, we need to
load the data into CAS.Then, we can create
a libref for it,since that's where
it's stored, and usethat libref in the data step.DATA Step will see
the data stored in CASand will automatically
run the data step in CAS.It's simple, simple, simple.Here's that code.So our CAS statement
will connect the CASand create a new CAS session.proc casutil's a new procedure.We use it to load our
SAS data set into CASacross the machines.Then, we use the
CAS libname engineto create a libref to where
the data's stored in CAS,and then my data
step is the same.It's the same data step
that I used in SAS.All I did was change the
libref to point the datastep to the data in CAS.Data step to Texas
and automatically runsthis data step in CAS.It is simple.It even gets simpler.You'll notice that
there's no proc sort.Well, there's no
magic going on here.Grouping is still happening.When using the BY statement in
a data step, in a CAS data step,CAS creates a temporary
table with the datagrouped on the
first BY variable,and then sorted within groups
on all the BY variables.And we'll see where this is
important in the next section.And when we run
our program, we'llsee that the results
are the same.And if this was
proc print output,you'll notice that the rows are
output in a different order.I've got in SAS it was in
alphabetical order by animal.Here, it's not
alphabetical order anymore,and why is the order different?Well, this is reflecting
the parallel nature of CAS.All of the worker machines
are reporting their resultsin at the same time.So the order that SAS
receives that is rowsis basically the order that they
just happen to be coming in.If the row ordering is
important in your reporting,you'll need to do some
sorting on the SAS side.So how does the
performance compare?With a million groups and
100 animals per group,SAS took about 41 seconds,
and CAS took about 22 seconds.So this was simple and fast.Whether you realize
it or not, whenyou're using CAS data sets,
the original order that rowswere added is maintained.When using proc sort,
the rows within a groupretain the same order as
the original data set.This isn't the case in CAS
and can seem not so simple,until we understand
what CAS is doing.Let's take a look at an example.Here's our animals
data set again.Let's assume the rows
in the animal's data setwere added in
chronological order.So to find the
latest animal weight,it would be the last row
for that kind of animal,because these were added
in chronological order.Now, to find this
last animal weight,one way to do this would
be to sort the dataset with proc sort.Proc sort retains the
original row order.So then, we could
write a data stepto output the last
row of each group.Let's take a look at
the code that does this.After we've sorted the
data, this data stepuses a set with a BY statement.And then on the last row of
the group, we do an output,and we see this result, if
we were to proc print it.So now, what would the
code look like in CAS?Well, we'll do like we
did in the last example.We will change the libref
off to our CAS librefto point us to where
that the data is stored.This will automatically
run the data step in CASand do the grouping
on the CAS side,and let's take a
look at the results.You'll see right away
that they're different,and what's going on here?This doesn't seem
very intuitive.The latest weight for
the ant is not the same.Bird happens to be the same.Cat's not the same.Dog's not the same.What is going on?Well, let's go back
to our diagramsto see what is happening.Once we've loaded the data
into three CAS Workers,the row order, the way that
we loaded the row order,is not maintained,
and you can kind ofsee this in the diagram.When we use the BY
statement in our data step,the machines will shuffle
the rows to form my groups,and the order within a
group is not maintainedfrom the original SAS data set.Then, when our DATA Step outputs
the last row of each group,it isn't necessarily
the latest weight,and we see a difference
in the results.What we've done here is
we've made a trade-off.This is comparing the SAS
results to the CAS results.We've made a trade-off
to do we take the timeto maintain the original order
of the rows back in the SASdata set, once we've
loaded the data into CAS?Or do we speed the
program by not havingto keep track of the order?We chose to speed
the program, but whatdo we do to keep the
row order within groups?CAS groups rows on the
first BY variable and sortswithin each group on
all of the BY variables.What we need is another
variable to sort onwithin our groups
that representsthe original order of rows.Oftentimes, oftentimes,
your data setwill already have
such a variable.Usually, it's something
like date observed.Like when do we observe the
weight for this kind of animal?Then, we could sort on
that, but in this case,this case that
I've manufactured,there is no such variable.So we'll need to add one,
and I'm calling it Row ID,and here it is in the diagram.If we load our
data set into CAS,and we have a new
row ID column thatrepresents the original
order of rows in SAS,we can then put our row ID
variable on our BY statementso that we sort within
groups by animal and row ID.CAS will do the shuffling
to group the rows,and you can see we haven't
done the sorting quite yet.Then, the sorting
occurs, and we'vemaintained the original row
order that we had in SAS.Then, we can write our
data step that picks offthe last row of each group,
and we'll get the same resultsthat we got in SAS.So here's what the
code looks like.Our first data step's going
to load our data into CAS.And what we're
doing here is we'rereading the rows with a set
statement from the SAS locationand writing it to
the table in CAS.And I'm creating a
new variable calledrow ID, which I assign the value
of underscore and underscorewhich essentially is
the row number in SAS.And then, I run the data step,
this next data step in CAS,where I added the row
ID to my BY statement,so that I group
based on animal type.And then, within
each group, I'llsort on the animal
type and the row ID.This gives my group the same row
order that we would see in SASSo while we've made a
trade-off for speed,the program is still
relatively simple,once you know how CAS forms
groups, simple and fast.The merge statement combines
rows from two or more datasets in SAS and in CAS.When used with a BY statement,
rows from the same BY variableare combined.SAS sequentially
reads each table,combining rows one
observation at a time,until there's no
more rows to process.In CAS, BY groups are
assigned to threadsto process in parallel.Each thread processes the
BY groups assigned to it,until there's no more
BY groups to process.We're going for
simple and fast here.Let's look at our diagrams
for merging two tables basedon a common variable
in SAS, and then we'lldo the same thing in CAS.In SAS, let's combine an
animals table and a plants tableon the variable Common.To combine the
tables, first theyhave to be sorted
with proc sort.Then, we can merge them
with the data step.After loading our data into
CAS, we have similar steps.When we use a BY
statement in a data step,CAS will automatically group
the rows on the machines,and then we can
perform the merge.Let's take a look at
the SAS code for thisand then the CAS code.In SAS, first we sought both the
animals and plants data sets.Then, we have a data
step that merges them,and this is what the
results look like.Now, let's change the libref,
like we did in the prior twosections, to automatically
have data step runthis program in CAS.In CAS, we don't
need the sorting,as the presence of
the BY statementtells CAS to automatically
group the rows,and here's our result.So this was simple.We used the same data step
we were using in SAS, changedthe libref to get
it to run in CAS,and we get the same answers.Note that the row order is
different because of the waythat all of the machines are
reporting their rows backto SAS, and they're all
sending at the same time.If we needed to
order this, we wouldneed to do some sorting in SAS.And here are the times it
took to do these merges.When we merged two tables
of 100 million rows,SAS took about 94 seconds,
and CAS took about 27 seconds.So this was simple,
and it was fast.Accelerating your
data steps in CASis simple, especially when
using BY group processing.If you have existing DATA
Step code that runs in SAS,you can simply change the
libref from a SAS librefto a CAS libref to take
advantage of multiple threadson multiple workers.If needed, you can use
a row ID to coax CASinto producing the
same results as SASwith accelerated performance.When my dad was coming to visit,
I needed a simple and fast wayto fix my house.Whether you are preparing
tables for analysis,computing new values,
or combining tableswith set and merge,
using DATA Step in CASgets you where you
want to be faster.Simple and fast.Thank you."
125,"Hi, everyone.It's Anna here at SAS.Today, I'm going to be
talking about SAS DataPreparation in SAS Viya.So within SAS Viya, you
have many data managementcapabilities that are built in.However, if you license
SAS Data Preparation,you get some additional
data management featuresand capabilities.We're going to be looking at
SAS Data Explorer and SAS DataStudio.Within SAS Data
Explorer, this iswhere you can ingest and
access your tables and files.So this is where you can
connect to your data sourcesand then load data
into memory in CAS.CAS stands for Cloud
Analytic Services.And this is our
runtime environment.So this is where we're going
to do our data wrangling, dataprocessing, and work with
our in-memory tables.In SAS Data
Explorer, we can alsoassess our data structure and
the content of our tables.So we can do something
called run a profile.We can profile our data.When we profile our data, we
get column-specific statistics.So for each column,
we get thingslike uniqueness, mean,
median, and mode.We can get some
statistics about the datathat we have to work with.If you have SAS Data
Preparation license,you get some
additional statistics.So you get things like
frequency distribution,some descriptive metrics,
and pattern distribution.And what's really neat
about these is youcan really learn where you
need to standardize and cleanseyour data.For example, if you
have a state column,by looking at the
pattern distribution,you can see if
states are entered--written out as North Carolina or
as two-character abbreviations,like NC.Now, once you've explored your
data in SAS Data Explorer--hence, the name--you can go over
to SAS Data Studioto transform and
cleanse your data.So this is where we do our
heavy lifting of our DataPreparation.We can create new
data and reallystructure our data
into the desiredformat for analytical
reports and for modeling.Within SAS Data Studio, we
create Data Preparation plans.Data Preparation plans work
with our various source data.And then they store
the transformsthat we want to
run on that data.We can then save
Data Preparationplans to access later or
share with other SAS users.The really cool feature
of Data Preparation plansthat is new to SAS Data Studio
2.5 is the suggestions feature.Suggestions can work
with your source dataand suggest possible transforms
that you could then includein the Data Preparation plan.So for example, let's look
at this CUSTOMER_NAME column.Using the suggestions
feature, a series of modelsare run against this column.And the data is analyzed.And then possible transforms
are suggested to you.So for example, we might get
the Remove Duplicates transformor the Standardize
or Parse transform.And then we can
choose which oneswe want to include in our
Data Preparation plan.I think suggestions are super
cool for a couple of reasons.One, your Data Preparation
process will be much faster.Because these models analyze
your data and suggest possibletransforms to you
you, don't haveto do quite as much
exploring of your data.And two, it's very possible
the suggestions could give youa transform that you wouldn't
think to use normally.So it kind of expands your Data
Preparation process, as well,while still making
it really efficient.So in this demo,
we're going to lookat Data Preparation
in SAS Viya and howwe can utilize suggestions.So we are signed into
SAS Viya as Christine.Christine is an admin user.So she has admin privileges.And we're currently
looking at SAS Drive.SAS Drive is kind of
like a landing pageor a home page for SAS Viya.It's where you can see
the content you've createdand you can share
content with other users.So as Christine, I'm going
to go up to the top leftand click on Show
List of Applications.And then I'm going to
click on Manage Data.And this will take us
to SAS Data Explorer.In SAS Data Explorer,
we have three tabs--Available, Data
Sources, and Import.The Available tab-- if we
had any in-memory tables,those would be located here.So this is where we could see
the data that was available,data that was in memory.If we click on Data
Sources, we cansee the CAS servers we have.So right now, we have one
CAS server configured.If we click one arrow down,
we can see the CAS libsthat we already have set up.Now, these are global CAS libs.We can see that by
hovering over this icon.But the snowflake icon does
indicate global CAS libs,which means that if
we, as Christine,sign in and out of
Viya, if we close outof our internet browser,
these CAS libs are stillconfigured and set up.So these are pointing
to various data sources.We're going to create a
new CAS lib connection.So I'm going to click on
this Connect icon here.And the Connection
Settings window will open.I'm going to call
this connectionSAS Data because we're going to
connect to some SAS data sets.And it is going to be a
file system-based connectionand a path source type.But notice we do have
other types of connectionsthat we can create.So we could connect
to an Oracle databaseand pull in tables
from that database.I'm then going to click
""Persist this connectionbeyond current session.""What that means is we want this
to be a global CAS lib insteadof a session-based CAS lib.Again, a global
CAS lib would meanthat even if I, as
Christine, sign out of SAS,this will still be there.We can still access
this CAS lib.And then this is path-based.So I'm going to give the
path on my CAS server.And the path is just /sgf.I'm going to click
Test Connection.This is to ensure
that that path isa path we can reach and access.And the connection
was successful.So I'm going to click Save.Over here on the left,
under Data Sources,we now have the SAS
data CAS lib thatis pointing to some
path-based data.And if we click
the arrow down, wecan see the sas7bdat files that
are located in that location.I'm going to right-click
on BANK_CUSTOMERS.And I'm going to
select ""Add to import.""""Add to import""
for a table loadsthe SAS data set into memory.And it creates a sashdat file.So there are two ways you can
load a file into CAS memory.We can do ""Add to import"" or
we could also just do ""Load.""""Add to import"" makes us also
create that sashdat file.So by clicking that, I'm
going to get this window herefor the item we're importing.We are going to call
it BANK_CUSTOMERS.And we're going to leave
that target locationas the default public CAS lib.I'm going to select
Replace File.So if, for some
reason, we alreadyhad a sashdat file
BANK_CUSTOMERS,we're going to replace
and update that.And then here, you can see
all the various formatswe can work with.We're going to use sashdat.But we could create
a different file.So again, this is going to
load data into memory in CASand also create
that sashdat file.So I'm going to
click Import Item.And then notice our
import was successful.If we come over
here on the add, wecan see all the different
types of data sourcesthat we can import into CAS.So that's really cool.So let's click back
on the Available tab.And now we can see that
we have BANK_CUSTOMERSas an in-memory data set.And that lightning
bolt does indicatethat it's an in-memory table.If I click on the little
Info Properties button,we can see it's
called BANK_CUSTOMERS.And we can see when we imported
this table and the default--or not default, but
the location, the CASlib we decided to use, which
was the default of Public.If I click on BANK_CUSTOMERS,
in the middle,I'm going to get some
information about my dataand my memory table.So again, this is where I can
do the exploring of SAS DataExplorer.The first thing we see
is this Details tab,which gives us some column
names and data types.Over on the far
right, it gives usinformation about
the actual table-- sohow many columns and rows exist.If I click on Sample Data, I
can get a sample of my data.By default, it's going to
sample 100 rows of data.But it can sample up to, I
believe, 5,000 rows of data.So you can see our data.Already, I notice
that my STATE columnhas some interesting
entries that we'regoing to have to do
some standardizing of--same with maybe
the CUSTOMER_NAME.It looks like our casing
could be an issue.Casing could be something
we might want to address.Now, if I click on Profile, this
is what I was talking about,where we can get some
profile information.We can learn some--about some column statistics,
explore our data further.We can run profile or
run profile and save.If we do Run
Profile and Save, itwill create a table
with these statisticsthat we could further work with.We're just going to run profile.I just want to do that
exploratory phase.So this is-- again, it's going
through all those columns,assessing for statistics
like mean, median, and mode.We're also going to get
some frequency distributionsand pattern distributions.So the first thing we
see is for each column,we can see uniqueness.So if we look at customer ID,
notice it is not 100% unique.So if this was something we were
going to use as a primary key,it looks like we
have some duplicates.So we're going to
address that later.And then we can also see
means, medians, and modes.If we want, we can explore one
particular column a little bitfurther.So I'm going to click on the
STATE column name right here.And this will give
me the metricsfor the individual STATE column.So you can see we have a
mode, minimum, and maximum.We can also click the down arrow
beside Frequency Distribution.And first thing I
notice is CA seemsto be the most frequently
incurring entry.But notice we have California
written out right here as well.So those two columns
would need to be combined.But that's not going to happen
until we standardize our data.We can also look at
pattern distribution.So again, we can already
see this from the frequencydistribution.But we can see that
a lot of entriesare written as capital
two-character entries.But then we also
have entries thatare written out in the
long form, like Californiaright now.So we're going to need
to do some standardizing.So that's just for
that individual column.I'm going to click
on the report up hereto go back and see
the overall profile.And then I do want to show
you in the right pane,we have this--it looks like a clock, almost.And if you hover over,
it says Versions.You can run multiple profiles
on this in-memory table.So we might do some manipulation
and standardizing of our dataand run another report to see
how metrics have changed--so kind of neat.You can have multiple versions.In this top right, I'm going
to click the down arrowbeside Actions.And I'm going to
go to Prepare Data.So that will take us
to SAS Data Studio.It's going to open up a plan,
a Data Preparation plan.And it's going to
use BANK_CUSTOMERSas our data source.So we can see BANK_CUSTOMERS
was selected as our data sourcein this plan.You can see a preview
of the data over here.On the right, you can see some
of the available transformswe have.These data quality transforms
are because we have SAS DataPreparation.They're really cool.But again, you have a lot
of different transformsthat you can work with here.Now, we could manually
just add these transformsfor each particular column.But I think it'd be neat to see
what SAS Data Studio suggests.So I'm going to click on
this Suggestions icon.And then I'm going to
click Get Suggestions.Now, this was a
little anticlimactic.The very first time that
you try and get suggestions,you do have to
register your models.So I'm going to click on--we have this note here that
says no models are registered.I'm going to click
Register Models.And then this Models
window will appear.And we can choose
to register models.So here's some of the
models that we have.So some of these
are going to taketransforms that are
already available,such as the parsing transform.Others might suggest
that you impute columnsand generate some code.But again, these
are the models thatare going to be
run on your columnsto assess what
kind of transformsthat you should possibly use.So our models are registered.I'm going to click Close.And then I'm going to click
on Get Suggestions again.Now, this time, it's going to
go through all of our columnsand populate a list
of possible transformswe might want to use.And wow, you can
see we have a lot.So I know that the STATE
column was a little wonky.It could use some standardizing.So I'm going to search for
STATE up here in the Suggestionsicon.And notice the standardized
transform was suggested to us.So I'm going to
double-click that.And that's going to
add it to my plan.So it's adding the standardized
transform to my plan.And the source column is STATE.I want verify that.The locale is English
United States.And then the definition is
State Province Full Name.So this is using the
SAS Quality KnowledgeBase to standardize our data.Basically, we have
a defined locale,which is based on a
geography and a language.So within English United States,
there are certain definitionsthat you would have.For example, we have states.So we have the state definition.And then we have a
variety of thingswe can do with the
state definition,such as standardize it--same with, say, an address.We, in the US, will write
addresses differentlythan you might write
them in Sweden or evenin another
English-speaking country.So the definitions
that are availableare dependent on the locale.And then within
each definition, youhave different things that you
can do, such as standardizing,parsing, et cetera.So this is the STATE column.It is for English United States.And we want to use the state
province full name definition.And there is an
abbreviated definition,as well, if we wanted
the two-character name.Now I'm going to
click Replace SourceFile because there's no need
to keep the messy STATE column.Why not just go ahead
and update the data?And then if you notice
right here, remember,we're doing all this data
wrangling and manipulationin memory.So right now, we've added a
transformation to our plan.But we haven't run the plan.So up in the top middle, it
says the session table is notcurrent to the plan, run the
plan to update the table.And you have this
half-filled circle.That means
BANK_CUSTOMERS has notbeen updated to
reflect this transform.So I'm going to click Run.And it's going to
standardize our STATE column.So over on the far right,
you can see, actually,this first transform
in our plan.And it's got the
nice green check markbecause we've applied it.We now have a green
check mark here.The session table is
current to the plan.And looking at our
STATE column right here,we can see that it is
nice and standardized.So all the states are written
out with nice, proper casing.So I'm going to exit out of
this filter in our Suggestionswindow.And then I'm going to
click Get Suggestions againbecause it will regenerate
suggestions based on our mostcurrent session table.And I'm going to search now
for the standardized suggestionand see--so it's suggesting a lot of
columns could be standardized.I'm not going to take
you through all of those.But I am going to
add a couple more.So I'm going to add
the standardize addressand standardize CUSTOMER_NAME.And you can add more
than one transform.You don't have to add
each individual transformand then run the plan.So I added them both.I'm going to, over on the
right, under the plan,click on the second
Standardize and make sureit is CUSTOMER_NAME or we're
using the locale English UnitedStates and the Name definition.So again, this is
the algorithm thatwill be used to
standardize CUSTOMER_NAME.I'm going to click
Replace Source Column.And then I'm going
to just go aheadand configured the
third Standardizeas well-- so Address, English
United States Address,and Replace Source Column.Then I'm going to
click Run so that bothof these standardized transforms
are applied to our data.Awesome.So if we look, CUSTOMER_NAME
is now properly cased,and then same with ADDRESS.It's been standardized.What's neat is things like
these cardinal directions,like southwest and south, and
then street, or Southeast 21stStreet--those things that you
see on an address--the ADDRESS definition has
a very particular algorithmfor how to standardize
those different entitiesof the address.So over here, I'm going
to regenerate suggestions.And then I'm going to be
adding a parsing suggestionfor the CUSTOMER_NAME
because right,now we have customer
name all in one column.Maybe I want a first
name and a last name.So I'm going to click
in here in the filterand search for Parsing.And notice we have the
CUSTOMER_NAME transformsuggested for parsing.So when we parse a name, we
are going to create tokensor we're parsing it into tokens.Two tokens you can think of
are first name and last name.But obviously, you might have
a middle name or a prefixon a name.So there are available
tokens over here on the left.We are just going to add the
Given Name token and the FamilyName token--so first name and last name.And then I'm going to click
on Options for New Columns.And I'm going to rename
these because theseare going to be really long.And I'm also going
to make the lengthof these particular
columns shorter.So I'm going to call Given
Name-- let's just go aheadand call it FirstName.In Family Name I'm
going to call LastName.And then I'm going to make
these both 50 characters longand click OK.And now when I run
this, I'm goingto have two new columns,
FirstName and LastName,with our customer
name parsed out.And these appear at the
beginning of our data set.So now we can see we have this
FirstName, LastName columninstead of-- we also still have
the CUSTOMER_NAME column, whichhas it all together.But here we have it
separate into two columns.I'm going to generate
suggestions one more time.And remember, we said that
customer ID column could notbe used as a primary
key because there weresome duplicate rows of data.Let's add the Remove
Duplicates transform.Now, I want to
show you over here,I'm going to click on the
properties for the resultstable.Right now, we have
483 rows of data.But we're going to
remove some duplicates.I don't want to
remove duplicatesbased on duplicate values
across all columns.I want to do it based
on a particular column.And we're going to use CUST_ID.So if there are any duplicate
values in this column,it'll remove the duplicates.And I'm going to click Run.Now, notice over here on
the right, my number of rowsis now 481.So two duplicate
rows were removed.And now customer ID could
be used as a primary key.I'm going to click on the plan--so this first icon here.And notice everything is good.Our session table is current.All of our transforms
have been added.But maybe you wanted to
remove duplicates first.That might be something
you do, for example,if you had a lot of duplicates.Go ahead and get rid
of them before youdo all your other
standardizing of your data.We can actually take this
Remove Duplicates transformand, by hitting this up arrow,
we can move it in the plan.So I decided I want to
do that thing very first.And then I want to
do everything else.Notice we will need to rerun
our plan so that the sessiontable reflects the plan and
everything is up to date.But it's really easy to
move around transforms.And that is actually a newer
feature of SAS Data Studio.So this is as much standardizing
as I want to do right now.I'm going to save the plan.And I'm going to save the
transformed table as well.So I'm going to click on this
Options icon and click Save As.I do want to point
out, notice, New Jobis currently grayed out.I'm going to click on Save As.And this is going to allow us to
save the plan and target table.I'm going to call the plan
Cleanse Bank Customers UsingSuggestions.And then I'm going to call
the table, the new table we'recreating,
BANK_CUSTOMERS_CLEANSED.So we're going to save
this as a sashdat file.But again, we could save
it as other file types.And then we're saving our
data plan in our folder.So I'm going to click Save.And now that we have
saved this plan,I can actually schedule a
job from this plan file.So if I were to click
here, if I want,I could create a new job
that would do standardizing.You could basically run this
plan file at a certain timeor for other reasons
based on other conditions.Now I'm going to
click this Actions.I'm going to hover over it.I'm going to go to Save Table
and Explore and Visualize.So that's a really
neat feature there--that you can go straight from
SAS Data Studio to Actions,Save Table, Explore
and Visualize,and it's going to
open up SAS VisualAnalytics with our saved table.So we can do some visual
reporting and analyticson that data that we
have just processed.So it's opening up a
page in Visual Analyticsand loading our cleansed data.If I click on the
Suggest icon here,we actually get some
suggested visualsthat we could use
with this data.So I think that's really neat.For example, let's drag
this Frequency one over.We can see the frequency
of occupations.And actually, first
thing I noticehere is ""law enforcemen"" at
the bottom doesn't have a Tand ""law enforcement"" does.These should probably
be grouped together.So it looks like
there's some morestandardizing we need to do.Now, that's all I'm actually
going to show you, though.That was a lot--going to click back on
Share and Collaborateto take me back to SAS Drive.There's our lovely
Data Preparation plan.And that is it for me.So I hope you feel like
you've learned something,feel like you're ready to use
the Suggestions feature in SASData Preparation.And thank you so
much for watching.Check out more really cool
tutorials on our SAS UsersYouTube channel.Thanks.Talk to you later."
126,"Hi, my name is Jagruti
Kanjia, and I'mfrom the Advanced
Analytics division at SAS.In this session, we will see
how you can incorporate Pythoninto SAS Visual Data Mining
and Machine Learning pipelinein Model Studio.There are many benefits of using
Python with SAS Visual DataMining and Machine Learning.For example, it
makes interactingwith SAS Viya easier for
a non-traditional SASprogrammer within Model Studio.This pipeline shows how you
can integrate open sourcetechnology within VDMML.You can use the open
source code nodeto do data preparation and
model building using Python.You can use Python
SWAT with SAS Code nodeto build deep learning models
using packages like DLPyand compare them with
traditional data miningand machine learning models.Our paper, ""Using Python with
Model Studio for SAS VisualData Mining and
Machine Learning,""provides more
details and exampleson using open source code
node and using DLPy packagesfrom SAS Code node.The open source
code node requiresPython and necessary
packages to beinstalled on the same machine
as the compute server for SASViya.What if you want to use
desired Python whichis set up on a remote system?In this demo, we
will focus on howyou can use Python in
an existing open sourceenvironment from SAS Visual
Data Mining and Machine Learningto authenticate
and transfer datato and from native
Python structure.We will discuss how you can
use your preferred Pythonenvironment for either data
preparation or model building,and call it through SAS Code
node for use or assessmentwithin in a pipeline.You can use SAS code
node to run Python scripton a remote server and use SWAT
to connect to an existing CASsession to access the data, run
the Python modeling package,and upload the resulting
tables into CASand transfer files
to SAS Viya system.To implement this, we will
use remrunner and Paramikopackages.In this case, remrunner
and Paramiko packagesneed to be installed in
Python environment set upon a system where SAS
compute server is running.The remrunner, which is
remote runner package,enables the user to
transfer a local scriptfile to a remote
host and execute it.The file is copied to temporary
location on the remote host.Permissions are set to 0700,
and script is then executed.Current limitation of
remrunner packages,it assumes that SSH keys, which
allows passwordless logins,are already in place.There is no option to
provide a password in a code.To implement this, we
created this pipelineand VDMML projects
in Model Studio.The SAS Code node is used as
a supervised learning node.In this example,
the SAS Code nodeis used to create and submit
Python file to a remote system,and display results from
executing the Python model.The Feature Machine node is
used to generate new featuresby performing variable
transformationto improve data quality
and improve model accuracy.The new features are generated
to fix the data qualityissues, like high cardinality,
high kurtosis, high skewness,low entropy, outliers,
and missing values.The CAS procedure is used to
create a Python file named,gb01.py, which
then, when executed,will run an extreme
gradient-boosting algorithm.This algorithm is
an implementationof gradient boosted
decision tree designedfor speed and performance.Here we are importing
the required packages.These packages need to be
installed on a remote Pythonenvironment.We use SWAT to
create a connectionto CAS server and upload
the data, xgboost to buildextreme gradient boosting
model, matplotlib to createthe reports, and
Paramiko packageto transfer necessary files.If your CAS server is configured
to use TLS for communication,you must configure your
certificate on the client side.Note that beginning
SAS Viya 3.3,encrypted communication
is enabled in the serverby default. If you don't
configure certificateon client, you
will receive errorwhen you try to connect to CAS
server from Python with SWAT.This error can be alleviated
by setting the pathto correct certificate in
the CAS_CLIENT_SSL_CA_LISTenvironment variable
in the environmentwhere you are running Python.The path indicated here
is a client site path,so the certificates
are typicallycopied to the local directory
from SAS Viya system.To enable a Python program
to work with SAS cloudanalytic service, you must
establish a connectionto CAS server.The SWAT package, that
is the SAS scriptingwrapper for analytic
transfer packagethat's simply
referred as a SWAT,is a Python interface to
SAS cloud analytics service.It allows users to
execute CAS actionsand process the results
all from Python.In fact, the SWAT package mimics
much of the API of the PANDASpackage so that using CAS should
feel familiar to current PANDASuser.We are using swat.CAS class
to create a connectionto CAS server running
on SAS Viya system.You must have an authinfo
file so that you can specifyyour credentials to controller.The dm_cashost and
dm_casport macro variablesare initialized to point to CAS
server used by Model Studio.The dm_cassessionid
macro variableresolves to CAS session
associated with the SAS Codenode.The SAS Code node editor
provides these various macrosand macro variables, which
makes it easy for a userto write efficient code.The easiest way to
get data from CASis using the data
loading method, to_frame.This is similar to PANDAS
methods, like read_csv.The only difference
is that the methodoperates on CAS Table objects
rather than PANDAS data frame.Here, the data
frame dm_inputdf iscreated from the entire
input table in CAS.Create a data frame called
X_train with input variableand series called y_train
containing the target variablefor the training observations.Similarly, X_valid and y_valid
are created for the validationdata.We also create an
evaluation set, eval_set,containing the training
and validation data.We create and fit
extreme gradientboosting classifier model
for our training data set.We use the eval_metric
option because wewant to generate the
plot of several metrics--for example, error and logloss--at each iteration for
our evaluation set.We would like to
create an output fileand display in SAS
Code node results.Python includes the
building file type.Files can be opened by using
new file types constructor.Open output.txt
files for writing,and print xgb model to
the output.txt file.We will transfer this
file to SAS Viya systemto display in the
Results window.You can make predictions
by using the fitmodel on the complete data set.To make predictions, we use
the function, predict_proba.You can add two new
posterior variablescreated by using
the predict_probato dm_scoreddf data frame.The dm_scoreddf data frame is a
copy of dm_inputdf data frame.The I_ variable, the
classification variable,is created using predict
function and addedto dm_scoreddf data frame.We use the plot_tree
function provided by xgboostto create and display
the tree plot.It is important to change
the size of the plotbecause the default
size may not be legible.The num_tree option indicates
which of the generated treesshould be drawn, not
the number of trees.Here, we set the value to 3 to
get the third tree generatedby xgboost model.We save the current figure
to rpt_tree.png file,and specify the resolution
in dot per inch while savingto image file.A train xgboost
model automaticallycalculates feature importance
on your predictive modelingproblem.This important
scores are availablein feature_importance_ member,
variable of trained model.We are creating a
new data frame calledvarimp, which contains a
variable name and importancescore of the variable.Here, we upload the
updated dm_scoreddf dataframe to CAS, running
on SAS Viya systemand convert it into a CAS table.We used upload_frame method to
upload data files to projectcaslib as nodeid
_score data set.This is the name of the scored
table that Model Studio expectsand it will use it to generate
model assessment reports.We also want to upload relative
importance of score generatedby xgboost model to CAS
in a project libraryas gb_varimp table using
upload_frame method.In order to take advantage
of some of the reportingcapabilities of Model
Studio, we convertthe evaluation results of our
evaluation set to a data framecalled stat.We upload and convert
the data frameto create the gb_stat CAS table.This table contains
four columns--the logloss and error for both
the training and validationdata at each iteration.In order to transfer the
output.txt and rpt_tree.pngfiles to SAS Viya system
from the remote system,where Python is running, we
use Python Paramiko package.The Python Paramiko gives an
abstraction of SSHv2 protocol,with both the client side and
server side functionality.As a client, you can
authenticate yourselfusing a password or key.And as a server, you can decide
which users are allowed access.An instance of
paramiko.SSHClientcan be used to make a
connection to a remote serverand transfer files.Paramiko requires that
you validate your trustwith the machine.This validation is
handled by callingset_missing_host_key_policy
method on the SSH clientand passing policy you
want to implement whenaccessing new remote machine.The file transfers are handled
by the Paramiko sftp client,which you get from
calling open_sftpon an instance of
Paramiko.SSHClient.Use the put method to upload
the files from Python systemto SAS Viya system.Now, run proc cas to
create a file thatcontains remrunner code, which
will run in Python environment,set up where a compute server
is running in SAS Viya system.The remrunner, remote
runner, transfers gb01.pyto remote host where Python
is running and execute it.In this case, gb01.py file
copied to temp locationon the remote host,
permissions are set to 700,and script is then executed.On cleanup, the PID
directory and all contentsare removed before
closing the connection.Finally, we create
custom reportsthat display tree plot
and output files createdon remote Python system.We then also generate a report
to display featured importancetable generated
by xgboost model.The dmcas_report macro is
used to register and describetree plot image, Python output
file, and feature importancetable.Sometimes images and
plots generated in Pythonand displayed in Model Studio
can have a readability issuebecause of resolution.We now create a
report such that wecan display either of
our calculated metricsfor the training
and validation data.We obtain the gbstat data
sets from CAS gb_stat table,by adding the iteration number
and modifying the structureof the table so we can plot it.The new variable, dataRole
identifies if the statistic isa training or validation value.The logLoss and error variable
contains the metric valuecorresponding to the
iteration and the data role.The dmcas_report
macro gives usersthe ability to create more
interactive reports than justthe images and tables.Here, we add the custom
""XGB Iteration Plot"" reportto the Results window
of the SAS code node.This report will be a line plot
of either the logLoss or errormetrics by iteration
for both partition data.The View option is
used to indicatethat we want to be able to
select the displayed metrics.When VDMML pipeline is
executed, SAS code nodecreates two Python
scripts, gb01 and scriptcontaining the remrunner
code during execution.The script containing
remrunner codewas submitted to
Python environmentrunning on the SAS Viya system
using the base SAS java object.The remrunner code transfers
gb01.py file on remote systemand submit it to Python
environment runningon that system.The SWAT was used on
a remote Python systemto connect to an existing CAS
session running on SAS Viyasystem to access the data.The input table is downloaded
from CAS to Python,and data frame is created.The Python modeling package
executed on a remote systemto build xgboost model.The score table, variable
importance table,were uploaded to CAS from
remote Python system using SWAT.The output.txt and
tree.png were transferredto SAS Viya system,
copied to node directlyusing Paramiko package.The node tab displays tree
plot, Python output file,and feature importance table.The XGB iteration
plot report alsoappears in the node results tab.The selector enables us
to control if the loglossor error should be displayed.The assessment
results are calculatedfrom the uploaded score table.The Lift report, ROC
report, Fit Statistic table,and event classification
chart is created and displayedin Assessment tab.So here, we saw how
using a SAS Codenode you can ran a Python
script on a remote serverand use SWAT to connect
to an existing CAS sessionto access the data, run a
Python modeling package,and upload resulting
tables in CAS.We also saw how
using the SSHClient,users can transfer files
from the remote serverto the SAS Viya system to
display reports in the SAS Codenode Results window.If you need more information,
you can refer to this paper,""Using Python with Model Studio
for SAS Visual Data Miningand Machine Learning.""You can find sample
code in GitHub.Please contact us if
you have any questionsor need more information.Thank you."
127,"HEATHER BURNETTE: Hi, and
welcome to my SAS Global Forum2020 virtual session.My name is Heather Burnette.I work for Teradata
Corporation, and today I'llbe talking about best practices
for enabling SAS Analyticsin the cloud at scale.I work on the-- for Teradata
on the SAS and Teradatapartnership, and
I'm lucky enoughto work at the SAS Headquarters
in Cary, North Carolina.I get to work with R&D,
QA, and SAS tech support.And as part of my
job, I get involvedwhen joint SAS and Teradata
customers have issues.And as our customers have
started to move into the cloud,we have noticed some
lessons that we have learnedand some best practices
that we can followto help all of our customers
make a smooth transitioninto the cloud and to get
the most out of their systemonce they get there.So the first set of steps
I want to talk to you aboutis how you set up
SAS in the cloudso that you have the best
performance possible.And the best tip that I can
give you to start off withis to make a plan for
your whole ecosystem.And that means to understand
that SAS is actuallypart of a larger analytic
ecosystem that you have.You may have other
applications that you run.You definitely
have data that SASuses that could be in SAS
data sets and a database.You have workloads that you
run associated with SAS,but it could also be open-source
programs that you have,like R and Python.And there's also security
and authenticationinvolved in that as well.And if you would like to
move SAS to the cloud,there are implications
because of the dependenciesthat will affect
these other thingsthat you need to be
aware of before youmake the move to the cloud.So the suggestion is to
perform a complete inventoryso that you know everything that
is in your analytics ecosystem.Do workload analysis.Do a sizing assessment.Make sure you determine
all of those dependencies--how those pieces are next
and how they'll be affected.With that information,
you'll be able to determinewhat you want to move
to the cloud, what'sgoing to be staying on
premise in your data center,and how and when you want
that migration to occur.Now, I understand that that
may seem like a daunting task,and you shouldn't do it alone.We recommend that you seek
outside expert advice.And getting help
from these expertswill definitely save
you time, money,and headaches in the long run.And at a minimum,
you should reach outto your SAS account team.If you're a Teradata
customer, reach outto your Teradata account
team or your database vendor.And you need to do
research to determinewhere it is that you're going
to place your SAS system.Which public cloud are
you going to put it in--AWS, Azure, Google Cloud?And reach out to them as well.They have lots of
experts that canhelp make this transition
successful for you.So now that you've
made a decisionof what is moving to
the cloud and what--and how you plan
to get it, you needto know where you're going
to place it in the cloud.And the experts will have
lots of advice for you,and you should definitely listen
to the advice that they have,but it's important to educate
yourself in this process too.So that's what part of
what I'm going to be doingis educating you on this.Because SAS uses so much data
and the data is so important,we recommend that you co-locate
everything as much as possible.The further away your analytic
components are in the cloud,the longer it's going to
take them to communicate.Anytime SAS sends
a request, it'sgoing to take longer for the
other component to receive itand to answer.That is called latency, and
there is increased latencyin the cloud.So for an optimal scenario,
what do you want to dois to take a single
cloud vendor.Don't put one SAS system
in AWS on the east coastand another SAS system on
the west coast on Googleand try to connect
the two, and expectthat you're going to have the
same performance that you havein your on-premise data center.It won't work that way.So again, for optimal scenario,
pick a single cloud vendor,take one geographic region,
and place all of your SAScomponents, all of your data,
all of the other pieces,any of your open-source--R, Python-- all of
those jobs place themall in the cloud in
the same location.So if we take a
closer look at this,this is the actual
architecture that wesuggest that you use to
link up SAS and Teradata.Now, if you've
heard about Teradatawithin the last couple
of years, you'veprobably heard of
Teradata Vantage.Historically, Teradata was a
traditional data warehouse.It was located on-prem.But as times evolve,
so does Teradata.So Teradata has evolved more
into an analytic platform,but if you have Teradata
database version 16.20or higher, believe it or not,
you have Teradata Vantage.So Teradata is
available right now.Of course, you can still get
it on-prem and in the Teradatahosted cloud.But it is available
in Azure and AWS.And coming next quarter,
Q3 2020 will alsobe available on Google Cloud.So all of the major
cloud platform vendorswhat you know and love--Teradata Vantage is
available for you there.I also like to point out thatTeradata Vantage has a
free developer version.I use that personally for
a lot of my own projectsthat I have going
on-- anything whereI need to send up an instance
of Teradata quickly and easily.Teradata does not
charge for that.You do have to have an
AWS or an Azure account.And if you have any
storage, of course,AWS and Azure will charge
you fees for those,but Teradata does not charge
you at all for the software.I will stop just now and mention
that, in the paper that I wrotethat's associated with
this presentation,it has links for all of
this and many other tips.So you're welcome to
get the direct links,rather than going out.Or you can go directly to
AWS or Azure Marketplaceand find those yourself.So when Teradata is created
in the cloud, all of the nodesare placed within one VPC.That stands for
virtual private cloud.In Azure, it's called a
VNET, a virtual network.And all of those are tied
to an availability zone.An availability zone just
means that all of those thingsare going to physically reside
in one geographic location.It is one single
data center in AWS,so you are guaranteed
that those nodes aregoing to be physically
close together as possible.SAS actually recommends
that you do the same thing.You put all of
your SAS componentstogether in one VPC or VNET, tie
them to an availability zone.And of course, we recommend
that you put Teradata and SASin the same geographic region.Now, Azure goes
one step further,and you can also D slides
the previous configuration.You can get one more
step of optimizationand get it placed within
the same virtual network.That provides just a
little one additional layerof optimization
if you have Azure.OK, so now that you
know where everythingis going to be located,
it's important to understandthat the default networking
speeds in the cloudare probably not
the same as whatyou're used to having in your
data-- on-premises data center.And the SAS Performance
Lab recommendsa minimum connectivity
speed of 10 gigabitper second between
each of your SAS nodes,and also between your SAS
nodes and your data sources.In order to get
something like that,you are likely going
to have to investin some high-speed
connectivity options.This is something that we see
very often with our customers,and so I've gone
ahead and I'd likefor you to spend some
time educating yourself.We know that it's not
a one size fits all.Every company is different,
and so the solutionthat works for you might not
work for another company.But I gave you some
suggestions to go aheadand get you started.Now, these suggestions
for AWS and Azure--our private link for Google
Cloud is VPC Network Peeringand all of these connectivity
options that I've providedfor you--they give you additional
speed, but they alsohave increased security as well.So that's another thing I
know that people worry aboutin the cloud.And so your only
consideration is not justconnecting all of your
components in the cloud,but it is also connecting
your cloud environment backto your on-premises environment.Now, there will always be pieces
that can't move to the cloud--well, not always,
but in most cases,we found companies can't
move everything there.And for things that can't
move, it's important to havea private, dedicated,
high-speed connectivity--connection between
your cloud andyour on-premise environments.Again, that gives you
increased performance,and you definitely
need it for security.So for AWS, I recommend
Direct Connect--for Azure, ExpressRoute,
and for Google Cloud,Cloud Interconnect.Now that I've given you some
ideas and some suggestionsof how to set up your cloud--SAS in the cloud
so that it wouldprovide some
optimum performance,you might think
that you're done.But actually,
that's not the case.Running things in the
cloud may require a shiftin your thinking of
how you run your jobs.And so I'm providing a list
of technical performance tipsfor you to use when you're
writing programs thatare going-- writing or
running programs that aregoing to be run in the cloud.So the first one
is you might haveto rethink how your analytic
workloads are being run,and think, are you running your
SAS jobs in an optimized way?You probably have a lot of SAS
jobs that you run regularly,and in fact, it may
just be you decideto run those jobs on a whim.Oh, I think, right now,
I'll run the SAS job.And when everything is in
your data center very close,by that can be fast, and
quick, and easy to do,but when you're talking about
sending data over a wide areanetwork, remember,
again, with the cloudcomes increased latency and
potentially slower networkingspeeds.Even if you've set your
environment up correctly,there are still
hiccups in the networkand things like that
you might encounter.So it's good to
think about, do Ineed to run my SAS jobs
in a different way?So I think it would be fun to
look at this little example of,what if you had a
large group of peoplethat you wanted to get
across the country?And maybe think about how
this relates to your SAS job.Does it make sense to send one
person on a plane at a time,or does it make sense
to send a small planeif you have a huge group of
people that needs to run?Would it be better if you ran--if you had multiple large
planes running at the same time?And how this might
apply to your SAS jobsor if you have a small
job that only requiresmoving a small
amount of data, thenmaybe you have four or five
of those that you need to run.Well, rather than doing
one at a time serially,maybe you should run those
all at the same time.You could also talk
to IP and determinemaybe there are
things that you'd liketo schedule for off-peak hours.There are things that
could be run overnightand be ready and waiting
for you in the morning.So one other thing
I do want to mentionis, what happens if you try
to run all of your SAS jobs,or what happen if
we tried to sendall of the planes in
the air and have themall land at the same time?Of course, there's
going to be contention.So again, think
about scheduling.And obviously, you don't
want to run everythingat the same time because
of scheduling and blocking,but it is something to consider.You can definitely run more
than one thing at a time.The next step is to
limit the amount of datayou are going to transfer.You should be aware
that, when you'retransferring data
in the cloud, it nowwill cost time and money.This used to be free.You could move lots of data
around in your data center,and it didn't cost you anything.You might have
heard the expressionthat getting data into
the cloud is cheap,but getting data out of
the cloud is expensive,and that is actually very true.If you set yourself up so that
you have your data and your SASinstance located close together
in the same geographic regionwithin a single
cloud vendor, that'sgoing to be the
best case scenario.It's going to be
the least expensive.But still remember that it
takes time to move that data.The more data you
move, the farther youmove it, the longer
it's going to takeand the more it plans cost.So as you write your SAS job,
keep those things in mind.Another tip is to run as much
as possible in the database.SAS and Teradata have been
partners for over a decade,and I've created a
list of SAS productsthat are integrated
into the databaseto help push processing--help push more processing
into the database.And this is a list
of those products.Some of them take advantage of
what is called the SAS embeddedprocess.Teradata was built with
an MPP architecture.For those of you that
are new to that term,it's massively
parallel processing,which means that Teradata
has lots of Teradata nodesthat process huge
amounts of data,and it does it all in parallel.The SAS EP is SAS software
that gets installedon every Teradata node.So any of the products
that use the SAS EPare actually running SAS code
on every node in Teradatain parallel-- pretty cool.So one of the best
ways to take advantageof all of the benefits of
running things in databaseis to do your data discovery
and your data preparationin the database-- to do
all of this analytic workwhere the data resides.So there's a couple
advantages of doing this.One, you don't have
to move the data.Of course, that saves us
time and it saves us money.We've learned that.But also, Teradata customers
generally have a lot of--they work in industries
that have governance issues,and so keeping the
data in one placemeans that they
are more compliantwith those regulations.So if that applies
to you, that'salso one thing to consider.But these are just a couple
of coding examples and waysthat you can run
some of the stuffdo your data preparation
in the database.I want to point out just
two things really quickly.One, there is an option
called dbidirectexec.That's an option that you
can put on your SAS programs,and it just makes sure that SAS
pushes as much as possible downinto the database.For example, you can see there's
a proc means right below it.So proc means, proc
free, those are,things that you would write
in normal SAS programs,and instead of calling all of
the data out of the databaseto perform a proc means
that would get pushed downinto the database.So you save the time of
doing the data transfer,and it increases the
performance of that activityby quite a bit.And of course,
Teradata runs on SQL,so you are more than welcome
to write your favorite SQLstatements and proc SQL.This is a great way
to do aggregationsand transformations
of your data before--to do any of your data
discovery and data prep .Another thing that you can do is
to score your SAS models insideof the Teradata database.Again, this takes advantage
of the SAS embedded processthat's installed on all
of the Teradata nodes.So it is SAS code that is being
run in the Teradata database,and you can see there's a way
that you can do it in SAS 9,and also in SAS Viya.So we know it's not
possible to run everythingwith SAS inside the
Teradata database,so when you're
running your programsand you realize you do
have to transfer data,we'd like for you to do
it in an efficient way.And there is a product
called the SAS Data ConnectAccelerator, which
allows you to do that.The default way is--as you can see on the
left-hand side of the screen,is a serial data connector.What this requires is that the
CAS controller request datafrom the Teradata
database, the databasereturns it to the
CAS controller,and the CAS controller
distributes all of the datato the CAS worker network.And then it can continue on
with any of the processing thatneeds to be done.Using the SAS Data
Connect Accelerator,each individual CAS worker can
connect to individual Teradatanodes, and you can upload all of
the data that's being requestedin parallel at the same time.So you can imagine,
that's quite a bit faster.So if you are score
this, this is the codethat shows you how to do it.You can see kind of in the
center it says dataTransferModeequals parallel.and that is how you tell Viya
that, when you do your CASactions, that you
want to do your datatransfers in parallel.So this example is a
parallel data load.You can do the same thing.And you can save
data to Teradataas well, so you can
do loads and save.And in the SAS log,
you'll be able to seethat the Teradata load was
performed successfully.You can see it says-- it
performs a parallel load tableaction, and it used the SAS
Data Connect Acceleratorfor Teradata.So that's how you
can know for sure.But it did it the most
efficient way possible.So if you don't have
Viya, of course,I didn't want to
leave you hanging.This is how it works for SAS 9.It's not the exact same.It doesn't work the
same way, but there isa Teradata product called the
Teradata Parallel Transporter--TPT.And in your data set option,
you can set TPT equal to yes.And depending on if you
are exporting data outof Teradata, loading it into
SAS, you can use FastExport.If you have a SAS data set that
you want to save to Teradata,then you can put TPT equals to
yes, FASTLOAD equals to yes.And again, you can
look at the logsto ensure it is performing
using the prefs setting.OK, well, thank you very much.This is just a summary slide
to talk about all the tips.I appreciate you
listening to this.Have a good day.Bye."
128,"SARADA PADATHULA: Hi.I'm Sarada Padathula, from team
Mining Minds, Oklahoma StateUniversity.Today, I will be presenting
our paper titled Hate SpeechClassification of Social
Media Posts Using TextAnalysis and Machine Learning.Before I begin, I would like
to introduce you to my team.Sarada Padathula, myself,
the business analyst,Venki Konduri, our project
manager, Ashish Pamuand Sravani Sigadam,
the data scientist.Disclaimer.Some content of
this presentationmay be perceived as
offensive and hateful.It is included with
no other intentionbut to emphasize that
use of such languageas commonly found on
social media platforms,and may negatively effect
some sections of the society.Hate crimes are on the
rise in the United Statesand other parts of the world.In hate crimes, victims are
targeted for who they are.They are threatened,
assaulted, or even killedon the basis of race,
religion, ethnicity, gender,sexual identity, disability,
or other characteristics.This chart from US Department
of Justice 2018 Hate CrimeStatistics shows that nearly
60% of all hate motivated crimesare motivated by a victim's
race, ethnicity or ancestry.So how is speech
related to hate crimes?Hate motivated
speech is consideredan offense in many
countries around the world.The US, however, protects
speech under the FirstAmendment of its constitution,
but only as long as such speechdoesn't interfere with the
civil rights of others.In the past decade,
the social media usagehas increased exponentially.About 45% of the
world populationare social media users.That is about 3.7 billion people
out of a total 7.7 billionworld population use different
kinds of social media,ranging from Facebook,
Whatsapp, Instagram, Twitter,to many others.So with these numbers, we know
that hate has a propensityto spread expansively and
rapidly, and even motivateindividuals who are
inclined towards hate.Now let us look at some
usage statistics of Twitter,which is the platform on
which we based our study.Twitter has as of
2018, Twitter hasabout 330 million
active users per month,who spend an average of
3.39 minutes per session.The users of Twitter produce
a massive 500 million tweetsper day.This contains all
kinds of content,like news, opinions, thoughts,
hashtags, and even hate.Twitter has been the
social media platformof choice for many eminent
people around the world,ranging from leaders of
nations to business leadersto celebrities to sports
stars and many others,along with common people.Many times we have seen
that none of these usershave been immune to
controversy, or sometimes evenlegal troubles, for
posting somethingdistasteful on Twitter.This is the whole idea
behind our analysis.That is, to come up with a
model that can classify tweetsbased on their content.Specifically,
hateful versus normaland alert the users before they
post it on the public platform.Now here are examples
of two tweetsthat caught the attention
of media and peoplearound the world.These tweets, as
you can see, arefilled with hate and prejudice.These tweets not only caught
the attention of people,but also cost the
tweeters dearlyin the form of lost career
opportunities, backlashand embarrassment.Now consider two
real tweets by users,say Jane Doe and John Doe.Take a moment to read them.Both these tweets
sound like threats.And that is the same reason
these tweeters got into troublewith the police, as reported
by a Business Insider article.Later investigation only proved
John to be an excited Pinkfan who was completely
unintentionalin the meaning of his tweet.Sometimes tweeters
are a bit too excited,or make a wrong choice of words.Or they might be even
in a bad state of mindwhen composing a tweet.Our model can come to
rescue in such cases,and give the users a
second opinion on the tweetthat they're about to
post, thereby saving themfrom costly mistakes, backlash,
embarrassment, or evenpotential legal consequences.Any good model begins
with good data.Our model needed tweets
labeled into categories,like hateful, normal, et
cetera, based on content.While many such data
sets are readilyavailable on the internet,
the issues with themare, is there a subjective
bias in labeling?And how did the
annotator label a tweetinto a particular category, when
some tweets potentially couldfall into multiple categories?With these ideas in mind,
we continued our searchfor a good data set,
and finally foundone that addressed
these problems.This data set is the output
of a research study namedLarge Scale Crowdsourcing and
Characterization of TwitterAbusive Behavior.This data set is the
output of a research studythat used crowdsourcing
for annotation of tweets.With multiple people working
on labeling each tweet,they used majority
voting on a labelto minimize subjective bias, And
also used statistical analysislike correlation
analysis, to consolidateoverlapping tweet labels
into one appropriate label.Our analysis needed
two date sets.One for training the
model and one for scoring.Our training data set is from
the research team at iDramalab.It had about 80k
observations whichare filtered out to
keep observationsof just hate and normal
labeled category.So with that, we had data
set of 58,000 observations.Our second data set
was from hatebase.org,which had a total of
about 25,000 tweets, whichwere again, filtered
to just have hateand normal, which are the
ones needed for our analysis.And this was our test data
set for the score data set.As you can see from
the legend below,the training data set had
only 8% of hate tweetsand 92% of non-hate tweets.Similarly, the scoring data
set had 26% hate tweetsand 74% non-hate tweets.The training data set
was severely imbalanced.In order to compensate
for this, and to overrideany over-fitting
problems, we performedstratified under sampling
over the dominant class.Now let's look at a high
level overview of our process.Our analysis began with
text pre-processing.This involved two steps.Namely, text parsing
and text filtering.Text parsing breaks text
into two kinds of words,detects parts of speech,
perform stemming, et cetera.Even the cleaning is
done at this phase.The cleaning, such as removal of
stop words, entities, like usernames, phone numbers,
and even punctuations.This was done using a
SAS Enterprise Miner TextParsing node.Once text parsing has
been done, text filteringwas performed by using SAS's
SAS Enterprise Miner Text Filternode.Text filtering helps
to remove termsbased on the term
importance through weightingtechniques such as TF-IDF.Simply put, TF-IDF is
a statistical measure,which is numeric, about
how important a word isto a document in a corpus.So using this numeric,
unimportant and insignificantwords are removed.Once these this
step is finished,we move on to the next step,
which is called topic mining.This is done using SAS's SAS
Enterprise Miner Text Topicnode.This text topic node
helps detects text topicsautomatically.Topics are collections of terms
that describe and characterizea main theme or idea.Using a score assigned
to each doc, documentand term to each topic,
the Text Topic nodedetermines whether the document
or term belongs to that topicby checking against
the threshold score.So our model generated
about 25 text topicsrelated to different
categories like hate,normal and other
general categories.Once the text
topics are derived,we split our partition
our data set into 70/30for training and validation.After this has been done,
we built models on our datafor performing
classification of tweets.For this, we use logistic
regression, decision tree,and gradient boosting.Once we built the
models, we comparedthem using SAS Enterprise
Miners Model Comparison node.Now, let's take a look at
some of the Text Topic termsfrom our model.This figure shows you
side by side comparisonof non-hate versus
hate terms, whichare part of our text topics.As you can see,
terms like country,birthday, goal, glad, beautiful,
celebrate, story, and others,are non-hate topic terms.Whereas in the
hate-themed terms,we see hate, bloodbath, disgust,
threaten, extremism, sexist,racist, and others.So this gives you a brief
idea about our text topics.Now coming to the
model comparison.From the results
shown below, youcan see that gradient boosting
has the highest validationaccuracy.And SAS's model comparison node
using this validation accuracydetermined gradient boosting
as the champion model.Accuracy is the
measure of the numberof correct predictions per
total number of predictions.So out of the logistic
regression, decision tree,and gradient boosting, we see
highest validation accuracyin gradient boosting.Whereas, the training accuracy
is same for decision treeand gradient boosting,
slightly morethan the logistic regression.Now coming to how the champion
model generalized on the scoredata.These charts show the accuracy
sensitivity and specificityof our model in classifying
the validation data and scoringdata.We can see that our model's
sensitivity on the scoring datais 66%.Sensitivity is the
true positive rate,whereas specificity is
the true negative rate.So we can say that about 66%
of hate labels in our modelare predicted as hate.Here are some examples of
tweets that our model labeled.Just like any other
classification model,our model has its share of
true positives, true negatives,false positives,
and false negatives.From these examples, let me
explain how our model labelssome of these tweets.The first tweet, as you
can see, is clearly a hate.And our model predicted this
hate tweet as a hate tweet.Second is a normal tweet.And this normal non-hate
tweet has been correctlypredicted again.The third tweet
is non-hate tweet,but it has been incorrectly
predicted by our model.The fourth tweet, as you
can see, is a hate tweet,but it is incorrectly predicted
as non-hate by our model.Potential applications.Our model can be
used in proactivelyalerting about hateful content.This can benefit the users in
carefully drafting their tweet.It can also be used by Twitter
to flag hateful content.To put our model
into implementation,an application may
be built on top of itand provided as a
feature to the user.Another feature that can go
hand-in-hand with this featureis a recommender system that
recommends replacement wordsfor hateful words.Next is the Future
Scope Model Enhancement.As a future scope, we want
to include additional datain our analysis.And this data, such as
retweets, comments and likes,might add some insights and
help in better classificationof our tweet.We would also like
to delve deeperinto the meaning of the tweet by
doing some sentiment analysis.Finally, we would like
to extend our applicationto other social media
platforms, and also includemore labels for categorizing.This concludes our presentation.And I would like to thank
SAS and the Global Forum teamvery sincerely on behalf of
my whole team for giving usthis opportunity to participate
in the student symposium.A big thank you to all of you.If you have any questions
regarding our presentation,you may reach us at one of these
email IDs listed on this page.Thank you."
129,"KEVIN RUSSELL: Good
afternoon, everybody.I'm sorry that we couldn't
meet in person today.But I would like to
welcome you and thank youfor viewing my presentation.This presentation's entitled
""Next Steps and ImportantConsiderations for Moving Your
Data and Formats Into CAS.""My name is Kevin Russell.I'm a consultant in
SAS tech support.I work in the Foundation group.My primary focus is the
DATA step and macro.But in the last few years,
I've been working quite a bitwith CASL.So in today's
presentation, we'regoing to discuss three
main considerations.First, I'll discuss
the change in characterencoding you'll likely
encounter when youload your data set into CAS.Next, I'll discuss loading
your data in parallelversus loading it serially.The third thing we'll consider
is that the resulting CAStable will likely be larger
than the source data set.And finally, we'll combine
everything that we've learnedand we'll go over two
examples, the first summarizinghow to move or load your
SAS data set into CAS,and the second showing
how you can load your userdefined formats into CAS.The first thing we'll
consider today is encoding.We'll discuss what is encoding,
the types of encoding,and then transcoding your
data into the new encoding.So what is encoding?When you're storing
your data, charactersare mapped to
numeric values, whichresults in a table of all code
points known as a code page.This set of integers
in the code pageare mapped to a
sequence of binary codesthat get a character encoding
scheme, otherwise knownas an encoding.So simply put,
encoding is the waycharacters are stored
on your computer.In your SAS session, you can
view your session encodingby submitting a proc options.This proc options will write
your SAS session encodingto the log.I executed this proc
options on Windows.And so we can see that
the encoding is WLATIN1.You can see the encoding of
your SAS data set, or CAS table,by using proc contents.This proc contents again
was executed on Windows.And so the encoding for this
data set again is WLATIN1.So now, let's discuss the
different types of encoding.Most of our users
in SAS 9 are goingto be using a single-byte
Western language encodingby default. This is
going to be WLATIN1if you're running on
Windows, and LATIN1if you're running on Unix.A single-byte
encoding just meansthat each character is stored
as a single byte of memory.The other type of encoding
we'll need to consider as UTF-8.UTF-8 is becoming the most
used and preferred encoding.This is because it's capable of
representing all the charactersused in modern software.So as a result, UTF-8 is the
only encoding available in CAS.This slide shows
the number of bytesneeded to display
different characters.So for US English
or ASCII, this isgoing to require one
byte to be stored.In the Eastern
European languages--Eastern and Western
European languages--including Turkish,
Hebrew, and Arabic.Those require two
bytes to be stored.With the Asian languages--Chinese, Japanese, Korean-- each
character requires three bytesto be stored.And some of the uncommon
Chinese, Japanese and Koreancharacters require four bytes.Let's take a look at an example.In this example, I'm
going to show youthe difference between
WLATIN1 and UTF-8,how it stores the value.In this example,
we're going to lookat the difference in how
these two encodings storethe registered trademark symbol.That symbol can be seen in
the middle of the slide.It's just a circle with
the letter R in the middle.In WLATIN1, this
symbol is stored as--or I'm sorry.In WLATIN1, this symbol is
represented by AE in hex.However in UTF-8, it's going
to be stored as two bytes.So it's represented
by C2AE in hex.The registered trademark
symbol is an exampleof a character that'll cause
an error if you don't takeany action when you're loading
the data set that containsthat character in the CAS.If we load a SAS
data set in the CASthat contains that
registered trademarksymbol without adjusting
for that symbol,we'll get this rather
lengthy error messagewhen we load the data set.Now as far as error
messages go, I reallylike this error message.This error message
is very specific.It tells us exactly
what the problem isand just as importantly,
it tells us how to fix it.This error message lets us know
that the character data waslost or some character data
was lost during transcoding,and that the data contains
characters that are notrepresentable in
the new transcoding,or the new encoding, rather.It also tells us that the
error message can be preventedby using the
charMultiplier= option,which is available
within proc casutil.The charMultiplier= options
available within the importoptions parameter, proc casutil.This option specifies the
number of bytes per characterto use when you're storing
your character data.So basically, this option
gives your character variablesmore room to breathe.The registered
trademark symbol can'tbe represented by a
single byte in UTF-8,so we need to give the character
data more space to store it.Here's an example of using the
charMultiplier= option and proccasutil.We know our data.So we know that we don't
have any characters thatwill require more than two
bytes to store the character.So in this example, we're
setting the charMultiplier=option to 2.So now when we load that
data set into CAS, runningthis code, there's enough space
for the registered trademarksymbol to be stored and the
data is loaded without any errormessages.Our second
consideration today isgoing to be the transfer
method we use to loadthe data set in the CAS.In this section,
we're going to compareserial loading of data versus
parallel loading of data.And then once we have a
better understanding of both,we're going to take
a look at an example.This graphic shows you a
serial load of a SAS data set.A serial data transfer involves
a sequential transfer the datathrough a single
channel from the sourceof the data to the destination.This data transfer method
is the most flexibleand it's always available.The most common
condition that willrequire you to use a
serial data transferis when your data source is
only accessible by the CAScontroller.In this example, the
CAS live that containsour data set is only accessible
by the CAS controller.So when the data's transferred
via a serial data transferto the controller, the data's
going to be then redistributedto the CAS workers.Although a serial data
transfer is always available,it's not the fastest way
to load your data into CAS.If a parallel data
transfer is available,then this is going to
be the preferred methodof loading data into CAS.A parallel data
transfer allows the datato be concurrently
transferred acrossmultiple independent
connections between the datasource and the destination.This method greatly increases
the speed in which your datasets can be loaded into CAS.There are two
primary requirementsfor a parallel data
transfer into CAS.The first is that your
deployment is an MPPdeployment, meaning that
it consists of multiple CASWorkers, like we
see in this slide.And the second is that
each of the CAS workers--sorry about that.I got a little ahead of myself.The second is that
each of the CAS workershas direct access to the caslib
that contains the SAS data setyou're loading.So in this slide, we
can see that each workerhas access to the
caslib that containsthe test data data set.So when this data set's
loaded in parallel,the data is loaded
simultaneously directlyinto each of those CAS Workers.Here's the code I
used to load the testdata set into CAS using
a parallel data transfer.Now what gives me the
ability to do thisis that my caslib
is a shared filesystem that's accessible
by each of the CAS workers.So this caslib statement
defines the mycaslib,which happens to be
a shared file system.With my data set
in this location,I can use the proc casutil code
to load my data set into CAS.If we look at the
import options,we can see that I'm
using the DTM= option.This option allows me to set
the data transfer method.So in this example, I'm setting
the data transfer methodto parallel.You also see that I'm
using the debug= option.And I'm setting this
option to a value of DTMNow what this
option does is it'sgoing to write a
note to the SAS logwhen we run this code
that's going to let us knowwhich mode of transfer we use.Did we transfer it in parallel?Or did we transfer it serially?Here's a log generated
by that proc casutil.This slide is a little busy.But I did want to point out
two pieces of information.The first is the note that
lets us know that the data wastransferred in parallel.The second is the amount of
time it took to load this data.This data set's fairly
large, about 15.6 gigabytes.So it took three minutes
and 10 seconds to load.To show you how much
faster a parallel load is,I loaded the exact same table.But this time I
set DTM= to serial.We can see the note
in the log thatlets us know that the data
was loaded in serial mode.Now if we look at
the real time, wecan see that it took about 8
minutes to load this data set.That's about 2 and
1/2 times slowerthan it was when we
loaded in parallel.So if you have the ability to
load your data in parallel,then as you can see, it's
going to be much faster.Our last consideration
is going to bethe size of the CAS table versus
the size of your source dataset.Something that you'll notice
when a SAS data set is loadedinto CAS is that the
size of the CAS tableis going to likely be larger
than your original SASdata set.Although CAS is designed to
run large scale analyticson big data, you still have
a finite amount of memoryavailable on each worker.So to maximize
efficiency, you'regoing to want to reduce
the size of your CAS tableand use the least amount
of memory as possible.This section discusses why
your CAS table can be largerthan the original
data set, and alsohow you can reduce the
size of the table usingthe new VARCHAR variable type.The first concept
in this sectionthat we're going to
need to understandis data structure alignment.Data structure
alignment is the waythat data is arranged and
accessed in computer memory.Aligned access is faster because
the external bus to memoryis not a single byte wide, but
typically 4 to 8 bytes wide.So we're accessing 8 bytes at a
time versus just a single byte.Well obviously, your data
isn't going to alwaysbe a multiple of 8 bytes wide.So your data's going to have
to be padded with blanksto reach that 4 8 byte boundary.And it's this padding that takes
place during data structurealignment that is the reason why
your CAS table is going to belarger than your SAS data set.A way to help with
this size increaseis to use the VARCHAR
variable type.The VARCHAR variable type is
the new variable type for CAS.The VARCHAR variable
type consistsof two main characteristics.The first is its
length is determinedby the number of
characters rather thanthe number of bytes.And secondly, the
length varies from rowto row depending on the
characters in that variable'svalue.So for example, if we
have a total lengthfor the VARCHAR
variable is 10, valueswith a length of less than 10
are not padded with blanks.An important aspect of
the VARCHAR variable typeis that there is
a 16 byte overheadto use this variable type.Because of this, the
VARCHAR variable type'sonly recommended for variables
whose values consistentlyexceed 16 characters.Also the VARCHAR variable
type provides the most benefitfor variables whose values
vary greatly in length.So let's take a look at an
example of taking advantageof the VARCHAR variable type.I have a sample data set with
5.5 million observations.It has 30 character variables.And 15 of these variables
have a length of less than 14,and 15 of the variables have a
length of up to 100 characters.So when we load our
table, data set into CAS,we want to make sure
that only variables thathave a length of 16 or
greater converted to VARCHAR.We can make this happen by using
the varcharconversion= optionwithin proc casutil.So to make only variables with
the length of 16 charactersor greater converted to VARCHAR,
we set the varcharconversions=option to 16.To share the benefits of
the VARCHAR variable type,I loaded this data set twice.Once using the code we just
saw to convert variableswith a length of greater
than 16 to VARCHAR.And once where no variables
were converted to VARCHAR.And this shows
the final results.Our original data set
was 15.6 gigabytes.When we loaded that data set
into CAS using only charactervariables, no VARCHAR
variables, the sizegrew to 16.1 gigabytes.However in this case, when
we converted the variablesto VARCHAR, the size
of the CAS tableactually was reduced
to 7.2 gigabytes.So as you can see using
the VARCHAR variable typeis a powerful tool you can
use to help you controlthe size of your CAS tables.So now let's put
everything together,and everything we discussed,
and work together through someexamples using the
charMultliplier=,the data transfer
method, or DTM=,and the VARCHARCONVERSION
options.This slide shows the
code we can use to loada SAS data set into CAS.The first line of code is going
to be our caslib statement.This statement creates the mycas
caslib located in the directorylisted on the path= option.The second step is going
to be our proc casutil.This step loads the
test data set into CAS.This data set is located
in the mycas caslib.We are going to write that data
set out to the casuser caslib.And the resulting table is
going to be called alltogether.When we're letting
this data set,we want the character
variables with the lengthof greater than 16 to
be converted to VARCHAR.So we set the
varcharconversion= option to 16.Also in this data
set, we know wehave characters like the
registered trademark symbol.So to accommodate
for this in UTF,we're setting the
charMultiplier= option to 2.Finally, we're going to set
the DTM= option to auto.A setting of auto will
always perform parallel datatransfer if possible.And if it's not, then it
resorts to a serial load.So the data set that we just
loaded also has a user definedformat associated with it.So we'll need to move
that format into CAS too.So the easiest way to
move a format into CASis to create an output data
set from the SAS format,move that data set to a location
that can be accessed by CA,then use the data set to
recreate the format in CAS.So the first thing we need to do
is have two libname statements.The first libname statement
points to the locationof my format catalog.And the second libname statement
points to the location of whereI'm going to write the output
data set created by proc format2/Our next step is
going to be actuallyto write the format
out to a data set.This is done by using the
cntlout= option on the procformat statement.So now that we've written
the format out to a data set,we can load that
data set into CASusing the exact
same code we justused in the previous slide.The only thing we need to do to
recreate the format into CAS isto run a proc format with
the cntlin= in the casfmtlib=option.The casfmtlib= option points
to the name of the casfmtcatalogs, or catalog that the
format's going to be writtento.This catalog could be a
new catalog or a catalogthat already exists.So this is the log that's
generated by runningthat proc format code.We can see that the
format was writtento the mycasfmt library.One thing that's
interesting about this logis we'll notice
the note, it saysthat both CAS
formats and catalogbased formats will be written.The CAS base formats will be
written to the session CASAUTO.Then it tells us that the format
library mycasfmt is added.And that the format search
updated using the parameterAPPEND completed.So what all those words are
telling us is that in Base 0.or SAS 9, we had to manually add
the format catalog's locationto the fmtsearch= option.In CAS, that location
is automatically addedto the fmtsearch= option.And so this is going to be
our last slide of the day.Chances are that you'll want
to use the format associatedwith the data set we just loaded
into CAS in a subsequent CASsession.To make the format
available, we'llhave to issue this
line of syntaxbefore we end our CAS session.This statement uses the
savefmtlib option to saveor format in a table.The fmtlibname= option points
to our CAS format library.The caslib= option points to the
caslib where the table will bestored.And then finally, the table=
option names the table thatthe format will be stored in.Before you start to move
your data sets into CAS,you'll want to take a few
things into consideration first.In today's presentation,
we discussedthree of these
considerations in detail.Hopefully, the information
we discussed todaywill help you take
the next steps neededto start taking advantage of
the power provided by the CASsystem.I'd like to thank everybody for
viewing my presentation todayand I hope everybody
has a great day."
130,"Hello, everyone.My name is Bartosz Jablonski
It's a great pleasureto be here-- take part in
the SAS Global Forum 2020.This is a very special
event, since we are workingin the virtual event set up.So let's dive into the matrix.So as I said, my name
is Bartosz Jablonski.I'm a mathematician by train,
and a data analyst by trade.I'm working with SAS for
more than 10 years up now.I'm also a bit SAS teacher.I'm involved in very, very
different SAS activitieslike Polish SAS Users
Group, here in Polandbecause I'm from Poland, and I
also hold second and a half danin sasensei online quiz game.Please check it out.And one of my
features, let's say,is if you would like to turn on
my problem solver, the best wayto do it is to say you
probably can't do it.Something like this in SAS,
and today's presentationwas burned during one
of such discussions.I'm going to talk today about
the concept and the frameworkI prepared especially
for this event.It's a SAS package--concept of SAS package,
and the tool, the idea,which allows you to, in an
easy and comfortable way,share your code with others.Before I jump to the
subject, just one more note.Here, I am representing Warsaw
University of Technology,and also I'm representing
Citibank Europe PLC.Those into institutions.In the first one, I'm learning.In the second one, I am working.And also there is a third,
I won't say institution,I would say group of people
called Polish SAS UsersGroup, which I am
representing here also.Please check out our
LinkedIn web page.Join us at
communities.sas.com/polsug.If you are a fan of SAS,
especially from Poland,please join us, and try to work
out with us in the Polish SASUsers Group.And now, let's go
to the subject.Maybe let's start
with the inspirations.So how it all started.Some time ago, I was working
with my colleague Filipon the project.We were doing this
with-- we have two tools.I was using SAS,
Filip was using R,and we are doing our
analysis for the project.And at the end of the
project, I ended upwith a bunch of
SAS macros, whichwere required to do
my part of the job,and Filip ended up
with one single file,which was R package.And all the codes required
to do his part of the jobwas encapsulated
in that package.And I look at
this, and I thoughtto myself, that's very, very
cool, very, very useful thingto have.But unfortunately, I
wasn't able to do somethinglike this in SAS.And that was the descent, as
you probably can't do somethinglike this in SAS.And I start thinking about it.I start to wondering how
this SAS functionality couldbe provided.And then I encountered
discussion on the SAS-Ldistribution list, the oldest
SAS distribution forum--the distribution SAS forum,
where Quinn MacMillan wasdiscussing the subject
of reading data from zipfile, and this was the thing
that turned on my focuson the solution of creating
something like a package.And when I did the first
one, I did just by hand,and I showed it to my
colleagues from the office.One of them was Filip, the
other one was Krzysztof.I showed them the
idea, and I asked themwhat they think about it.And they simultaneously
said that's quite cool idea,but we think that you
could try to writesome kind of a framework
which would allowusers and developers
create their own packagein the automatic way.And that was the moment when I
start working on the subject,and a few days later, after
some scratching of my headand thinking about it, I
present them the first versionof a solution.So the SAS package.You can see this O'Reilly
style book cover here titledSAS Packages the Way
to Share, and thereare two animals, as usual,
on O'Reilly's books,and there is this
sentence at the top.""The wolf is sated and
the sheep is whole.""This is an old
polish saying, whichis that here the phrase
is literal translation,and this the saying means
more or less that if you havetwo parties of an
argument, discussion,or some kind of
interaction, you areable to find solution
which satisfiesboth of those parties.And the same we
have with the ideaof packages because when
you are creating package,when you are a
developer, you havesome requirements for such two.When you are a user, and you
want to use a created package,you also have some requirements.And I tried, in this
framework, whichI will be presenting
in a moment,combine those requirements,
and I hope I managed to do it.So what I by a SAS package.First of all, the concept of
a package in the SAS ecosystemis not something new.There was already something
like Procedure DS2 packages,SAS-IML packages, they
are also SAS-ODS packages,and there are also SAS
Integration TechnologyPublishing Framework packages,
also the EGP Enterprise Guidefiles kind of packages
because they are zipfiles with a bunch of metadata.But none of these is what
I am going to talk about.The framework, the idea
which I am presenting hereis something new.Since I am a
mathematician, let'sstart with some small definition
which gives you the idea.So SAS package is automatically
generated single standalone zipfile containing organized
and ordered code structures,created by a
developer, and extendedby additional automatic
generated drivingfiles and metadata which
combines everything together.The purpose of package is
to be single, simple, easyto access code sharing medium,
which allows, on the one hand,to separate developers work
with complex dependenciesof the code, which separates
it from the user's experiencewith the final product.And the second one is to reduce
both developers and usersunnecessary frustration
related to remote deploymentof other codes.And there is also third one--purpose, which I not
explicitly showing hereis the building of community.If you ever program with R,
you know that the packages in Rare something
fundamental, and wheneveryou create something which could
be valuable for the community,you encapsulate it in
the package and share it.And that's also my
goal here, to give youa tool which allows
you to share code youcreated to build a community.We are at SAS Global
Forum, the placewhere all people from
the SAS community meets.So that looks like the perfect
place to share with such ideas.So as I said, there
are always two partiesin this game, the user
and the developer.So one who is creating, and one
who is looking at the creation.So let me briefly
give you an ideaof how to work with this
framework of SAS packagesfrom both sides.Let's start with the
user point of viewbecause it's really simple, and
a very short and comfortableuse.So the user.Process takes three steps.First step is created
with files and folders.So user had to create a folder
in which the packages willbe stored, and then user have
to download the load package SASelement of the framework.You can see this
little asterisk here.So here in this moment,
I am giving a little winkto a SAS Institute.Maybe if you find this
concept interesting,maybe you will put this macro
provided by the frameworkinto the base SAS Echo
system in the future,so user then won't
have to download it.It will be just in SAS.And the second thing that
user needs to downloadis the package itself.So basically, you're creating
a folder download two files,one required for a framework,
and the second with the packageyou want to use.The second step of the process
is to execute some code.So this code, three
lines of code,allows you to
enable the frameworkand load the package to use it.Basically, three lines of code.And the final step of the
process on the user sideis the so-called what next.So when you load up the
package, first read the log.Check out what this package
is doing, what is the version,just familiarize yourself with
the package you've just loaded.You can also run the
health package macro,checking up what's more
the package is giving you.And as the final step,
just use the package.You are user.Use the package.And that's all.Those three steps
are all that the userneeds to do to work with
the package, providedto him by whom?By developer.So now, let's look at the idea
from the developers' pointof view.It's also very simple
and straightforward,and if you already have a bunch
of code macros, functions,formats, definitions, you
don't have to-- as a developer,you don't have to
do almost nothing.You have to do almost
nothing to make it usable.So the developer
side of the story.There are also three
steps which are required,and they are almost the
same as the user steps.The first one is the
files and folders.So the developer has to
download the framework.So both parts of the framework--
the generate package SAS file,and the load package SAS file.Again, here is this
little asterisk.So this is second wink
to the SAS Institute.Maybe you would like
to incorporate thisinto solution in SAS.Then, if the developer
wants to create a package,there is a need for a
folder in which the packagefiles will be stored.And the last step here, details
about how this file should looklike, I will show
in the next slide.But assuming that we already
have the framework download,and the packages files
prepared, now whatdeveloper should do
to create a package,just run three lines of code.First two to enable the
framework, and the third oneto generate the package
to run the framework,and create the package.As a result, developer
will get the zipfile containing all the packaged
content, and some summary.What next step?Developer should
read the summary,check out the log for
any information about howthe process went,
then good practicewould be to test
the package, whichis also a feature
available in the framework.And when everything is OK,
when tests are positive,then share your
package with the users.So that's basically
the very simplethree steps of the process.As I said earlier,
files for a packageshould be prepared
in proper way.Let me show you some
example of such structure.So this setup is very simple.In the package folder, you
embed the description.sas filewhich contains information
about metadata for your package,and then in a very simple way,
you structurize your code.So whenever you have code
creating a library, or a macro,or a function, or a format,
or any other available codestructure, you put it
into separate folder.Like name for libraries,
macro for macros,function for
functions, and so on.And each code reading either
library, or macro function,or format is a separate
file with the same nameas the function or as
the macro name itself.So basically, the
standard SAS wayof doing such things, if
you've ever used a SAS OUTERcalls framework for macros.There's also this file
named license.sas.It's not obligatory.If you don't provide
it, the frameworkwill generate automatically
the MIT license file for youfor your package.The description.sas file, the
one which contains metadata,is very simple structure.Let me share with you
example of such file.It contains two parts--the header and the
description part.The header is a set of
metadata used by the framework.The package name, the
short description,the package version, the list
of authors and maintainers,also then the type of license
on which the package is shared,and encoding queues created,
and also those two, I call them,tax required, and
required packages.They are not obligatory,
but very useful.The first one is the
list of SAS componentswhich are required
for your packageto work, like base SAS software,
or some kind of SAS accessinterface to any
external database.And the second one, ReqPackages.This is a list of
other packages whichare required for your
package to work properly.And the second
part description isthe free text in which you can
elaborate about your package.You can put there some
additional informationabout your package, some help
information, some examples,describe the content
of the package,and this part will be
used as an element of helpfor your package.So very simple structure.Very little set of
additional requirementson the developer's side to
create a package which couldbe shared with other users.So we know how to
use package, weknow how to create the
package in very general way.If you want to learn about
details, usually at this momentI would give you a
live demo, but weagreed that in
this virtual event,we are having a bit
shorter presentation.So let me just share with you
this link to my GitHub webpage.When I put all the
information up about packages,I put a copy of
the article, whichwill be published within SAS
Global Forum 2020 proceedings.I have put there all the
codes for the framework.I put the group of
packages, which I alreadycreated for you to play with
them, and to check them.If you are interested in such
things like live demo of howthe packages works, or maybe
you would like to contact me,have an online
call, or even kindof hands on workshop
about SAS packages,don't hesitate to contact me.Use either the email address
from the first slide,or contact me through
either GitHub,or any other social
media like LinkedInor something like this.And usually at this moment,
when the presenter is ending,there is a slide saying,
do we have any questions?But I decided to
change the slide.I would like to
ask you would youlike to build your
own package with me?So at this moment, I
would like to thank youvery much for your attention.I hope that you like
the idea, and you wouldlike to dig deeper into it.If you're interested, there
are some references whichmay help you to work it out.And at the very
end of this talk,I would like to express
my big gratitudeto two of my colleagues
from my office, Filip Kulonand Krzysztof Socki,
who spent a lot of timewith the early
version of my article,and spent some time
listening my ideas,and gave me the very,
very valuable clueshow to develop this idea.Filip, Krzysztof, thank you
very much for your effort,and that would be it.Thank you very much, I
hope you enjoyed my talk,and I hope we can talk online or
offline about it in the future.Thank you very much."
131,"Hello, everyone.Thank you for joining
us today for multinomialversus ordinal regression--
does model selection makea difference?My name is Corey Leadbeater,
and I am a data scientist,consultant, and adjunct
professor of Applied DataScience at National
University where I alsoreceived my graduate education
under the tutelage of Dr. TylerSmith and Dr. AnnMaria De Mars.I also sit on the WUSS
2020 open source advocacycommittee which--WUSS may or may not
happen this yeardepending on how
the COVID continues,but I invite you all to
join us in San Diego.Of course, pending whether
or not that occurs.We'd love to have
you on the site.You don't necessarily have to be
from the west coast to attend.So all you East Coasters, come
on out and get some sunshine,and we can all shave off our
apocalypse beards together.So I wanted to take a little
bit different approachto this presentation today.Really because I feel like
understanding practical usefor multinomial versus
ordinal regressionmethods is a little more complex
than the actual coding itself.And this is kind of one of the
things I like most about SAS,anyway, is that it is
really simple to switchbetween multinomial
versus ordinal models.And while both
models perform well,the user intent--
like what you wantto present with your
model-- is reallygoing to dictate which
one is best for you.So that's what we're
going to talk about today.And we're going to
explore how to use each,and we're going to present our
models alongside the problem,and then, again,
with the solution.So that being said, let's start
by addressing a common problemin data science.And I know that these are being
recorded, so you probably--probably wouldn't help
to see a show of hands--but I'd like you all
to think about a timewhere you had a conversation
with a colleague, or a boss,or a client, and you
want to leverage datato identify a problem or a
solution for all of the above.But they weren't really
sure how to begin.And this is a question that
I get all the time, actually,with my clients is we
want do data science,but we're not really
sure how to start.So I want to deconstruct this
a little bit because we can allempathize with these
clients, and itcan be very challenging to
choose how to attack a problemor even to identify a problem.So let's break this down so
we can stop being overwhelmed,and really just
kind of get to work.So where do we start?Well, you start by
picking a problem.OK?If you're like me when you're
going through meetings,or data, or whatever the
case may be, you're going,oh, you know what
would be reallycool to do with
this, or somethingthat we could probably
solve with suchand such is X, Y, and Z.
So just pick a problem.Doesn't matter which one.I would recommend maybe
not the biggest problemthat your company ha
or your organizationhas because that can
create challenges.But if you're just
wanting to get started,choose a smaller problem.Something you can kind
of knock out, and thenmove on to the next one.And what this'll do is
have a snowball effectin creating change
for your organizationor for your client's
organizations.The next thing you
want to do is youwant to articulate that need.So you want to demonstrate
why this problem is a problem.OK?And then, finally, you
want to propose a solution.So the nice thing
about solutionsis that there are many
of them, and you reallyonly need one solution
to solve a problem.Oftentimes, one solution
you come up withis going to actually
solve many problems.So you got kind of this cool
solution effect which is--I don't know, but it works.OK.So now moving onto our problem.And maybe some context and
some background on this.At NU, we were exploring this
Monitoring the Futures studydata set, right?And this was basically
occurring at the same timewhen I was enrolling
my son into school.So, understandably, I was
a little bit more sensitiveto the school level
type subjects.And then I began noticing
these ads all over school.Posters and things on fences
for a no bullying campaign.And then I started thinking
about all the schoolviolence, and shootings,
and stuff thathad occurred in recent years.And it didn't really dawn on
me that this was a possibilityto occur at my
kids school until Isaw a these no bullying signs.I saw like-- OK, we understand
why these things arebeing posted here.To present violence-- harsh
environment for childrenbecause, ultimately,
kids that have troublesat school or at home and
don't really get help,end up becoming more troubled.So I knew that from
reading news articlesand stuff that kids that were
having these difficult livesat home and at school existed.But while the
schools were reallynow taking these
proactive measuresto teach kindness and
discourage negative behaviorslike bullying, many kids
who really need this supportactivity didn't really get
it until it was too late.And they weren't identified
until one of these violent actswas committed.So what I started to learn
in my hypothesis was,can we identify kids who
need this support usingdata points that are available,
really, at every school.And I thought, well, if
there's a data set thatwould support that,
it's going to bethe Monitoring
the Futures study.And so, our problem
was troubled kidswho don't get help
become more troubled.Well, the Monitoring the
Futures study investigates--essentially, changes in
the beliefs, attitudes,and behaviors of young
people in the United States.And this is actually pulled
directly off their website.In recent years, the US has
experienced tremendous changesin public opinion for
such diverse issuesas government and policies,
alcohol and other druguse, gender roles, and
protection of the environment.And this study
looks at all of itand basically,
makes an ideal dataset for the problem at hand.So now we have our problem.Now we have our data set.What's the next step?Well, we want to
articulate the need.So in our case, articulating
the need might--articulating the
need might be, howcan we show the school board,
or parents, or local governmentsthat we suspect
a problem exists?So we have to share
with them the data.And you need to
show them the databecause it's really easy to
say like I did-- yes, thisis a problem, we're aware of
it, but not at our school,not in our town, not
in our neighborhood.OK?So the big question, again,
how do we show them the data?How do you show
them this problem?So do I use--as a data scientist,
I'm thinkingabout what visualizations
make the most sense.You, I know, are too.So do you use multinomial model?Do you use the ordinal model?What type of visualization
would best communicate the need?Well, if we want to articulate
a multifaceted problemin a binary fashion--for example, can we
identify kids thatneed behavioral interventions?Yes or no?Then, multinomial is
probably not the best choicebecause multinomial is more
of a prescriptive utility.Whereas, ordinal is more
of a descriptive utility.And a lot of this has to do with
how the output is formatted,in that, ordinal is more
for like a higher levelperspective-- a top down view.And multinomial starts getting
into a more granular view.And let's just full ordinal
modeling a little bit.So ordinal modeling is a
type of regression analysisthat's used for predicting
an ordinal variable.An ordinal variable--
there's a variablewhose values is on an
arbitrary scale whereonly the relative ordering
between different valuesis significant.So just low, medium,
high, low, moderate, high,as we see on our slide here.So in our use case, here,
what level of the risk--low, medium, high-- does a
student's behavior present?I won't worry about
these other examples,here, just because I don't
want to get off into the weeds.But the point is, you
only use ordinal modelingfor presenting a high
level view of the problem.OK?So this is our ordinal modeling
output and visualization.And again, I want to point
out that ordinal modeling isdescriptive.So what we're doing is we're
looking at our problem here.And what we're doing
is we're showingthat there is an association
between several factorsthat we selected
from our domain.In this case, it's a
letter grade, how much timeis spent alone after
school by the student,and the level of parental
communication the child has,and the likelihood of a
behavioral intervention, whichin this case is an
unwanted office visit.So the frequency of poor
behavior, for example,impacting the student's
education or the classmateseducation, well, it's
clear that this exists.This is a very easy model read.And we can see
here that studentsthat tend to be in
the office more,tend to have lower
grades, and tend to haveless parental communication.OK?So what this doesn't tell
us is why the correlationexists, only that it does.So let's dig into this
a little bit more.And there we go.All right.So just to recap.So far, we've suspected
that a problem exists.OK?And so, we create
our hypothesis.Kids that have problems
that go unattended,tend to have greater
problems, right?So kids that don't get
help become more trouble.We've articulated that need
by using our ordinal modelingoutput.And what's next is that we
want to find a solution.And for our practical
use case examplehere, this is where our
multinomial modeling comes in.So let's go ahead and talk
about multinomial modeling.Multinomial modeling, it's
basically a topical regressionanalysis that's
used for predictingdependent variables with more
than two possible outcomes.So for example, scoring the need
for behavioral interventionsaccording to a
predictor variable.Multinomial is appropriate
for the second investigationbecause it avoids-- there is
sort of like a one size fitsall solution.And it provides a detailed
drilldown type perspectivefor our problems.So in other words,
multinomial canbe thought of as being
prescriptive, in that,you can use it to develop
a matching solutionfor multi-spectrum problem.Let's move on.So this is our
multinomial model output.And for whatever
reason, you guyscan't really see the
bars that I put up here.What I'll do is--let me see if I can share
the right version here.OK.So you guys should all
be able to see that now.So what we're looking at here
is, basically, students with Ascompared to students
with lower gradesaccording to the frequency
of office visits.And at first glance, this
looks pretty straightforward,just more messy than
our ordinal model.But when we start breaking
it down, what we start seeingis these group level variations.So of particular
interest are the C group,here, and the B group.So you see the students with
Cs are just slightly greaterrisk for needing behavioral
interventions than studentswith Bs.Very, very interesting.And students with Bs,
interestingly enough,are more likely to be in
the office less often--some more seldom--
than students with As.And this is counterintuitive
at first glance.Why would students with
Bs be in the office moreor need behavioral intervention
more than students with As?Well, let's think about it.So maybe this really
isn't that far out.But this sort of
modeling effect--oh, and again, I want to
point this out of here too.So this effect can be
seen again with the levelof parental communication
and the frequencyof behavioral intervention.So students with no
parental communicationor moderate parental
communicationtend to find themselves
at the office more--you know, behavioral
intervention,more than those who have lots
of parental communication.My takeaway from
this was of course,if you want your kids to stay
out of trouble, talk to them.But don't be their
best friend, whichis what the data presents.And then, this last
explanatory variable,which was the amount
of time kids spent homealone after school,
did not end upbeing statistically significant.So we aren't going
to discuss it here.Just know that it was considered
within the model itself.Back to it.So let me stop
this screen share.And we'll give that
a second to pop off.There we are.OK.So taking our multinomial
modeling here,what can we use this for?What can we make of
this information?Well, what we can
make of it is wecan use our multinomial model
to develop a selected preventionprogram to target
elevated risk groupsand provide a level
of intervention that'sappropriate for the
intervention for the individual.So in practice,
this means somethinglike allocating
resources like staff,or maybe a counseling program,
after school program, whateverthe case may be, where
it's most needed.So for the students
who most need it.And as we saw, the Bs would
need it more than the As.And obviously, the Cs and the Ds
would need it increasingly morebased on that.And I'm sure other
factors would beadded in a real practical
use case as well.But it illustrates our point.We've now presented a
solution, which in our casewas a system that IDs
kids with varying levelsof behavioral interventions.A scoring system
where we can presentthese additional resources
to the individuals thatneed it most or at a level
that's appropriate for that.And the cool part about
our regression approachis that it doesn't have to be
limited to school behavior,right?So it could be extended to
customer, member, or employeedata where various levels
of intervention or attentionare warranted.So our summary about ordinal
and multinomial modelingis that ordinal is
more descriptive.It's for high level views.And multinomial is prescriptive.OK?So let's go ahead and
start talking about someof the coding.What makes the difference--
what makes a multinomialversus ordinal modeling?So we're going to start, first,
with our ordinal regression.In both models, we're calling
it a logistical regression hereon our model.Our model data is titled MTF.And we're calling
descending on thisbecause proc logistic
models order valuesin an ascending order.All right?The proc function calls odd
ratio plots, specificallyso we can pair outputs
visually rather than a table.Classes are the
explanatory variables--parental communication, letter
grade, and time spent aloneafter school.And then, of course, our model
is our dependent variable,which is the need for behavioral
intervention predicted basedon our class variables here.We did call two options.STB, which generates
a maximum likelihoodestimates, and R squared, so we
can measure how much varianceis explained by our model.And then, we ran that, and
we got our ordinal outputthat we saw earlier.So how do we make
this multinomial?Well, this is the cool
thing about SAS, right?Since the SAS is so
versatile and so thoughtfuland understanding that we want
to drilldown these high levelviews, they gave us
a simple solution.Link glogit is the
exact same model.The difference is that you
include the optional argument,length equals glogit.And that will produce
the multinomial model.OK?So to wrap things
up here you guys,the big takeaways are
intent dictates use.What's the story that
you're trying to tell?Are you presenting a problem?Are you identifying an area
where there's an opportunityto expand a commercial business,
retail business, whateverthe case may be?OK, well, then use ordinal for
your high level viewpoint--your high level presentation of
that problem, more opportunity.Ordinal modeling is
more descriptive.So we're describing
your problem,describing your opportunity.OK?And then, multinomial
is what you'regoing to want to use
for prescriptive detail.So how do we resolve
this problem?How do we attack
this opportunity?How do we drill down on whatever
this is so we can come upwith the solution for?So multinomial is for a
granular drill down view.OK?And we've got a couple
references here for you.I'd like to say thank you
to everybody for attending.I know that this is
the virtual conference,and I hope that you
learn something today.I hope you have
some big takeaways.I'd also like to thank my
SAS Global Forum mentor, KirkLafler, who did provide me
with some wonderful feedbackfor presenting.Some great advice.In general, just a great person.And I'd also like to
thank Dr. AnnMaria DeMars for suggesting I
participate in the SAS eventsto begin with.So without them, I don't think
that I could get this done.And I appreciate all the
SAS staff for the hard workand moving this from an
in-person presentationto providing these online
capabilities very quicklyand in very short order.So thank you all very much.Here's my contact information.If you have any questions,
feel free to reach outto me directly.Thank you very much.Hope you have a great day."
132,"Hello.My name is John McCall,
and welcome to the session""Getting the Most Out of the
SAS Macro Language and SQL.""So we will take a look
at some SQL syntaxalong with some macro
programming syntax,combining the two to
generate some results.So let's get started.I want to first give you an
idea of what we will cover.The first will be a simple
listing with PROC SQL.I will show you how to generate
a listing for specific columnsand embedded title.By default, you get a
report with PROC SQL.Secondly, we will use a
macro program in SQL codeto generate a statistic and
assign it to a macro variable.And that macro
variable will displayin the title, a second title
along with the first title,display in the columns.The third program will
be to generate a listof distinct or unique values.And one of the things you can
do is put it in an a footnote.So let's say that I want a
report on a particular carmake, such as Chevrolet, and
I show-- or a particular cartype.And I can look at the
particular models, the types,and the MSRP.But in a footnote, we
have the vehicle typesassociated with the make.Now it might be instead of
displaying the distinct listin your footnote, perhaps you
want to do some validation.So let's say you
write a macro programand embedded in it is some code
that generate some statistics.And you give it
to me and you say,well, you're welcome to
use this macro program.All I have to do is supply a
make, like Audi or Chevrolet.But let's say I mistype it.Well, let's say that
you've accounted for itby doing some
validation just in caseI don't type it
the way you expect.So what you've done is supply
a customized message, ERROR:No Chevys, because that's
the way I typed it.But you also supply a list of
values that are acceptable.So we could use a distinct list
to help us do some validation.Those are just three of
the things we'll look at.There are other scenarios.In fact, there are 10
scenarios we will look at.But complementary to this
session we have some links,support.sas.com/
training/getstarted.You can go there and see
some of the free eLearning,like Programming 1 and
other courses that we offer,free Statistics 1, and so on.We have online documentation
which is outstanding.You can look up the statistical
procedures, functions,more on SQL, more on the macro
language, and many more topics.We also are listing two courses
that complement this session--the SAS SQL 1 Essentials course.And you can get to this
link and search by citywhere the course is offered.As well, the Macro
Language 1, youcan search by city to see
where the course is offered.SQL will cover the
full range of topics--doing selects, creating
columns, doing joins, summaries.And then the macro
language, creatingmacro programs that
generate code based on data,based on conditions.We show the power of both.So this is the end of
the slide presentation.We are about to go in SAS Studio
and look at the 10 scenariosthat I mentioned.So we have 1 through
10 scenarios.The first three will illustrate
what I did on the-- showed youon the slides.But before I do that, I
would like to show youtwo of the data sets we
will use in this session.And you have access
to these data setsin the SASHELP Library.If you go to the
SASHELP Library,you will see CARS, which
is one of the tables.And it has like a car make, like
Acura, Audi, the model, type,origin, drive train.You'll see things
like the MSRP, whichwe will investigate, the invoice
amount, engine size, and so on.That's one of the tables.A second table has to
do with fish species.So if I go to FISH,
you will see Species.You will also see Weight,
Length, and thingsassociated with these species.So those are the two tables
that we will use for the mostpart in this session.So let's get started.I want to go over
the first scenario.I'm going to my
files and folders.And I'm going to start
with Scenario 01,which is that listing report.By default, PROC SQL
generates a report.If you've never used
PROC SQL before,it's a great data management
tool complimentaryto the data step.It does a lot of features.Let's go through
some of these aspectsof this particular step.I start with PROC SQL.And then there are some
options I can put up hereto show like the row number.I can control how many
observations get written out,read in.And then there's
a title statement.I can have up to 10 titles.On the select, you can choose
everything with an asteriskor list the columns you want.So you may not want to
look at all the columns.You want to pick
the ones that youwant to look at and just
type them on the select.You don't put a comma
after the last column.You can also create
columns with PROC SQL.The from is indicating where
do data is coming from.From and you put
the data set name.A where is optional.If you want to
filter, go for it.An order by is optional.If you want to sort, go for it.You can list more than
one variable separatedwith a comma.The default is ascending.You can make it
descending if you'd like.And then there's a
quit instead of a run.Now I'm going to right click.There's a feature that's
available in SAS Studioand in Enterprise Guide.You have the ability to right
click and format your code.I'm just trying to
make it little bitmore readable so it's
not shifted to the left.I'm going to do a little bit
more customization here, movesome things around, and
have each part of the clauseso you can see it, my select,
from, where, and order by.I'm going to run this
and here's that listing.The title's Chevrolet.And you see the Model,
the Type, and MSRP.Just to show you something
I mentioned earlier,I could type an asterisk.That means select everything.And we run this and here we go.You see all the columns
that I showed youwhen I gave you a view of
the table in the beginning.I want to take you
to Scenario 02.Here's Scenario 02.And there are 10 of these
that we will walk through.I actually have a macro
program definition.And I haven't even shown you how
to create a macro variable yet,but I'm about to.One way to create a macro
variable is as a parameter.But let me go to
the outside first.I have an options,
mcompilenote, macro compilednote equals no auto call.So that if I have an autocall
macro program, then I'm sayingdon't give me notes about it.And that's no big deal, because
I'm already interested mostlyin seeing if this macro
compiles successfully or not.So I have %macro, give
it a legitimate name,and then the parentheses are
optional if I have a parameter.Now this is called a
positional parameter.There's also a key
word parameter,where you can set a default
value with an equal sign.And you could put a comma and
put another, like Atlanta,if you wanted to create
a second parameter.Most of the time when
you create a parameter,it will be used as
a macro variable.Notice when you type
it, no ampersand.But when you use it in your
code, you put an ampersand.I'm going to right click
this again, format it.I'm trying to make it a
little bit more readable,as opposed to having
it all left aligned.All right.That's optional.It will still work,
but I'm lookingto improve readability in
case somebody inherit my macroprogram.So here's the beginning, %macro.Carfinder is the name.I have one positional parameter.Then I have my proc sql
step to get started,an optional title
statement, my selectto choose which columns I want,
from to indicate the table,the where to filter if I
want, order by to do a sort.Instead of putting
a column name,I can also type what
position column I want.This is the first, second.So that one refers
to model as well.Then a quit.And then I do a %mend to
conclude this macro definition.All this does is define it.And it will be stored
in a work libraryin a catalog called macro.And you could save
it permanently.This will be temporary.But once I create it, I can
use it as much as I want.I can call it for Chevrolet.I can call it for BMW.So I'm going to run this
first with this option,call mcompilenote
equals noautocall.And this option in conjunction
with this macro definitionindicates if it was
completed, if itwas compiled without any
error, so no syntax errors.The next step is to call it.Now I'm asking it to
execute when this executes,it will create what's
called a local macrovariable called car.And I will assign a
value of Chevrolet.So Chevrolet would be filtered,
resolved here and here.And then it will
execute the proc sql.So here we go.And there is the report.There is the title.There are the three
columns and the log.Notes look OK, real
time, cpu time.Real time would include
a waiting period.CPU time, how much
time it took to run it.Let's try a different vehicle.Let's try BMW.And I will go ahead
and tell you it's notgoing to work because the value
you type is case sensitive.Here we go.We just get a title, no report.So it didn't find BMW
because it's case sensitive.But I can handle that.What I can do is use a
function called upcasefor the data set variable.And then I can use a function
called %upcase for the macrovariable.So this function will
apply to the data setvariable, the one in cars.This upcase will
apply to whateverI type for that parameter,
which is essentiallya macro variable.So now if I recompile this--and I make it a habit
to check my log notes--should do that consistently.And this time when I check
for a mixed case BMW,it gives me a report.Now in reality, the value
is in capital letters.But regardless of how I type
it now, even with a big M,I've got myself covered.Now if I wanted change that
value to uppercase that I type,I could.But it still works.I'd like to go to
the third scenario.The third scenario is
intended to do two selects.You might recall in
one of these slides,I showed you a statistic
in the second title.So that's where I'm going.From the top, I have the macro
definition again, carfinder.And it's just going to write
over the old macro programand create it and replace it.I have an option called noprint.Well, why would I want no print?If I don't use
noprint, I will geta listing report for whatever
I ask for on the select.And let's take a look
at what I'm asking for.The only thing I'm asking
for is a statistic.I want the mean of the MSRP.By the way, I can
format it if I want.The dollar format displays it
as currency with a dollar signand commas.Now I need some syntax, a
clause called the into clause,in order to create a macro
variable dynamically,so to speak.So I have to type it
like this-- into colon.Take this result
and put it into--you put a colon in front
of the macro variable nameyou want to build, the
one you want to create.Select what you want, into
colon, the name you want.From the table and that's
where MSRP is coming from,filtering for that make, that
will create the macro variable.That's all it does.The next select I
do want to report.And by the way, you can
have multiple selectsbetween one proc sql and quit.Or I could have started
a new proc sql statement.But I want it within the same.I reset print, so now I will
get a report if I have a select.I'm issuing two titles.The first, we
already talked about.That will display the make
of the car, like Chevrolet.The second title, the goal is
to display the resolved valueof that macro variable which
should be the mean of MSRPfor that car make.Then I'll select the
columns, indicatewhere it's coming
from, where, andorder by for that
particular automobile.So good practice.Run this.Make sure it compiles first.Notes are looking good.Then go back to my code.And we're ready to run
this for Chevrolet.And here we go.We have that statistic.I could have put
it in my footnote.The nice thing about
this, let's saythat Chevrolet makes
some new models next yearand I rerun this.It will recalculate the average
MSRP automatically for me.So it's all dynamic.It will create it.Soon as I run it,
get the result.Well, let's try a different car.Let's try one of my dream cars.I know I won't get a Mercedes
Benz, so I'll try for BMW.Notice this time, I am
typing it in capital letters.And we should get a different
average, and indeed we do.The average MSRP 43,285.Very good.Let's move on.scenario number 4.This time, I'm outside
of a macro program.I just want to illustrate
how to generatea list of distinct values.There could be a
number of reasonsyou're trying to use it.Perhaps you want to
put it in a footnotefor a particular car make.Or maybe you're trying to
use it to do some validation.I will show you both.Noprint because I
don't want to report.All I want to do is
create a macro variable--you see the into clause--called list.Well, let's talk about
what we're getting.I want to select a
distinct list of car types.And I want that list to
be separated with spaces.You can put whatever you
want in the quotes here.I put a blank.So I'm going to create a
list of distinct types,put that distinct list
separate it with spaces,went with a space,
put each valueinto a macro
variable called list.I could have called it
xy, whenever I want.I just decided to call it list.It comes after the
colon from sashelp.cars.Now to show you that list,
in the SAS log I can usewhat's called a %put statement.The %put is a macro trigger
that writes to the SAS log.Now to make this stand
out, think about what colorthe error is in the log.It's red.So I can put in ERROR: and it
will write the word ""ERROR"" outalong with this list.And here's the list--Hybrid, SUV.What if I really don't want that
word ""ERROR"" to be displayed?I just want the color.Absolutely, you can type a
hyphen instead of a colon.I don't need to rerun the SQL,
because that macro variableis indeed out there.And here we go.We have the list of distinct
types for that car make--Sports, Truck, Wagon, and so on.The next step I want
to use this list.In fact, I decided
to put that proc sqlstep that generates a list
and make it a macro program,build it in a macro
program so that Ican supply two parameters-- the
variable I want to list fromand which data set to
retrieve that variable from,where that variable resides.So if you look at this macro
called charlist or charlist,however you want
to pronounce it--I'll go with charlist.I have two parameters--
the variablethat I want a distinct list of.You see, select
distinct, that variable.And the data set name where
the value's coming from-- yousee the from dsn.Again when you put the
parameters, no ampersand.When you use it in the
code, you do put ampersands.The other thing that's different
is I have key word parameters.You see the equal signs?Well, you can set
default values.But I'm just making
those null values.The other thing I'm doing
inside of the macro,is I'm making that macro
variable list a global macrovariable.So it it'll be globally
around my entire SAS session.When you start your SAS session,
SAS creates a global tableand it sticks automatic
macro variables in there.There are some available in
there that we haven't used.Plus any macro
variables you createcan be placed in a
global symbol tableif they're not parameters
in a macro program.Because default, by
default, they will be localand they go away when the
macro finishes executing.But the first
thing I did in herewas to assure that
the list macrovariable that I'm
creating with SQLwill be put in a global table.So that whenever I
run this macro programand I create lists, when
that macro program finishes,that list will still be
around available to me.So I'm going to run this
and follow some principles.Always check the log.That's the best practice.And the next thing
I will do, I'mgoing to do three calls to
this charlist macro program.The goal is to generate a
distinct list of whatevervariable, legitimate variable I
tell it to generate a list of.The first one I'm going with is
the car make from sashelp.cars.All right.So I'm going to do
charlist, run this.All right.It tells you how much
time it used, CPU time.And then I want to see the list.You notice I used to %put
ERROR- so I can see it in red.This percent goes with
the list I just generated.Maybe I should have put
them all on the same line.But here's the distinct list--Acura, Audi, Chrysler,
Rover, Land Rover,Dodge, Mazda, Mercedes Benz--one of my dream cars--still a dream car.Porshe, definitely a dream car--will probably
remain a dream car.Anyway, let's try the next one.I'm generating a
replacement list,so if I overwrite it,
whatever the new list isis what is now in the new value.I have species from
the sashelp.fish table.And then I'm going
to do a %put again.I'm not using a color this time.So I'll run this, call
this macro program again.And if you look, Bream, Parkki,
Perch, Pike Roach, Smelt,Whitefish.That's the distinct list
separated with a space.All right, the
last one, I'm usinga numeric value called age, so
charlist var age sashelp.class.I'm going to use
a different word.If you think about it,
in your log warningsare typically in green.Again, if you put a
hyphen, it uses the color.If you put a colon, it writes
the word and shows the color.So it looks like
I missed my %put.Let me try this again.Maybe I missed it in the log.Here we go.11, 12, 13, 14, 15, 16, all in
green separated with a space.Remember, you could choose
whatever separator you want,just put it in quotation marks.So if I rerun this
to compile it first,and then I'll try the one for--let's try the species again.You see, we're separating them
with an asterisk-- whateveryou want, whenever you like.I really want to
go back to a spacebecause I'm actually going
to call this charlist macrofrom a macro--well, macro program from a
macro program called carfinder.So I want to make sure I do
separate these with a spacefor the list that I generate
from a data set for a givenvariable.I'm going to show
you how to referencethis macro variable,
this macro program,from another macro program.So I'm going to
scenario number 6.And I have a macro
program called carfinder.Now before I call it, I
will treat them independent.I have %macro carfinder,
a positional parameter.So I'm going to give
it a particular car.And I want it to generate the
mean statistic on the MSRPfor that car, put it in that
macro variable, mean_msrpusing an into: clause.From sashelp, we're going
to make a SQL to whatevermake I specify for the car.And remember, I'm
doing a noprintbecause I don't want
to create a report.I only want to create
a macro variablewith this first select.Then I would return or
reset the printability,so I can reset print back on.And the reason I'm doing
this is because I'mdoing two selects in
the same proc sql step.If I had a second
proc sql, I wouldn'thave to turn the print back on.Here's my first
title for the car,the second one for the MSRP
macro variable to resolve that.And I have a footnote.Check it out.I have a footnote for that
list of distinct values.So where's that
going to come from?Well, if you look further down,
notice I'm calling that first.I'm calling that first
on the variable typefrom sashelp.cars.So anyway, let's
go back to this.I have a from, a where,
the order by, and quit.And I have a %mend.So this is the
conclusion of carfinder.Let's compile it,
make sure it runs OK.We follow the best
practice, always checkyour log, compilation with no
errors, go back to the code.And I'm going to call charlist.And I'm going to do a %Put.And I need to make sure I spell
the word ""WARNING"" correct.So I'm going to
call this charlistto create a distinct
list for the car makeor the type of the car make.And I want to show you
what that list looks like.Now notice I have two periods.One will actually go away and
I'll get to that momentarily.So let me run this.And I have the distinct
list of the types in green.Now, I'm going to
call carfinder whichgenerates the macro variable
mean_msrp, filters for the carmake, turn the reporting
ability to back on,have these two titles, a
footnote with that list.And remember, I put that list
in the global symbol tableso it will still be around.So here we go.For Chevrolet, MSRP.If I scroll to the footnote,
you can see the vehicle types.And I'm going to change
this to a capital BMW.Now, if I was to give
this macro programor you were to give this
macro program to someone,it'd probably be a
good idea to validatewhat they type in for the make.And I'll get to
that in a moment.So here's BMW, the footnote
reflecting the types.Let's go to the next
scenario, number 7.So what if someone doesn't type
the car the way you expect?So now we are
accounting for that.We're actually doing
some validation.Remember that list of types?We're going to use that to help
validate what someone typesfor the value of the parameter.So check it out.After I call charlist
or charlist--and by the way, this time
I'm embedding that macrocall within a different macro.I have a macro program
called carcheck,one positional parameter.And I'm making use of an
option called the minoperator.That stands for
macro inoperator.So why am I doing that?Well you see the %if, which
is a macro if statement?I'm checking for the value of a
car to see if it's in the listthat I generated.So this in, for it to behave
at the macro language level,I have to tell the macro
processor treat it like a macroinoperator, not the
regular inoperator thatwould go with a where or an if.So in order to not
treat it like textor to treat it like an
operator at the macro level,I have to turn this option
on with the forward slash.So now, I can check
for the valuesof the macro variable,
not the data setvariable, the macro variable.I can check its value in a list.And by the way, I don't need
a put parentheses around thisbecause the macro variable
will be compared to whateverI generate separated by spaces.Each one of those, it will
compare until finds oneor not find one.So if it's in that list,
%then %do, do the following.For every %do block, just
like in a regular do,you need an end.But at the macro
level, we do %end.Notice there's a
percent symbol in frontof the if, the then,
the do, and the end?Those are macro
language statements,not data step if then else.These can only be
used-- well, thesecan be used mainly
in the macro program.That's what they're
generally used for.So if the car is in the list
from that macro variable,then I'm going to
generate a validationlist on the type for that car
make, then I'll call carfinder.So this macro carcheck is
actually potentially callingtwo other macro programs--one, charlist, and
the other, carfinder.Remember, we get the
listing with the nameof the car making the first
title and the mean_msrp.And then here's the %end.So what if you don't type it
the way I want, or maybe you'vewritten this and someone doesn't
type it the way you expect?That's why you're
providing validation.Here's the %else %do.You write a note to the log
in the color red with a colon,so you get the word
ERROR, No ampersand cars.Now remember before,
I said I put two dotsso I could keep one.And I should have
pointed that out.But here's the deal
with putting a dot.Think about the name
of this parameter.It's called car,
not cars plural.Well if I don't put
the period here,the macro processor will look
for a macro variable call carsplural.And there is no such macro
variable called cars.And you will get an apparent
symbolic reference not resolvedfor that.You get a warning about
that and potentially--well in this case,
just a warning.So in order to make it
look for the correct name,I put a dot after
the r, the letter r.And here's how it works.You can put a period next to
a macro variable reference.And the way it works, it's
used as the delimiter.So that if you have an issue
with some text followingthe name of the
macro variable, youwant to avoid the
compromise of the name.And the way you do that is put
a period and the macro processorwill see &car.And here's the
other part of this.When the macro
variable resolves,that period disappears.It goes away.It goes away, I'll get
the make of the car,and there will be an s
put after the last letter,like Chevrolets or
Audis, or Porsches,or whatever it is
that I specify.So the first period is
used as the delimiter.The second period
to the right is usedlike the end of a sentence.Next, %put, NOTE.I think it's probably
blue, Cars include.And get this, I'm helping
whoever uses this,I'm going to show them
what the list looks like.Remember, the first period
is used as the delimiter.And when that macro variable
resolves to that long list,that period disappears.So I put a second
period so it will looklike the end of a sentence.And then %end with
that do block.I'll try for Chevy
spelled right and I'lltry for Chevy misspelling.Let's try both.Here's the good one first.And here we go.Chevrolet.And here's the distinct
list at the bottom.And now I'll
purposely mistype it.So if you give this
to someone, you'revalidating what they supply.You're going to be out
all week on vacationand you want to make sure
you account for a mistake.They run this.Oh, ERROR: No Chevys.NOTE: Cars include--
oh, I mistyped it.So all they have to do is go
back, spell it the correct way,and you're good.Now you could put some
additional validation.Remember the case?I had the issue with
the case of the value.So I could check the %upcase
here, if I wanted to.Very good.Let's look at number 8.Number 8, 9, and 10 are related.I've created a number
of macro variables.Let me show you
what they look like.I'm going to do a %Put so
that I can show you the macrovariables in the log.I'm going to use _user_.There's also %Put _automatic_ to
see the ones that SAS creates.You can do _all_ to see both
those that SAS created and youcreated.Or you can do %Put _user_
just if see the ones that youcreated.So I'm going to put WARNING and
the word WARNING and a dash.I think maybe with
these keywords,I half to just put the keyword.Here we go.Here we go.Yeah.I have to do that
with the keyword.So where you see global,
these are user-defined macrovariables for the most part.Remember the list, the last
list we had was for these makes.And there are a number of
other macro variables in here.When I run proc sql,
it automaticallycreate some SQL macro variables.One is called SQLOBS to
indicate how many records thatwere generated.This 38 is equivalent to these
38 different makes of carsif you add them up.Let's see what else is in here.You've got some macro variables
created from SAS Studio.The new user name
is John McCall.That's my logon.And there are other macro
variables created in Studio,and server name localhost,
directory, working directory,and so on.Now some of these
are write protected.Some of these you cannot delete.So I'm about to show
you a macro programto delete the macro variables
that I am able to delete,such as the list
and some others.But some of these
are write protected.All right.So let's give it a shot.Here's the first one
called deleteAll.And it's not a bad idea to
clean up these macro variablesif you have a lot of them.If you have a few, no big deal.Maybe they'll be in the
global symbol table.When you shut down, that global
table is cleared out anyway.But let's say I'm
trying to be, you know,cognizant and just
clean things up.And maybe I want this
to be a utility macrothat I run whatever I
want, clean things up.I can put it in what's called
the Autocall macro facility.So I call it whenever I want.It automatically
compiles and executes.There's more to that.There's so much more
to macro programmingthat we don't really get
heavily into it here.But just think of
this as a utilitymacro, where I'm deleting all.I'm just emphasizing in
a windowing environment.Options nonotes, %localvars.I'm just creating a macro
variable called vars locally.Here's the proc sql step.And I'm generating a report.I'm selecting the name
from dictionary.macro.So what is that about?Let me give you a little
background on this.Whenever you use
PROC SQL, there areseveral what we call dictionary
tables that you can accessand they track all kinds of
metadata in your environment.Think about this.There's a dictionary table
called dictionary.tables.It will give you a list of
every table in every libraryin your SAS session--the Maps Library,
the SASHELP Library,any others that you
have active-- work.There's a dictionary file
called dictionary.columnsthat gives you the name,
the type, and other metadatainformation on every single
column, in every single table,in every single library.They're high, I would say
descriptive type filesthat you can get
to with PROC SQL.You can only access
them with PROC SQL.But we have some views of those
dictionary files in the SASHELPlibrary.Anyway, let me just
give an example.I'm going to do a select * from
the dictionary.tables wherelibname equals--and I'll choose maybe
let's go with SASHELP.So I want to get a list of
all the tables in the SASHELPLibrary.And the star means
all the columns.So I can see every SAS table,
or really, every SAS member typeincluding tables and catalogs.And then when they were
created, how many observations,and other metadata information,
descriptive informationfor each data set.Now, you're actually
looking at labels.So I'm going to
type OPTIONS NOLABELso you can see the
true column names.Once again, they're
several of these out here.So you want to be careful about
what you select because youcan get tons of information.Here I can see the true name,
lib name, m name, m type,creation date, modified
date, no of obs, and so on,number of variables.Very useful information.There's one called
dictionary.columns,but the one I want to look
at is dictionary.macros.Get rid of this where and
I just want to show youwhat you get from this.You get a list of
global macro variables,some that you created,
some that PROC SQL created,or SAS Studio created.Here's that list.Remember that list we generated?All these macro variables.And then you'll see the
automatic ones that SAS createsthat you cannot delete.These are write protected.Now some of these
values you can change,but they're write protected.There are some global ones.See, Client and some of
these are writ protected.So if I run this macro program--and notice in the log, I'm
getting some error message,I'm out of proper.Remember I say there are
some that I can't delete?I got, but I do get a note
saying 30 macro variableswere deleted.So the ones like list and
any others that I define,I can delete.So let's go to
scenario number 9,where this time I'm
eliminating the onesthat I know that I can delete.So check it out.Select the name of
the macro variable.And I should have
mentioned it earlier,into a macro variable that
I'm creating in this proc sqlstep called vars, separated with
a space from dictionary.macros.So that list of macro variables
that were user definedwill be placed in yet
another macro variable thatwill hold that list of the
names of the macro variablesseparated by a space.And I'm excluding
where the scopeis equal to global, which means
user define for the most part,and the name not
like any of these.Because I know I
can't delete these,so I'm trying to
avoid even trying.And then I'm using
a like operator.Whenever you see
an underscore, thatmeans any single character.A percent symbol means
in a number of charactershere and here.And I put a forward slash if
I want the percent to not beused the way it normally is.I want that slash to be
like an escape character.And then what's
most important, yousee that percent
symbol delete, symdel?That would delete one
or more macro variables.So whatever variables resolves
to, that list of macrovariables that I created,
and I'm avoiding these.I want those macro
verbals to be deleted.So I'm going to run this again,
well, with the customization.It compiled with no errors.But remember, I
already ran this once.And so there's
zero to be deleted,because I already deleted them.I'm going to go to
my final scenario.It's very similar to
what I just showed you,but this is for Studio
and Enterprise Guide.I happen to know which
macro variables arecreated automatically.And I want the ones that
are write protected,which means I can't delete
them, I'm trying to avoid those.I didn't have to, but
I'm doing it anyway.And then I'm going to have the
%symdel to create that listthat was supplied to this new
macro variable whose value isthe list of the macro
variables that I created.Now just so that you'll
see that it works,I'm going to use a %global.Remember that %global statement
creates macro variables?I just make something
up, x, y, z.We're not giving
them initial values.They will have no values
but they will exist.And I'm going to put city,
maybe state, and season.Just put a few macro variables
so I can test this out.So let's run this
and check our log--six macro variables deleted--1, 2, 3, 4, 5, 6.The six that I just
created with the %global.They're now gone.So this is a very
good, folks, thisis a very good utility
macro called deleteall.I can delete the macro
variables whenever I want,the ones that I've created.So there's the place you can
put this and just call itwhenever you want.And there's a lot
more to this wherewe explain the benefits
beyond what you'veseen, beyond the scope
of what we cover in here.But this is a great
start for you.So this is pretty much it.As to recap in this
session, we saw some SQL.There's much more to it.You could do joins.You could do set
operators comparing tablesto say what's in table A that's
not in B, what's in both,things like that, union those.You can compute
columns, do summaries,create macro variables
like we did here.And then in the macro course,
we create macro variables,macro programs.I show you how to write macro
programs that generate codeconditionally or repetitively,
and basically usethem to write programs
for you on your behalf.So thank you for your time.I was looking forward to
doing this session for you.And let me just
put this back up.My name again is John McCall.Here's my email address.Feel free to reach
out to me if youhave questions
about this sessionor questions about the course.I'll be more than
happy to help you out.My number's (404) 754- 0530.And listen, you
have a great day.I appreciate you taking the
time to observe this session.Thank you very much."
133,"This tutorial is
for users wishingto code in Python using
Jupyter Notebook on SAS Viya.Hi.I'm Charu, an instructor
with Sas institute.Here's a little
background on me.You can also find
me on blogs.sas.com.On the agenda today
we'll cover three topics.I'll first show you how
to start a CAS sessionand connect to
CAS, and then we'lluse CASTable objects
like a data frame.And finally, we'll explore data
and gather summary statisticsin Python.Programming in SAS or against
one of the SAS enginesis one of the most
powerful and flexible waysto deliver results to SAS users.SAS has continued to
evolve through time.From classic SAS
display manager,to GUI SAS enterprise guide,
to thin client SAS studio,SAS continues to evolve
to embrace open sourceintegration with Python
using Jupyter notebooks.So let's get started
with Section 1.Let's get started with
Jupyter notebooks.I'll launch Google
Chrome from my desktop.And then I'll hit the
Jupyter Notebook tab.I already have a notebook
that I built before.I'll open it up.And when Jupiter
Notebook launches,you'll see that
it looks literallylike a notebook, a
running notebook.It will be useful to just
examine our interfacefor a quick moment here.Up above you have something
called a markdown cell,and this is where you
could have titles.Down below, you have cells
where you can have code cells,and the code cells include both
input, and as you submit code,we will also see output.The run button would
be useful for us.We'll hit that
every time we wantto execute a piece of code.Some other things.Jupyter Notebook likes to put
comments with the hashtag.So you want to put the hash
sign before a piece of code,and when you execute it, it
will be considered as a comment.In Section 1, we'll be
starting a CAS sessionand connecting to CAS.And to do that, we
have a couple of itemsthat we need to
take care of first.We'd like to first
import the SWAT package.And the SWAT package is a
Python interface to CAS.Before you use a
SWAT package, youwill need a running CAS server.So here, I'm going to
just remove that commentand execute this code.I don't really expect to see
any messages or any commentswhen I execute.Next, I'd like to
create a CAS connection.In order to submit
this connection syntax,I need a few things here.I need four things to be exact.I need the host name.That is server.demo.sas.com.I need a port number.In this case, it's 8777.I need a user name, which
is student, and a password.That is Metadata0.Here, I've typed
in the code I need.And I'm using the SWAT
package and connecting.We've already connected
to a CAS server,and now we'll create the CAS
connection by hitting run.At this moment, again,
no messages, no notes.Not sure what's going on,
so what I'd like to dois just get some help.And I can do this
with CAS actions.CAS actions perform
a single task.And to ask CAS
directly for help,I'll use help
action, just like so.conn.help.conn happens to be the
connection we made to CAS,and within that, help
is an action set.So if I punch in
conn.help and execute,I'll see a plethora of
help available to meand the definition of
each of the help actions.I'll scroll further down here.And I'm going to run a simple
action called serverstatusbecause I still don't know
if the connection is working.So conn.serverstatus
is what I'll execute.I'll get a note about grid
note action status report.And then in there I'll
get more informationabout the node status, the
controller, the server status.At any one time, there
is one caslib thatis considered to be active.And active caslib live means
that any table referencesare assumed to be
the active caslib.How do we find
out what's active?We'll use an action
called caslibinfoto validate which
caslib is active.So once again, I'll
remove the comment,and I'll submit this code.I'll execute it.So I get some information
about the libraries in my CASsession.CAS user is one
of the libraries.There's formats.And when you see the
column that says active,a 1 indicates that this
is my active caslib.A 0 indicates inactive.So now we have an
active CAS session.We have imported a SWAT package
to be able to write Python.Next step for us
would be to load data.And we're going to load
it from the client side.This would be the
simplest way to load data.And loading data from the
client side, just as a note,will be slower than loading
data from the server.And clientside data
loading is basicallyintended for smaller data sets.We have a method
called read_csv.It works just like the
pandas read_csv function.In the following
scenario down below here,I'm pointing to a URL reference.And by the way, all of the
code, this Python notebook,the data that I've
used for this tutorial,is all going to be
available on GitHub.And even the notes here.So perhaps you won't
need me after that.So remove the hash sign
here and execute this code.And then we get a note saying
CAS made the uploaded fileavailable as, and then it gives
kind of an odd looking namefor the table.We should have the CARS
table in our SAS session.However, if you don't like this
difficult to read generatedtable name, you can certainly
specify one using the casout=parameter.This one right here.And when we execute
this, we mightbe happier with the identity
of the CARS table in caslib.We can get quite a
bit of informationabout our data using cas actions
and do some simple statistics.Nothing major, just
some basic stuff.And these actions are in an
action set called simple.It should already be loaded.And we can verify this
by running actionsetinfo.Scrolling down in
the output, we seethat the simple action set is
already available and loadedfor me.Something that might be easier
in the iPython environment--in the notebook environment--and getting the help content
to pop up in a separate pane,is to simply add a question
mark after the actionsetattributename.Just like this. conn.simple,
and then a question mark.And then we're going to
get a little table openup down below.And within that
table, you're goingto get the type of the
action set is simple,and within that, the kind of
actions that I can execute.If, however-- I'm going
to x to close this off.If you'd like to see the
help for a single action set,you'll specify the actionset
name as a parameter.So I'll take conn.help, and in
brackets, actionset='simple'.And these are the results
for the simple action set.What we've mentioned
earlier is these methodsand the CAS object
use pandas functionsunder the hood, which
means to our advantage,we can also use all of the
pandas parsing options.For example, I'm going
to use cars.summary.So this gives me some
descriptive statisticsfor my cars dataset.And for those of you familiar
with the SAS programming,you'll certainly
remember it lookslike a proc means or a proc
summary in the results.Certainly, CAS comes with a
few preloaded action sets,but it's likely
that you might wantto load action sets with other
capacities, other capabilities,like percentiles.You might want to do some
DATAstep, SQL, or even machinelearning work.The action used to load action
sets is called loadactionset.So I'll go ahead and load the
action set called percentile.And then once it's loaded,
I can use this actionset on the cars table.So I'll get percentile
information on the cars table.Now that we made a
connection to CASand the use of action sets--we've also imported
the SWAT package--will move on Section 2.Here we'll use CASTable
objects like a dataframe.The CASTable object is
the most important objectin the SWAT package other
than the CAS connection objectthat we used in Section 1.It keeps our CASTable
settings in one object.CASTable objects don't
contain actual data.They are simply
a clientside viewof the data in a CAS
table on a CAS server.Some definitions
might be useful here.What is a dataframe?What is a series?A data frame in pandas
is similar to a SAS dataset, a two dimensional data
source with labeled columns.And a series
represents one column.So you want to think
about a data setbeing similar to a dataframe.A single column in a data
set is like a series.Let's head over to
the demonstration.Let's move on to Section 2,
where we use CASTable objectslike a dataframe.It might be helpful
to look at a compareand contrast between
CASTables and dataframes.So what is the difference
between these two?The main difference
between the twois location, location, location.A CASTable is, of
course, in CAS,and the dataframe is local.In addition, there are two other
fine points about a dataframe.Firstly, you can load data
locally in the dataframeand alter it in Python before
you decide to load into CAS.Also, secondly, it's typical to
build models on the CAS serverand then download the
CASTables to local dataframesin order to do some data
wrangling in an open sourcelanguage.So what about CASTable objects.They don't contain actual data.They are simply
a clientside viewof the data on a
CASTable on a CAS server.So the very first
thing we'd like to dois to create a
data table in CAS,and we'll begin first by loading
iris.csv into a dataframe.Now, dataframes are
not built into Python,so we must first
import their definitionfrom the pandas module
with this import statement.Once that's done, I can load
data from iris.csv sittingin this location using the
read_csv() method in pandas.Subsequently, I can now
interrogate this DataFrameby using a method
called columns.The columns method
returns, for me,all the columns
in this DataFrame.I had been playing around
a little bit with this IRIStable.So I'll go ahead and first give
it a drop using the droptable()method.So once it's dropped
from my caslib CASUSER,I can go ahead and use the
loadtable() method to load thistable into a CAStable,.The definitive note indicate
that Cloud Analytic Servicesmade the file iris.csv
available as IRIS in CASUSER.Now does the table exist?Does it really exist?We can go ahead and supply
the tableinfo() method,which will return, for me, the
information that this table isactually in my CASUSER.Along with the
IRIS table, I alsosee the CARS table,
which, if you recall,we had loaded in section 1.Further down, I'd like to do
a little bit more analysisof this table, very
basic reporting, such as,give me the top
five observations.The head() method gives me,
by default, the top five.With the n=4 option, I
can supply a restrictionand request just
four rows of data.There we go.We have the first
four rows of data.A pythonic thing that
you're noticing hereis observation number
1 begins at the 0.And that is different from SAS,
where first observation is 1.Typically, we are
trying to use CASwhen we have large data sets.And so we might have
hundreds of variables.We can actually
subset the numberof variables you're bringing
in using the columns parameter,like so.The tail() method gives me
the bottom five rows of data.The columns parameter
indicates herethat I want to restrict
the columns thatappear in the result to, say,
the sepal length and petallength.There we go-- the
bottom five rowsof data for the two columns
SepalLength and PetalLength.Now that we've loaded
data into a DataFrame,and we also loaded
into a CAStable,we'd like to do some
compare and contrast.We'll first like to see the data
types using the dtypes method.On the DataFrame, the types
are float64 for four columnsand object for the
species column.I can also see the
same for my CAStable.You'll notice a
small difference.The data types supported
by DataFrames and thosethat are supported by
CAStables in the resultthat we just see here
have different names.The different names
really don't mattersince the Python float64 is
equivalent to the CAS double.The Python object type is
equivalent of varchar in CAS.In this final
section, we'll see howto explore data with histograms,
analyze, visualize, manipulate,and also do some data cleansing.Now for some more visual fun
stuff, we'll begin section 3.And in this section,
we'll explore dataand gather summary statistics.In our data exploration segment,
we'll build a histogram.Histograms are able
to help us viewthe distribution of numeric
variables within a data set.And typically, we
look to histogramsto identify outliers.We'll use the Python hist()
method to create a histogramof all numeric variables.If you recall, in
the IRIS table,we had four numeric variables.So we are going to see the
distribution for these fournumeric variables.An optional config
size in the codespecifies the size of the plot
which you'd like to create.So I'll resubmit this code.And voila-- some
nice-looking visualsfor histograms for the
four numeric variables.We'll do some more
data work here.We'll move on to
data analysis next.The describe() method is a
common method for a DataFrameto provide us some descriptive
characteristics of the data.We'll get some basic summary
statistics like mean, minimum,quartiles, et cetera.So I'll use the
describe() method.And then I'll submit task.And this will give me
descriptive statisticsfor the numeric
variables in my table.So these are the results
of the desc method,or the describe
method, on a DataFrame.Something we'd like
to observe hereis something that'll
come in handy later.Sepal width has a maximum
of 4.4 centimeters.So we'll just park
that for a moment.We can continue.And out of curiosity,
we'd like to see the typeof this describe method.And it'll return
for us that thisis a DataFrame within pandas.We'll do similar actions
for our CAStable.We'll ask to describe the
table and then submit casdesc--similar-looking result
with descriptive statisticsfor the four numeric columns.Again, requesting the
type will reveal that thisis a SAS DataFrame within swat.One last thing--I submit tbl.And this returns to me the name
of the table and its location,which is a caslib of CASUSER.Within the realm of
data exploration,percentiles are
another useful toolfor numeric data exploration.And by default, the
percentile actionthat we'll use subsequently
computes the 25th, the 50th,and the 75th percentiles
for the selected columns.So I'm using the describe()
method, and within there,the percentile action,
and supplying these valuesfor percentile.And lastly, one last
thing we'd like to dois execute the summary action
to generate summary statisticsfor all the numeric
columns in the IRIS table.And there is some detailed
summary statisticsfor the IRIS CAStable.Next we'll try out some
data visualization.Since the tables that come
back from the CAS serverare essentially subclasses
of pandas DataFrames,you can pretty much do anything
that works on DataFrames.So there is a method that
you're going to use here calledthe plot() method.And that's going to give me
some very nice plot, nice visualhere, for the numeric
variables in my table.Now we'll move on to
data manipulation.If you recall, earlier,
I had pointed outthat the maximum sepal width
was greater than 4 centimeters.And we wanted to identify
which ones they were.To identify this, we can use
a head() method on a subsetof the CAStable.In this code, we are filtering
out, from the IRIS CAStable,any sepal widths greater
than 4 centimeters.So these are the rows of
data where the sepal width isgreater than 4 centimeters.We'll do some cleansing next.We've identified the
three observations.Now we need to go ahead
and do some cleansing.So decisions need to be
made about the outliers.Of course, there are many
ways of dealing with outliers.You could, for example,
either replace a value,or you could completely
remove the observation.For this record, what we'll
do is temporarily replacethe value of the reading
score to a null value.And the replace CAS action is
called the replace() methodon the CAStable.So you can replace
any value of 4.4.We had one value of 4.4.And we're going to
replace any value of 4.4with a value of NaN.NaN, in Python, indicates
a Python missing value.We also have an inplace option.We'll set it to True, which
replaces the value directlyin the data set.So that's done.This is the CAScolumn that
we were trying to influence.Now we'd like to validate
that no rows existwith a sepal width
of greater than 4.So we'll request
the head() method.And we'll filter SepalWidth
for any rows greater than 4centimeters.And the only two
rows that are writtenare the other ones, which
had a sepal width of 4.1and another sepal width of 4.2.To terminate the CAS
session, you'll just submitconn.terminate().That brings us to the
end of this tutorial.If you have any questions,
feel free to reach out.Thanks for watching."
134,"Hello, thanks for tuning
in to see the presentationon common tasks done with CASL.My name's Jane Eslinger.I'm a senior technical
training consultantat SAS in the
education division.CASL is a language
specification usedby the SAS client
and other clientsto interact with the CAS server.CASL is comprised of actions,
functions, and statements.Much like the SAS
programming language,CASL is used to input
and output data,perform variable assignments,
and analyze data.The focus of my paper, and this
presentation, is common task.Tasks that you're used to
performing in other languages,I'll show you what they look
like when done with CASL.I've broken those tasks
into three broad categoriesof housekeeping--which I could have
just as easily calledtable management--summarizing, and ordering.What I'm going to do is
focus on one subsectionor example for each of
those broad categories.Within the
housekeeping category,I walk through a
looping example.For summarizing, I'll focus
on the RUStats action.Ordering is a slippery
topic in the CAS world.So I'll discuss a little bit
of what you can and cannot do,and show an example of groupby.Before I get into
any of the examples,I want to level set and make
sure everyone understandswhat a CASL action
statement looks like.This is the syntax you would
use from a SAS programmingenvironment, like SAS Studio.An action is one long statement.It starts with the action
set name, if one is needed.Actions are grouped into action
sets based on functionality.The same action name can be used
in more than one action set.If the name of the
action is not unique,then you must specify
the action set it is in.Follow that with a period,
then the name of the action.For example, a summary action is
found in three different actionsets.I want to use the one from the
simple analytics action set,so I put simple.summary.After the name of the
action are the parameters,and the values and options
associated with that parameter.Depending on the
action you are using,this statement can
get pretty long.Proc cas does run
group processing.That means it keeps a line of
communication with the serveropen until it hits a quick
statement or a step boundary.You can [AUDIO OUT] actions as
you need in one proc cas step.I said we would look at
an example of looping,a do over loop specifically.Base SAS programmers might find
that interesting because doover is not supported there.In my programs, I have found
that I need to loop over tablesa lot, without necessarily
knowing how many tables thereare or what they're called.How do you figure out how
many iterations you need?How do you reference the thing
you need to iterate over?I have found the described
statement is a helpful toolfor answering those questions.If I want to loop over all the
tables in a specific caslib,I would first need to know the
tables that are in that caslib.The table info action gives
me the names of the tables.I have funneled the names into
a result table here, called ti.Before I do any looping, I
run a describe statement.In the log, I can see the result
table contains a dictionary.The dictionary name happens to
match the action name, tableinfo.The log also lists the columns
in the table info dictionary.The one that is important
to me is called name.It holds the names of
the tables in the caslib.I now know that the result
table I need to use to loop overis ti.tableinfo.And the name of the column
that I care about is name.I now take that information and
add it to my do over statement.I still start with
the table info action,and create a results table.Again, the name of the
results table is ti.In the do over statement,
I want to iterateover the entire
results table, whichhas how many of our
tables' names it has in it.Remember, we've found out
from the described statementthat the name of the
dictionary is table info.The reference to
it is ti.tableinfo.My goal is to drop all of the
tables in my casuser caslib.Therefore, within the do loop,
I use the drop table action.In the name parameter
for the action,instead of hard coding
a name, I referencedthe column from the table
info table called name.I prefixed it with the
i iterator I specifiedin the do over statement.Running this code, I dropped all
the tables in my casuser caslibso I can start fresh.But as you can imagine, you
can put any other action insideof the do over loop, whatever
works for the task youare trying to accomplish.I put this example
in the paper, so I'mgoing to show it to you here.Though a word of
caution, this codemight drop more tables
than you intended to.I learned that one the hard way.What I want to show you is,
just like other languages,you can have nested
loops in CASL.From the
table.caslibinfo action,I create a result
table called c.The first loop iterates
over all the caslibs.You see here, do j
over c.caslibinfo.Then you'll see the
same table info actionfrom the previous
example, creatinga result table named ti.The second loop iterates
over all of the tablesin each specific caslib.The dangerous part is
the drop table actionwithin the nested loop.You might not actually
want to do that.Because depending on
your user positions,you might drop tables from
caslib that you reallydidn't want to drop.Hopefully, you see that
this same kind of task Iwould do in Base SAS.I'm looping over
a set of elements,like tables or variables.The takeaway is that you
can use a result dictionarytable like you would
use an array dimension.The other takeaway is that
the described statementis very handy.Analysis is taking a
large amount of dataand condensing it into
bite-sized pieces.Everyone has to perform
some kind of summarization.CASL has several
actions that summarize,and the paper covers
three of them.In this presentation, I want
to discuss the RUStats actionbecause it lets me
calculate percentiles.If you ask for percentiles
from a CAS enabled procedure,all of the data is moved
to the compute serverand processed there,
not on the CAS server.And we don't want
that to happen.We want all of the processing
to be done on the CAS server.Enter RUStats.You might be wondering why
you are seeing proc sqlcode when I'm talking about
using the RUStats action.Well, the reason is
that the RUStats actionrequires that you list
all of the variablesthat you want to summarize.And well, none of
us wants to typeout a whole list of variables.I'm using good old proc sql to
create a macro variable callednumvars, containing a
list of the names of allthe numeric variables
in my CAS table.The MYUSER libname
points to a caslib.Just because I'm using CAS
to do my summarization,doesn't that all of my
code has to be CASL.Here is the RUStats
statement thatwill calculate the
percentiles for me.The table parameter
takes the nameof the table I'm interested
in and the caslib it sits in.I just mentioned the
inputs parameter.Notice I am referencing
the macro variablebecause my table has
a lot of variables.If I only cared
about one or two,I would go ahead
and hard code them.Note that each of the
column variable names hasto be inside of
quotes, which I diddo with the quote function
in the proc sql step.Request packages sounds
weird for a SAS programmer.That's just what it is.I need it, because inside of it
is the percent house package.For its arguments, the
percentiles packagerequires a list of quantiles
or percentiles to compute.While I was at it, I went ahead
and used the scales packageto get the standard deviations.The action has other packages,
like centralized momentsand schemas.The results are sent to a
table named mydata_rustatsin the casuser caslib.Just so you believe
me, here's a screenshotof the mydata_rustats table.From here, you can do anything
you need with those values.That could include sending
them through another action,a procedure, making a
graph, or putting itin a report for your boss.Today, I do have to give you
a little bit of bad news.CAS is a distributed
environment,which means there's no
concept of ordering.Because it is a
distributed environment,the order of rows in a CAS
table is not predictable.That's not a bad thing.CAS returns rows as the
processing completesfrom each node, and that's
what helps it run so quickly.Before you get mad at
me, let me assure youthat doesn't mean ordering
or grouping can't be done.CAS will group rows on the
fly when necessary, likewhen using the groupby action.The groupby action
creates a tablewith unique values of a variable
or the unique combinationsof multiple variables.In this example, I want to get
a table with the unique valuesof color.In Base SAS, I
would use proc sortto do this, or maybe
even a proc freq,which would order the
values behind the scenes.Here groupby, we see
the inputs with color.And I'm creating
an output table,in this case, named example21.The fetch action is
just a way for meto see the results
in my results window.These are the results
that are in the example21that the fetch action
are showing me.When I ran this code,
results came backin alphabetical order.That won't always be the case.I talk about it in
the paper, but I alsowant to mention it here.A lot of actions accept
the sort by parameter.Sort by we'll give the printed
results in a sorted order.In whatever programming
language you're used to,there are some tasks that you
do in almost every program.CASL is no different.You need to save
tables, drop them,look at the contents of the
caslib, loop over information,summarize data, and order data.I've given you a quick look
at what some of those taskslook like in CASL.As I said, the paper
has a few more examples.So be sure to check it out.But thank you for tuning in."
135,"Hello, ladies and gentlemen.My name is John Guerard.I'm the Director of
Quantitative Researchat McKinley Capital Management
in Anchorage, Alaska.And we're going to
be talking todayabout robust regression
in portfolio constructionusing SAS.Now, I appreciate
the help of SASallowing me to present
this in a WebEx format.I'm terribly sorry
that I could notbe with you in Washington D.C.
as we were all looking forwardto that.I want to tell you at
the beginning todaythat our research that
I'll be presenting todayhas been done in conjunction
over the last 25 yearswith Harry Markowitz, the Nobel
Prize winner, and Ganlin Xu.Harry and Ganlin serve on
our Scientific Advisory Boardat McKinley and we work
together at Daiwa Securities25 years ago.In terms of my background,
I was an undergraduateat Duke University in economics.I have a PhD from the
University of Texas in finance.Now, I'm excited to be with you
today and let's talk about whatwe're going to be doing.First of all, we are going to
be talking about three modelsin particular.We're to be talking about
a forecasted earningsacceleration model,
a CTEF model.We're going to be talking about
a eight factor model, whichwe're going to designate REG8.REG8 model has earnings,
book value, cash flow, sales,and how these
variables move relativeto their five year means.We're also going to
be talking-- so we'regoing to be calling
that equation 9, thathas to do with the paper that
has been submitted for the SASform.OK, the next slide, please.We're going to be talking
about regression 10, whichis a 10 factor model.And to this model of earnings,
book value, cash flow,and sales, we add our forecasted
earnings acceleration model,CTEF.And we add a price
momentum variable,a 12 month minus 2 month
price momentum model as à laFama-French.And so we have a
10-factor model.We have tested this model
over the last 10 years.We believe that the
model is statisticallysignificant in
almost all universesand we're going to be showing
you those universes today.The eight model, the 8-factor
model was developed at 1993and continues to be
statistically significant.Well, we want to
share with you todaythe reason we're so excited
about this is becauseof the fact that over
the last 25 years,we have built and
implemented modelsand we've continued
to test models for twoprimary degrees of testing.Number 1, we want to make sure
that our model ranks securitiessuch that we can use
this for stock selection.This is our information
coefficients, the ICs.This has to do with
statistical significance.You will find this discussed
in our book in Chapter 3and in chapter 4 of
the SAS press book.The 10-factor model
has updated research,it's statistically
significant and wewant to make sure that we will
show you different trade-offcurves.See, the ICs are our
first level of test.Our second level of test has
to do with a full blown HarryMarkowitz efficient frontier
with transactions costsfrom ITG and
turnover constraints.We're going to turn
over 100% a year.We're going to assume a
traditional mean varianceapproach of 4% of our balance
ITG transactions costs,which average about 60 basis
points each way going forward.Our next slide, please.And this is our summary
and conclusions.We're going to tell you what
we're going to tell you now.And we will remind
you at the end.First of all, we're going to
show you a forecasted earningsacceleration model that
is highly, statisticallysignificant.This model was published
in 1997 and continuesto work post publication,
obviously post sample.We're going to show you that
momentum enhances portfolioreturns, primarily non-US
markets over the periodfrom 2003 to 2015.We're also going to show you
updated evidence through 2018.So momentum works in
the US but works betterin the international markets.All right, our
next slide, please.Have markets changed since the
publication of Bloch, Guerard,Markowitz, and Xu in 1993
and Guerard and AndrewMark in 2003?The answer is no.Over the 25 years
since Bloch, Markowitz,Guerard was published, our
models continue to work.CTEF, 8-factor models
are highly, statisticallysignificant.The 10-factor model
continues to dominate as wellin international markets.And we want to stress
two things in particular.Number 1, our models
will never be perfect,but they can be statistically
significant based on ICs.And we're going to
introduce a level 3test, which is that data
mining corrections test.We will show you, we will talk
about those results today.And we passed the
data mining test,which means that our
models are statisticallydifferent from the
average of the modelsthat we could have tested
which includes 36 models as wetalk about in Guerard, Gillam,
Markowitz, Xu and Wang.Now if we go to our
next slide, please.In terms of references.I want to stress in particular
the first reference, Portfolioand Investment
Analysis with SAS:Financial Modeling
for Optimization.This is our SAS press book
that came out last year.We also want to show
you that our model hasbeen done with many, many other
models over the last 25 years.Rob Arnott has done a
particularly great jobof modeling.He of course is the
research affiliate's person.You can see our papers
with Harry Markowitz thatwere published in the
IBM Journal of Researchand Development.And next slide, please.And also in the International
Journal of Forecasting.The International Journal of
Forecasting piece of 2015 isimportant because we talk
about earnings forecasting,how the I/B/E/S database was
developed primarily in 1976domestically, 1987
internationally.And most people started to
look at I/B/E/S forecastingin international
markets around 1992.We did not have enough data
at that time for our datamining corrections test
in 1993 when we publishedour original paper, but
we published a followup paper in 2015 in
the InternationalJournal of Forecasting.It is an Elsevier journal,
highly peer refereed.And this is interesting, we
continue to work, Harry, Ganlinand me, and you
can see papers thatare coming out in Annals
of Operations Researchwith a special issue
on data mining.We are always concerned
with data mining.People talk about data mining.Harry and Ganlin published
their original piece--next slide, please.Harry and Ganlin published
their original data miningcorrections piece in the
Journal of Portfolio Managementin 1994.We want to stress the fact that
our models are statisticallysignificant.There is a book that you
should all be reading.It's Rubust Statistics--
Theory and Methods.It's by Maronna
and Martin, 2019.Its actually by Wiley.And that's a book that
people should be reading.It's what the SAS PROC
ROBUSTREG is based on.When you're looking
at ROBUSTREG,that proc is based on the work
by Doug Martin and Maronna.Next slide, please.OK, so let's now talk
about efficient markets.Do we believe in
efficient markets?Markowitz, Guerard and
Xu believe that marketsare relatively efficient.What that means is the average
person will not make money,but a person who
has well trainedstatistical and financial
analysis and lots of computerscan beat the market by a
statistically significantlevel.And what we do in this table
one is we show you paradigmsand portfolio anomalies in
the period of Compustat, CRSP,and I/B/E/S. That's the
period from approximately 1976to the current time.You see the work of
Jaffe on insider trading.Latané and Jones.Latané of course is a great
portfolio theory teacherat Chapel Hill.Elton, and Gruber, and
Gultekin from 1981.Their paper for
management scienceis one of the most
important pieces.That's where we came up with
the underlying concept of CTEF.Next slide, please.Thank you.We continued to show you the
history of financial modelingthe way we do it.You see Bloch and Guerard, 1993.Fama, French came out
about the same timeand several of their papers.We have different variables and
different portfolio approach.We believe in traditional
mean-variance analysis.Next slide, please.We are not alone.I want to refer to the great
work of Andrew Lo of MITwith his price momentum work.The work of Bob
Haugen, H-A-U-G-E-N,who did a lot of work
on fundamental analysis.Bob is a brilliant person.He passed away about
three or four years ago.It was a brilliant person.Bernell Stone is a co-author
and a fellow Duke alum.And we've done lots of work
together over the last 25years.Next slide, please.The work continues.We continue to work on our
data mining corrections test.We continue to find that
financial anomalies areimportant.Buybacks right now
are part of a variablethat we use in our
corporate exports variableand that's in chapter
3 of our book.And you can see that that
work continues to be.Right now you're going to
see that the stimulus bill isbasically coming down
on companies that havebeen buying back for stock.The reason they're trying
to do that quite franklyis the following.Buybacks have helped
stock returns.And our corporate
exports variable,which is driven by dividends
and corporate exportsin terms of buybacks, has
been highly, statisticallysignificant the last 15 years.Next slide, please.Now, in portfolio analysis.If we were live, I would be
holding up the Markowitz bookfrom 1959, Portfolio Selection.MVM59 means we're
going to createa Markowitz mean-variance
efficient frontier.It's a 4% weight on a global
universe from Decemberof 2002 through May of 2015.We're going to use
the APT optimizer.We're going to pick
a lambda of 200.Lambda is a measure
of risk aversion.It's a fairly
aggressive portfolio.A fairly aggressive portfolio.Now, if you look
at this portfolio,we're going to compare Markowitz
1959 with Markowitz trackingerror at risk, which
is a First Amendment,kind of a friendly amendment.A first cousin to
the original modelwhere instead of simply
maximizing returns, minimizingrisk, we're maximizing
returns and minimizingthe underperformance of a
benchmark for the next year.So what happens
with mean-variance?Mean-variance will almost always
give you a higher active returnand will almost always give
you a higher active riskand tracking error risk.However, models are highly,
statistically significantin terms of asset
selection, the t-statistics,5.59 for the WLRR, that's our
weighted like route regression.That's in chapter 4.We're doing outlier analysis
and linearity modeling.So it's primarily the beaten
Tukey key robust procedure.The M method of 1974.And if you look
at this analysis,you can see that number
1, information ratios allexceed 1, that's
on the second line.We've taken into account
transactions costs of 500 basispoints each year.They are substantial
tracking errorsthat you see of 11 and 1/2 on
the traditional mean-varianceanalysis and the tracking
error goes down to 9%with tracking error at risk.So that's why you would
use a tracking error risk,you can reduce your
tracking error.If you used a lambda of
100 as opposed to 200,you're tracking error
would fall to about 7%.Obviously, the
higher your lambda,the higher your
information ratio,the higher your Sharpe ratio,
the higher your geometric mean.So notice this, the benchmark
of Markowitz created in 1959is an extremely hard
benchmark to beat.Next slide, please.This is in the global universe.Now, for those of you that
get excited about data mining,and we all do.We all get excited
about data mining.This is our lambda of 200.We're producing lots
of different models.Lots of different models.This is from our Annals of
Operations research paperthat's forthcoming on the
special issue of data mining.And we're using the
BHY false discoverytest, ad this is what Campbell
Harvey has popularized,the false discovery method.And if you look
at all the modelsthat have used our
weighted light and rootregression that we used in
1993 for our first paper,continues to be the
second best model.It's t-statistic is 3.88.It's highly,
statistically significant.Notice LAR a t value
in that chart of 3.84.And those are the monthly
returns of your models.So our weighted latent root
regression does extremely well.Notice for the
individual variables,our CTEF, our forecasted
earnings acceleration variable,has a t value of 3.14.Its highly statistically
significant.And models go down from there.Our overall model is highly,
statistically significantwith those t values.We can reject the null
hypothesis of data mining.That's in table 3.Next slide, please.Now this is where we get
excited, really excited.This is table 4
where we're talkingabout robust regression.This is from our SAS
press book, chapter 4.Look at the different
methods we can use.We can use the M methods.We can use the MM methods.We can do the Fair, the
Huber, the Tukey methods.And notice one thing
that's important.All of the t values
for active returnsare highly, statistically
significant.This is in the global
universe for active returns.Specific returns
are statisticallysignificant for all of
the 10 factor models.Notice momentum has
high t statistics.This is in the global model.So if you were to look
at this, you would say,wow robust regression
is important.Which method of
robust regressionis not nearly as important
as using robust regression?What an exciting conclusion.Next slide, please.OK.Now as we go through, we're
going through a lot moreof these slides.We're going to show
you some returnsnow having to do with
another universe,and this is our EM universe.And so as you go
through these, we'regoing to show you different
universes where they'reapplicable, the same results.You always find highly,
statistically significantactive returns.What's your active return?Your portfolio return
minus your benchmark.If you subtract out all
your factor returns,then you get specific returns.You want your specific
returns to have t statisticsgreater than two for
statistical significance.Next chart, please.Oh, I'm sorry.This is the EM universe right
here because noticed the highervalues.The first chart a is
non-US, b is global,c is robust regression
of the EM markets.Notice you have much higher
active returns, much higherspecific returns, and
momentum is positive but notnearly as high a set
of momentum returnsover that period from 2002 to
2015 as the other universes.Momentum returns post 2015
have been extremely high.Next chart, please.Thank you.Now, so let me summarize
what we've done.Let me summarize what we've
done and that is the following.Here are our conclusions.Our markets efficient?No.Can we build models that are
statistically significant?Yes.What factors do we use?We are using our public
models at McKinley.See, we're a asset manager
in Anchorage, Alaskadoing primarily global
and non-US stocks.I can't show you our real
models, our proprietary models,but I can show you our
public models basedon earnings, book value,
cash flow, sales, CTEF,and price momentum.These factors have exactly
the same exposures,the same attributes as
our proprietary model.So I can show you
our public model.And last but not least, I
want to show you our pictureof Guerard, Markowitz, and Xu.And so from our point of view,
I say thank you for your time.We had 20 minutes.Conclusions.Are markets efficient?No.How can we beat them?We have to look at forecasted
earnings acceleration.We need to look at
robust regressionand we need to use Markowitz
mean-variance analysis.You can use traditional
mean-variance analysisor mean-variance analysis with
tracking error risk, the TARmethod.The TAR method gives you high
returns, higher active returns,high specific returns, but it
lowers your tracking error.From the point of
view of modeling,we've shown you
different universesand our models are
statistically significant.Our model is perfect?No, but they can be highly,
statistically significant.Our models are.We passed the IC test, we
produce efficient frontiersthat you can see in
tables 4 and in chapters 4and 5 of our book.In chapter 6, we do our
data mining test, whichwe have shared with you today.From table 3, our models are
statistically significant.We're excited to be
making this WebEx.Thank you so much.I appreciate your time.Goodbye."
136,"DEV KAKDE: Hello, everyone.Thanks for joining.Today, I'm going to talk about
two analytical techniques whichare useful for visualizing
high dimensional sensor data.This talk is IoT focused,
but the analytical techniquesthat I'll be discussing
are not domain specific.My name is Dev Kakde, and
I'm a member of the R&D teamin the IoT department at SAS.I've been working at SAS
for close to 12 years now.Prior to SAS, I worked
in the transportationand manufacturing industry
within their qualityand reliability functions.At SAS, my focus is on
solving engineering problemsusing analytics.If you think of a
critical equipment,such as a chemical plant
or a tractor trailer,it has traditionally
been monitoredin real time using
multiple sensors, oftenfor control engineering.If this sensor data is analyzed
using right techniques,it can also provide some
useful insights regardingequipment operations and result.
For example, such analysis candetect deviations from the
expected stable operations,and can provide early warning
about impending processor equipment faults.Any intervention
based on such alertshas potential to improve
the equipment up time.The sensor data collected
from such equipmenthave some common
characteristics.Data are multivariate since the
data represents measurementsfrom multiple sensors.Second, the data are nonlinear
and highly correlated,as is true for most
real life systems.Third, the data are
often multi-modal,representing multiple
operating modes.Sometimes these operating
modes are known--for example, in the washing
machine, wash, rinse, and spinare three known operating modes.But sometimes, these
operating modes, or regions,may not be obvious, especially
when process behavior changesas a result of a disturbance
or system overload.In this presentation,
I will demonstratetwo analytical techniques
for visualizing sensor dataand getting useful insights.Now consider a machine with
several hundred sensors,and I want to
visualize such data.I can look at one
variable at a timeand monitor these
variables independently,or I can look at two
or three variablesat a time using multiple
2D or 3D scatter plots.But what if I am interested
in visualizing morethan three variables at a time?It is possible to
understand the structureof a high dimensional
data using a 2D plot.The idea is to take the high
dimensional data as an inputand plot it in a 2D space
such that certain propertiesof the data in high dimensional
space are preserved.For example, using
such plot neighboringpoints in the high dimensional
space and the best neighborsin the two dimensional plot.Additionally, any groupings
in the high dimensional datais preserved in the 2D map.Such 2D plot which allows you to
visualize all sensory variablestogether is often very useful.It provides many
useful insights whichcan help better understand
equipment operations.The artifacts in the 2D
plot, such as clusters,local structures,
and manifolds haveinteresting interpretations with
respect to equipment operationsand diagnostics.These artifacts
provide informationsuch as the number
of operating modes,onset and propagation
of degradation,and local disturbances
in the machine.Today, I'm going to
showcase two techniques--the t-distributed
stochastic neighborhoodembedding, or t-SNE; and kernel
principal component analysis,or KPCA, which can
be used to visualizehigh dimensional sensor data.t-SNE and KPCA are
both available in SASvisual data mining and
machine learning, version 8.5.t-SNE is a nonlinear
dimensions reduction technique.It maintains a local
structure of datain 2D plot which is
similar to the one presentin the high dimensional
data, meaningthe neighborhood of a
point in the 2D plotis similar to the
neighborhood of the pointin the high dimensional space.It can provide a lower
dimensional 2D or a 3D plot.KPCA is a nonlinear version of
principal component analysis.You can use the coordinates of
the data along the first twoprincipal components
to opt in a 2D plot.We have observed
that KPCA providesa more compact representation of
the data as compared to t-SNE.Now let's look at some examples.In this first example,
you will develop a 2D plotof NASA Turbofan Data.These are simulator flight
data for 218 aircraft engines.For each engine
flight, data wereavailable from the time the
engine went into the servicetill the end of its life.Each engine completes a
different number of flightsbefore it reaches
the end of its life.For each flight, there were 21
different sensor measurementsand three variables
describing the operatingconditions of the engine.For this example, we
selected a set of 99 enginesfrom the total of 218 engines.The data are multivariate and
exhibit nonlinear structure.Here is the t-SNE
plot of engine data.Note that here, we presented
24 dimensional sensordata using two dimensions.The plot on the left was
created using the first 60cycles of each engine where
we assume normal engineoperations.This plot shows six
distinct clusters,which correspond to six
different operating conditions.The t-SNE plot on
the right was createdusing all cycles from the start
to the end of the service.As the engine
accumulates flights,the engine starts
to slowly degrade.The t-SNE plot capture
the degradation pathalong the tails emanating
from the individual clusters.Here is the KPCA
plot for engine data.Here, we have represented
the 24 dimensional sensordata using the first two
principal components.Similar to the t-SNE
plot, the plot on the leftwas created using
first 60 cyclesof each engine where we assume
normal engine operations.The plot shows six
different clusters,which correspond to six
different operating conditions.The KPCA plot on the
right was createdusing all cycles from the start
to the end of the service.Note that, similar
to the t-SNE plot,the KPCA plot capture
the degradationpath along the
tails which emanatefrom the individual clusters.This example uses
the vibration datathat we collected during an
experiment at the SAS IoT lab.The picture on the right
shows the experimental setup.The motor rotates the
shaft connected to it.The sensor Ai1
measures the vibration.Vibration is measured
as displacementalong the axis which is
perpendicular to the shaftaxis.First, we collected
vibration measurementsunder a stable
experimental setup.Next, we introduced a shaft
imbalance in this setup.We slowly increase the
magnitude of imbalancewhile the shaft was rotating.We recorded vibration
measurementsunder imbalance
condition as well.The vibration measurements for
normal and imbalance conditionwere recorded at three
different motor speeds--of 25, 35, and 50 RPM.Next, we processed the
raw vibration signalto obtain power in 13
different frequencybands obtained using
short-time Fourier transformon sliding windows.Here are the t-SNE
plots of vibration data.In these plots, we represented
13 dimensional datarepresenting power in 13
different frequency banksin a two dimensional space.The plot on the left
shows separate clustersfor different speeds.In both plots, the
imbalance conditionis captured by the
tails emanatingfrom the normal
condition clusters.In this slide, I'll show you
an animated plot of t-SNE.In this plot, the
observations areplotted in the sequence
they're collected.The blue markers indicate
normal operations,and red markers indicate
imbalance shaft condition.The plot shows
movement of the datapoints away from the normal
data clusters as the extentof imbalance increases.These are the KPCA plots
for vibration data.Again, we represent
13 dimensional datausing the first two
principal components.Similar to the
t-SNE plot, the ploton the left shows separate plus
transfer difference speeds.The tails emanating from the
normal condition clusterscapture imbalance.Note that the
clusters correspondingto the stable
operations obtainedusing KPCA are much more
compact than the clustersafter using t-SNE.Here is the animated
plot of KPCAfor vibration data,
where observationsare plotted in the
sequence they're collected.In this plot, the blue markers
indicate normal operations,and red markers indicate
imbalance shaft condition.The plot shows
movement of the datapoints away from the normal
data cluster as the extentof imbalance increases.To summarize, t-SNE and KPCA
are two nonlinear techniquesto visualize
multivariate sensor data.The plots obtained
using t-SNE and KPCAcan provide useful insights
regarding the motion condition.These plots can indicate a
number of operating modes,initiation and
propagation of faults,and local structures
in the data.For more information regarding
the techniques, data,and methodology, you can
refer to these publications.Thanks for watching."
137,"Hello, welcome to the online
edition of the SAS Global Forum2020.My name is Gordon
Brown, I'm from SAS.Today, I'm going to talk to
you about new procedures in SASVisual Statistics 8.5.If you're not familiar
with Visual Statistics,it's a product that
sits on SAS Viya.8.5 was put out in
the fall of last year.Specifically, today,
I'm going to talkabout four new procedures
that were releasedin Visual Statistics 8.5.The first one is proc GAMSELECT.This procedure allows
you to do model selectionfor generalized additive models
The second is proc SANDWICH.Proc SANDWICH uses a robust
variance estimation methodto perform analysis.The third one is proc SIMSYSTEM.SIMSYSTEM generates data
based on the momentsof a distribution.And the last one
is proc NMF, whichstands for non-negative
matrix factorization.Here are some of
the procedures thatare available in Visual
Statistics that allowyou to do model selection.The first one
listed is GENSELECT,which allows you to
do model selectionfor generalized linear models.LOGSELECT is for logit
models, including multinomial.PHSELECT is for Cox's
proportional hazards model,which is for survival data.QTRSELECT is for
quantile regression.REGSELECT is for
ordinary least squares.And the new one is
GAMSELECT, whichallows you to do model selection
for generalized additivemodels.So let's talk a little
bit about GAMSELECT.As I stated, it is for
doing model selection.But it specifically
works on splines, insteadof working on the
individual parameters.So what happens in
GAMSELECT is you do nothave to specify the functional
form of the potential predictorvariables.GAMSELECT will figure
that out for you.So the spline terms
are used to determinethat non-linear dependency.So is this going to be
quadratic, cubic, et cetera.You don't necessarily know, you
don't have to worry about it.GAMSELECT will figure
that out for you.In addition, it
will provide graphs,which will allow you to see
what these distributionsand functions look like.The default method in
GAMSELECT is boosting,but there's also
the shrinkage methodavailable for model selection,
if you wish to use it.Here's an example, modeling
mortgage applicationsusing the GAMSELECT procedure.And here, you can see
syntax that shouldlook fairly familiar to you.We have a class
statement, which allowsus to specify
classification variables,and we have the model statement.In the model statement
after the slash,we specify the
distribution equals binary.That tells us we're
going to be lookingat a binary distribution.The event we are
most interested inis when the variable Bad
takes on the value one.That is going to be our
outcome of interest.To the right of
the equal sign, wehave our independent effects.The ones that have the PARAM
statement in front of itmean that they are going
to be modeled as is.There is not going to be
anything done to those.The ones with the spine
statement in front of it,such as spline CLAGE, are going
to use a spline term in orderto model the
outcome of interest.In addition, we can
include interactions,such as loan and value,
and mortgage and value.At the bottom of
the procedure, wehave the SELECTION and
the PARTITION statement.The SELECTION statement allows
you to choose the method.In this case, we chose boosting.And the PARTITION
statement allowsyou to create both the
testing and training data setfor performing validation.Here are some of the output.You get fit statistics,
you can alsoget betas if you want them.But in this case, I'm
showing you the graphs.On the right side, in
the graphs over there,you'll see four separate graphs.The two at the top,
one is for CLAGE,and it shows its functional
form with respectto our outcome of interest.Same with the DEBTINC.The bottom two are
for the interaction.So that's the value and
MORTDUE interaction,and the value of
loan interaction.And here, you can see
how these interactionsare affecting the outcome.Where you see the bright red--
or the brightest red color--is where the event is
most likely to happen,and where you see the
darker colors is whereit's least likely to happen.The second procedure
we're going to discussis the proc SANDWICH procedure.And proc SANDWICH uses a robust
variance estimation methodto perform analysis
and inference.In this case, we're
going to be usingthe classic robust
sandwich estimator.Now the beauty of the
sandwich estimatoris that you, the user, do not
have to specify the correlationstructure within clusters.The sandwich method will
look at the clustersand will change the
variance accordinglyto take into account
misspecificationof the correlation structure.As a result, this
method is very usefulwhen we have a lot of
clusters, as we no longer haveto worry about what that
variance structure is,the sandwich matrix will
figure it out for us.Here's an example of
using microarray datawith the SANDWICH procedure.So again, here is some
syntax that shouldlook pretty familiar to you.First, we have proc sandwich.And we specify that we want
to use the microarray datafrom the mycas library.We have the class statement, so
we're specifying some variableas categorical variables,
and then the model statement.Our outcome of
interest is density,and then we have gene treatment
dye and a few interactions.Our cluster statement
tells us that microarraysare going to be our cluster.So what's going to
happen in this caseis we're going to fit a
linear model for densitybased on the independent
effects specified.We're going to use the sparse
option for sparse matrixtechniques.Microarray is going
to be our cluster,and so we're going
to get the sandwichestimator for the
variance based on cluster,which is microarray.So here's some of
the output that wecan see from proc SANDWICH.We asked for the type
three model ANOVA table.And that's what we get in the
lower right hand corner, whichshows us which effects
are significant,or more importantly, it gives
us the F value and the p value.And again, all these
tests are basedon the sandwich estimator.So we don't have to
specifically requestthe sandwich estimator
in this case,it's automatically
determined for us.The third procedure is
the SIMSYSTEM procedure.And this is a procedure that
you can use to simulate data.Now, it's specifically
for simulating datafrom a continuous
univariate distribution.It needed the Pearson
or the Johnson system.The whole idea of
SIMSYSTEM is insteadof trying to specify parameters,
you simply specify the moments.So in this case, you can do the
mean, the standard deviation,the skewness, and the kurtosis.This allows you a lot of
variability in the type of datayou want to simulate.So let's look at
a simple example.This is a case where I'm
going to be simulating datafrom a distribution that
has moments as specified.The mean is five,
standard deviation is two,the skewness is one,
and the kurtosis is 4.5.So down in the syntax,
you see I'm specifyingthe system is Johnson.The n equals 10,000 means.I want 10,000
generated observations.I'm using a seed because
this is pseudorandom numbers,and I want to make sure I
can replicate my results.My moments are then specified
in the moment statement.Mean equals five, standard
deviation equals two,skewness equals one,
and kurtosis equals 4.5.This is going to
generate a data set,and it's going to be stored
in the mycas library as the SBdata set.Here's another example
of things that youcan do in proc SIMSYSTEM.On the left, I have
the moment ratio mapfor the Pearson distribution.And this tells you what
type of Pearson distributionyou will get for values
of kurtosis and skewness.For example, if I specify
a skewness of zeroand a kurtosis of three, I
get the normal distribution,which is illustrated by that
blue circle on the graph.You can also see a series of
x's moving across that graph.And those x's are
represented by the graphon the right side, that centered
and scaled Pearson densitiesn equals zero.What's going on in
there is I'm changingthe value of the skewness--I'm making it larger--but I'm keeping the
kurtosis at three,and you can sort of
see what happens.The larger that
skewness gets, the morepeaked that
distribution becomes.And you can see the shape
of that distribution.The last procedure we're
going to talk aboutis the NMF procedure.And the NMF stands for
non-negative matrixfactorization.So the whole idea
of the NMF procedureis to take some
matrix x, and I wantto decompose that matrix
into two new matrices,or two low ranked non-negative
factor matrices, W and H. Wis going to be the
features matrix,and H is going to be
the weights matrix.Once I get these
two new matrices,I can use them for a
variety of different things,including dimension reduction,
feature identification,and feature extraction.Thank you very much for
listening to my super demo.My name is Gordon Brown.Here is my contact information
if you have any questions."
138,"Hello, everyone.My name is Michael Lamm,
and in this presentation,I'll be talking about
the new GAMSELECTprocedure for generalized
additive model selection.This procedure is new in the
8.5 release of SAS VisualStatistics and SAS Viya.And it was developed by my
manager Weijie Cai and myself.This talk will use a
simulated data exampleto demonstrate the
selection methods supportedby PROC GAMSELECT, and it
focuses on the following threequestions.First, how does PROC
GAMSELECT fit and selectgeneralized additive models?Second, how does PROC
GAMSELECT compareto other related
procedures in SAS software?And finally, and maybe
most importantly,what are the benefits
of using PROC GAMSELECT?So what type of
problems might itbe able to help you solve
in your day-to-day work?To begin, though,
I want to providea brief high-level overview of
generalized additive models.Generalized additive
models, or GAMs,can be used to build flexible
and interpretable modelsfor a variety of
different response types.A typical setup will assume
independent observationsof a response from
a distributionin the exponential family.So that could be using
a normal distributionto model continuous
response data,a Bernoulli distribution to
model binary response data,or a Poisson distribution
for count data.Like a generalized linear
model, for GLM, a GAMmodels the mean of the response
by using a link functiong, and an additive model
comprised of componentfunctions, f1 to fp, each a
function of some predictors x.As the name implies, a
generalized linear modelfurther assumes that each
of the component functionsare going to be linear.GAMs extend GLMs by allowing
the component functions to bean arbitrary smooth function--
typically estimated by usingsplines--which allows for
increased flexibilityin terms of how you can model
that mean of the responsewhen there might be unknown
nonlinear dependencies.To illustrate this
point, here isa small example where I've
simulated 500 observationsfrom a normal
distribution, wherethe depends on one predictor
x through a sine function.The figure on the left shows
the traditional GLM fitusing a normal distribution
in the identity link.And the figure on the right
shows the corresponding GAM fitwith the same distribution
and link function,and it's clearly doing a better
job of capturing that nonlineardependence.SAS Visual Statistics
and SAS Viya currentlyprovides a number
of tools designedfor building GLMs and GAMs.In particular, some of you might
be familiar with the GENSELECTand GAMMOD procedures.To understand how the new
GAMSELECT procedure fitsinto this picture,
you can considerthree features of a procedure--first, if it allows you
to control these sparsityof the model fit--so does the procedure
support model selection;second, if you can control the
smoothness of nonlinear splineterms in the model; and
third, if the methodsthat the procedure
implements to fit a modelare designed
specifically for GAMs.The GENSELECT procedure
is designed primarilyto support the
selection of GLMs.While you can use regression
splines and PROC GENSELECTto model nonlinear
dependencies, itdoes not support
tools for easilycontrolling the smoothness
of the spline terms.Moreover, because the procedure
is designed primarily for GLMs,there are likely to be
performance implications whenyou try using it to
fit in select modelswith a large number
of spline terms.The GAMMOD procedure
is designed to fit gamsby using low-rank
regression splines.It will allow you to control
the smoothness of the model fit,and it is designed
specifically for GAMs.However, it does not
support model selectionand does not provide
tools for controllingthe sparsity of the model.The GAMSELECT procedure supports
methods for both fittingin selecting GAMs, while
controlling the smoothnessof the model fit.It does so by using
implementationsof a boosting and shrinkage
selection method designedspecifically for GAMs.One point of comparison
not on this slideis that the GAMSELECT
procedure does notsupport inference
on the fitted model,whereas PROC GAMMOD does.Also, while there are
procedures in SAS/STAT softwarethat are analogous to PROC
GENSELECT and PROC GAMMOD,there is no procedure
in SAS/STAT softwareanalogous to PROC GAMSELECT.So with that overview
covered, let's beginby discussing the
boosting selectionmethod in PROC GAMSELECT.PROC GAMSELECT implements
a componentwise versionof a functional gradient
descent or boosting algorithm.The boosting selection
method in PROC GAMSELECTsupport selection of parametric
effects, some components thatassume a linear model,
univariate spline terms,and bivariate spline terms.At each iteration of
the boosting algorithm,componentwise fits are
obtained for each term,and one component is selected
and used to update the model.Parametric effects specified
by using the PARAM syntax arefit at each iteration by
using ordinary least squares,and the univariate and
bivariate spline termsare specified by using
the SPLINE syntax,and are estimated by
using penalized B-splines.So it's those penalties
on the B-splinesthat allow you to control
the smoothness of the model.To illustrate the steps involved
in the boosting algorithm,consider the following data
with a response y and fourcontinuous predictors, v1 to v4.The algorithm begins
with an initial modelthat, by default, is an
intercept-only fit, shown hereby the red line.The algorithm then
repeats a second stepuntil a stopping
criterion is met.The second step
begins by computingthe derivative of the
loss function, giventhe current fit.For PROC GAMSELECT,
the loss functionwould depend on the negative log
likelihood function determinedby the distribution and link
function that you've specified.And evaluating that
derivative gives the UI valuesplotted here based on that
initial intercept-only fit.Next, for each of
the individual terms,a model is fit to the UI values.In this example, that gives us
the for component functions,gamma 1 to gamma 4,
plotted here in red.Note that shown here the
algorithm, by default,fits spline components at each
iteration with a small degreesof freedom--so a more smooth fit.This is because the
goal of the algorithmisn't to quickly converge
to an optimal solution.Instead, we want to
use the algorithmto efficiently generate a file
exploration of our model spacefrom which we then
select our final model.Given the gamma 1 to
gamma 4 functions,we now select for update the
component with the smallestASE, with respect to the UI
values, which, in this case,is the gamma 4 function.So this effect is now
updated in our model.Since this is the
first iteration,it is now the only effect
in the model at this point.Note that the model updates use
a small step size by default0.1, and once again, that's
to try to support a fileexploration of the model space.The process of computing
the derivatives,and fitting the
component functions,and selecting a
component for updatewould then be repeated until
a maximum number of iterationsor an early stopping
criterion is met at iteration,say, m end.At this point, there are m
end plus 1 possible modelsto choose from, with
one model correspondingto the cumulative fit at
each one of those boostingiterations.The final model is
selected from this set,and here, you can see
the selected modeland the partial residual plots
for that model in our toyexample after 500
boosting iterations.Let's now look at
a larger examplethat demonstrates how you can
use the boosting selectionmethod in PROC GAMSELECT.For this example, we'll
consider a simulated dataset with 10,000 observations.The data include 200 continuous
predictors, v1 to v200,and the true model
for of the response ydepends on only the first
four predictors, v1 to v4.So in this example,
there's goingto be a very large
number of noise terms.Here I've plotted the true form
of the component functions f1to f4.We have a sine function
of the variable v1,a quadratic function of v2,
a linear function of v3,and an exponential
function of v4.And the error terms
are being generatedfrom a standard
normal distribution.The syntax for specifying
a model in PROC GAMSELECTis consistent with
other SAS proceduresfor fitting generalized
additive models.Here I'm defining the
SplinePrefixList macro functionand using it to specify spline
terms for the variables v1to v200, all using the
default spline constructionin the model statement.Using PROC GAMSELECT, both the
model and selection statementsare going to be required.And in the selection
statement, I'mindicating the use of the
boosting selection method.Looking at some of the
output from these statements,the iteration summary table
shows that 500 iterationswere executed--that is the default number for
the boosting selection method,that the selected
model correspondsto the final boosting
iteration, and that the selectedmodel includes eight effects,
not counting the intercept.The selected effects table shows
as the eight selected splineterms, the iteration at which
a term first entered the model,and how many iterations a
term was selected for update.You can see that the four
true effects were selectedfor the majority
of the iterations,and that the four
noise terms wereselected a small number
of times towards the endof the algorithm.This model yields a good fit,
with an ASE of about 1.04.And an Oracle model should have
an ASE of about 1 in this case,so this is a very good fit.One very nice
property of GAMs isthat the univariate and
bivariate spline termscan easily be visualized, which
allows you to better understandthe contribution of each
term towards the model fit.So GAMs.Provide you with a very
flexible and interpretable classof models.These smoothing
component plots wererequested by using plots equals
option in the PROC GAMSELECTstatement.And here we see
well-reconstructed curvesfor the variables v1 to v4.The plots for the spline terms
constructed from the noiseterms selected in the model
show small contributions,as indicated by the
scale of the y-axis.One concern you might have when
using boosting selection methodis that, when run for a
large number of iterations,you can end up overfitting
the training data.Two options you can use to
attempt to prevent overfittingare to allow for early stopping
of the selection process,or to change the
selection criterionto be either cross
validation of the ASEor the ASE on a
validation data partition.Here I'll refit the model
by using cross-validationand early stopping based
on a relative changein that selection criterion.With these options added,
the selection processnow terminates at the
300th boosting iteration,and selects the model with
only the four true effects.This model has comparable
training data ASEand cross-validation of the
ASE with values of about 1.08and 1.1, respectively--so again, a pretty good fit.Looking at the
smoothing componentplots for the selected
model, the curvesfor the variables v2
to v4 look pretty good,but the curve for
the variable v1shows a more linear
fit in the tails,suggesting some possible
over-regularizationin this case.Over-regularization
can be controlledby adjusting the
degrees of freedomfor the spline
term that's fittedeach iteration of the
boosting algorithm,as increasing the
degrees of freedomwill decrease the
penalty parameter usedwhen constructing the
penalized B-splines.When using the boosting
selection method,you can use the df equals
option in the spline instructionto specify the degrees of
freedom for a spline term.Here I refit at the
model one last time,now using only the
four terms, v1 to v4,with the spline v1 fit,
with 10 degrees of freedomat each iteration,
instead of the default 4.Doing so results in a
very good model fit,and the smoothing component
plots for this modelshow well-reconstructed
curves for all four variables.Now, because most people,
when they think of boosting,probably think of the
type of boosting algorithmas implemented by the GRADBOOST
procedure in SAS Visual DataMining and Machine Learning,
I want to quickly comment onhow the GRADBOOST and
GAMSELECT procedures comparein terms of their usage
of boosting algorithms.And ultimately, this comparison
is pretty straightforward.PROC GAMSELECT implements
a componentwiseboosting algorithm to build
a generalized additive model.Because the parameter
estimates for eachof the components
of that model areadditive across the different
boosting iterations,you end up with one generalized
additive model at the end.PROC GRADBOOST implements a
tree-based boosting algorithmto build a tree ensemble.So in my outline of that
boosting algorithm earlier,what would change is,
in step 2, now a treeis being fit at each iteration.And because those trees aren't
additive in their estimatedparameters across
the iterations,you end up with a tree
ensemble at the end,as opposed to a single tree.For those who are interested,
in an example that furtherillustrates these
differences, the paperthat corresponds to this talk
in these SAS Global Forum 2020Conference proceedings
includes an analysisof the example data using PROC
GRADBOOST that, for time, I'veomitted from this presentation.So to summarize the boosting
selection method and programselect, you can use it to
build generalized additivemodels comprised of
parametric effects,univariate, and
bivariate spline terms.At each step in the
algorithm, one effectis selected for
update in the model,and that model can be
based on the training dataASE, cross-validation
of the ASE,or by using a validation
data partition.So let's now move on to
discuss the shrinkage selectionmethod in PROC GAMSELECT.Because the shrinkage and
boosting selection methodsuse very different
algorithms, they'regoing to support different
modeling options.Shrinkage selection
method only performselection for spline terms.All of the parametric effects
that you specified in the PARAMsyntax are going to be included
automatically in the model fit.Splines are modeled by using
natural cubic splines, insteadof penalized B-splines.The shrinkage selection
method does notsupport bivariate spline terms.The shrinkage selection method
performs penalized likelihoodestimation by optimizing a
function that involves upto three penalty terms.While I show here the
full equation thatis going to be optimized
by this method,don't worry too much about
the individual components.And let me just try to summarize
some of the main portions.First, we have the negative log
likelihood function and thenthese three penalty terms with
tuning parameters lambda 1to lambda 3.The first penalty term with
tuning parameter lambda 1is like the group
lasso penalty that canbe used to induce sparseness.The second penalty with
tuning parameter lambda 2is the smoothness penalty
on the second derivativesof the individual spline terms.And the final penalty with
tuning parameter lambda 3is like a penalty
from ridge regression.You can either specify
the values of these tuningparameters lambda 1 to
lambda 3, or by default,PROC GAMSELECT will perform a
grid search over their valuesand return the model
with the best fit.So let's now see how you can use
the shrinkage selection methodfor our simulated data example.Here, I once again used
the SplinePrefixList macrofunction to specify a model
with 200 spline terms.The shrinkage
iteration summary tabletells you the number of
penalty parameter setsthat were tried--in this case, 200--the number of those tries for
which the model fit converged,how many iterations
were performedfor the selected model, and
the number of selected effects.In this example, there
are six selected effects,now counting the
intercept, and theyinclude the four true spline
terms plus one noise term.By default, two
tables are producedto summarize the penalty
information for the selectedmodel.The regularization
parameter summary tabledisplays the information
about the tuning parameterslambda 1 to lambda 3, and the
spline regularization tableshows the penalty specific to
these selected spline terms.These involve the lambda values,
the spline-specific weights,phi and gamma, and the
penalty function values.Smoothing component
plots for this modelshow well-reconstructed curves
for the variables v1 to v4and a small contribution from
the one selected noise term.The shrinkage selection method--you can further
refine the model fitusing a data-adaptive approach
to set the spline penaltyweights, which would be
similar to the adaptive lasso.Here we use that approach,
refitting the model usingdisplaying terms selected
in the first model fitby using PROC GAMSELECT and
the shrinkage selection method,now with splicing smoothness and
splicing sparsity weights setbased on one over the
respective penalty functionvalues from that
first model fit.Doing so results in a
model that now selects onlythe four true spline
terms plus the intercept.And this model has a very good
fit with an ASE of about 1.2.So to summarize the
shrinkage selection method,you can use this method
in PROC GAMSELECTto fit models comprised
of parametric effectsand univariate spline terms by
using a penalized likelihoodapproach.Unlike the boosting
selection method,selection is performed
only for the spline terms,and the spline terms are modeled
by using natural cubic splines.Models can be fit based on
the fit for the training dataor by using a validation
data partition.So no cross-validation
is supportedwhen using the shrinkage
selection method.And you can use those
spline penalty weightsto fit a model in a
data-adaptive way.So to summarize this talk in
terms of those three openingquestions.PROC GAMSELECT fits and selects
generalized additive modelsby using boosting and
shrinkage selection methods.It provides tools
that you can useto control both the
sparsity of the model fitand the smoothness of the
spline terms, a combination thatis unique to this
procedure in SAS software.And it implements
selection algorithmsthat are tailored to
generalized additive models.You can use PROC GAMSELECT to
build models that are flexible,interpretable, and
parsimonious, even in situationswhere there might be possibly
unknown nonlinear dependencies.Thank you for watching,
and for more informationabout the GAMSELECT
procedure, youcan see the corresponding paper
in the SAS Global Forum 2020Conference proceedings.Thank you very much."
139,"JAVIER DELGADO: Hello,
everyone, and thank youfor taking the time to
attend this presentation.And so what I'll
be discussing todayis the cloud-based time series
processing system that'sbehind SAS Visual Forecasting.And this is a system
that's designedto scale to millions
of time seriesif your infrastructure
allows it.And this has been a very
well-received system.Now, one thing that
we've constantlybeen asked over the past couple
of years when we presented itis whether users can use
languages besides the SASlanguage itself to
interact with the system.And in particular, people are
interested in the time seriescommunity to use Python
and R. And so I'mhappy to report that as of the
end of 2019, the answer to thatis yes.You can now use
Python and R codeusing our new External
Languages Package.So that's what I'll
be discussing todayis how to use this package.But before that, let me
quickly give a primeron the back-end system
behind all of this.So what we refer to as TSMODEL
is this cloud-based time seriesprocessing infrastructure.And what it is an
infrastructure thatworks with facets of
Cloud Analytic Servicesto allow users to run your
time series on the cloud.And it scales very well.It handles data
movement for you,as well as data processing.I'm not going to show any
empirical results here,in the interest of time.But we've done tests with
millions of times seriesand it scales very well.And there's multiple ways you
can interact with this system.So you can use the SAS Visual
Forecasting Graphical UserInterface.Or you can use SAS
code if you're a coder.You can use, for example,
the TSMODEL Procedureto interact with the system.Or if you prefer using
one of the other clientsthat CAS supports, you
can use those as well.So let's set the scene for
a scenario in which youcould use this system.So if you're a big corporation
or some other entity thathas millions of time series
and you have a huge cluster,you can use this system.For the sake of just
describing the system,let's go with a
more modest clusterof four worker nodes connected
to a distributed file system.And so the distributed
file system is essentiallya set of hardware for
storage and softwareto manage that storage
in an optimal fashion.As a user, all you're
concerned with isthat you have data tables,
in our case, time series datatables, and you want to submit
those to the worker nodesto work with them.So we're going to ignore the
distributor file system detailsin the interest of time.Now, we'll get
back to that later.So as a user, what
you're concernedwith is that you will
have some processingyou want to do on
these worker nodes.So what you're going to do is
you're going to write a script.Either manually write
the script yourself,or if you're using the
Visual Forecasting Interface,then your interactions
with the interfacewill create that script for you.And the system will
replicate the scripton each of the worker nodes.You don't have to worry
about any of that yourself.And the big news here
is that you're notlimited to just SAS anymore
with your users script.You can use any
combination of SAS, Python,and R code in there.So let's go a little bit into
the details of the script.So the first thing that's going
to happen when the worker nodesreceive this, they're
going to compilethe script into
the machine code,so that it's as
quick as possible.And so that should
the SAS code itself.But as I mentioned, you can
now have Python and R code.So how does that work
with this system?So in order to
discuss that, let'smake the script a little
bit less abstract.Let's show this hypothetical,
very simple SAS scriptI call an external
language code,in this case, Python code.All you need to know
here is that anythingwithin these submit
blocks, that'swhat's going to run
on the worker nodes.And you can see here that
we've pushed some Python code.So you can think of
this code as beingwritten into its own
little code.py file.And it's the same concept
for R. But for this example,I'll just use Python.All right.So we're back to
our cluster here.We have our compiled script
on each of these nodes.So now, we want to run some
Python code, in additionto the compiled SAS code.So by using Python objects
in here with the systemnodes that it has
to run to Pythoninterpreter, which on
Windows, python.exe.If this was UNIX, user
then Python, et cetera.And remember that code.py file
where we extracted the Pythoncode, so that's going to be
fed to the Python process.So what's missing
now is the data.So back here, we go back to
the distributed file system,which is going to take
the data and send itto the worker nodes.I won't go into the details of
how the load balancing and allthat work.In this case, it's
pretty simple.We have seven time series on
each of the two data tables,and we have four worker nodes.And so that results
in three of the workernodes having two times
series from each table,and then the last,
the third nodehaving just one time series
from each of the two tables.Presumably, these are
modern nodes wherethat leads four CPU cores each.So we can actually process
all of these time seriesin parallel at the same time.Now, once that's done
processing, what's leftis to send the data back to
the distributed file system.And that's all automatically
handled for you as well.You just specify what
your output data sets are.One thing I didn't
show here is howthat data is shared
between the SASprocess and the Python process.So you don't need to
worry about that either.We're going to do that for you.All you need to
do a specify whichare the variables
that you want to sharebetween the environments.But we're not going
to unnecessarily shareall the variables for you.And I'll show how that
works in a bit whenwe go into the actual objects
that you use for interactingwith the external languages.This is just a quick slide that
shows how the system scales.So here, we ran
up to 400 threadswith a data set size
of 87 gigabytes.And you can see that it
scales very well, culminatingat about five minutes for using
400 processing cores to runthis really large data set.Comparatively, if we
did the same thingusing our serial processing
framework, it's 130 hours.All right, so now, let's
get into the detailsof the External
Languages Package.And here, I list all
of the packages thatcome with Visual Forecasting.But I don't really have
time to go into each one.But I list them here just
to emphasize the factthat in addition to the fact
that you can parallelizeyour Python and R
code now, you can alsointegrate that code with
all of these features thatcome with Visual Forecasting.So that really augments the
power of what you can do.As far as some highlights
of this External LanguagesPackage, so first of
all, you can run any codeyou want with this system.Obviously, it's designed
to optimize time seriesprocessing.But we don't limit
what you can do.If you want to run some hello
world with it, that's fine.Go ahead.We also don't limit what
version of Python and R you use.Obviously, there's some
practical limits here.If you want to use a
version of Python thatwas released 10 years ago,
that might be a problem.But any relatively recent
version, like 2.6 for Python,2.5 for R, that will work.We also don't require
you to installany third-party packages
for either Python or R.We only installed the
libraries that youneed for your own processing.As I mentioned, we automatically
transfer data to and from SAS.So basically, we
share the variables.Whatever variables you
specify your want to share,we're going to do that
data transfer for you.And I'll give an example how
you specify those variablesin a bit.For those of you who
are concerned nowbecause you have support
for these general purposelanguages, like Python and
R, that allow you to accessfile system, the key
processes and all that,we do have a flexible access
control methodology in placeto restrict what users can do.As far as launching the
External Language Process that'ssupplying which Python or R
interpreter they want to use,things like that,
you can control them.All right, so now, let's
go into a little bit moreof the specifics of
the package itself.As with our other Visual
Forecasting packages,we take an
object-oriented approach.And we can break down the
objects into three main kinds.The first are the
interpreter objects.So these are the objects we use
to actually interact directlywith the external language.So the first thing you
can do with these objectsis insert code.And we provide three
ways of inserting code.One is directly
into the SAS script.So basically, you
just create a stringthere with the Python or
R code you want to run.If you have a very
long piece of code,that's obviously not
going to be as convenient.So you can also load
the code from a file,provided it's a file that's
accessible to all of the workernodes.And you can also load
code from a CAS table.With the interpreter
objects, you can alsospecify shared variables.So these are the variables
that will be made availablein the Python or R process.And you can specify
environment variables.If your external
language code relieson environment variables,
you can specify thosewithin the SAS program.They'll be made
available for you.Obviously, you can run the code.And then there's several
other miscellaneous functionsfor things like
getting exit codes,run times, things like that.So I won't go into those
specific little details now.The next set of objects
are the collector objects.So these are the ones you use
to store other kinds of datainto CAS tables.So, for example,
if you're runningon millions of
time series, you'regoing to have millions of logs.By logs, I mean
anything that's readinto standard output
or standard errorin your Python or Oracle
code using print statementsor whatever.So you can store
these into CAS tablesso that you can
look at them later.So, for example, if
something didn't go rightwith just a subset
of the time seriesor if you just want to take
a look at a few random onesjust to eyeball
what went on, youcan do that by storing it in
the table using this collectorobject and then looking at it
later using whatever modalityyou use for looking tables.You can also store
shared variable status.So occasionally,
shared variablesthat are supposed to be
written back to the SAS processare not, for one
reason or another.But you can use
this status tableto get a better idea
of what went wrong.And you can store source code,
which you can then load backusing these repeater objects.Well, in this case, we just have
one repeater object to load.That stores the source code.All right, so with that,
we can go into the example.And we'll describe a
very simple example,which calculates the moving
average of a time series usingPython.And in this case,
we have a data setconsisting of many times
series, so this can allbe done in parallel.For those of you
viewing this, I'mnot close to a monitor,
like in a classroomsetting or something.Don't worry.I'm going to go into
the details of eachof these sections of code.But I wanted to show like a big
picture view of the code itselffirst.And this is an
actual working code.So this kind of
demonstrates that youdon't need to have
a lot of setup codeto run your Python program.So the first thing
to emphasize hereis that the code between the
submit and then submit logs.Keep in mind,
that's what's goingto actually run in the
cluster on the worker nodes,in other words.The rest is basically
setup code whereyou're going to specify the
tables you use for inputand output and whatnot.And then this
section of code herein the purple,
that is just extendthe language specific code.As you can see, this
program is mostly just usingexternal language codes.The rest is boilerplate stuff.All right, so let's get
to the first part here.So the first thing
is we're goingto specify the parameters
of our PROC TSMODEL call.The first thing is the input
data that we're going to use.And here, we're using
the skinproduct dataset in the mycas CAS library.So the skinproduct data set
is a synthetic data set.It has several times
series, consistingof a fictional
skincare products saleswith a weekly time interval.Next, we're going to
specify where we'regoing to put our output data.So we're specifying here
that output array data.That's going to go in
a table called outarrayin the same mycas CAS library.Similarly, scalar
output data is goingto go into this OUTSCALAR table.And then objects that are
part of the Visual Forecastinginfrastructure that we use,
they'll go on their own tables.So in this case, we just
have one such object,which is a log object.And that's going to store
all of our Python logs.And that's going to go
in this pylog table.Here, we specify the name of
the variable in the data setthat contains the temporal data.So in this case, it's
a variable called date.And the interval is weekly.BY, so that's where we
delineate how we want these timeseries to be parallelized.So in this case, each
distribution centeris going to be its
own time series.So depending on the number
of distribution centerswe have in this
skinproduct data set,we'll have a separate
process for each of those.And we'll automatically
send that sub-seriesof the time series to the
appropriate worker node.And then VAR revenue,
so this is a variablewithin this skinproduct data
set that we want to use.And when we accumulate
the data setinto equal sized
buckets of time,we're going to sum those data.And then runtime, that's going
to be our only OUTSCALAR.And that's going to store
the runtime of the pipelineprocess, just so
you know how long ittook to run that pipeline code.And then our only output
array is M average,which is a moving average.All right, so now, we can
get to the next section.So the first thing
we do is we specifythat we want to use objects for
the External Languages Package.And we do that with
this REQUIRE statement.If you're a Python
programmer, that'slike an import statement.If you're R programmer, you've
got the library statementNext, we start our
SUBMIT block, whichrecall this where we start
the code that's goingto be executed on the cluster.So we're going to create
our pipeline object.In this case, we want to use
the Python 2 interpreter.So we're going to create
a Python 2 object,and then initialize that object.Next, we're going to specify
our shared variables.In this case, we have
two shared variables.And to do that, we use
this AddVariable method.And the AddVariable method
takes various parameters.I'm just going to go over
the two main ones here.So the first required parameter
is the variable name itself,in this case, revenue.And so the first common optional
parameter for AddVariableis the alias.So this is used because you
might have existing Pythoncode that doesn't necessarily
take a variable calledrevenue, but a
more generic name,like y for an
independent variable.So by specifying
this alias, insteadof making the variable known as
revenue in the Python program,it'll be known as y.So you can just
refer to it as Y.So you don't need to change
your Python programming,in other words, just
to accommodate this.So the other shared
variable is M averagefor the moving average.And the parameter we're
passing here is read only.So by default, we're
only going to send datato the Python process.And, again, this is
just to limit the amountof unnecessary data movement.So in the case of
revenue, we're onlyusing it as an input variable.So we then have
to bring it back.So we keep the default
read only attribute there.In the case of M average,
that's what we actuallywant to calculate and
bring back to SAS.So we're going to have
data that's not read only.We want that back
to the SAS process.And then finally, we
can add our Python code.I'm not going to go into the
details of this code itself.Basically, it calculates
the moving average,using convolution.And then you specify
that you wantto run the code using
the run method call.You can get the runtime.And then we're going
to write the codeto collect the log from
each of these Python runs.And for that, we use
this OUTEXTLOG object.And we call the collect
method using the Python objectas an argument.And then we specify that
we want all the logs.And that's it for
the parallel code.And we can close
TSMODEL statement.Once you run that, you can
retrieve the data any wayyou want.You can use SGPLOT to plot it.You can use PROC print to
print table, et cetera.So that was an example of how
to use the SAS programmaticallyor how to use the External
Language programmatically.You can also use
it through the GUI,although that's kind of limited
since this is a new package.But here, I'll just
show some screenshotsof how you would integrate
external languagecode into Model Studio.So you can use the
Exchange to adda, for example, an
open source nodeto use as part of your automatic
model selection process.And once you do
that, you can seewhen you create a
pipeline that itwill be part of your pipeline.In this case, for
these screens, whatI'm showing
screenshots of here, weused an R model,
an R ARIMA model,as part of the automatic
model selection.And you can see that it was
actually used in some cases.And so that's all
I have for now.Thanks for your attention."
140,"Welcome to this presentation
about Amazon Web Services datalakes and enabling access to
data lakes on AWS through SAS.My name is Dilip Rajan.And I'm a partner solutions
architect with AWS.I work with various ISV in the
data and analytics verticalsto build joint solutions
and integrations with AWS.I've been with the company
for just over four years.And I worked in
various industries,from e-banking to
fulfillment operations.In this presentation, I'm going
to talk to you a little bitabout data lakes on the AWS,
how to build data lakes on AWSusing Lake Formation,
which is a service thatwent live last year.And I'm going to
also talk to youabout some of the
frequent data lakepartners and analytics
on AWS and howusers can interact with data
lakes in a seamlessly way.As you know, the data that
has been available nowadayshas increased exponentially in
the last four to five years.And because the
data has exploded,it has caused a
variety of challengesin terms of making
their data democratized,as well as making sure that the
right governance and controlcapabilities are
available for datato be available in
a seamless fashion.These challenges are
important challengesthat happen in any
organization wherethere is a large
amount of data thatcomes in from internal
and external sources.At AWS, we think
about a data leakas a centralized in
the security Baldessariwhere users can analyze
a wide variety of datasets very simple mechanisms to
discover and govern shared datasets, both data sets from a
structured and unstructuredfashion, such as-- such as
Apache Parquet formats areavailable in a data lake.And they can also
access via open sourceanalytical frameworks
such as Apache Spark.In the world of AWS, there
are a variety of servicesthat help build a data lake.At the center of all of this
is the object storage of S3.S3 allows for all of
the data to be storedin a centralized repository.However, it also
provides the abilityto interact with
multiple serviceswithin the portfolio of AWS.For example, in order
to move data into S3,it is important that we have
integrations with kinesis,which is the streaming
service, as well as Snowball,which can help in loading
data in a batch-wise fashion.Same time, you
have AWS View thatcan help with the cataloging
and searching of that dataonce it's available in S3.User interface services such
as AppSync, API Gateway,and Cognito to allow
for interactionthrough visual interface
to the data sets in S3.And finally, these
data settings areavailable via
analytical servicessuch as Athena, Redshift,
EMR, Glue, and QuickSight.Staging all of these things
together, once it's done,it's important that we
have the right permissionsand compliance model, which can
be enforced through AWS IAM,Identity and Access Management.And once the identity and
access management frameworkhas been set up, monitoring
and logging off access,it can be enabled through
CloudWatch and CloudTrail.Building a data lake
is not an easy task.It can take months for a team
of data engineers, data securityofficers, and data scientists
to set up the storage,move the data there,
clean and prepthe data through a
catalog, and forcedto write function
policies, and makethe data available for
the data scientists.Having the necessary
infrastructureand then removing these
silos of interactionbetween multiple teams can
be a cumbersome process.And this is not
something that makesit easy for the organization
to set up and operationalizethe data lake quickly.Last AWS launched
a Lake Formationwhich allows users
to set up datalakes in a matter of weeks.It provides the three
pillars of setting upa data lake in
terms of ingestionand cleaning security
and analytics and ML.And within ingestion and
cleaning the building blockthat is in terms AWS glue,
which provides a frameworkto write the workflows of moving
data from source to destinationusing so on a Spark.This framework is built
on top of blueprints.Blueprints are nothing but a
pre-computer, pre-developedtemplates that allow for data
to be moved between sourceto the destination.We won't go through
ML Transforms,but in ML Transforms there
is ability to de-goop dataand clean lots of data
sets in ML Transforms.In this presentation,
I'll also show youa workshop link that will allow
the users to set up a datalake from the basic
components to analyzingthe data in a cohesive fashion.So we'll take a look at that
workshop towards the endof the presentation.It's important to look into what
are the main components of Gluebecause Glue is the building
block on top of which LakeFormation is built. The Glue has
three main components-- a datacatalogue, a serverless engine,
and an orchestration modelthat stitches all of
these workflows together.Within the data
catalogue it providesthe capability to automatically
crawl and discoverthe metadata from the
underlying S3 data sets.In a high compatible data
store once the metadata hasbeen discovered it
has the capabilityto interact with other
AWS services whichallow it to discover
these metadataand provide them for search
and cataloging capabilities.The same capabilities are
also available similarlyin the serverless
engine where userscan use to be Apache Spark or
Python Shell scripts to buildjobs, either for interactive
purposes or for batch purposes.And once these jobs
are set out thereis an orchestration
model that allowsusers to build a workflow using
a visual designer on the AWSconsole.Stitching up all
of this together,once the workflows are set up
there is a mechanisms in placefor monitoring and alerting
of various workflowsand how those workflows are
implemented at the base level.Let's take a look at
how Lake Formationbuilds on top of AWS Glue.For Lake Formation itself there
are three main components.One is the monitoring component,
the blueprints, and thensecurity collaboration
and search.For the blueprints,
as you can see,the blueprints are nothing
but pre-formed templatesthat allow data to be moved
from source to destination.And that's built on
top of workflows,and workflows are build on
top of ETL jobs and the GlueCrawlers.So you can see that the
core components of workflowsare built into a blueprint.Similarly to search
and collaborate on datasets you need to have
a Glue to that catalogand once that Glue catalog has
been built a lake formation islayered on top of it with the
security and collaborationcapabilities.Putting all of
this together helpswith monitoring where any
access to a lake formationcan be monitored on the
dashboard for off lakeformation.In order for us
to understand howthe data is moved from
source to destination,let's take a look into how data
is loaded through blueprints.Blueprints are nothing
but pre-built templatesthat so many common
use cases of ingestion.For example, you might
have data that movesfrom a OLDP database such
as Amazon RDS or from accesslogs in CloudTrail
or in from ELD.Once that data is available
a Glue workflow automaticallyis built using a blueprint
and Glue crawlers and jobsautomatically discover and
transform the structured datafrom the source.A Glue catalog is populated
and all of the datacan be then loaded from the
source to the destination S3buckets in an incremental
or full fashion.This is how the data is moved
from source to destinationand commonly built use cases.Once the data has been moved
into the centralized locationpermission models
are overlayed on top.This is an example of how a
user might want to get accessto the underlying
data sets usingcommon analytical tools such
as Redshift Glue, EMR, Athena.Once the user asks
for data accessa lake formation call is
made and the permissionswhich have already been
overlayed by the administratorare provided and
the user gets accessto the data in the
underlying data lake.It's also important to
understand how the data isavailable at various levels.In previous data
lakes you would noticethat you would have had to
apply to permissions at a bucketlevel.However, with the
lake formationspermissions can be specified
on tables as well as columns.And this permission
model also allowsusers to grant and revoke
permissions and alsosee all of the Glue permissions
at one particular location.At the same time
and also providesthe ability for folks to
see what kind of accessis available and audit that
access whenever there might besome challenges to user access.It's also important to
understand the capabilitiesdata cataloging and
metadata managementwithin Lake Formation.Lake Formation allows users
to perform text-based search.It also allows users
to add attributes.For example, who could
be a data owner or datasteward and an additional
table properties.And also at the
same time the userscan also get access to
look at the data, whichis available in the stables
using Amazon Athena.It's super important
that we have the abilityto monitor and audit all of
the access to a lake formationor to underline data
lake in real time.As you can see in
this example, thereis detailed activity for
CloudTrail to track eachand every one of the audit logs.And from CloudWatch we
can create notificationsand publications to how the data
is being ingested into a datalake and perform any Glue cost
analysis or investigationsif there are certain issues
or challenges with monitoringand managing that
real time activity.Let's take a look at some
of the comprehensive setof integration tools.AWS has pre-built interaction
for Lake Formationwith Redshift, EMR,
Glue, and Athena.And these tools have the
necessary systems in place.And returns the API access
to provide the accessby our permission models.And all of the access
can be logged and trackedwithin Lake Formation itself.For the purposes of
this presentation,I'm going to move
through Amazon Athenaand see how Athena
interacts with the LakeFormation and some of the
benefits that Athena provides.Athena is a interactive
equity services.It's a standard
SQL-based interfacethat allows access to the
underlying data industry.Some of the key
benefits of Athenaare that it is a decoupled
storage and computeenvironment.It's a serverless model that
has no infrastructure or costsetup.Customers are only required
to pay for the amount of datathey scan.And it is a compliant product
from other open source fileformats and also is built
upon powerful open sourcecommunity-based solution.A recent addition to this
feature set of Athenawas the Federated
Query that allowsusers to interact with
multiple creating enginesbehind the scenes.And this is a powerful
capability thatadds benefits to Amazon Athena.Athena is a familiar
technology under the covers.For example, users can still use
the DDL capabilities of Createor Alter Table, have
support for multiple datatypes, multiple formats, and
it supports data partitioningwhich allows for improved
and fostered accessto the data which
is available in S3.And uses in-memory database
distribute engine of Prestowhich is pretty much the same
as writing a standard SQL code.Users can take a look at
some of the Presto-based SQL.Here you can see
an example of howa Presto SQL can be written.It provides the same
capabilities of standard SQL,such as joins, nested queries,
and windowing functions,and supports multiple file
formats such as, arraysand structures and maps.That are file format such as--Presto, Avro, and ORC are
also supported in additionto CSV and JSON.And ultimately does strong
interaction and integrationwith AWS data catalog.Now that you have seen
a few of the services,including Glue, Lake
Formation, and Athenathat form the nuts and
bolts of our data lake,let's take a look at what
are the mechanisms thatare available for SAS users to
interact with a AWS data lake.SAS has many tools
and many productsand it is important that
we have the common talkingmechanism with AWS data lake.For Athena which isn't ODBC and
JDBC JDBC compliance service,we have SAS access interface to
ODBC-JDBC that can be layeredand on top of we
are SAS toolkit.In addition, there is also
the needed capabilitiesof PROC S3, which allows
access to S3 buckets usingaccess keys and secret keys.For data sets, which
are large in number theynative capabilities of S3 in
terms of transfer accelerationcan also be used for
accessing the data setsand loading and unloading
the data in a speedy fashion.For Redshift Spectrum there
is the native capabilityof downloading in ODBC-JDBC
driver from Redshift thatcan allow for interaction
between your SAS tools and AWSdata lakes biometric spectrum.And at the same time,
you also have the abilityof using SAS access interface
to ODBC-JDBC on the SAS accessinterface Redshift.There are various
benefits of usingone mechanism over the other in
terms of speed and performanceand users should typically
take into considerationthe media's workload
requirementsbefore making a decision
of which mechanismis the most suitable for their
workload on a day to day basis.Finally, I want to take
you through the linkto the workshop, that workshop
that I brought to you before.In this architecture
or in this workshop,you can actually see how
a MYSQL data source, whichcontains a few data sets that
are being updated from logs,etc.I loaded to an S3 bucket using
a Glue workflow that containsa Glue job and a Glue crawler.There are various personas
and terms of admin, developer,and business analyst who can
then set up the architectureand also have the ability
to interact with the datasets in S3 and then store
those resulting data sets in S3as well.So this is a good
workshop that can give youa hands-on experience of what
is required to set up a datalake using Lake Formation.We are constantly
looking for feedbackon how to improve either
the workshop or these sointeraction between
SAS and AWS and we'dlove to hear about your
feedbacks and your questions.I welcome your questions.And feel free to
interact with me.Please note down
my email addressand let us know
how we can improvethe interaction between
AWS data lakes and SASand we can take that back to
our product engineering teamsand improve the product
offerings that currentlyexist between SAS and AWS.Thank you, and have a good day."
141,"Welcome to this demo for
SAS Global Forum 2020.I'm Guohui Wu at SAS.In this demo, we will talk about
spatial econometric modelingfor big data using
SAS econometrics.First, a little bit
background informationabout spatial
econometric modeling.Spatial econometric
models extendstandard econometric models
to address spatial interactionand heterogeneity in cross
sectional and panel data.The motivation behind
spatial econometric modelingis the first law of
geography, which says,everything is related
to everything else,but near things are more
related than distant things.From the modeling point of
view, the first law of geographysuggests that data collected
across different units in spaceare often correlated.And the strength
of such dependenceis determined by the proximity
of two units in space.In spatial econometric modeling,
spatial weights matrix,often known as W matrix,
plays an important rolebecause it describes
the proximityof two units in space.Our choice of models also
plays an important rolebecause we need to
address various formsof spatial dependence
in the data.In this demo, we will focus
on the CSPATIALREG procedurein SAS econometrics,
which was developedfor spatial
econometric modeling.As we have already mentioned
in the previous slide,W matrix describes
neighbor relationshipsamong observation units.Generally speaking,
W is an n by n matrixwith non-negative
entries, where nis the number of unique
observation units in the data.Wij, or the ij-th
entry of W, is positiveif units i and j are neighbors.We use W matrix to parameterize
spatial interactionand dependence.As an example, let's look at
a general modeling frameworkthat is described by the
two equations given here.In the first equation,
the first summation termaccounts for spatial
endogenous interaction.Whereas, the second
summation termaccounts for spatial
exogenous interaction.Moreover, the second
equation accountsfor spatial interaction
in the error term.To create a spatial
weights matrix W,we can use neighbor criteria
such as contiguity, distance,and many more.Now let's look at two examples.In this first
example, we considera data set that contains median
home values for 506 censustracks in Boston.Four variables in this data set
are selected for the analysis.Our dependent
variable, MedValue,refers to median value
of owner-occupied homesin 1,000 US dollars.And it is measured on log scale.Two explanatory variables
are PTRatio and Status,both of which are
measured on log scale.Here, PTRatio refers to
pupil-teacher ratio per town,and the status
refers to percentageof lower status population.Tract is the variable that
refers to census tract ID code.The figure on the right panel
plots MedValue, from whichwe can see that median home
values for neighboring censustracts seem to be
similar, which mightsuggest some kind of spatial
dependence in the data.For this analysis, we
consider a spatial weightsmatrix W that is created
based on sphere of influence.You can fit various
spatial econometric modelsto Boston housing data
using PROC CSPATIALREG.For the purpose
of demonstration,we only consider six models and
provide their AIC values here.These six models ranked
by the AIC valuesare linear regression
model, spatial lagof X model, spatial
autoregressive model,spatial error model,
spatial Durbin error model,and spatial Durbin model.If AIC is used for
model selection,spatial Durbin model, or
SDM, is the winning modelbecause its AIC value
is the smallest.The lower right panel
shows you the SAS code thatis used to fit SDM and to
test the null hypothesis,rho equals zero, using both
Wald and Lagrange multiplier,or LM tests.The impact option is specified
to request impact estimatesummary table.The test results table here
shows you the results from bothWald and LM tests, from which
we cannot accept the nullhypothesis at 5%
significance level.Now, let's look at parameter
and impact estimates from SDM.The first table here shows
you the parameter estimatesfrom SDM, whereas
the second tablesummarizes impact estimates
for two explanatory variables.Here, we note that the average
direct impact for PTRatiois negative 0.435, which
is different from its pointestimate of negative 0.419 in
the parameter estimates table.Moreover, the average indirect
and total impacts for PTRatioare negative 0.184 and
negative 0.619, respectively.As a result, we conclude that 1%
increase in pupil-teacher ratioin a census tract leads to a
total of 0.62% decrease in homevalue.Similarly, we conclude that
1% increase in percentageof lower status population
in a census tractleads to a total of 0.57%
decrease in home value.The take home message here
is that parameter estimatesfrom spatial econometric
models do not alwayshave the same straightforward
interpretation asin classical linear
regression models.PROC CSPATIALREG also supports
two approximation methods.These two approximation methods
are Chebyshev approximationand Taylor approximation.For the purpose
of demonstration,we also fit SDM to
Boston housing datausing Chebyshev approximation.The table on the
left panel showsyou the parameter estimates
from SDM without approximation,whereas the table
on the right panelshows you the parameter
estimate from SDMwith Chebyshev approximation.The comparison between
these two tablesshows that the
parameter estimatesfrom SDM with and without
approximation are very similar.However, fitting SDM
to Boston housing data,using Chebyshev approximation
can reduce computation timesignificantly.To further demonstrate how the
CSPATIALREG procedure scalesto big data.We consider a simulated data
set in our second example.In this simulation, we create
a spatial weights matrix Wfor 64,999 census tracts
in the 2000 US Census,based on three nearest
neighbor criterion.Using this W, we
then simulate datafrom the true model, which is a
spatial auto-regressive or SARmodel using the true
parameter values given here.When fitting the true model
to this simulated data set,we'll be facing two
main challenges--memory-wise, storing
the full W matrixrequires about 30
gigabytes of memory.Computationally,
fitting the true modelto this simulated data
set with exact computationis almost infeasible.However, these
two challenges areresolved in the
CSPATIALREG procedure,which allows the procedure
to scale to big data.You can fit the true model
to this simulated dataset using PROC CSPATIALREG
with approximation.And the total computation
time is about one minute.The table on the
right panel showsyou the parameter estimates
from PROC CSPATIALREG.We compare the
parameter estimateswith their true values and
find that the parameterestimates are very close
to their true values.Now, let's look at the
values of dependent variablein the simulated data
set and their in-samplepredicted values.The figure on the
left panel showsyou the plot for the
dependent variable yin the simulated data set.Whereas, the figure
on the right panelshows you the plot
for predicted values.The comparison between
these two figuresshows that the
predicted values capturethe overall pattern in the
simulated data very well.This is expected given that
the parameter estimates arevery close to the true values.Thank you for your time.And I hope you enjoy this demo.If you have any questions,
you can reach me by emailat guohui.wu@sas.com."
142,"Hello, my name is
Caroline Walker.And I'm speaking today about
principal component analysis.So just to provide a brief
overview of where we'llbe headed in this
talk, we're goingto spend most of this
session focusing on a verysmall dimensional data set.In fact, a data set
with just two features,which is quite different
from the type of dataset that you'd probably be
executing PCA on in real life.You're going to be
in a machine learningcontext working with a data
set with many, many features.The problem is that
data sets such as thatdon't lend themselves
well to visualization.And our goal is to build
a mental model of whathappens when PCA is executed.So our hope is that by focusing
on this small two-dimensionalfeature space, and being
able to really visualizewhat's happening there, we'll
deepen our understandingand our intuition about
what's happening when we'reexecuting PCA in much
higher dimensions,as we would be in practice.All right, so let's
jump right in.First, of all, very
broadly speaking,what is Principal
Component Analysis?Well first of all,
it's a mouthful to say,so I'm going to refer to it
throughout this talk as PCA.And it's a dimensionality
reduction technique.So it's one of many
dimensionality reductiontechniques that are available.But just broadly
speaking, when we'rethinking about
dimensionality reduction,we're imagining starting
out with a feature space,with tons of features.I put 10 up here
for as an example,because that's all I
can fit on my screen,but in reality, we
could have 100 or more.And for any number
of reasons, wedon't want to start building
a model off of 100 features.We want to reduce that, distill
the information in those100 features down into
something that it couldbe advantageous to start with.And so we're taking a large
data set, a large feature space,we're reducing it to a
smaller number of features.And it's important
to keep in mind,we're talking feature
extraction here.So we're not just
throwing out some featuresand keeping some others.We're changing the
features we haveto arrive at a new
set of featuresthat we believe will be
more informative to builda model off of.And when we're using
principal component analysisto find those new features,
the name of those featuresare principal components.so that's a very big
picture principal componentsand dimensionality
reduction in general.Let's take a look at some of
the specifics, the things thatdifferentiate PCA.So when we use PCA to
find a new feature setto build our model with,
the new features we obtainare going to be
linear transformationsof the original features.Those are the only
options that we'reexploring with this method.When we find those
features, they'regoing to be linearly
uncorrelated with each other.And here's the really big one,
kind of the driving motivationin PCA.We're looking for
a feature set thatwill retain maximum variance
from the original feature set.So if we think really
big here, say westarted with a feature
set with 100 features,and we reduced it
down to just 10using PCA, well, PCA promises
that set of 10 new featurescaptures the maximum variance
from the original featureset that can be captured
in just 10 dimensions.All right, so there we go,
maximum variance, big ideato take away from that overview.Now let's take a look at this
very small dimensional dataset I promised you.So here it is on the
screen, just six datapoints and two features.We're calling them
feature 1 and feature 2.Looking at that data
plotted in two dimensions,we can already see there's
some linear correlation there,which is an indication
that there's redundancy.The information
contained by feature 1is replicated somewhat in
the information containedin feature 2.And so we feel hopeful that
we can achieve good resultsby reducing the
dimensionality here,by finding a single
feature, whichcan adequately summarize what's
happening in two dimensions.So because variance is so
central to this idea of PCA,it's worth taking a moment here
to notice the variance that'spresent in our original data.So feature 1 has
variance just about 1.5.Feature 2 has variance 0.7.So all together, we're
talking about a variancein our feature set of 2.184.That's kind of going to be
our benchmark moving forward.We're trying to capture as much
of that variance as possiblein a single dimension.So when we think
about trying to reducethe dimensionality of this
feature space via PCA,we can visualize any
new feature that PCAwould create as a line in this
two-dimensional feature space.So let's put one up
there just to start with.So I've labeled it
line A, and thiswill represent a new
feature we're considering.Let's call it feature A.So we can see the values
that our data points wouldtake in terms of feature
A by projecting those datapoints onto the line and looking
at the values they yield.And what we're
hoping for, we wantto retain maximum
variance, we'rehoping to see very
dispersed data points.those blue dots, we'd
like them to be very far--very greatly spread apart.That's not really what
we're seeing here.And in particular, let's take
a look at these two pointsthat I've highlighted in yellow.When we think about
those two pointsin the feature
space F1, F2, thoseare very different points.They're capturing very
different information.However, when they're
transformed into feature A,they're taking nearly
identical values.So that's a good
indication off the batthat maybe we don't
feel so warm and fuzzyabout feature A. We may be
losing a lot of informationhere.And we can quantify
that mathematicallyby looking at the distribution
of the data points in featureA, measuring their distance
from the origin along line A.And we can calculate
the variance.And it's 0.328.So that's not just a little bad,
that's pretty much horrible.If we look back at
the variance that eachof our original features had,
and remember our big picturegoal was to take this from a
feature set with two featuresto a feature set
with one feature,well, we could've done better
to just throw out feature wand keep feature 1.Feature 1 has higher
variance than feature A.If we want to think about
it in terms of percentof the variance retained,
feature A is capturing just15% percent of the
variance that was presentin our original data set.So we can cross feature
A off the candidate list.This isn't the
direction we want to go.Let's look for a better feature.So I put a new line
up here, labeled B,representing feature B.
And just off the bat,taking a look at
this line, we cansee this is a pretty nice
trend line for our data.Our gut may be
telling us we're goingto get better results here.Looking at the projection
of those data pointsonto the line, we can
see that they indeedare more widely dispersed
than we saw with feature A.We can calculate the variance.It comes out around
1.9, which isa big improvement
over feature A,and as a percent of our
overall variance it's 86%.So that's great.Here, we would choose feature
B over either feature 1or feature 2.Feature B alone is
capturing more variancethan either of those original
features did independently.But the question remains,
is this the best we can do?Is this the feature
that captures the mostvariance possible for our data?So PCA is the method that
provides the solutionto that problem.When we follow the
steps of PCA, we'reusing a method that
has been mathematicallyproven to find the
new feature set thatcaptures maximum variance.So what we'll do next is talk
through the steps of PCA,and then we'll apply it
to this example data set,and look at the
results it yields.Here we go.So I'm going to put
four steps up here,summarizing how PCA is executed.But it's really
important to keep in mindthat when you're doing PCA or
using this method on your data,you're not executing these
steps one by one, in order.It's happening behind the scenes
in whatever implementationyou're using.So if you're working
in SAS 9, you'regoing to be using
PROC Print Comp.If you're working
in SAS Viya, you'regoing to be using PROC PCA.Or maybe you're using SAS
Visual Data Mining and MachineLearning, in which
case you're goingto be sending all your
parameters in the userinterface there.These are the steps that are
happening behind the scenes.And each of those
methods offers youinformation about
these various steps.We want to really
know what's going on.So here are the steps.So the first step,
we're going to calculatethe covariance matrix of
the original feature set.And I've starred the
word covariance there,because there's a number
of situations wherethe correlation matrix
is actually preferable,and any implementation that
I mentioned where you couldbe using this gives you
the option of choosingbetween the correlation
or the covariance matrix.For our purposes, in the
example we're looking at,we'll illustrate use of
the covariance matrix.So once you've got
that matrix, you'regoing to calculate the
eigenvectors and eigenvaluesof the matrix.And you're going to order
them from largest to smallest,according to the magnitude
of the eigenvalues.And then notational
convention hereis to call the largest
eigenvalue lambda 1.And the eigenvector associated
with it, we'll refer to as V1.And that eigenvector V1 is
really the star of the show,because here in step
for four, eigenvector V1is going to show the
direction of maximum variancewithin the data set.So that's the very
direction we're looking for.If we put our new feature
line spanning that vector,we will capture
maximum variance.So before we apply
these steps to our data,let's just take a moment
to think about thema little more deeply, further
develop our mental modelof these steps.So now, I'm showing on the
screen three different datasets with far more data points
than our two-dimensionalexample we're
looking at, but stilldata sets in just two
dimensions, the feature spaceF1 and F2.And here we have
enough data pointsin each data set to really begin
to see the shape of the datacloud there.And we have three very
differently shaped data clouds.And now I'm going to
superimpose upon eachof those plots the
covariance matrix.And when we think about what
the covariance matrix is doing,it's capturing
important informationabout the shape
of the data cloud.It's not complete information,
but it's telling usthe things we need to know,
for example the directionof maximum stretch
of that data cloud.Because you can see
in each three examplesthose data clouds are
stretched in different ways.So now I'm going to superimpose
over those data cloudsthe eigenvectors we
were just talking about.There they are.And in green, there's the one
we're really interested in,the eigenvector 1.That's the eigenvector
with largest eigenvalue.And we can see that
it indeed is pointingin what appear to be
excellent trend linedirections, those directions
of biggest stretch.We'veGot the second eigenvector, V2,
in each of these plots as well.And those are almost
very difficult to seebecause they're so short.And that's because
in this plot, Ifollowed the convention
that's commonlyused when we're visualizing
eigenvectors in PCA.I've plotted the
lengths of these vectorsto reflect the magnitude
of the eigenvalues.So you can see those
second eigenvectorshad very small eigenvalues.They're very short vectors.Just to get a
better look at them,I plotted them here
as unit vectors,so we can see that orthogonality
that PCA also promises us.So now let's return
to our example datasets and apply those steps
we just talked about.So here, I've shown
our covariance matrix.From there, we can calculate the
eigenvectors and eigenvalues.And over on the left
is the eigenvectorwe're interested in,
V1, and its eigenvalue.So let's plot that
in our data space.You can see V1 looking
like a great trend line.And we'll draw the
line that spans it,representing our new
feature we're considering.And we'll call this new
feature principal component 1.And we can see that it
is an excellent trendline for our data.In fact, three of
our data pointsare falling right
on that trend line.Let's see how they all fall,
as we draw our projections.Look at the dispersion,
they're quite dispersed.We can calculate the variance.We can see we're achieving
variance of 2.038.So as a percent of our
overall variance, it's 93%.And because we used
PCA, we know that thisis the optimal solution.This is the most variance
that can possiblybe captured from this data
set in a single new feature.Before we move on from here,
I just want to take a momentand focus a little bit more
on one number on the screen.That's that variance
number, 2.038.We've seen that number before.Let's take a look at where.Back when we were talking
about eigenvectors.So 2.038 was the value of lambda
1, the eigenvalue associatedwith that first eigenvector.And this is not a coincidence.This is in fact
always true in PCA.And this becomes
extremely helpful whenwe move from thinking
about this PCA,where we're reducing a
two-dimensional data setto a one-dimensional,
one-feature data set.Let's go back to
that example wherewe had a feature set
with 100 featuresand we want to use PCA.So now, we can use
the covariance matrixand the eigenvectors to find
100 principal components,but we have this added question
of how many of those do we wantto keep in our final model.Probably not all 100, because
then we haven't reallyreduced the dimensionality.But do we keep 80 of them?Do we keep 50?Do we keep 5?How do we gauge how
many we want to keep?Well, these eigenvalues
provide the key therein making that decision.Because each eigenvalue
will tell youhow much additional
variance willbe retained when the
principal component associatedwith that eigenvalue
is kept in the model.So just to delve a
little deeper in that,I put together a dummy data
set, or an example data set,with 10 features in it.And we can take a look
at how the process wouldgo, looking at those eigenvalues
and deciding how many to keep.So here's my example
feature set of 10 features.Principal Component
Analysis was applied.It yielded originally initially,
10 principal components,and we need to decide how many
to keep to move into the modelbuilding phase.So here is a visualization of
those 10 principal components.And each one is going
to have associatedwith it an eigenvalue.And remember PC 1 and
lambda 1, lambda 1is going to be that
largest eigenvalueand they're going to decrease
in value as we move downthe list in size.OK, so let's take a look.For this particular example
data set I put together,it's probably useful to know
that the total data set hadvariance 58, and
here, we can seethat the first principal
component has variance 38.So we're capturing in just one
dimension, 66% of the variancethat we originally had
summarized over 10 dimensions.So that's pretty great, right?Let's see how that
improves when weadd the second principal
component to the model.So in this example data set,
the second principal componenthas eigenvalue 9.So put together, if we built
a model off of just these twoprincipal components,
we would beusing a feature set that
had 81% of the varianceof our original feature set.And we can keep going,
add in that third--notice here, we're seeing
a pretty steep dropin the magnitude
of the eigenvaluesas we go through this data set.And that is pretty
typical of many situationswhere Principal Component
Analysis is applied.OK, so if we add principal
component 3 to the model,now we're capturing 90%
of the variance thatwas in our original data set.And we may decide
that's a good stoppingpoint, that's enough for us.We're pleased to be
able to use just threefeatures for model building.So we decide to throw
out all the others.It's just not worth it to add a
whole another dimension to add,say, just 2.5 variance
units captured.OK, so we visualized this here.If you're implementing PCA
in SAS Visual Data Miningand Machine Learning,
it's going to producea number of nice
graphics for youto help explain the
choices made about whichprincipal components
were retained.So I want to take a minute
and look at those graphicsas generated from
this same example dataset here with 10 features.So the first one and you're
going to get is this.So along our horizontal
axis, we justhave the principal components
from first to last, 1 to 10.Our vertical axis is
showing us the magnitudeof the eigenvalues.And what's really
interesting here,what we're looking for
in a graph like this,is sharp elbows in that plot,
points where we're reallyseeing deeply diminished
returns from addingadditional components
to the model.It's just showing us that
extra feature is probablynot worth adding.The second diagnostic
we can look at here,I really like this one.We've again got
principal componentsalong the horizontal axis,
but now the vertical axisis showing us the cumulative
proportional eigenvalue,or a nice way to think
about that is showing usthe proportion of
variance that willbe captured based on the number
of principal components youinclude.And you can see that
when this model was run,I specified we'll keep as
many principal components asnecessary to retain 90%
of the original variance.So that's where that number of
PC selected line is coming in.All right, so one more
topic I want to touch onbefore we wrap up here.And that's the idea of
reconstruction error.Because we've talked
about the benefits,how PCA can be used to reduce
the dimensionality of our dataset, but there's a
flip side to that,which is that we could
potentially have informationloss, and we want to have a way
to visualize and measure that.So here again, I've got our
original two-dimensional dataset up there.That's what we started
with in the 2D featurespace, feature 1, feature 2.And we turned it into
this, a single feature,principal component 1.So now we want to
flip that around.We want to say let's start with
the data values of principalcomponent 1 and try to
reconstruct our original datavalues in the F1, F2 space.And of course, we don't get
our actual original data valuesback.We get an approximation.We're going to get all points
falling on that line, PC 1.So now that's superimposed,
let's look at both thoseplotted at once.And we can see our
original data pointsand our reconstructed
data points.And when we think about
reconstruction error, whatwe're interested
in is the distancebetween our original data points
and our reconstructed datapoints.So that's visualized
with those dotted lines.And those lines should
look very familiar.Those are the same
lines we saw whenwe were projecting our
original data pointsonto the new feature line.But now, we're thinking about
the length of those lines,or the distance
between the pointsas representing how
much information wemay have lost when we reduced
the dimensionality of our dataset.And we want to measure
that mathematically.What we do is we square
all those distancesand average it over
the whole data set.And that's referred to as
the reconstruction error.And our hope is that if
those distances are small,it's quite plausible to
think, well, it wasn't reallyinformation that was
lost, it was noise.We've actually removed
some noise from our data,and perhaps are
creating a new featureset that will better
enable us, enableus to build a better model.But if that reconstruction
error becomes very large,well that isn't quite
so plausible anymore.And we might become
concerned that we've reallylost something that could
have benefited our model.Well, the good news
of PCA is, throughsome lovely
mathematical symmetry,the goal of minimizing
reconstruction error isactually equivalent to
maximizing variance retained.So in finding the features at
the retained maximum variance,we also found the feature set
that minimizes reconstructionerror.All right, so in
closing, just to revisitsome of the main ideas
from our talk here,PCA is a dimensionality
reduction technique.The main goal of PCA is to
find the new feature set whichretains maximum variance.And that's equivalent to
minimizing the reconstructionerror.And we have these
other constraints.The new features found
are linear transformationsof the original
features and they'regoing to be linearly
uncorrelated.All right, well, I hope
this talk was informativeand maybe enjoyable.If this has wet your appetite
to learn more about PrincipalComponent Analysis,
then I highlyrecommend the Springer
text by Jolliffe.It takes a very deep dive
into many aspects of PCA,which we obviously couldn't
touch on in the 20-minute talk.If you're looking
for a shorter read,but something very
mathematically rigorous,I highly recommend
these next two articles.They come at PCA from slightly
different perspectives,but they both have
beautiful derivationsof the linear algebra
proofs for why PCA providesoptimal results, as well as
very interesting discussionon the topic.And if you're just looking
for a little more fun,I recommend this fourth link.This is an interactive
visualizationof PCA in both two dimensions
and three dimensions.So spending some time
playing around with thatcan further develop that
mental model of the process.All right, well, thank
you so much for listening.Have a good day."
143,"Good morning, and thanks
for joining me today.My name is Margaret
Warton, and Iam going to speak today
on Time After Time:Difference-and-Differences
and Interrupted Time SeriesModels in SAS.And this is part
of the work that Ido with the Kaiser Permanente
Northern CaliforniaDivision of Research.So Time After Time, why would
I call it something like this?Well, these studies
designs are basedon an outcome of interest.It's a longitudinal design,
so we're measuring our outcomeover time.And we're looking at changes in
this outcome measure over timebefore and after
an interruption.So in most of these
designs, we compareboth exposed and
unexposed study groups,and we also are able to causally
interpret these results.So the results that
we find, we say,can be caused by
the interruptionthat we're talking about.So there are two
different variationson the same idea of
measuring before and after.We have
difference-in-differences--or what we call D-I-D--which is where the
outcome is measured twice.It's measured in both
exposed and unexposed groups,and it's measured
once before and onceafter the interruption.In the interrupted
time series model,we actually measure our
outcome multiple times.So we measure it multiple
times before and afteran interruption.And the key here is
that the measureshave to be equally spaced.So we have to sort of variations
on the interrupted time series.We have the single
group ITS, whichis where we only look
at the exposed groupbecause they're really the ones
we are the most interested in.But there's also
a robust version,where you use a control group,
essentially, and unexposedgroup to compare to
your exposed groupto find out if there
were changes goingon in the background that might
explain the difference you sawjust in the exposed group
and to account for that.So before and after what?Well, there are
different characteristicsof the interruptions that
we're often interested in.It's an interruption
or shock, where we knowthe timing of the interruption.So there's a single
interruption.It happens at a
known point in time.And generally,
these interruptionsneed to be ones
that you hypothesizewould have an immediate
impact on your outcome.So the outcome changes
quickly and systematicallyafter the interruption,
and it lasts long enoughfor you to measure it.Something that makes a
blip doesn't really count,and it doesn't work well
with these types of models.So some examples would be
a planned intervention,say, in a health care
setting, some typeof operational change--maybe moving to a new lab test--a natural disaster, and
new laws that are enacted.So lots of these
studies are doneon things like helmet
laws or seat belt laws,that type of thing.So for our example
study today, we'regoing to be talking
about insurance benefitchanges and patient costs.So we want to control
patient health care costs.Everybody's been
wanting to do that,and employers want to
control their costs too.So a lot of employers
are moving to offeringhigh deductible plans,
which have lower premiums--and so cost the employers less.However, when they do this, our
patients and their employeesincrease their
out-of-pocket costs.So employers can offset these
increased employee costsby maybe passing along
part of that lowerpremium to their employees,
but they can alsoadd a value benefit insurance
designer-- or VBID plan.And what VBID plans do are
they reduce patient costsfor proven,
effective treatments,preventive measures,
and procedures.And the idea is that by putting
these plans in and encouragingemployees to use these
effective treatments,the employees and patients stay
healthier, the insurer paysless later, and, therefore, we
can prevent health issues downthe line and reduce costs.So the cohort we're going
to be talking about todaywas a cohort of patients
in our health planwho had a zero deductible
health plan in 2013.In 2014, their
employers switched themto a high deductible plan.They didn't have a choice.And all of these members
had continuous coverageand continuous
drug plan coverageand were actively
taking medicationfor a chronic condition
like diabetes, hypertension,and high cholesterol.And the VBID pharmacy
plan that we alsobegan offering in
that year includedfree medications for chronic
conditions like diabetes.So the patients could actually
get the most common medicationsto control these chronic
conditions absolutely free.So in 2013, both arms of
our study had VBID coverage.In 2014, both arms
of the study wereswitched to a high
deductible plan,and only one of those
arms got the VBID benefit.So we had about 1,500
folks with no VBID in 2014and about 1,000 folks
with a VBID in 2014.And we're going to call the
people who got the VBID in 2014VBID, even when they
didn't have the bid.So they're the VBID
arm of our study--the exposed arm.So basic design, we have
the two exposure groupsare VBID and no VBID bid.And the outcome we're
interested in looking atis out-of-pocket medication
costs for all medications,not just for
chronic medications.And we want to see how much
this changed from 2013 to 2014.Because going onto a
higher deductible planmeans that drug costs
are going to increase,and we wanted to see
whether the VBID plan helpedmitigate that increase.So our interruption happened
on the 1st of Januaryand 2014, where both
groups experiencedthis new high deductible,
but only one groupgot the VBID plan to
help them offset costson their prescriptions.So first, we're going to look
at this with a D-I-D model,and this is what we call
the In Greek slides,where you can see all of
the equations and the graphsthat sort of explain
the equation.So here, beta 0 is
our intercept term.It's the amount in our
example that the patientswould be paying in 2013 who were
in the unexposed group-- the noVBID group.Beta post indicates
the slope of the--or the amount at
the total cohortincreases between 2013 and 2014.Beta exposed tells you how
different the exposed groupwas from the unexposed group.And in our little graph
here, the unexposed groupis the blue line,
and the exposed groupis the dashed red line.And we can see that the
exposed group is starting lowerthan the unexposed group.And then the interaction
term tells usthe difference in slopes
between the exposedand the unexposed groups
between our two time points.So one of the things that's
important to do when you'redoing a D-I-D model is to take
a quick look at the raw data.And we can see here that we
have the VBID group, whichis the red dashed lines and the
no VBID group, which is blue.And it's pretty
obvious, just lookingat this, that we've got the
VBID group has lowered costsoverall.They appear to have a little bit
less in the way of variability.You can see the blue lines tend
to have much steeper slopeseither up or down compared
to the red lines in general.And another thing
to do with the D-I-Dis make sure that the
difference that yousee in your trajectories
between the two study pointsthat you're interested
in is not somethingthat had already been going on.So I did here was I graphed
2012, 2013, and 2014 reallyquickly.You can see that there's
really no differencein the slope between the red
and the blue lines between 2012and 2013.But then at 2013 and 2014,
we see some divergence.And so this is an indication
that it's a reasonable thingto do to use this for a
D-I-D. So should we D-I-D it?We have no sign that
those slopes differedbetween our groups before
the study period began,and so D-I-D is likely
an appropriate modelfor this analysis.However, we might want to
do some propensity scorebecause the covariate
distributions may not be equal,and maybe that's what's
causing our baselinedifference in costs, where that
red line is so much furtherdown than the blue line.And I did actually
do the examples herewith some weighting.So I did some propensity scores.I'm not including that
here, but the codeand the results of
the propensity scoreare in the paper.So here's an example of
what your data shouldlook like for D-I-D modeling.If you look
carefully, you'll seethat every study
ID has two rows,and we have a flag for whether
the member was VBID or not.For study ID 1004, both of
those have an exposed flag.Because even though they
weren't exposed to VBID in 2013,we're still calling them
exposed to differentiate themas a group.And then in post, we
have zeros and onesindicating whether or not
this is the post period.So 0 is 2013, 1 is 2014.I have my propensity score
average treatment effectweights in a column, and
then we have the overall costfor the year that the patient
had in out-of-pocket costs.So this rather busy little
slide is a quick macroto run the models
weighted or unweighted.You can see we're using
PROC MIXED because wehave a continuous outcome.And we've got the
post and the exposedvariables as class variables.The model statement uses
the little handy shorthand,the post pipe exposed
to give us POST EXPOSEDand the interaction term.I've requested the solution.And we're using a
random intercept modelwith the study ID
as our subject.I'm using unstructured
covariance matrixhere because we have a large
enough sample to do so.And I have a little
WTSTRING macrovariable that allows us to put
in a weight statement or not.And I've requested the
least squares meansfor all of my variables with
the differences in less squaresmeans.And then I'm doing a quick
estimate of the D-I-Dto give me the values
for the actual differencein the slopes.So you can run this macro
either weighted or unweighted.And I ran it weighted,
and these are the results.So our beta 0, which
is our intercept term,is $238.39, which is the
mean cost for the medicationsfor the non-VBID group in 2013.The beta exposed
is the differencefrom that in the VBID group.And so our VBID group
actually started outpaying $73 a year less on
average for their medications.Both groups would
increase by $38.76when they moved to the
non-deductible plan.However, the VBID group actually
drops $44.34 in comparisonto the no VBID group.So they actually had a
slight decrease in costs.And then the least
squares means gives ussort of the averages
for these groups.So are no VBID group
went from $238 up to 277,whereas our VBID group went
from $165 down to $159.So this is an indication that
in the big group, the VBIDitself did actually offset
the increase in cost,whereas in the non-VBID
group, it did not.So let's move on to the
interrupted time series.And this is a
figure from a paperby John Adams, who is
a biostatistician downin our Southern
California group.And this basically
indicates, whenyou have two exposure groups
in a robust test, whatthe equation is
going to look like.So the top line here, we
have our unexposed group,who are gradually increasing
between months 0 and 9.And I didn't put all
of the little dotsin because these are just
least squares regressionestimates of all of
the different measures.So we have one measure
at every time point.And so in the first
nine time periods,we have a gradual increase in
both the exposed and unexposedgroups.And then you can see at month
9, our interruption point,we have a jump.And the top group actually
starts increasing even fasterafter this jump, and the bottom
group decreases after the jumpinstead.And so all of
these betas in hereindicate the different
sizes of the jumps,differences in starting points,
and differences in slopes.So moving on to
the equations here,the single ITS is basically
only the top sectionof the illustration.So all we're looking at is
a pre/post slope and a jump.And you can get that from
this equation with the beta 0through beta 3.And it's very
similar to the D-I-D,except that you have this beta
2 around the interruption thatgives you how much the
interruption actuallychanged toward the baseline
from where it was in period 9to where it jumped
to in period 10.Then the robust ITS equation,
we put both of thesetogether so that
the first row isexactly the same
as the single ITS,but then we start adding in
all of those same variablesfor our exposed group.So we have beta 4
exposed, which tells ushow different the
starting points are.Then we have the
exposed times timeto give us the
slope, exposed timesinterruption to give
us the jump, and thena triple interaction term with
exposure interruption and timeto tell us how different the
slopes are between the twogroups before and after.So again, it's always useful
to take a look at your datain a random sort of way.So I grabbed 30
random study IDs.And for this
particular one, we'regoing to use a slightly
smoothed outcome.We're going to look at
a three-month rollingaverage of medication costs.The reason I'm doing that
is because at Kaiser, youcan often get a 90
to 100 day refill.And so oftentimes, you'll see
a jump in one month, and thentwo months, and nothing
and a jump in one month.So we're using a 30-day rolling
average to get our monthly costand smooth it out a little bit.And again, we can see it looks
like maybe these VBID folks arestarting out at a lower
value and that thereis more variability
in our non-VBID folks.So I also plotted the means
of these month-by-monthso you can see.And it looks, again,
just like the random.There's more variability
in our no VBID group,and it looks like the bid group
does tend to be lower overall.So this is the summary picture.There's a little break
in here because we took--what I did was I removed
a couple of monthsas the interruption
was taking placebecause we had some difficulties
with our pharmacy costswhen we first roll this out.So we figured that we would
have a better view of itif we took out that extreme
variability in those monthsin the middle.So here are the data for
the single ITS model.We have our study ID, and
you can see that each studyID has 18 different measures--9 before and 9 after.We're starting with month
4 because we're usinga three-month rolling average.And when I label the
months gap month,that is actually just the number
of months, starting with month1, going up to month 18.You can see month 10 is where
the intervention starts.And then in the far right, we
have the three-month rollingaverage for all
medication costs.And then down at the
bottom, you couldsee we start with
the next study ID,starting, again, with
gap month number 1.And so this is the
code for the PROC MIXEDagain, with our single ITS.It's pretty similar to the
D-I-D. We have our outcome,we have our month
counting variable,we have an intervention flag,
and we have the interactionterm between the two.Again, I'm asking
for the solutionsmy repeated statement has my
study idea as the subject.And I'm asking for the
pre-period slope, the postperiod slope and then
the pre/post gap.And note here that
the interaction termhas the value of 10,
meaning the first measureafter the gap is month 10
in this particular data.So here are the results
from this model.We have our average
monthly cost at baselineis $12.67 across the cohort.And of course, again, we're
only looking at the VBID folkshere because they're the
ones who had the interruptionthat we're interested in.And as we see sort
of from the graph,the cost is kind of drifting
up a little bit over time,maybe $0.08 a month.And then we have this big jump
of $10.41 at the interruptionpoint, which is
offset, actually,by 10 times the minus
0.99 interaction term.And so what this means is
that our overall pre/postgap for these VBID folks
is only $0.63 a month.So again, the
conclusion we woulddraw from the single yes
looking just at our VBID folksis that the VBID definitely
offset the increase in costthat they would experience
due to the changein their deductible.So very simple
additions to the dataset in order to look
at a robust ITS model.We add our exposed and
unexposed groups both in here.So our first person is
an unexposed member.Again, we've got that
count from 1 to 18,and I've put the
weights in here as well.So that just adds those two
columns of to the right.The very bottom, you can see
is one of the exposed folks,who starts out in month 1.And that's all you
need to do, is justto add that exposed
flag and add your weightif you're using them in order
to run the robust ITS model.And so here's the code for that.Again, it's really similar
to what we've seen before,at least at the very first.And then we add in all
of our exposed groupvariables times the gap
month, and the intervention,and the triple interaction term.This time, I'm weighting
them by the ATE_WTs,which is the average
treatment effect weights.Study ideas my repeated
subject to study ID.And we run this model and end
up with these 8 coefficients.And we can see that,
on average, whenwe add in these non-VBID
folks into our cohort,the intercept goes up to 18.65.The non-VBID folks,
with their beta1, look like they're
increasing about $0.13 a month.And that is actually
significant,whereas our exposed
group, looking at beta 5with exposed times, time
is actually going downby about $0.09 a month.And that is not significant,
so it's probably just 0.And once you put all of
these different coefficientstogether, you can
generate a figurelike this with
the estimates thatgives, to me, the clearest way
of interpreting these results.So we start out with
our no VBID group.Our blue line starts
at this $18.65and climbs up to about
$19.79 by the timethe interruption happens.It's a pretty steady increase.And then after the
interruption happens,it looks like their
medication costs, they go up,but then they stay
relatively stable.We don't see those medication
costs increasing againafter the jump.In the VBID group,
however, we see somethingthat's a little bit different.We see that their initial nine
months, they didn't reallyincrease all that much.They only went from about
$12.88 to $13.23 on average.And so they had a jump--maybe not quite as big as the
jump in the non-VBID group--but then their costs started
going down dramatically.And because we're using a
three-month rolling average,the reason we're seeing
this dramatic decreaseis because, at first,
they weren't refilling,and then when they started
refilling those refills,it got less and less expensive
because they were takingthese chronic medications.And so, basically, it
tells the same storyas we've seen before, that
the VBID plan definitelyoffset and even really decreased
the cost of medicationsfor this group.And one of the reasons that
it took us a little whileto figure this out--
why the baseline, whyall of our weighting
and stuff didnot actually change
the baselinesand make them more similar--
is because the factor thatis different between
the two groupshappened to be strong
union leadership.And what happened with
this group was their uniontold them all to go home,
and look in their medicinecabinets, and choose
every medication that theyhad in there, and then
they asked for thatto be put on the VBID list.And so that probably explains
why this drop was so dramatic.So that's just a
very quick overviewof the difference-in-difference
and ITS models.In conclusion, these are
really useful study designs,especially for those of us in
the health care research area.They have a causal
interpretation.We can actually
say that that VBIDcaused their costs to decline.These are really important
tools for analyzingthe effects of things
like operational changes,but also natural
experiments and law changes.And the other great
thing about thisis they're really accessible
data sources these days,so you can often
use it in existingadministrative data like the
electronic health record.You can do a retrospective
data collection, whichreally lowers your study costs.And often, you can
do these studieswithout involving members
of the health plan directly.You can really
reduce the overheadand the operational burden of
doing these types of studiesand yet still come up with
very important results.So thank you for
your attention today,and you're welcome to
contact me at this emailhere anytime with any
questions you might have.And I highly
encourage you to readthe paper, which
has far more detailthan I was able
to present today.Thank you."
144,"ASHLESHA: Hello, everyone.Adolescence, a period of
transition between childhoodand adulthood, is one of
the most challenging phasesof life.It includes some big changes
to the body and the waya young person
relates to the world.Our topic of presentation
today, analyzing the factorsimpacting suicidal
behavior in American youth,looks at high schoolers.This work has been
performed by Data Jockeys,namely Mounica, Nitesh,
Venkat, and myself, Ashlesha.Here is a brief outline.I'll begin with the
introduction and give youa brief overview of the data.Nitesh will be
presenting our analysis.And he'll be closing
the presentationwith few recommendations.But before going
into the details,here is a small disclaimer.This presentation
addresses sensitive topicsthat may make some people
feel uncomfortable.Viewer discretion is advised.Let me begin with a quote from
Sam Finch, a suicide survivor.""Imagine a scale being
tipped back and forthuntil one side is finally
outweighed by the other.A trigger, a moment
of impulsivity.A window of opportunity that
disrupts the precarious balancethat allowed us to survive.""That, ladies and
gentlemen, is suicide.Suicidal thoughts
are like snowballsthat turn into an avalanche
that can wipe out anyone.Suicide is the second leading
cause of death among youth age12 to 19 in the United States.In 2017 alone,
nearly 6,000 teenshave parted ways
with their loved onesand left the world by
committing suicides.That innocent mind
is very delicate.In today's world, where
technology plays a key rolein our day-to-day life.And people live their
life on social media.Unfortunately, there are more
ways in which one can hurtthemselves, including games
such as rumoured Blue Whalechallenge, where people are
challenged and encouragedto commit suicide.With this study, we would like
to help understand and identifythe factors that are influencing
these suicidal tendenciesand hopefully
contribute actuallyto the society in reducing them.Data used in this
analysis was providedby Center for Disease
Control and Preventionas a part of the Youth
Risk Behavior Survey.The CDC conducts a survey
on high school kids,grades 9 through 12,
bi-annually since 1990.It has 99 questions.And the latest data
available on CDC websitewas of the year 2017.144 school districts
participatedin the 2017 survey.And nearly 15,000
survey responses,ranging from demographics,
drug and tobacco consumption,alcohol use to sexual
activity have been captured.There were missing values in the
data you due to non-responses.These missing values
have been imputedusing SURVEYIMPUTE in SAS, which
uses [INAUDIBLE] imputation.[INAUDIBLE] information
from multiple variablesinto a single variable
for the ease of analysis,four new dichotomous
variables have been created.One set feature
that was created wasbullying which
indicates the person wasbullied at school or online.The second feature
that was createdwas abuse of narcotics
which indicates a usageof non-recreational drugs.The third feature
was sexual minoritieswhich indicates the person
identified as LGBTQ.The fourth feature
was suicidal attemptswhich tells if a person had
attempted to suicide or not.A trend analysis of the
suicide of risk behaviorshas showed a statistically
significant increasein the following, a significant
increase in the number of teenswho claim to have
suffered depression,a significant increase in the
number of teens who seriouslyconsidered suicide, and
a significant increasein the number of teens who
actually made suicidal plans.Although we can
see an upward trendin the percentage of teens
who attempted suicides,the data did not show
statistically significantincrease.However, it should
be acknowledgedthat the actual number of
teens attempting suicidescould be higher due
to associated stigmapreventing an honest
response on the survey.Of all the teens who
were surveyed in 2017,it was seen that more girls
attempted suicide than boys.Moreover, it was seen that more
kids from 9th and 10th gradesattempted suicide than the
kids from 11th and 12th grades.Furthermore, the data showed
that more African-Americansattempted suicide than
of any other race.Overall, this indicates
that girls, younger teens,and African-Americans are more
vulnerable to attempt suicides.Our initial exploratory
data analysis alsofound the funnel of despair.Of all the teens
who were surveyed,45% of the depressed teens
had suicidal thoughts.29% of the depressed
teens made suicidal plans.And 16% attempted suicides.This means 16 out of 100 people
who are undergoing depression,are taking the extreme step
towards attempting suicides.According to American Foundation
for Suicide Prevention,it was reported
that, in 2007 alone,nearly 6,000 teens
have committed suicideswhich is a significant number.And taking appropriate
measures can definitelyprevent these suicides.Given the seriousness
of the issue,one might be wondering, what are
the common characteristics themon these teens who are
attempting suicides?So a latent class
analysis was performedto understand the same.From the analysis, it
was seen that therewas a high probability of
depression in all the threeclasses.Moreover, two classes
were characterizedby high probability of teens
who had been sexually assaultedand bullied, respectively.No other distinguishing
characteristicswere found in the
third class whichis a depressed class,
which indicatesunknown causes of
depression, or requires morecomplex analysis or more data.In all, 20% of the teens belong
to sexually assaulted class.And 40% each belonged to
bullied and depressed classesrespectively.Furthermore, in
profiling, it wasseen that the sexually
assaulted class had the highestpercentage of teens attempting
suicide multiple times, morethan four times on average.Now, I would like to
request Nitesh to continuewith the further analysis.NITESH: Thank you, Ashlesha.So far, we have
understood that thereare specific factors
which pushed teenagersto the brink of suicide.However, not all the factors may
be directly related to suicide.In order to find the factors
that are drivers of suicide,we used the PC algorithm to
show the causal associationbetween the variables.Here, we have a graph that
shows variables that directlyinfluence suicide attempts.Moreover, we found that
some of these variableswere co-occurring.PROC SURVEYLOGISTIC
was used to quantifythe importance of
each direct driveras found by the PC algorithm
from the previous slide.We found that depressed
teens were eight times morelikely to attempt suicide.But we know that depression may
not be a root cause in itself,but the symptom.Thus, we removed depression
from our analysisand built our final model.We found that the most
important variablewas sexual assault. A teen
who was sexually assaultedwas nearly five times more
likely to attempt suicide.Bullying was seen to be the
second most important variable,where a teen who was bullied
was nearly four times morelikely to attempt suicide.Next, we found that teens
who abuse narcotics,such as cocaine, heroin,
and other hard drugs,were nearly 2.5 times more
likely to attempt suicide.Finally, we found that teens who
belonged to sexual minoritieswere nearly twice as likely, and
those who perceived themselvesas obese were nearly
1.4 times morelikely to attempt
suicide than their peers.In summary, we have seen
three distinct classesthat were observed
in the data, analyzedthe cause-effect relationship
between the variables,and built a model
than to quantifythe effect of these factors
in suicide attempts.We have also seen that
not all of adolescentswho attempt suicide do
it for similar reasons.There are three distinct
classes observed.One was characterized by
sexual assault, anotherby bullying, and
lastly, by depression.With this, we move on
to our recommendations.The US government currently
uses a multi-level frameworkto deal with suicide prevention.We believe that
data and context canhelp improve existing policies
and spend funds effectively.Having contemporary teen
icons such as Billie Eilishtalk about suicide
prevention couldbe more effective than spending
millions on generic mass mediacampaigns.Creating an atmosphere that
teens voluntarily seek helptakes time to achieve.We propose the creation of a
data-driven application thatuses analytics to identify
teens that fall outof normal behavior patterns,
such as falling grades, missingclasses, skipping lunch,
among other things.This will enable
the identificationof high-risk teens for
an early intervention.Teens who are sexually
assaulted go down a rabbit holewhere they suffer
from depression,feel inadequate, abuse
drugs, and attempt suicidemultiple times, a
clear cry for help.Creating a safe space at
schools where they can seek helpwithout fear for social
stigma, in conjunctionwith the aforementioned
app, can prevent these teensfrom going down the rabbit hole.There is a lot of focus
laid on kids being bullied,but what about the bullies?Studies have shown that
bullies themselves oftensuffer from domestic issues
and misinformed notions.School programs that can
identify and address the reasonfor these behavior problems
can be more effective than justplain punishments, which might
make bullies more aggressive.Finally, depression
is a serious issuethat often gets
swept under the rugs.From our analysis, we
found that depressed teenswere eight times more
likely to attempt suicide.Promoting the notion that
depression is a disease,just like the flu, could
make teens seek help sooner.We believe that
additional data canhelp improve our understanding
of suicidal behavior.Inclusion of
socioeconomic data canhelp quantify the impact of
wealth on suicide attempts.A kid who grows up in
a rich neighborhoodis probably better equipped
to face suicidal thoughtsthan a kid who grows
up in a poor suburb.Social acceptance
data can help quantifythe impact of
people's perceptionson suicide attempts.An LGBTQ teen who goes to
school in a liberal neighborhoodsuch as in coastal US faces
less social stigma thena teen who goes to school in a
more conservative neighborhood.Knowing the sources
of negative mediavulnerable children
are exposed to canhelp shut them down faster.For the rumored Blue
Whale Challenge,it was alleged that
bk.com, a Russian website,was a big source
of suicide games.Some of the points we
recommended may seem generic,yet why is it important
to talk about them?Because data is the new
weapon for policy change,the sickle of the modern
day revolutionary.We know that many school
systems are low on fundingand may not be very quick
to prevent suicides,but when we can show
how the lack of fundingcould kill children,
we can hope that thatwould promote voters to strive
for a data-driven policychange.Suicide doesn't
take away the pain.It just gives it
to someone else.We appreciate you
for taking the timeto listen to our presentation,
and we'd love to hear from you.Please reach out to us with
any questions or feedbackon the below email addresses.Thank you, and stay safe."
145,"KARIN KOLB: Thank you for
joining us for our presentationof the plight of the honeybee.I'm Karin Kolb, and my
co-presenter is DaMarkus Green.We have one more member of our
project team, Rachel Bishop.We are all graduate students
at Kennesaw State Universityin the Masters of Science in
Applied Statistics program.All of us are guilty of swatting
away a bee if it comes near us,but we must be mindful of
the importance of bees.They are actively pollinating
the majority of our food crops.In the United States alone,
14 billion in crop valuecan be attributed to the
honeybee's pollination.And of course, we
rely on honeybeesfor their production of honey.DAMARKUS GREEN:
Even though honeymay be a big export
of the honeybee,it's also responsible
for a lot of crops--responsible for
various crops as wellsuch as watermelon, different
vegetables, differentcrops nationwide.So if bees go extinct,
there will be a decreasein these crops as well.So that will have an impact
on the economic value, food,everything.So the honeybee is very vital
to the nation's backbone.So we wanted to see if the
pesticides that farmers useare having a big
effect on the honeybeebecause they're using
these pesticidesfor different reasons,
but it's havinga negative impact
on the honeybeesas our underlying reason.So we wanted to see how these
pesticides are affectingthe honeybee population.KARIN KOLB: The
production of honeyis the evidence of
healthy, active bees.Since it is a measurable
entity, it can be analyzed,and then conclusions can be
drawn about the viabilityof honeybees.The United States
started widespread useof neonic pesticides
on crops in 1995.So there's data
available now to analyzethe effects of the use of
some of these pesticides.This analysis could
reveal if these pesticidesare contributing to the
decrease in honey production.DAMARKUS GREEN: So for
our research question,we wanted to see if the specific
use of neonic pesticideshave negative consequences
for honeybee productionand their total yield as well.We chose to look at five
specific neonic pesticides,and my co-presenter
will tell youabout those in a little bit.KARIN KOLB:
Neonicotinoid pesticidesare insecticides that are
derived from nicotine.They're the most widely
used class of insecticide.As of [INAUDIBLE] they were
being used in 120 countries.These pesticides
work by attackingthe central nervous system of an
insect, which eventually causesparalysis and leads to death.Three of the neonics that
are included in this studycan be seen in this diagram.Imidacloprid,
thiamethoxam, clothionidan,and two additional ones are
also included in our study:acetamiprid and thiacloprid.As we continue the presentation,
we will refer to these neonicsby the two letter
abbreviations IM, TH, CL, AC,TI respectively.DAMARKUS GREEN: And
these pesticideshave already been seen to
have some type of impacton the honeybee colony count
and the honey bee yield.We saw in a previous
study that a littleover 31% of the honeybees
exposed to the pesticidefailed to return to their hives.So that means that
they died whenthey came in contact
with these pesticides,meaning that their colony
count was affected,and in turn, the honey bee
yield and the honey yieldwould be affected as well.And if they are coming in
contact with these pesticides,and they are actually able
to return back to the hive,they're going to have a more
long term effect on the colonycount because
they're going affectmore of a widespread area.Honeybees already
in the colony willbe affected, meaning
the honeybeesthat go to different colonies
will be affected as well.So the colony count,
colonies are being affected,the honeybee yield
will be affected,and the honey yield will
be affected as well.So them coming in
contact with pesticideshas a very long term effect.KARIN KOLB: The data set
used for our analysisis provided by the National
Agricultural StatisticsService.The US Department
of Agriculture usesthis service as their
primary reporting source.This data set also
includes resultsfrom the US Geological Survey.In all, we have 20 years of data
about honey production, colonycount, and pesticide usage
for most of the United States.DAMARKUS GREEN: For
our first graph,we chose to use a heat map
that shows the pesticideusage across the country.If you see bar legend, as it
goes from burnt orange to blueis the heavier that it gets.As you see, mainly in
the South and the Eastand somewhat in
the west Midwest,it's all about the same.They use about the same
amount of pesticides.So the pesticide usage is
pretty common in those areas.But when you get the Texas,
California, the bigger states,and then you get around
the Great Lakes region,you see that they get to
the blue, or the heaviest.So we see California,
Texas, Illinois;they are the big users
of the pesticides becauseof the size of the state,
and it may have somethingto do with their crop
production as well.And then based off of
that pesticide usage,we show a heat map of where
the yield and the colony countdecrease based on that usage.You see that the darker regions
that we saw in a previous heatmap with California, Texas,
those regions, and the GreatLakes, they have more years that
the yield actually decreased.So you see that the
pesticide usage and yieldare showing hand-in-handKARIN KOLB: The USDA
provides a websitefull of information pertaining
to the agriculture industry.There is a section dedicated to
their pesticide data program.The addition of an interactive
tool such as a decision treecould be a resource
for concerned growersto access when making a decision
about which pesticide to use.A predictive model
could give growersinformation to make decisions
about which specific pesticidethey should use to reduce
potential consequencesto the honeybees, to
increase pollination,and most importantly to them,
to increase their crop yield.Observations for 1994
in our data set onlyincluded data about
the annual honey yield.So the data was
used as the controlfor the amount of
increase or decreasein the pounds of honey produced
for the following year.Prior to developing the
decision tree model,a new target variable was
created called effect.Two binary variables, production
decrease and neonic increase,were also created to be used to
determine the value for effect.Production decrease compared a
year's total honey productionto the previous year's
total for each state.If there was a
reduction in the yield,then the observation was
assigned a value of 1.Neonic increase compared
a year's neonic usageto the previous year's
total for each state.Since there was no
information for 1994,a quantity of zero kilograms
was used for the controlfor the 1995 comparison.If there was an increase
in pesticide usage,then the observation was
assigned a value of 1.If both production decrease
and neonic increasehad a value of 1
for an observation,then effect was also
assigned a value of 1.This was based on the
assumption that honey productionis negatively affected by an
increase in pesticide usage.We hope to develop
a helpful toolto determine which
pesticides wereleast harmful in each state.But a decision tree model did
not return the results needed.It only return two of
the five pesticidesas important
variables: TH and IM.DAMARKUS GREEN: After we
built the decision tree,we decided to kind of look into
building a predictive model.And this can be used for
the National Farmers Union.It will be a comparison
model based on the pesticidesyou use.So maybe our end
term goal would bethat farmers would
look at this modeland determine,
""Maybe I shouldn'tuse as much pesticide
in this regionbecause it's having
a wider effect.""So we chose to use regression
to build this predictive model.And as we were
building the model,we chose to break
our data set downby region because different
regions had different climates.They have different crops.So you want to look at it
region-by-region based.The first region we're going
to look at is the south region.In the South, you see
that all the pesticidesare significant on the total
yield and the colony countas well.You see that CL, IM, TH,
and AC pretty much allhave a negative impact
on the total yield.But you see that TI and
TH have a positive impact.This can be something that's
seen as a positive thing,meaning that the
colony count grew,or it can also be
seen as a deterrentto the growth of the honeybee
because there's no telling howthey would grow without
coming in contactwith these pesticides.But you see all the significant
impacts on the total yieldalso have a significant impact
on the colony count as well.The next region that we
have is the west region.And we see that in the
West that IM and CL havea negative impact
on the total yield,and TH again we see showing
up as being significantand also has a positive impact.So it may be that the
TH pesticide mightbe a good thing, might
be a helpful pesticidein the growth of the honeybee.But you see that IM
and CL are actuallyhaving a negative impact
on the total yield,and again that you
see anything thathas an impact on the total yield
will also affect the colonycount as well.So it'd be nice for
farmers to look at.Maybe the TH pesticide might
be the only thing that'sbeneficial for the honeybees.KARIN KOLB: As we continue to
look at the specific regions,we see that in the
Midwest, TI and CLhave significant impacts, with
TI having a negative impacton honey production,
while CL hasa significant positive impact
on honey production and colonycount.And finally, we see
that in the Northeast,none of the
pesticides are havinga negative impact on the honey
yield or the colony count.In fact, we see
the two pesticides,TH and TI were having a
significant positive impact.A positive impact
might be somethingthat should prompt further
research in that region.DAMARKUS GREEN: So
looking at our results,we see that in the South,
all the pesticides playsome type of significant
role in the yieldand the colony count as well.We see that TH,
it played a factorin pretty much
everything, and THwas the only one that pretty
much showed a positive impacton all regions.So maybe TH might be
the only beneficial oneof all the pesticides.So the other four, they might
need more further researchto determine maybe we need
to throw this one out,maybe we need to
tweak something,maybe we need to
tweak the usage.Something needs to
be done because it'saffecting these honeybees.And also with our
further research,we want to see the
climate, what kind of cropsthey're using, what
kind of soil they have,because these factors will
play some type of rolein affecting our colony count
and affecting the honey yieldas well because of how these
pesticides are being used,what kind of bees they
have in this regionwill have some effect on the
reaction to these pesticides.And we're doing this
because honeybees may notseem like a significant
factor in this country,but as the research
shows, they havea huge impact on everything
that goes on in this country.So we want to be able
to research this, hearthe results, and use these
results to help farmers,to help everybody to determine
we need to be doing somethingbetter to increase honeybee
yield and save our honeybees.KARIN KOLB: These are the
four articles and websitesthat we gleaned information
and data from for this study.The appendix includes
further informationabout the different
pesticides that westudied in this project.Thank you for watching
our presentation.DAMARKUS GREEN: And
feel free to email usif you have any questions."
146,"Hello, everyone.My name is Deanna
Schreiber-Gregory,and my presentation is
on A Doctor's Dilemma--How Propensity Scores Can
Help Control for SelectionBias in Medical Education.Here's my bio.I won't go into any
detail about that.You can always read it later.So this is an overview
of the presentationso you can jump to any
point in it if you'd like.However, I do recommend
listening to itin its entirety.So first, I'm going to
go over an introductionto propensity scores
for those of usthat aren't really familiar with
what this type of analysis is.I'm then going to talk about
the current study breakdown.So what this study in
particular is about,and what the question is, and
what the variables look like.We're then going to talk about
propensity score creation-- howto create a propensity score.And then we're going to go
onto, after creating it,what kind of analytic
methods we canuse with a propensity score.We're then going to
wrap it up by lookingat the different
model comparisons.So we're going to look at
each of the different analysisand compare how they performed
against our original model.And then we're going to
have a conclusion, summary,and a wrap-up.OK, so I'll begin
with the introduction.So propensity scores.As we know, there are two main
types of research methods.So we have randomized
control trialsand observational studies.An advantage of randomized
controlled trialsis that they have
randomization, which is greatbecause we can actually get more
of a causal relationship outof a type of study
that has randomization.Observational studies,
on the other hand--they don't have that advantage
of having randomizationbecause a lot of them
you can't actuallyrandomize people into different
groups for ethical or justfeasibility reasons.One thing that they do have,
though, is generalizability.So because we're
observing peoplein their actual environment
or observing the effectsin their actual
environment, we can actuallyget a little bit better
generalizability outof these types of studies.The problem with having a study
that doesn't have randomizationlike I mentioned
earlier is that wearen't really able
to pull causationfrom these studies,
which can result--which in essence, can
result in a selection bias.So we're not able
to say that we haveequal types of groups in our
different treatment groups.They can have their own
little quirks or relationshipswithin those groups
that kind of makethem more part of those groups.So when we do have
this issue, wecan use a variety of different
statistical techniquesto help control or
to help with the factthat we're not able to
control for randomization.Some of these techniques
that can accountfor these differences
are matchingstratification and regression
adjustment techniques.This can be limited,
though, in useof these techniques if you
have too few covariatesin the adjustments.Propensity scores
is one way that youcould utilize these techniques.A conditional probability
of being treated or includedin a particular group based
on identified covariatesis what makes up their
propensity score.Propensity scores
also do a great jobof accounting for imbalances
within the different treatmentgroups, which tends to
resemble randomization.It's basically kind of called a
psedu-randomization technique.And then in essence, what a
propensity score ends up doingis summarizes the
covariate effects into onescore that you can then add back
into the model to help controlfor that lack of randomization.All right.One thing to note is that
even with propensity scores,we do have a set of assumptions
that must be supportedor must be present
within the modelbefore we're able to use a
propensity score analysis.These assumptions are the
stable unit treatment valueassumption, positivity,
and unconfoundedness.Once we have these
assumptions metand-- so you've tested
for these assumptionsand you've determine that these
assumptions have been met,then a propensity score can be
used as that balancing score.If these assumptions
are not met,I don't recommend
using propensity scoreswithin the model, because
of course, you probablywould get a result that is
not consistent with whatthe propensity score uses
would be intended to do.So please, just test
for these assumptions.And then if they're
right, then youcan move on to actually creating
and utilizing propensity score.So the creation of a propensity
score is actually very easy.It can be created in two steps.The first step is to
calculate the propensity scoreas the probability of a
patient or participantinclusion in a specific group.That can be done through
several different methods.One of the most popular
is through proc logistic.And then once you've
created that score,just include it in your model
through one of the techniquesthat we'll discuss in
today's presentation.The method of logistic procedure
in the creation of propensityscores, like I said, is one
of the most popular waysof doing it.The logistic model itself
provides a descriptionof the relationship between the
variables, which allows for usto create the propensity score.Logistic regression is then
used to predict the probabilityof an events occurring ,
and then pushes that outinto a variable, which we
can use within the model.If you don't want to use
a logistic procedure,there are a couple of
alternatives to usingor to creating and
utilizing propensity scores.That would be through
the proc GENMODprocedure using the
output statementand predictive keyword.There's also the PSMATCH
procedure, which is very new.The reason I don't
use that procedurein particular in our model--I do talk about it in the
paper and how it's set up.But in this presentation
I don't talk about itbecause for my particular
version of SAS,I really didn't
have access to it.So I wanted to be sensitive
to people who might nothave access to it and provide
this alternative way, which,you know, all of us
should be able to do.All right.So now that we've gotten
a good introductionand how to create
the propensity Swe're going to talk
about our current study.So this study, I
want to state againthat it does not use
real data, but itis based on analysis of real
data from US medical schools,and the structure, the
specific structure of coursesthat one US medical
school in particular uses.The data was constructed to
represent about five yearsof medical student data.The reason I didn't
use real data--actually, I have a
lot of reasons for it.But the main one
is that I wantedto protect the identity
of the studentsand the implications of
this particular analysis.Since this is a theoretical
exploration thatwas going to be put online
and presented at a conference,I wanted to make sure that it
was strictly just theoretical.I didn't want anyone pulling
any actual meaning from it.I just wanted to
perform this analysisto make sure that
it was feasiblefor me to use propensity scorers
within the question that I had.I didn't want to use the
real data to do that.So I created my data
based on real data,and then ran my analysis just
to see how it would look.So that's why none
of this is real data,but it's very
close to real data.It's using the same
boundaries and pretty muchthe same structure.OK, so I have a couple
of research questionsthat I wanted to look at
for this particular study.The first one is to explore
the possibility of a connectionbetween the sequence one
chooses for their clerkshipyear of medical school
and the resulting gradesthey obtain on major
examinations, specificallythe big examinations of the
USMLE Step1 and Step2CK.A secondary goal of this
presentation and the studyis to explore the possibilities
and complications of utilizinga propensity score
analysis to controlfor selection bias in an
observational education dataset.Those of us that work in
medical education or educationin particular know
that it's verydifficult to
separate individualsinto groups, especially
with somethingas sensitive as choosing your
medical school trajectory.So I wanted to just
see if this was evena feasible or able to be
used in this type of study.The biggest selection
bias concernthat we have for this
particular study,and the reason I thought
propensity scores would helpcorrect for it, is that the
sequence in which a studentgoes through their clerkship
year of medical schoolis extremely
difficult to assign--I think maybe for
every medical school--I'm not going to say that
as a blanket statement.But most medical schools,
at least at this time,you're able to choose
the sequence in which yougo through your clerkship.So basically which
of those specialtiesyou are able to study first
and the order in which you'reable to study those.That is more up to the
student than it reallyis to the school.So you're not able
to have randomizationin this type of environment.There's also been a
lot of speculationabout the effect of
clerkship order on NBMEand USMLE performance--
mainly, doescompleting a certain specialty
like internal medicinebefore your more complex or
seemingly more complicatedspecialty such as
surgery, could thathelp you score higher
on, maybe, a surgeryclerkship, or the surgery
clerkship examinations,or the USMLE examination?So does having that
kind of compound routethrough your medical education
experience in clerkship,does that actually
help you score betteron presentation-- or
not presentations,but on your examinations?Or is there one sequence
that maybe gives youa handicap over another?Just a thought.Other factors that
may be responsiblefor why a student would choose a
certain clerkship over another,or clerkship order over another.There are several
different reasonsa student could do that.One of them could
be they might havedecided on their concentration.So maybe they are like, hey,
I want to go into surgery,so I want to save
that one for last.Or I want to do surgery first
to get it out of the way.So maybe having
that choice of whatyou want to do
after you graduatecould be something that impacts
a student's choice as to whichclerkship order they choose.Another one would
be maybe they wantto delay a clerkship
for later in the yearbecause they deem it or think
of it as more difficult.Or there's certain aspects
of it that make them nervous.Or they think that if I complete
one clerkship over anotherbefore I do the
other one, it'll makethat one a little bit easier.Lots of different
ways, lot's for thingsthat could go through
a student's mindwhen choosing their
clerkship order.So we wanted to
just consider thosein the creation of
this kind of a study,or in looking at
this kind of a study.These factors could also
contribute to the likelihoodof USMLE performance.So maybe if a student
was strugglingin surgery-related
activities, so theysave that one for their last
clerkship in their sequence.Maybe they're a struggling
student overall.So that could also contribute
to their USMLE performance.There's lots of
speculations, justlots of thoughts that kind of
go into a study like this, whichis what makes medical education
such an interesting and complexstudy design.So that's why we wanted to see
if the propensity scores couldhelp with something like this.So the variables that are
in this study, the outcomevariables that I chose were
USMLE Step 1 and Step 2 CKperformance.I chose the scores themselves,
not just the pass/fail reports.The independent variables, the
predictor of interest I chose,was clerkship sequence.I wanted to see if clerkship
sequence itself didhave an effect on the
outcome while controllingfor covariates such as
pre-clerkship variableslike pre-clerkship
NBME scores, locationrestrictions, MCAT score,
college science GPA, ageat admissions, gender,
and class year.And then of course, the other
covariates I wanted to choosewere those that happened
during the clerkship timeframealongside clerkship sequence.And that would be the
clerkship NBME exam scores.Those clerkship NBME
exam scores could alsoserve as their own outcome, but
that could be another study.For now, we're
just mainly lookingat USMLE Step 1 and Step2 CK.In this presentation, I'm
going to concentrate on Step1.All right.So this is one of
the complicatedparts in the variable creation.So this is what the different
clerkships look like.So on the left-hand side,
you see the different rounds.There are nine different
rounds that you can choose fromor that--I'm sorry-- that the student can
assign or get a specialty in.So we've got two internal
medicine specialtyrounds, which would be
ward and ambulatory.We have psychiatry, two surgery
one's, general and specialty,obstetrics and gynecology,
family medicine,and pediatrics.Now you can see the
numbers I have there.1, 2, 3, 4, through 9.That is the order
in which you wouldtake that certain specialty.So maybe psychiatry
for one studentwas their fifth clerkship round,
where maybe for another studentit was their ninth
clerkship round.So that's what those numbers.So when I looked at that
variable in particular for--or the values for each of
those different variables,I got one of those numbers.The next column, so the middle
column you see, is block.The clerkships themselves
are actually paired.Internal and psychiatry
are together.Surgery and obstetrics
and gynecology together.Family medicine and
pediatrics are together.What this means is that when you
look at the different rounds,internal medicine and psych will
always be around each other.So you'll only see a
combination of those three kindof being switched
within each other.So you might have ward,
ambulatory, psychiatry,or psychiatry, ward, ambulatory.Any of those combinations.But you'll never see surgery,
OB/GYN, family medicine,or pediatrics in between
those different concentrationsbecause you do them kind
of in the same block.And then for surgery
and obstetricsgynecology are done
in the same block.Family medicine and pediatrics
are done on the same block.For those variables I
see different valuessuch as the 1, 2, 3.So is it the first block,
is it the second block,or is it the third block?The sequence variable
that I created--I could have done it in
several different ways.I could have actually
went and lookedat each of the
different rounds and howthey compared to the
blocks and createda ton of different values
that represented that.Because there are so many
different results thatcould have came
from that, I decidedto try to make it as
simple as possibleand concentrate on the
order of the blocks.So did the student do their
internal medicine, psychiatry--or psychiatry block first?Or did they do their
surgery block first?Or their family
medicine block first?And then which one
did they do second?Which one did they do third?So I ended up having eight
different groups, or sixdifferent groups,
sorry, instead of the--I can't think of the number
at the top of my head--the number of
groups it would havecame from if I had actually
taken into consideration roundand block in the
creation of the sequence.So this was just one example.For the propensity score, I
needed to have them ordered.So what I did was I assumed
for this, like I said,theoretical
exploration, I assumethat ideally you would
want surgery and obstetricsand gynecology to be your
last in the last block,because theoretically, they
may be the most complicatedand benefit the most
from having the other twoblocks in front of it.I then chose to have
internal medicineand psychiatry be the second
block, with the assumption thathaving internal medicine
as close to surgeryas possible would
be most beneficial,which left family
medicine and pediatricsto be the first block that
a student would go through.So they would first go through
family medicine and pediatrics,then internal medicine
and psychiatry,with the result to
surgery medicine--I'm sorry, surgery and
obstetrics and gynecology.And then, of course, the
rest of them behind itkind of fell in order as to how
far in surgery and obstetricswas, and internal
medicine and psychiatry.All right.So the propensity
scores I wantedto create for this
particular study--my treatment groups were
the clerkship sequence,and I wanted to balance these
treatment group groups so asto reduce the bias
to group selectionand obtain a better idea of
the effect of the clerkshipsequence on USMLE performance.So I wanted to see how my
covariates were affectingmy clerkship sequence
and make surethat that was
appropriately handledwhen looking at its effect
on USMLE performance.One thing that I
ran into was that Ihad a lot of continuous
variables in my model.So when I was looking at
my explorer statistics,it was very difficult
to just, in look at themgraphically and
everything, to have themas continuous variables.So I chose to do some binning.So I used high-performance
binning proceduresto kind of bin the
continuous variableswhen I needed to into different
just kind of groups, whichmade the exploratory
analysis a little bit easierboth visually and conceptually.And also when exploring the
effect on propensity scoreanalysis, I was able--having them binned
in that mannerwas making it easier for me
to do it that way as well.And then what I did through the
binning procedure is I employedeach of the different methods--
so bucket, winsorized,and pseudo quantile--and chose bucket.So that's a completely different
analysis and thought process.And I won't cover it in this
other than this is what I did.But always happy to talk about
it if you have questions.All right.So before I created
the propensity score,I wanted to first see how
my original model wouldperform without it.So I ran two
different procedures.One was proc glmselect, simply
because of the sheer numberof covariates and variables
that I was having in the model.And I want to just
throw this out therethat if you have several
different variables,it might be a good idea
to narrow them downthrough a glmselect
or other procedureso that you don't
have as many in there,because that will affect
the fit of your model.Next, I see that my proc
logistic procedure whereI just ran the model as is,
keeping all the variablesin there, since, like I
said, it's theoretical,so that I can see exactly how
that model without propensityscore analysis would perform.Oh goodness, I'm
almost out of time.OK, so this is my results
for the initial study.And then here's my creation.So I wanted to create my
propensity scores to be--to represent the effects of my
covariates on the propensityscore analysis-- or, I'm
sorry, on my sequence,and probability of falling
into a certain sequence group.So I did that through the
proc logistic procedure,using out into the
allpropen dataset.And prob was the
propensity scores.I then did proc univariate
to pull up the histograms.And I could see that when I
created my propensity scores,they were all pretty
evenly distributed.A simple regression
adjustment would be justto add the propensity
score itselfinto the model as a covariate.And then you could
do a weighted onewhere you can add
it in as a weight.Certification would be creating
a strata which you can see--you can do through
the rank procedure.And then putting it back
into logistic regressionas a strata.Here's see results.Matching I didn't
do for this onesimply because I had
six different groups Ineeded to match them
to, and that I knewwasn't going to happen.So I chose to skip that for now.But then I wanted to
touch on it brieflythat this is a possibility you
could do is through matching.And then here is my
model comparisons.So you see my original model
was actually the best fit model.So having the propensity
score analysis the waythat I had structured
it initially actuallydidn't really help
my model a lot.It actually put in more
noise than it needed to.Plus it didn't
actually adjust my rsquared in a positive light,
with the exception of weightedwhich jumped it up to 98%.And that I don't really trust.I want to go back
and look at it.But both simple and
stratified, their R squaredlooked very close
to the original one,and their model fits
were still much worse.So I want to go back
to the drawing boardand make sure that the way I
had the groups and everythingstructured and how I did my
propensity score analysis wouldprobably want to be revisited.So it wasn't as easy as I
initially thought it might be.But like I said, it's a
theoretical expiration.So on one thing I
wanted to considerwas that I could have had
too many covariates in it.The structure of my group, the
structure of my covariates,all these different
things I needto go back and
consider if I wantto continue looking
at propensity scoreanalysis, which I do.I think it actually could
help my model significantly.Another thing I
want to consider isthat the goodness-of-fit
test and rsquare tests that I chose
aren't always the best tests.I could look at doing
it a different wayand maybe seeing
if there is moreof a difference between the
different propensity scoreutilizations that I might be
seen by using the defaults.So in conclusion, I apologize
for the speed of the last part,but there are several different
ways to do propensity scores,such as regression adjustment,
stratification, and matching.By utilizing
propensity scores, youcould help reduce
your selection bias.Lots of different things you'd
want to consider in that,such as the structure
of your variables.And you also need to consider
the theoretical nuancesof your data and models, such
as mine was medical education.If you have a different
type of the field,you want to consider
what implicationsthe different variables
and structurescould have on propensity
score analysis.Also, make sure that you're
comparing your modelson things such as
goodness of fitand r squared to make sure that
propensity score analysis isactually helping your model and
not making it more complicatedor adding noise,
like I found in mine.So our question was controlling
for the possible confoundingeffect of sequence.And it ended up not
really improvingthe model for this example.But like I said,
I'll go back and lookat different ways
of structuring itand talk with my fellow
researchers and everythingto see if maybe I had
approached the question wrongor I could approach
it differently.Here are some references I
just wanted to add on here.If you have an interest in
propensity score analysis,I reckon there's lots
of different thingsout there that you can look at.And I wanted to thank
everyone for tuning in.Here's my contact information
if you have any questionsor would like to
bounce ideas off of meor anything like that, I would
love to talk more about it.So thank you, everyone,
for tuning in,and I hope you have
a wonderful day."
147,"Hello everyone, and thank you
so much for joining me today.My name is Sara Richter.I'm a senior statistician
at a small consultingcompany in
Minneapolis, Minnesotacalled Professional
Data Analysts.And I am so excited to
be here today talkingwith you about stepping up
your SAS ODS Graphics game.Effective data visualizations
are so important.They can make it easy
to understand trends,to see variability,
and to recognizepatterns that might not
be obvious in tabular datadisplays.They're also really great
communication tools.Honestly, it's becoming
an expectation, almost,that analysts create clear,
easy-to-read graphics as partof their everyday work.It can also be pretty
challenging to do.I'm going to admit
I was not alwaysan advocate of graphing in SAS.In fact, when I first
started learning SAS backin version 9.1, I
didn't graph in it.I'd export the data and
graph and another programbecause I thought the graphing
in SAS was kind of difficult,and it never really
turned out quite right.Eventually I needed to
start creating reportsin SAS, including graphs.And SAS's graphing
capabilities reallyexpanded by leaps
and bounds, allowingme to turn things like
this basic bar chartinto much more
visually engaging,easier-to-read versions
of the same chart.And the best of all, I was
able to do this in Base SAS.And by the end of
my presentationtoday, you will also be
able to do this, or at leastyou'll learn all
the tips and tricksthat I used to put it together.As with anything in
SAS, there are many waysto accomplish the same thing.Today I'm going to talk
about some of the featuresin Base SAS that I found
most helpful in my datavisualization tool kit.I'm going to give
applied examples showingsome of the ways that I've
used graphical annotations,and I'm going to go
into some exampleswhere I demonstrate
how to combine outputfrom graphs and tables into
more complex data visualizationelements.In the limited
time I have today,I'm not going to go
through all of my code.All of the code and
detailed explanationscan be found in my paper.Instead I'm going to focus
on the process of how I putthese visualizations together.Let's first take a look at the
graphing tools in Base SAS.I think it's important to note
that the ODS graphing that I'mgoing to talk about
has not alwaysbeen available in Base SAS.Through version 9.2, all of the
graphing was done in SAS Graph.And in 9.2 they introduced
this output deliverysystem graphics, or
statistical graphics, a new wayof creating graphs.In 9.3 they moved
it to Base SAS,meaning everybody had
access to these features.And since its
introduction, it's beenexpanded with additional
procedures and a lotof additional features.A lot of my examples
today are goingto use the SGPLOT procedure.As I mentioned, I started
graphing in SAS before ODS,so I was used to
graphing in SAS Graph.There are a few key distinctions
between these two graphingenvironments that I'd
like to point out.First, GOPTIONS will not control
the graphical environmentin ODS Graphics.To do things like remove borders
or to set graph dimensions,you have to use a ODS
Graphics statement.Along those same lines,
axis and pattern,and the other global
statements in SAS Graphare not going to control the
visual elements of your graph.Instead, you're
going to have to usethe options available
within the SG procedures.The last thing
that I'll point outis that while both
environments haveAnnotate facilities available,
they're not interchangeable.But let's pause for a second.What are the
Annotate facilities?The're really how you add the
embellishments to your graph.It's kind of like the
equivalent of adding sprinklesor decorations to a cake.You can add arrows,
shapes, text, even images.Let's see it in action.We can take this basic
scatter plot drawnusing the sashelp.cars dataset.And we can call attention
to certain areasby adding some custom text,
a rectangle, and an arrow.We can add a data
label, includingan image of the vehicle
that draws that data point,and outline it in a
custom designed text box.And if you have some
free time on your hands,you can even combine
all the shapefeatures available
within Annotationto draw a stick figure car.Now that we've seen
it, how do we do it?There are two steps to creating
a graph with Annotation.First, we have to create
an SG annotate dataset.You do this within
a DATA step, as youcreate most of your datasets.And you're going to
have one observationfor each annotation.Observations can be created
using SG annotate functionsor SG annotate macros.Both of them do the same thing,
but I prefer to use the macros.I just find them
more user friendly,so that's what we're
going to focus on today.Then, once your dataset with all
of your annotation observationsis complete, we're going to
do the second step, whichis apply it to a graph
created with ODS Graphics.Let's go through a
quick example thatgenerates two text statements.One of them, the first one, is
going to use the text function,and the second one is going
to use the SG text macro.So in this example, I'm
creating a new datasetcalled MyAnnoData.This first observation
is being createdusing the text function
to generate a textbox with this label.I'm going to place the top
left corner of my text boxat the x and
y-coordinates specified,and I'm going to use the
output statement to writethat observation to my dataset.My next observation is going to
be basically the same text box,but created using
the SG text macro.Before I can use the text macro,
I must call the SG anno macro.This macro compiles all of
the other annotate macrosand makes them
available for use.You only need to call
the SG anno macro onetime each session, and
after it's been called,the other macros are available.And you can see here
that the optionsare very similar to
the function options.In this case, I don't need to
specify the output statementbecause it's wrapped
into the macro.So this is my resulting dataset.Two observations, just
as we see in the dataset.Next I'm going to apply the
annotate dataset to my scatterplot.So you can see I've used a PROC
SGPLOT with a scatter statementto make the scatter plot.And to apply the
annotate dataset,I'm going to specify
the SG anno option.And I'm just going
to say equals,and I'm naming my dataset
that I've just created.And the resulting graph can
be seen here with my twolines of text, just that easy.In addition to knowing what
annotations you want to add,you also need to know
where you want to add them.SAS offers four
different draw spacesto work within, the data space,
the wall space, the layoutspace, and the graph space.All of the graph spaces you
work within using percentagesor pixels, and
always originatingfrom the bottom left
corner of the draw space.The default draw
space in ODS Graphicsis the layout draw space,
specified in percentages.And really, for
most of my purposes,this works really well.Sometimes I'll need to narrow
it down to the wall space,or sometimes expand it
out to the graph space,but for most of my purposes,
the layout space works well.I'd also like to note that
SAS does not reserve spacearound the graph
for annotations.So if you want to
include annotationsin the margins of
your graph, youneed to use the pad option
on the PROC SGPLOT statementto make room for it.If you don't, the
annotation willbe laid on top of your graph.Whew, that was a lot of setup.Let's see some of these
fun things in action.In the first examples
that I'm goingto be going through today,
I'm going use annotationsto recreate portions of
the ambulatory glucoseprofile, or AGP.The AGP report is
a one page reportthat clinicians and
people with diabetescan use to monitor the
blood glucose trends.For the purposes
of our talk today,the contents of the report
is really not important.We're going to be
focusing on recreatingthe graphical elements.And I'd also like
to note that allof the data in the presentation
today is simulated data.The first example is going
to recreate the line graphin the middle of the report.So as I approach
these visualizations,I try to imagine what
SAS procedure canget me close to
my desired resultsand what features
of that can I use.And then I'll use the
annotations to putin the last remaining bits.In this case, I can use a PROC
SGPLOT with band and seriesand refline statements to come
very close to the final desiredproduct.However, this line graph
is missing a few keylabeling elements.It's a good candidate
for annotationsbecause of the rotated text
and the extended lines,those green lines.So how to do it.To finish the graph,
I'm going to haveto add three green
line segments, twohorizontal and one vertical.And I'm going to do that
using the SG line macro.Then I'm going to use the SG
text macro to add the fourmissing labels.Let's take a quick
look at the code.Here is my SG
annotate data step.The first things
that I'm going to addare the three green lines.And the thing I want to point
out here with those SG linemacros, getting the x
and y-coordinates correcttakes some trial and error.You make a good first
guess, but you're notgoing to get it right
the first time around.So have a good guess, and
then refine it from there.The last part of the dataset
adds those four SG text labels,and it's the same code
that I used beforewith a few extra
options specified.I'd like to call
attention to two of them.The first one is
the rotate option.Rotate equals 90.I'm using that option
to rotate my textto read parallel to the y-axis.I'm also using the
fill color option.This is going to fill my text
box with a white background.This is important for the
label to look correct,and I'll show you
why in a second.The thing to note first is that
SAS creates the annotationsin the order that we
specify in the dataset.So the order of
the observations isgoing to be the order that
SAS creates the annotations.So we're almost there.The other two things
that I'll noteis that we're going to use a
PROC format to get a customformat on our x-axis.We're also going to specify the
ODS Graphic statement to setthe dimensions of our graph.And then we're going to
apply those annotate datasetand the format to
our PROC SGPLOTto get our desired result.And here's where I want to call
attention to that white textbox that I mentioned.So the white background
behind the targetrange text on the
y-axis, it makesit seem like the green line
is breaking for the text.But in reality, I've just
laid down a white text boxon top of the green line.So the order is important.But this gives us our
first finished product.The second example recreates
the stacked bar chartin the upper right
of the report.As I look at this desired
outcome, I see two parts.I see the stacked bar chart,
and I also see the labels.This is another good
candidate for annotationsbecause there's not
really a great wayto do that extensive labeling
or to add those leader lineswithout doing the annotations.This two part approach is
going to work in our casebecause we're sending the output
to the PDF destination usingODS PDF with an absolute layout.If you're not familiar with
this output destination,I really highly recommend
you look into it.It's beyond the scope
of my talk today,but it really is a game changer
for reporting out of SAS.To give you the quick overview
if you're not familiar,I'm going to send
this output to PDF.And when I do that I'm going
to specify regions, basicallythe areas that I
want to work in.And then I'm going to
tell the PDF what contentI want to fill those regions.So you can think of the regions
like the blue boxes here,and the content being filled
with the bar chart and labels.It's great because then I
can align the two components,making them look like
one visual element.So let's first consider the
stacked bar chart parts.To get this part
right, we're goingto create a free
floating bar chart,and then we're going to annotate
it with four leading lines.Let's take a quick
look at that code.The only really
remarkable thing hereis to point out that we are
turning off all of the borders,all of the axes,
all of the legends.We're getting rid of
all of it so that itappears to be free floating.The one thing that SAS will
not turn off in a bar chartis the baseline axis.To get that to
disappear, we're actuallygoing to trick SAS
by coloring it white.So in that free
floating bar chart,it actually does
have a white axis,but it blends in
with the background.And you don't see it.And that's done using
that baselineattrs option.Then for the second
part, the text part,we're going to use a bunch of SG
text macros to add the labels.But now, hold on.I said that we are
doing this in two parts.This is the part, and
that an annotationdataset has to be
applied to a graph,and there's not a graph here.And really we don't
want a graph there,but we do want the flexibility
of the SG annotate facilities.So again, we're going
to kind of tricks SAS.And we're going to
print this annotationdataset over an empty graph.Let's take a quick look.So up top in gray are
all of the SG textfunctions creating
those text labels.After that's done, we're
going to create a PROCSGPLOT with just a line chart.And again, we're
going to suppressall the borders and the axes.And we're going to print
our series, or a line,in white, using that
lineattrs option.And that's going to
make it, again, blend inwith the background.And then we can print
the annotate dataseton top of that.And printing out to
the ODS PDF destinationand align the regions.It gives us this
one very cohesivelooking data visualization.In the third
example, we're goingto recreate the table in the
upper left of the report.This is going to be an
extension of the examplethat I just talked about.If I thought about doing
this using a different tableproducing procedure, like a
report procedure, an ODS textprocedure, it would be difficult
because of the varying formatswithin each row and because of
the differing column widths.But instead, I can do
the same trick I did,and use a bunch
of SG text macrosand a couple SG line macros,
and print it on an empty datasetto get a totally custom table.Let's move away from
the SG annotations,and move into combining
output from graphs and tables.First we're going to embed
spark lines in the table.And again the
context of the datais not important, or
focusing on the data vis.In this case, the
end goal is reallyto get small line charts
within the style of a table.So to create this, I again
envision two elements.The table element and
the graph element.In this case our
elements overlap,so I'm again going
to send them to PDFwith that absolute layout.We're going to start by creating
a table using PROC REPORT.Again, this is one of my
favorite table producingprocedures.I use it all the time.And we can, within PROC
REPORT, use some of our datavis best practices, like
right aligning the numbersand only selectively
printing orders.To save a space for
our graph element,we're going to add a blank
column that's eventuallygoing to hold our lines.That's the first element.The second element is
the stacked line chart.I'm going to do this
using a PROC SGPLOT,and I'm going to
make sure that it'sthe same dimensions as my table.And then I'm going to
strip out the axes.I don't need them.And when I send them
to PDF, I'm goingto overlay those two regions,
making it look like one datavisualization element.In the final example
I'm going to show youhow to recreate the graph
from my introduction.This graph was created
as part of a reportto track combinations
of servicesreceived by people enrolling
in a tobacco cessation program.So you can see that
we've ordered from mostpopular to least popular.But it's difficult to tell which
services are really being used.So I asked one of my
colleagues, a data vis wizard,if she could help.And she did.She came back with this mockup.She kept the ordering, but
she turned the text labelsinto a table with one
column for each service.She added the icons
up top to helpgive a quick visual reference
of what we were talking about,and she color coded the
bars based on whetheror not an evidence based
treatment was included.And then she said,
can you do it in SAS?And I said, yeah, I
can do this in SAS.So I'm going to challenge
you, how would you do it?I've already mentioned
that I've given youall of the tricks and strategies
that you need to do it,so where do you start?What components do you see?How would you add the icons?I'll tell you how
I approached it.I see two components.I see the graph component and
I see the table component.In the graph
component, I'm goingto do that using a PROC
SGPLOT with HBAR statement.I'm going to create a
horizontal bar chart.But I'm going to
group my data now,so that if you have an
evidence based treatmentyou're in one group,
if you don't, you'rein the other group.And then I can apply the
green and gray coloring.To create the table element,
I'm going to use a PROC REPORT.And in this case, the
underlying dataset for my reportis a dataset that
looks essentiallyjust like this table.It has ""ones"" if the
service was includedand ""zeros"" if it was not.And as I print it--and I'm using PROC
REPORT, in this case,as a fancy print procedure.I am applying a
format that turnsthe ""ones"" into the filled dots
and the ""zeros"" into blanks.To get the two
elements to line up,I'm going a print to PDF
with an absolute layout.And then to add the
icons, I'm goingto use an SG image annotation
the same way I addedthe picture of the Toyota Prius
in one of my first examples,I'm using here.And then to get a
custom label, I'mgoing to use an SG
text annotation.And altogether it makes
this visualization.So in the last 20
minutes or so, you'velearned about some of my most
often used tools in my databasetoolkit.Building new skill sets
does not happen overnight.It takes a lot of practice.But it is possible.And once you figure it
out, it's really satisfyingand kind of addicting.The parting thought,
I guess, that Iwould like to leave
you with todayis that with a lot of
creativity and not so much work,we can create some really
awesome visualizations in BaseSAS.If you haven't
yet, I would highlyrecommend you check out the
SAS Graphically Speaking blog.Really great for
inspirations and for how-tos,and using the ODS system and
getting some really amazinggraphs.Thank you so much
for your time today.My contact information
is here on the sideif you have any questions about
any of the code or examples.And if you want to
reach out to me,please feel free to email
me at this email address.Thank you again."
148,"JEREMY CHEN: Do you know
that SAS Decisioningis helping customers make
better decisions every dayaround the globe?Have you heard of
Intelligent Decisioning--our most innovative product
for decision management?I'm Jeremy Chen.I'm part of a product management
team for decisioning at SAS.Today, let me show
you a demo thatillustrates the power of
intelligent decisioning.On top of that, I
will also show youthree new features that can be
used to build complex decisionprocesses.But before we start, let's
first review why SAS IntelligentDecisioning is transforming
the landscape of decisionmanagement.With SAS Intelligent
Decisioning,customers can put
their machinery modulesinto production
with ease and speed.Because Intelligent Decisioning,
and many other SAS Analyticsproducts live in
the same platform--SAS Viya-- customers
can perform every stepin their analytics lifecycle
ad decision lifecyclewithout ever having
to go somewhere else.Enterprise decisions often
require complex decisionprocesses.We know that this
Intelligent Decisioning cansupport pretty complex
decision processes,but customers want more.They also want these
decision processesto be easy to maintain.They want there to be
minimal duplication of logic.They want different teams
to be able to collaborateon common decision.At the same time, they
also want the teamsto have clearly separated
concerns and decisioningresponsibilities.In today's demo, we will
highlight three featuresthat can be used to
help customers createcomplex decision processes
with a goal of maximizingreuse and maintaining
separation of concerns.These three features are nested
decision processes, evocationof mass directly in
a decision process,and using data grid
for micro batch.Let's get our demo started.First, a few words
about our demo.Our demo is based on the
decisioning process for creditcard transaction fraud.When a card transaction
is submitted for approval,card issuer bank uses
this decision processto determine if the transaction
is a potential fraud.If yes, the transaction
will be denied.And the case will be sent
to another team for review.The customer will also receive
an email when this happens.Naturally, our decision
process is a lot simplerthan what is used in real life.We start our demo in SAS drive.Here we find our demo project
in the favorites panel,and navigate to
Intelligent Decisioning.Now, we're looking at our
top level decisions flow.We will first check if
our card is black listed.We'll use an SQL task for this.And you can see here, the
code is pretty simple.But let's validate that our
code is actually correct.And now we'll add a simple rule
to block all the transactionsthat have been black listed.Remember, I said I'll show
you three features to supportcomplex decision processes?Here is our number one feature.Use nested decision flows.With nested decision
flows, you willhave an easier time explaining
the very complex processto the stakeholders.And you will be able
to reuse sub-decisionsacross organizations.The modular design also makes
it easier for both maintenanceand for collaboration.So here is our top
level position flow.This decision flow embeds
two nested sub-flows.Here, let's open the
fraud decision sub-flow.The sub-decision flow use to
check past history, geography,and also to apply different
machine learning models.The second feature for complex
decisions is data grid.Do you know that
you can actuallyuse data grid to support
micro patches thatrequire the use of
aggregated variables?Of course, data grid
is also super usefulwhen you need to carry a data
store to get multiple recordsinto your decision
process in real timeto erase the decision data.So here's an example, to use a
bigger grid with a SQL query,and to use the data grid
for calculating variables.The value returned
from the SQL queryis assigned to a data grid
variable named transaction lookup out.And the data grid
has two columns.The data grid structure comes
with many useful functions.We use them to calculate
aggregated variables,among other things,
so that we can usethese variables in our rules.While we focus on the three
features for complex decisions,I want to quickly demonstrate
how easy it is for you to bringanalytics into decisioning.Here, we have a
branch node, wherewe apply different
machine learningmodels for different regions.All I need to do is to navigate
to the model repository,and pick the right model.So here, we see we have imported
three machine learning modelsinto our fraud decision flow.Once we score the
transaction with the model,we can use the
score in the rules.Here we combine a score
and other attributesto determine the resolution
of the transaction.The third feature I
want to highlight todayis the ability to invoke
another micro analytic servicewithin a decision.This capability provides
another way to a modular designand for reuse of
common self services.The main difference between this
one and this nesting decisionflows is that
here, as a consumerof the micro
analytic service, youdo not need to know or care
to know how it is implemented.Maybe you do care.But the key point is,
its implementationis the responsibility
of another team,and they have their
own release cadence.And here, in our
second sub-flow,we have a step to invoke
a micro analytic servicefor anomaly analysis.This step is implemented
using SAS code.Intelligent Decisioning
and its highly extensible.For example you
can use Python codeto call out to external services
hosted on other platforms.In the last step of our
fraud detection flowhere, we will send an
email to those accountswith suspected fraud activities.This step is
implemented in Python.Now you've seen the
whole decision flow.Let's run a quick test.And now we have the test
result. Let's bring the datainto SAS Visual Analytics
for a quick review.Here we can see that,
by state, Georgiaseems to have the highest
number of transactions denied,and California has
most suspicious casesthat we should
investigate further.So today we demoed
three featuresof Intelligent Decisioning
that support our customers'complex decisioning needs.We also showed how easy
it is to bring analyticsinto a decision.Please hit the like
button if you wantto see more demos like this."
149,"Well, hello.Welcome, my name
is Mark Schneider.I'm a product manager here at
SAS, whose main responsibilityis SAS Viya, which is
why I'm coming todayto talk to you a bit about how
one migrates from SAS version9 to SAS Viya.I have a lot of domain
expertise in general areasof architecture, and
deployment, and administration.But today, we're going to
focus on the activities relatedto moving your version 9
environments over into the Viyaworld.Before I do that
though, it's alwaysimportant to pause
and say, well,why am I going to SAS
Viya in the first place?And there are a lot of
key motivating factors.I'm not going to
belabor them, but I'mgoing to reiterate a few of
the most important ones thatare driving people to
adopt SAS Viya technology.First is SAS Viya's
capability to speakin your language of choice--whether it's Python, or
R, or Java, or ALua--being able to do analytic
data science typefeatures in a language
with which I'm familiar.Then, of course, SAS
viya does bring to itadvanced analytics that are
not present in version 9environment.So doing some of that deep
learning in some of the latestand greatest technologies.That is unique on
the SAS Viya side.Another motivator for people
moving to Viya is speed itself.SAS Viya is based on a
distributed in-memory engine,the CAS server.We'll talk a bit about
that in a second.And people are taking
the analytic problemsof largest size and trying to do
those in an efficient fashion.Of course, many people
are moving to the Cloud.A lot of the customers are
adopting Cloud technologies.And Viya was architected
with the Cloud in mind.And over time,
more and more Viyais integrated closer and closer
with both public and privateCloud providers.High availability, that's a key.No single points of failure
in the SAS Viya architecture.If any one component
drops out, I'vegot a copy of that
component that'sready to pick up the slack,
and make sure that there'sno incurred downtime.And then we talk
about democratizationof the analytics.And that's just a
big flowery termfor analytics for everybody,
whether you're a trade datascientist, or whether
you're an end businessuser who has no knowledge
of programming languages,but still can use
user interfaces.Which are adapted
to allow them to doadvanced analytics, high
forecasting types of functions.Basically, SAS Viya encapsulates
the entire analytics lifecycle.We talk about going from data,
input, data manipulation, allthe way to deriving
business value from makingthe right business decision.So these are all motivators
for why you'd move to Viya,now let's talk a little bit
about how you accomplish that.Most of the people who are
adopting SAS Viya today,already have a SAS
version 9 footprint.And so they're seeing
over time, thisneed to adopt more and
more unbiased capabilities,and rely less and
less on version 9capabilities in the process.So when we talk
about adopting Viya,it essentially incorporates
three different phases.And actually, most
of the customersthat I speak to who
are adopting Viyaare going through each
of these three phases.And SAS along the way is
enhancing each of these phasesto make it easier and easier.The first phase that
I'm going to discussis interoperability,
and this oneI would say SAS
supports completely.The ability to use
version nine clientsto communicate directly with
that SAS Viya server, the CASserver, to do that distributed
in-memory processing there.We'll talk a bit about
how we accomplish that.The next phase of migration
that you could choose to adoptis content promotion.And this essentially
means if I havea SAS version 9 environment
stood up next to a SAS Viyaenvironment, being
able to move content--when I talk about content, this
is user created collateral,these are the
reports, and views,and jobs on a pipeline that you
created on the version 9 side--and to export them and import
them into the Viya side.So that you can use them
in native Viya fashion.And not just use them, but to
edit them and use them there.And then the final phase that
I'll discuss is replacement.And this is kind of the nirvana
when it comes to migration.This is, I have moved all of my
SAS workloads over to SAS Viya,and now I can turn off the light
on my version 9 environment.No longer requiring
sort of this dual head,having to maintain both
a version 9 and a Viyaarchitecture environment.So let me take each of
these three separately.And we'll start with the
interoperability first.So as I mentioned, we've
already got this covered.SAS has done a
great deal of workin order to make sure
that version 9 clientscommunicate well with
SAS Viya servers.So if you're familiar with our
standard version 9 clients--like Enterprise Guide, and
Data integration Studio,and Enterprise Miner--these clients actually generate
SAS code, either explicitlyor behind the scenes.But your good old standard
SAS foundation data stepand procedure code, which then
is executed using a foundationserver in version 9.Well, what we've done as
of the fifth maintenancerelease of 9.4, which
actually came out over a yearand a half ago.We're already on the sixth
maintenance release right now.But as of at least the
fifth maintenance release,all of these clients
were Viya enabled.And what that means is, that
they can generate and leverageprocedure code that can only be
executed in a Viya CAS server.And the way that's handled
is, the clients actually dotheir data step and procedure
submission into a 9.4M5 server,like a workspace server.And then the workspace
server actuallysees that, well,
I don't know aboutthis particular procedure.This was a Viya procedure.I need to farm that out to
the CAS server in my Viyaenvironment, and have it execute
in that highly performantdistributed fashion there.And have the results sent
back then to my client.Or similarly, the data
step itself is somethingthat has been Viya enabled.And so if you're using, for
example, data sources thatare available to
SAS Viya, the datastep itself can be distributed
over on the CAS server.So on the version
9 side, I've gotsort of this middle man
of the workspace server,or stored process server,
or any foundation server,that's processing the data
step of the procedure codeand farming out what it can to
CAS server to take advantageof the faster,
higher performanceanalytic server in
the Viya environment.A lot of good interoperability.And that's available
to you today.The next thing you talk about
interoperability though,it's not just about the client,
but the data themselves.And what they're accessing.And in many cases, data can be
accessed both from your version9 environment and your
SAS Viya environment,without the need to
replicate the data.So you see on this slide, the
lower portion of slide talksabout different data types.Obviously, the SAS7bdat format.That's your simple
SAS data set format.And Viya speaks data set
format as well as version 9,obviously.They both can read in
the same CSV files.They both communicate
with Hadoop well.They both can access third
party databases, like Oracle,and Db2, Teradata, et cetera.There are a lot of
different ways for youto load that data
into a CAS serverin order to make it available
for Viya based processing.You can, of course,
use those SAS 9 clientsthat I just talked about
on the previous slide thatsupport that interoperability
in using CAS librarysteps in order to set up
those libraries, whichpoint to the data that
was previously onlyaccessible on version 9.You'd, obviously, also use
Viya clients themselves,like SAS Studio, to
pull data in and makeit accessible to CAS server.You certainly can write batch
jobs, and schedule them--either on the version 9
side or on the Viya side--to pull that data into
the CAS server memory.And interestingly enough, if
you're a Visual Analytics user,for example.You're already familiar
with the SAS LASRserver, which is its version
nine in-memory distributedserver.We can actually point
CAS libraries directlyto LASR libraries.So there's no need for you to
replicate that data somewhereelse, exporting it out of LASR.But I can actually directly
access the exact same dataas it exists in your
version 9 LASR environment.So Visual Analytics, for
example-- on the Viya side--can now look at the same data of
Visual Analytics on the version9.Version 9 and Viya can
share that same data.And, in fact, CONNECT actually
precedes the fifth maintenanceversion of 9.4, in allowing
data and processing actuallyto go back and forth
between version 9 and Viya.Now, should point out just
before I leave this slide.There are some data
characteristicsthat are unique to Viya,
certainly that in-memorydistributed processing
that we have right now.The approach that we're
taking with the CAS serveris unique to SAS Viya.As well as some
different data formats,columnar data formats, which
are especially efficient whenwe're talking about
analytics processing.Things like Parquet and
OR, those are uniquelysupported on the Viya side.So there's not interoperability
there because version 9doesn't support it.But for what version
9 does support,we have a lot of
different avenuesto pull the data in on Viya.So enough about
interoperability.Let's move on to the
second phase of migrationthat I talked about earlier,
and that's content promotion.I have a lot of user
created collateralthat I want to move over
from version 9 to SAS Viya.And those of you who are
familiar with version9 administrative
tools, you probablyalready know about SAS
Management Console.And the way in which you can
package up version 9 content,and move it from one
version 9 environmentto another version
9 environment.Well, thankfully, the
process for content promotionis very similar when
I'm trying to movethat content to SAS Viya.I can use SAS
Management Console.I can create similar SPK files,
which is the SAS packagingformat that it produces in
wrapping up all those reportsand jobs, et
cetera, to make themavailable for
another environment.Now what Viya adds to the mix,
though, is this mapping filethat you see on the slide.So we have a transfer
utility, whichactually looks at
the package and says,OK, I see the different
object types here.I need a little bit
more informationin order to map, for example,
those SAS version 9 librariesto the Viya CAS libraries.And so the mapping file is
produced with documentation.And allowing you
to edit it directlyto provide the additional
mapping information,in order to accomplish
that contentpromotion across version
9 to Viya's approaches.So once I've edited the mapping
file, I've got my package file.Those two things together
are used and inputinto the import process.Which is SAS Viya's
transfer server,which imports it and
makes it, therefore,available in your
target Viya environment.Now before I hop off this
process, it's important for youto take into consideration
the order of operationsthat perform this promotion.Because when I'm moving
from version 9 to Viya,I don't typically do
that holistically.I don't typically take my
whole version 9 environment,create one big honking
package, and then importthat whole package into Viya.And then I'm done.Rather, I typically break
things down by department,by workload, et cetera,
doing it piecemeal.Well, for each of those
pieces that I choose to do,I need to do it in
a certain order.First of all, I
should create packageswhich contain the security and
identity information first.So then once I've exported
those into my package,done my mapping, and then
imported it into Viya.Then I move on to
library definition,mapping LIBNAME statements
on the version 9--library definitions and metadata
on the version 9 side to CASlibs on the Viya side.And then I do the same
thing with my data tables.And then last, but
certainly not least, is Icreate promotion packages,
which contain my reports.And all the goodness,
all that collateralI created on the version
9 side, which dependon those previous three steps.So doing those in order
very, very important.Now SAS has provided a utility
called the Content Assessmenttool to assist with the process
of determining what you haveavailable on the version 9 side,
in determining what needs to bepromoted on the Viya side.And, actually, you engage your
SAS account representativein order to gain
access to this utility.Again, it's the Content
Assessment tool.The account rep will make that
available for download to you.And then it runs
on your local site,gleaning information from
your metadata server,to get all the different
objects that you've defined.And it produces a
report, then you view it.And you can see this as
an example of a top levelreport that has resulted
from one run of that ContentAssessment tool on one
specific environment.There's a lot of
detail in this report.I'm not going to run
through every aspect of it.But you can see here,
it's a quick synopsisof all the different
object types--the data tables,
and the libraries,the OLAP cubes, the
number of server context.Those are all called out here.And it's a drillable report.So you see tabs across
the top, and you alsosee links within
the report itself.So I can get more and
more information on this.And here's an
example of drillinginto the OLAP information.So this gives me a better feel
of the types of OLAP cubesthat are being built
on the version 9 side.Now SAS by itself does
not have an OLAP server,but it does have
methods by whichOLAP cubes, and the reports that
are produced from those OLAPcubes, can be manifested into
Visual Analytics reports.And so having an understanding
of the types of cubes,and how those should
convey under the Viya sideis important.So I see this
detailed report, whichhelps me with that process.The same thing with
Enterprise Miner.I need to have a handle on
all those different pipelinesand sequences that I've created
within Enterprise Miner.This at a glance
shows me everythingthat was gleaned
out of metadata,to see in this
particular version9 environment what I need
to carry forward with me.Now, full disclosure, not
every pipeline sequenceis currently supported
in SAS Viya yet.And so this, again, informs me
on what I'm currently using,and better prepares
me for what isand isn't compatible
on the SAS Viya side.So I've talked about
interoperability, bothof the client and the data.I've talked about
content promotion.How I get that SAS
version 9 content and usercreated collateral into
a SAS Viya environment.And the last phase of
migration is replacement.This is what I
called the nirvana.This is I want to turn off
my version 9 environment,and just live in
the SAS Viya world.But there are four key
considerations whenyou consider that prospect.And the first and foremost is
functional replacement value.I just talked about
Enterprise Minerand how not all of
the pipeline sequencesare available for
support in Viya.So I need to ensure that as I'm
moving workloads to SAS Viya,all the business value that
I've been using on the version 9side is available
for me on Viya.So I can work with
my account repto see the different
products whichare available on SAS Viya, and
the features that they offer.And in many cases--yeah, Visual Analytics
is the poster child--we've got full
replacement value.It's a no-brainer.It's easy.Full functional
replacement value,I can turn off my VA on 9.4 and
move everything over to Viya.So that's the first
consideration,but there are three others.I've got to consider capacity.So certainly, SAS has
created system requirementsdocumentation to help
assist you with determininghow much muscle, how much
CPU, and memory, and IOare required for your
destination Viya environment.So I need to ensure that I've
got the horsepower in orderto support that,
before, obviously, Imove my workloads to it.And SAS has its Enterprise
Excellence Center,which can help you with
determining those capacityparameters.Third is the data access, right?The analytics is only as good
as the data that you can get to.If you're going to turn off
your version 9 environment,you better well be sure that
your SAS Viya servers haveaccess to the same data or
that business critical datathat is going to drive your
whole analytics lifecycle,and those decisions
at the tail end.So in many cases,
what I talked abouton the interoperability
slide, SAS Viyadoes have great access methods.In fact, all those
access enginesthat work in the MBA world,
well, those access engines,in many cases, work on the
Viya world in the same way.But then Viya actually
brings to the table a lotof additional efficiencies.For additional access method,
certainly parallelized access.But I do need to have internet
access to those data sources,right?So in my SAS Viya environment,
as I'm standing it up,I need to make sure that
my service can actuallyconnect to those third party
databases, or those fileservers which hosts the
data that I'm pulling in.So that's data access.And then the fourth
considerationis just your users.Are they ready for this change?Yes, SAS Viya brings with it
a great new industry standard,modern HTML5 look and feel.But change is change.And a lot of users
need to sort ofbe eased into turning their
heads around and seeinga new interface of that ilk.And so educating
those users, ensuringthat you're phasing in
those new interfaces,is important as you're
considering replacement.So those are the
four considerations.And so then, in summary,
three phases of migration.First was interoperability.SAS has this covered end to end.All our version 9 clients
that deal with SAS programmingand SAS program generation
communicate directlywith CAS server.Got that one covered, just need
to learn to stand up a Viyaenvironment and leverage it.Second is content promotion.SAS has a similar approach to
what we've done from version 9to version 9, as with
version 9 to Viya,in packaging up my user
collateral and exporting it.And then importing
it on the Viya side.And then the third
is the considerationsassociated with replacement.I've now got all my business
value on the Viya side.I'm ready to turn off version 9.And I have to consider
the considerations, whichI enumerated earlier.Now there's a great
deal of detailwhich I wasn't able to cover
in this brief video, whichare described in the
Global Forum paper.A shout out to Susan
Pearsall, who co-authoredthis particular paper with me.You'll find a lot
more informationon the mechanics behind,
especially content promotion,both in the paper, as well
as the references that itcites to SAS documentation.So I appreciate your time.And I hope you enjoy the rest
of your virtual conference."
150,"CHRIS BAREFOOT: Hi, everyone.Welcome to my SAS
Global Forum 2020 talk.My name is Chris Barefoot.I'm a development
manager here at SAS,working with conversational
technologies.And my talk today
is entitled GivingYour Model a Voice, Natural
Language Studio and SAS VisualData Mining and
Machine Learning.Before I get too much
further in the talk,I do have to make
one correction.Between the time when
I submitted this ideaas a session to the
time when it's actuallygetting recorded
now, the name of oneof the main products we're
going to show has changed.And so if you remember talking
to me at the booth last yearor seeing any of the
demos we've given,you might have heard the
name Natural Language Studio.And that has since changed
to SAS Conversation Designer.And Conversation
Designer is a productthat we're very
pleased to be releasingthis year in our next release.So be on the lookout for that.So let's talk a little bit about
what I hope to accomplish todayand what I hope you can
take away from this talk.So I hope you'll know that at
the end of this that machinelearning based models, when
placed inside of a chatbot,really extend the reach of
the data that you've got,the valuable sort of
models that you've built,decisions that people can
make based on those models.I really want you to think
about chatbots as another userinterface into that data.Just like an actual
graphical user interfaceallows certain uses of
data, text based interfaceshave their own unique
benefits and reasonsthat you might want to
expose your models there.We'll take a pretty extensive
look at how a chatbot is made,especially using SAS
Conversation Designer.And then, lastly, we'll look
at, given the Viya platform--so you're creating
this bot, you'vegot your models-- how
do you link the two,and even link things that
are outside of maybe the SASecosystem using
the Viya platform.So let's talk about the
scenario that we'll use here.So for this we're using
the scenario of a bank.So a bank might have
an existing chatbot.Maybe that chatbot does
things like returnsthe amount in a user's
checking account,maybe it allows users
to transfer moneyfrom one account to another.But maybe the bank
is now lookingto really expand the number
of people that are qualifyingfor loans and also remove the
barrier to entry that, oh, youhave to go to a website or
maybe even walk into a branchto get preapproved for a loan.So let's say the bank
wants to add loanpreapproval to their chatbot.The first thing
you're going to dois think about what are
the existing assets wehave to sort of enable this?And so, firstly, they're
going to have a ton of dataaround existing and
historical loans, right?So they're going to have
all sorts of informationabout the people who
took on those loans.And then they're going
to have informationabout whether that loan
was successful or not,whether the user completed it--the client completed
it with all paymentsor whether maybe they defaulted
or struggled with the paymentsat any point.And so you can imagine
rows and rows of datathat include things like the
client's income and existingdebt and whether they
defaulted on that loan.And so then using
those rows of data,you could pretty easily
make some machine learningbased models that
then take that dataand would score new rows,
basically, new clients, giventheir data, and whether they
might default on the loanand what the risk factor
there is for that.So with that, let's take a
quick look at the chatbotthat I've built for this.And then we'll look
into how it's made.Just a precursor to me
showing this to you,this bot was specifically made
for this demo sort of as a toy.So I didn't want
to clutter up boththe bot and the UI behind
it with a bunch of things.So you'll notice
it's pretty limited.But that's on purpose.And so a production
level bot would obviouslyhave a lot more of everything
go into it to make sureit's robust for users.But for this purpose,
let's keep it simple.So the bot first
greets me and basicallyasks am I interested
in getting approvedfor an equity line of credit.We'll go ahead and say, Yes.I've got buttons here in a
couple of these questions.And buttons are nice when you
want to guide user's input.Certainly, at any point, I could
enter natural language texthere as a response.But buttons are a nice way to
keep people on some guardrailsif you know you want specific
answers to a question.The nice thing is, though, you
can still keep it fairly casualand natural language based.So even though I
clicked the Yes button,the bot pre responded,
you know, am Ieligible for a loan, which is
kind of a nice back and forth.But here I could just
type Yes, as well,if that's what users prefer.So the first question
the bot's asking,do you own, mortgage, or
rent your current residence?So we'll just say we
have a mortgage on it.What's the size of the
loan we're looking for?So let's say we're trying to
put in a fence in our backyard.So we need $6,500.What's the combined
income for your household?And you'll notice
here some italic text.So each of these
responses can be rich,and we'll look at
that in a little bithow that's accomplished.But let's say the combined
income of our house is $85,000.And we'll also
look at, basically,the ways the bot
responds to keep thingslight and conversational.But now it's asking
how much debtdo you have excluding
any current mortgage?So let's say, we're
still paying off a car,or maybe I have a little
bit of credit card debt.So we're going to say $14,250.How many years have you been
with your current employer?I've been here eight years.Final question, which
of these categoriesbest describes the
purpose of your loan?So I'm going to say
Home, consideringwe're doing an addition
into the backyard.So let's review your answers.So now it's basically
just regurgitatingthe answers that
I've given it, justto make sure these look right.If they didn't,
you could say No.And the bot could then repeat
and ask for the same dataor ask for which
pieces need correcting.But in this case, we'll
say, Yes, this looks right.So it says, congratulations,
the probability of defaultis 11.4%.We'll look in a second at how
it arrived at that answer.And it says that
we're prequalifiedfor a loan of $6,500.Got a little bit
of a mistake therethat would be easy to correct.And I can show you where we can
do that in just a little bit.And you can visit this link
to start your application.Great.Let's go back to
our presentation.So the first thing I
want to talk about is--and briefly, mostly
because I'm notan expert in this area at all--I have to give huge credit
to a colleague of mine,Andy Christian, for
actually creating this modelpipeline using Model Studio.So this takes in the data that
we were talking about earlier,and then forms a
couple of pipelinesusing some different machine
learning based algorithms,and then, at the end,
brings them all backin with a model comparison.And so this is the pipeline
that we just executedas part of that chatbot.We scored a new row, which
we'll look at in a little bithow that gets
accomplished, and thenthat score returned
us a confidence valuethat we then
provided to the user.And it should be
mentioned that a lotof the nuances into
creating a chatbotare up to the chatbot designer.A bot designer might
decide they don'twant to provide a user with a
raw 11.4% chance of default.But there are ways to
hide that away and stillcraft nice responses to a user.So all of that's sort of
an art of actually creatinga good chatbot.So let's talk about
building the bot.And we're going to talk
about three main pieces.And then I'll show you the user
interface Conversation Designerand where those three
pieces get built.The first is Natural
Language Understanding.So natural language
understandingis that the bot is going to
take in natural text from users.That text comes
in a variety ways,because users speak and
type in a variety of ways.And so natural
language understandingseeks to categorize
everything usersare saying into a known
set of intents which we'lllook at in just a little bit.So once you've understood
what the user is going to say,it's time to talk about
Conversational Flows.So this is the chatbot
need to retrieve data,maybe needs to ask
more questions.So there's all sorts of
ways that chat flow--you can think about
it just as a flowthat you and I might be
having in real life--that the bot might need to flow
and answer back to the user.And so, lastly, the
piece we'll look atis Natural Language Generation.So this is a bot needs
to respond to a user.And they need to do so
in a way that's natural,that feels like you're
actually talking to something.We don't want to fool people
into thinking we're human.But it's nice to have it
be conversational ratherthan robotic.And lastly, you need to be able
to take the data that we'reretrieving, as part of the bot,
and formulate that into a waythat the user can read
and consume as well.So let's take a look
at that interface.Jump to a different tab here.So here's SAS
Conversation Designer.Normally, I'd have a
list of other bots here.But right now, I've filtered
it to just the specific botthat I want to be using.This in my SGF 2020 bot.We're going to start--there's several tabs up
here for different purposeswithin the bot.We won't look at
all of them today.And we'll start
actually in the middlehere, in the Intents tab.So Intents, Entities,
and Utterancesall three deal with natural
language understanding.The intents, as I
mentioned before,are the broad
categories of thingswe want the bot to be
able to understand.So again, this bot's
relatively simple.And we'll really just look
at qualifying for a loanhere, in the interest of time.So inside this
intent, which we'velabeled Qualifying For a Loan,
there are about six utteranceshere.And these utterances can be
viewed as sort of the trainingdata for this intent.So we're providing
the bot with hereall the ways we
expect people to askabout qualifying for a loan.And it's important to know that
there's some machine learninggoing on behind this as well.So we're not we're
not asking youto provide every different
permutation here.But you do need to
get good variety hereto be able to capture a wide
breadth of things peoplemight say.So within these
utterances, you'llsee a couple words highlighted
here in different colors.Those words are
actually entities.So entities represent a single
word or a group of wordsthat you want to
call out as somethingspecial within the utterance,
for lack of a better word.And it could be special
in a variety ways.One, it could be an
important term in the sensethat you know you
want to be there,you want to call it out to the
natural language understandingengine.It could be that you
want to provide variety.So it could be that Loan
also is sometimes referredto as Credit or Line of Credit.So it could be possible that
you want to basically call--you want to be able
to provide variety.And without having to
specify, can I get a loan,can I get a credit, can
I get a line of credit,you don't need to
tell us all those.All you need to do is specify
that Loan is an entityand that it has a variety of
ways that it could be said.And lastly, we're not going
to see it in this bot,but entities allow you to pass
data on to the actual chat flowif you require that
in your conversation.So it could be you
could have an utterance,can I get a loan for $10,000.That $10,000 could be provided
to the conversational flowso that then that's one less
piece of information you'dneed to query for.But for this bot, let's
keep it simple for now.So let's then look at the
conversational flow aspects.And for that, we'll
go to the dialogues.I'll zoom in on
this in a second.And this might look intimidating
just because of how big it is.But really, this is
a fairly linear flow.These could obviously take
any sorts of twists and turnson top of this.But this is a fairly
linear flow that'sbeen collecting the information
that we saw earlier.And I want to call out
something for a second here.You'll see that the
first piece of this flowis actually asking the
homeowner if they own,mortgage, or rent their
current residence.And you might say,
well, that's not reallywhere the bot started, I
mean, it started somewhereelse with asking me if I
was interested in a loanand if I was ready
to start the process.So let's actually go look
at one of those real quick.Or actually, I'm going to
go to this flow and start.So what I want to call
out here, and then we'lljump back to the
qualification whereI want to spend
most of our time,is that Conversation
Designer has the abilityto link these dialogues
together usinga node that allows you to
jump from one conversationto the other.And what's nice about
this is it allowsyou to manage pieces
without havingduplication all over your bot.So let's say there's
eight different placeswithin a chatbot that
people might ultimately needto get qualified for a loan.Well, rather than having
that logic sprinkledall over the place and have to
remember to go and update itin several places, you can
just have one central dialogueabout loan qualification,
and then youcould jump to that dialogue
whenever it makes sensewithin a conversational flow.So let's jump back to this
rather lengthy dialogue.We'll zoom in here a little
bit so you can actuallyget an idea of what's going on.So the bot, as we
saw, asks the user,do they own, mortgage, or
rent their current residence.What you'll notice here is
that this is an HTML response.And that means that if
we want line breaks,we have to insert those
line breaks ourselves.But what it also means is
if we want rich responses,and we'll look at,
I believe, this onehas an italicized
slightly smaller fonthere at the bottom, so it
allows us to tailor our responseand make it a little
bit easier for usersto see different pieces
and us to style it,essentially, the way we
want it to be styled.The other thing I want you
to notice in that node--if I scroll down just
a little bit more--is that we're calling
out to something calledNumber Tool here, and
what we're callingis Loan Amount, as well.So we're saying we want to
format this variable, LoanAmount, using this format.And this is where,
later on, you'llsee that there was
a mistake in partof the bot assuming that it
was always a five-digit number.But what's important
here is to notethat this templating language
here, which is Apache Velocity,allows you to format numbers.It allows you to have logic,
if/else statements, and loops--so all sorts of things that help
you take the data that you'vebeen collecting as part
of this bot and spit itback out to the user
in a meaningful way.And to that point, we'll
look at this blue node here.So after we've asked the
user what kind of resonancethey currently have
and we've provided themwith some buttons where they can
select Rent, Mortgage, or Own,we're now going to collect that
information in a text inputnode.And that text input node's going
to assign it a variable nameof Home in the context.So now, in any other part of a
bot flow, we can refer to Home,and it should give us the value
of what that user provided.So without going through
all these, suffice to say,it continues to collect
this information, right?Just as we saw in the flow,
it asks about your employment,about the purpose of the
loan, and then, finally, itbasically responds to
the user, saying, here'sa review of your answers.And we'll see just
a bunch of template,and show the full text here.We'll see a bunch of
formatting of the numbers.We bolded, in HTML, these values
to call it out easier to user.And then, at the end, it
asks, do these look right?So the user can then respond
with either Yes or No.We'll match that they
said either Yes or No.So in this case, we'll
look at the Yes call.And really, now we're
getting to the heartof after we've collected
all this information,what can we do with it?So in this case, what I
really wanted to call outwas that we can make
a web service call.So if you've got some web
service sitting out there usingrests-- so you can GET
it, POST it, PUT it,you can instrument that
using your chatbot.So in this case, we're
calling this URL,which happens to be another Viya
instant sitting on a machine.We are posting some data to it.We've got some JSON here,
which, again, is templatized.So for instance, I'm
using the value Homethat we just saw earlier
in this JSON template.And I've got a
couple headers here.So we are authorized, and
we're saying that we wantJSON back out of this service.And then, lastly,
I'm going to takethe results of whatever
the service says,and I'm just going to put it
in a variable called Score Out.And now let's look at the
response just after this.And this is where the natural
language generation getsa tiny bit more complex, but I
hope it's still relatively easyto see what's going on.So after making that call, we've
got a variable score out here.Using our template
based language,we're now going to
parse that as JSON.Because it came
back as JSON output.We're going to get one of
the pieces of that JSONcalled Outputs.And we're just going to get the
first element, not the zeros,but the second
element in the JSON,essentially, and get that value.And we're just going to assign
that to a local variable calledPercentage.So this line just
essentially meanswe don't have to
keep copy and pastingthis entire getter everywhere.So now, this percentage is
basically the confidence valuethat the machine
learning based modelreturned on whether that
user will default or not.And so now if that
percentage was above 50%,we're going to say, ah, you
know, sorry, but, you know,the percentage of you defaulting
returned was pretty high,we're not going to be able
to extend that line of creditfor you today.But in the case we saw,
when that percentage isless than 50%, we have a
nice congratulatory message.And so that shows
you how the datacan affect your output in
natural language generationusing these templates.And in this case, I'm not
sure if this would everhappen unless maybe--yeah.So the author of this
bot chose to have,basically, some
debug messages here,something else would happen.So we're going to
click OK on that.And then, lastly, they
respond with the link.So you saw, in our case, when
it was successful, the linkto go apply for the loan.But even in the case--this is sort of another bot
designer decision-- evenin the case where
it was above 50%,they're still going to
send them into a link.Maybe that machine
learning based modelhas some edge cases that
disqualify this person.But once they talk
to a real person,they could go into the
system and override those.So that could be useful as well.The one thing I want
to call out here, too,and we'll talk about
in just a second, if Ilook at the types of
nodes that could be addedat any point in a
conversation, thereare a couple others that
allow you to retrieve data.So this one happens to be
calling out to a web service.But there are others that allow
you to run code, essentially,while you're executing a bot.So it'll allow you to write SAS
code in the middle of a bot--so if you wanted
to execute that.We also have a couple WYSIWYG
Visual Analytics nodes herethat allow you to call out to
some reports you might haveor even create reports
on the fly, given datathat you've already
got loaded into VA.And in the future, we plan to
have a couple others here aswell.Let me jump back over to my
PowerPoint presentation here.So like we just talked
about, bot connectivity--and this is the last piece
that I want to call out,the power is the
Viya platform here.So using one node, where
we made a very simple restcall to, essentially, the
VDMML pipeline there to scorea new result, we were
able to very easily dropin the power of that
machine learningmodel right into our bot.And so if you've got your
information already in SAS,in Viya, it's extremely easy
to include that in a bot,to add value to your chatbot.There are, of
course, other ways.So the power of that
web service nodeis that if you've got code
running at some other webservice outside of
SAS, as long as youcan get to it from the chatbot
machine that's being hosted on,that is also enabled as to
provide value for your bot.And coming soon, after
this release, we alsoallow users to write DS2 code,
DATA Step 2, or Python code.Because we know
those two are alsovery popular among the
data science community.So in conclusion, I hope this
gave you a really good overviewof chatbots, of how you can use
machine learning based modelsinside those chatbots,
and how the Viya platformand Conversation Designer allows
you to seamlessly integratethose pieces for your users.So thank you very much.I appreciate your time."
151,"Hi everyone.Welcome to the SAS Global Forum
2020 virtual presentation.My name is Brad Morris,
and I'm the authorof this paper, SAS Visual
Analytics SDK, Embed SAS VisualAnalytics Insights in Your
Web Pages and Web Apps.So let's get right into this.A little bit of
background on me.I work for SAS.I manage the SAS
Visual Analytics SDK,and SAS Graph Builder teams.I joined SAS in 2001.I have a computer science degree
from North Carolina State.And I started as a developer and
worked on SAS Visual Analyticsin some form or fashion
through the lifecyclesince its inception.So what is the SAS
Visual Analytics SDK?This really starts with a
description of what an SDK is.SDK stands for software
development kit.And that can take on a
few different meaningsdepending on the context.In this case, what
we're describing,what software
development kit meansfor us is this is a JavaScript
library that providesJavaScript APIs and custom
made HTML web elements.And the functionality
that providesis to allow you to interface
with SAS Viya DigitalAnalytics.This allows you
to embed insightsinto your own web pages.This could be reports,
report pages, containersindividual objects, and also
provides a JavaScript APIfor more deeper integration
using data drivencontent and report parameters.So what I'm going to show
you all today is give youan overview of both the custom
element APIs that we haveand how to use them.I'll show demonstrations
for embedding reports,report pages, and
report objects.I'll also show the
JavaScript API,and show some examples of what
you can do with data drivencontent and report parameters.But I first want to start
off with pointing youto a resource that's going
to be very handy if you'regetting familiar with the SDK.And that's our developer guide,
our developer API documentationis out on developer.sas.com.A quick overview of that now.This is the main page, and we've
got a lot of information here.Some general getting
started information,which includes some information
on setting up your Viya systemin order to allow
connection back to the SDK.That includes topics like
cross-origin resource sharing,cross-site requests forgery,
and setting up guest accessif you want to allow that.These are all things that need
to be set up ahead of timeand configured on Viya
an order for the SDKto be able to connect from
a different origin backto SAS Viya order to read and
then render the report content.We've also got more
detailed guideson things like data
driven content, as wellas API references on all of our
JavaScript and custom elementAPI.Jumping from there, let's get
right into the meat of this.And the first thing that I want
to show is how to load the SDK.Now there's really two
ways to access the SDK.We publish the SDK out on NPM,
which is Node Package Manager.So if you're familiar
with JavaScript all,you're likely familiar with NPM.This is where all third
party modules are published.And there's no difference
here with the SDK.| module is under the
namespace SAS software,and the actual module name
is VA Report components.So being published
out on NPM, youhave the option of doing an
actual NPM install, whichwill pull down this SDK library
onto your machine, whichthen you can deploy
alongside with your web page,if you want.But the easy option
here, and the oneI'm going to show today, is
actually accessing the SDKthrough a CDN.CDN is a Content
Delivery Network,and there are a couple public
CDNs that duplicate and mirrorNPM content.One of those is unpackage.com.What I'm showing
here is the abilityto create a script
tag and your HTML,and add it into your
head of your HTML,setting the source to
unpackage.com of VA reportcomponents.And you can see in this case,
I'm pointing to version 0.4.0.So just a quick note, what
I'm going to demo todayis this version 0.4.0
that is not available yet.It's going to be published
in the next week or two,so by the time you view this,
hopefully 0.4.0 is out there.So what this does is this
brings in the actual JavaScriptlibrary, and what's loaded
makes the JavaScriptAPI and the custom web
element declarationsavailable to you in your HTML
page or in your JavaScript.Moving on quickly from
this, is to a discussionof the custom elements.I mentioned, we've
got the JavaScript APIand we have the custom elements.So the custom elements
provide a declarative wayin your HTML markup to define,
in this case, a SAS report.So this is showing our
SAS report custom element.And it's really quite simple.This allows you to
treat SAS reports justlike any other built
in HTML elementthat the browser provides,
like a div, or an image tag,or an anchor tag.This is just another tag in
HTML that we've provided.And it is made available to
you through loading the SDKJavaScript.So what can you do
with the SAS report?Pretty much what you expect.This is embedding an entire
report into your page.It's got a couple of
required parameters here.One is authentication
type, and you'llsee me mentioned
this a couple times.There are two types
of authenticationthat we provide in the SDK.One is guest authentication.So if you have guests
setup on your system,you can set authentication
type to guestto have the report automatically
log in as the guest user.So this skips any type of user
intervention authenticationthat would happen
on the web page,and it uses the guest user.This is perfectly if you have
a public system with guestsalready set up and
you want everybodyto view this report as
the same guest user.The other type of
authentication is credentials.So I can say authentication
type equals credentials.The difference here is that this
will prompt the user for logincredentials, and it does this
by redirecting to the SAS logonpage.So this is us utilizing the
security and the authenticationcapabilities of SAS logon.And what that means is
that out of the box,the SDK is going to
automatically support.Single sign on so if you have
Viya set up for single sign-on,and using the SDK with
authentication typecredentials, we'll also
honor single sign-onand allow that to
work seamlessly.So the second
parameter here is URL.And this is just a
URL pointing backto the location of
the Viya server.The third parameter
here is report URI,and that's that
unique identifierto identify the report that
you're trying to embed.Let's skip over for now how
you get that report URI.I'll show you a really easy
way to do that later on.For now, let's jump over
and see what this does.So I have a Viya system that
has the built-in SAS VisualAnalytics samples.One of these being
report analysis.So just to give you
a brief overview,this is the report
analysis report.I'm going to show you what
it looks like embeddedin a web page using the SDK.So I have this page.I've got my own custom header.And it's really straightforward.Basically, the only thing on the
page is the SAS report element.And as you can
see, what this doesis it brings in the SAS report.It makes the connection back
to Viya, gets all the data,and renders the
report in line herein your page, fully interactive.So you can see making
prompt changes,doing filters, we're making
data calls back to the serverand getting that
here on the client.This is all alive with a
live connection back to Viya.You can also see that we've got
other interactions like drill.And all the UI and
interactions youexpect to see in
the report lookingthe way that you've designed it
from within Visual Analytics.So that's the key
here with the SDK.You design your report,
your insights, all the datamanipulation that
you want to do,and harness the power of the
VA through the Visual Analyticsapplication itself.Then, you can embed
that within the SDK.So this is a real
quick overview of whatyou can do with just a simple
embed the entire report.Sometimes that's enough.You want to provide a general
portal into some set of reportsthat you have and expose
that in a certain wayon your own web page.This is the way to do it.Moving onto the next custom
element that we have,and that's the SAS report page.And as you guessed, that is
not embedding an entire report,but picking at an individual
page from a reportand embedding that.So I've got two examples here.A lot of the same
parameters we have before.Real difference
here is page name.In this case, I've
specified the page name,and that will let me
embed a specific page.I can also alternatively
do it using page index.This a 0 based index.So in here I'm specifying
index number 2.So that's the third
page on the report.So let me show a
quick demo of this.And the real thing
to note here isthat while in the report
demo, we had all the full pagenavigation, when you
embed an entire page, youforgo that navigation.You go straight to the
page that you're embedding.Again, this is all interactive.It's all the same functionality
you had with the full report,just with an individual page.So if you want to target this--and in this case, I'm
actually embedding two pages.So I have two separate
pages of the same reportthat I want to show at the
same time on the same web page,embedded in the same page.So this gives a little bit
more of an integrated look.You don't get your page tabbing.Maybe you want to provide your
own page navigation mechanism.That could easily be
done programmaticallywith your additional
HTML and JavaScript,and then have that load
individual pages on the fly.So that's kind of a use of the
individual SAS report page.So going to the next,
it's SAS report object.And this is the most granular
of our custom elements.And this allows the embedding
of individual objectsor containers.So containers are considered
objects in this case.And again, it's got
all the same parametersthat are provided before with
the addition of object name.And I'll show in a bit how
we can generate automaticallythis definition of
what the report URI is,what the object name is,
or what the page name isfor the SAS report page case.You'll also see, . line 6 here.I've got an additional
style property herethat's being specified.And that's really just
to point out to youthese are just extensions of
the built in HTML elements.This is like a div.This is like an anchor
tag, like I said.These inherit all the properties
that come from HTML element.And so that includes
style, class name.I can style all
of these with CSSjust like I would style
any other visual, so--or any other HTML tag.So in this case, I'm setting
width and height explicitlyhere with 100%
height 400 pixels.And it really depends
what kind of layoutyou want to have in your page.So let's get to the quick
demonstration of that.So in this case, I'm
pulling individual piecesout of that report.I've got this top section
here is actually a container.And it's got five
different visuals,and it's got a prompt
which I can seeis filtering by year
into these key values.So I've got four different
key values down here.This is an individual element,
specified by one SAS reportobject tag.I then have two other
visuals down below,each one individual
visuals in that report.And you can see that these
are each individual SAS objecttags, yet because there
is interaction definedin the report, I still get that
exact same interaction filter--filtering, or brushing,
or linking to other pages.All that is still
supported here.So that-- this is
really, when youget to what I would call maybe
like a mash-up situation.These don't have to all
come from the same report.I can have multiple SAS
report objects in a page,pointing to different reports.Now of course if they're
pointing to different reports,you don't have any interactions
to find between them.But there are cases where you
have data, different insightscoming from different
locations, and youwant to pull them all together
in a way that I can showthe user this overall view.It's doable with the SDK
here using report objects.And it's really easy to do.So I've showed you all the three
custom elements that we have.And I guess the big question
this point is, OK, great.How do I determine
what the report URI is?How do I know what
my object name is?And I promised you
I would show you.There's a real easy
way to do this,and that's using SAS Visual
Analytics the application.There's a functionality
called copy link,which will automatically
generate the custom elementtag that you need.So this will generate SAS
report, SAS report page,and SAS report object tags.And it'll let you choose
between guest or credentials.So let's jump back over
here to this report.And I'm going to select this
object here, cost by labor--cost by primary labor group.If I go to the object
toolbar, select copy link,you'll see this dialogue pop up.And I've got a couple options.I click embeddable
web component.This is the option that
will change the linkto be an actual
custom element tag.You can see this is generating
the SAS report objecttag for me, pre-populatng
the object name,and the URL, and the report
URI, and then optionallychanging it to
guest or credentialsbased on what I've chosen.So I can just hit copy link,
and paste that directlyinto my HTML page source.And this allows you to
easily go to your reports,and cherry pick those
visuals that you want,and dump them right into
your HTML page with ease.So moving on, we're going to
jump into the JavaScript API,and then try to cover
this really quick for you.We covered the declarative
HTML custom elements,and that's the
way if I just wantto statically bring some things
over and show them on my page.But if I want to do
anything dynamic,I have to get into
the JavaScript API.So this example shows here how
to access the JavaScript API.And there's a
really key bit here.On line 3, you see this
object VA report components.This is a global that is defined
by the JavaScript library.And off of that, lies all
of our custom elements.So you see for the
SAS report, youhave the equivalent SAS
report element custom element.And on that, we've
got the propertiesthat mirror those
properties thatare available in the
custom element tag.So things like, line 4,
you see myreport.url.Line 5, you've got report
URI authentication type.The same goes for SAS report
page element and SAS reportobject element.Again, all of these APIs can
be seen in our documentationand they're fully
describe there.So you can see I'm doing various
things through JavaScriptimperative API here, and then
I'm adding this new customelement, and I'm appending
it to the document body.So this is essentially
achieving the same thingthat we did declaratively
through custom elements,we're doing it here
through JavaScript.And that's not that
interesting, but whenyou want to get into using
some of our other moreextensive API, like data
driven content or parameters,you need to figure out, OK, how
do I work with this JavaScriptAPI?So I want to show an example
of data driven content here.And the code example here
is not that interesting.But I'm going to give you
a quick overview here.Essentially, data driven
content allows you to--or the SDK allows you
to connect directlyto a data driven content object.So instead of embedding it
using a SAS report objecttag like you would, you
would connect to it directly.And essentially you're
harnessing its powerto pass data in and out of the
SAS Visual Analytics report.And so we can do that
through this registerdata driven Content API by
passing a handler for the datamessage.So essentially by doing this,
we can capture the data that'sbeing passed to a given object.And we can take that, and we
can render our own visualson the page.This could be, essentially,
rendering your own datadifferent content
on your web page,or you can modify
your web page howeveryou want based on that data.So I've got a--it's a little bit of a
contrived demo to show this,but for a data driven
content, I've taken this page,and I've hooked up a data driven
content to gross claim amount.And we're not seeing the
data driven content here.This is going on
behind the scenes.And I'm actually going
to modify the pagebased on the value of
the data driven content.You can see what I did
was I put, essentially,my own type of display rule.And I have said, if
the total warrantycost, if the gross claim
amount is over $30 million,then I'm actually going
to turn my banner red,produce an alert for the user.And this is, like I said,
this is a contrived example,but it shows that I'm
essentially creatingmy own visualizations out of
the entire web page itself.I'm having it
respond to the data.So this could be really
powerful if you'vegot complicated calculations
going on in your VA report,and you want to expose
those calculations backout to your application to
show some feedback to the user,to make some automated decisions
in order to drive processesthrough other systems.So there's a lot
of opportunity hereto get creative, and
use the power of VAand harness that
in your web pagehere through data
different content.So next, let me show
report parameters.And this is another way to
get a deeper integrationwith your report.Now if you have a report that
actually exposes parameters,you can easily set
those parameterson the report object.So here, you can see
that I'm taking my reportand I'm setting parameters.Let's say in this
case, the example,I've got a parameter
called origin parm,and I'm setting
the value to USA.And so in this case, right, I
could have some UI in my page,like some custom UI, that
changes report parameters,things of that nature.So in my example here,
using the same report,I've actually taken the
filtering out of the report.And I have each of these visuals
filtered by the ship yearparameter, right?And what I've done is created
my own custom component herein the web page that
allows me to passthe year in as the
report parameter,and you can see
as I click these,I'm calling set parameters
with the ship yearparameter to 2017.And that sets that
on these visualsthrough the report
parameter feature.And again, there's a
lot of potential here.Parameters, in this case,
I'm using a parameterto do filtering on
the report data.I could use parameters to create
more complex calculated columnsusing those values.You can use it for ranking,
display rules, et cetera.So this has a lot
of potential there.So just as a recap
of what I've shown,I've shown how to load the SDK.I've shown how to work with
our custom HTML elements.And I've shown how
to use our JavaScriptAPI to get deeper integration
with report parameters,as well as data driven content.So I've got a couple
extra resources herethat should be useful.Again, this is the--what I showed initially
was the developer.sas.comdocumentation.We also have a GitHub page
for the Visual Analytics SDK.And that's where you'll find
some straightforward exampleHTML pages that we've
published to showthe various
capabilities of the SDK.And also I've got a link
here to the NPM locationfor the actual package.So this will-- if you're
interested in seeingwhat versions are out there, you
can get that from NPM directly.So thank you very much for
attending this virtual paperpresentation.And I hope you've learned
a lot about the SDKand seen the value
that it can providefor you on top of your SAS
Visual Analytics installation.If you have any questions
for me on this presentationor the SDK in general,
feel free to reach outbradley.morris@sas.com.Thanks again for joining."
152,"Hi.My name is Andy Ravenna,
and I'm a technical trainerat SAS Institute.What I'd like to show you
today is a hands-on workshopand give you an introduction
into SAS Visual Statisticson SAS Viya.So at a very high
level, I'm justgoing to give you a
nice quick introduction,and then I'm going to get
right into my demonstration.Let's go ahead and get started.So what I'd like to talk to
you just a little bit aboutis what SAS Viya is and
how SAS Visual Statisticsfits into SAS Viya.I also am going to
cover some regressionbasics at a very high level.Hopefully you're already
familiar with regression.That's probably why you're
watching this video.If you're not, it's pretty
easy to follow along.And then I'm going to
do a demonstration whereI'm actually going to perform
a logistic regression for you.What exactly is SAS Viya?Well, SAS Viya is the
latest piece of softwarethat you can get from SAS.It has some really great
features available in it.And one of the features
that I want to focus in on,which is really most
important to you and I,is the concept that
SAS Viya is goingto allow you to take big
data, bump it up into memory,and then not only put
the data in memory,but also perform the
analytics in memory.This works really well
for us, especially whenwe have algorithms that are
very computationally expensive.And so that's one
of the reasons Ithink SAS Viya is
most important to us.What is SAS Visual Statistics?Well, SAS Visual Statistics
is a web-based productthat is an add-on to
SAS Visual Analytics.I think in a very general
way, a typical sessionfor you might be you might log
in to SAS Visual Analytics,and you would actually
explore your dataand clean up your data.And then you would use
SAS Visual Statisticsto start to perform some
unsupervised and supervisedanalysis.I want to talk to you just a
little bit about regression.Regression has been around
for a really long time.And we like to use
regression for a coupleof important reasons.One of the main reasons
we like to use regressionis because it will
allow us to come upwith a prediction formula
that's going to allowus to predict new cases.So we're going to take
data from the past,and then we're going to use that
data to help us build a model.That model's really just going
to be some sort of predictionformula.And then we can use that
prediction formula on new data,for example, new
customers, new patients,and we can make
predictions about that.Another feature that
we can take advantageof when we're
performing a regressionis it can help us narrow
down the informationthat we want to
plug into a model.Some of you out there might
have a lot of informationabout what it is that
you're trying to understand.For example, maybe
you have a lotof information about your
patients or your customers.Maybe you even have 50
columns or 50 inputs.Well, typically, one of
the things that we foundis that it's not very
helpful to our modelto have many, many inputs.A more complex model is just
more difficult to maintainand more difficult to execute.So what we want to do is
narrow down that list of inputsto only those that
are most important.And that's the process
of sequential selection.Some of you might
even be familiarwith some of these
selection methods,for example, the forward
selection method, whereyou start with an
empty model, and thenyou add inputs one at a
time and keep the onesthat are most important.We're actually not
going to be performinga typical regression,
where we wouldbe making a prediction
on a continuous target.For example, maybe we're
trying to predict how mucha customer was going to spend.What we're going to focus
in on is a binary logisticregression.And I have an example here,
where we're asking ourselvesa question, hey, can we use both
credit score and home ownershipto help us make a
prediction aboutwhether somebody is going
to default on a loan or not?So what we're trying
to do in this caseis we're trying to
make a predictionabout a categorical variable.And so the values for the
whether a customer defaultson a loan or not are yes or no.So we want to know,
looking at informationabout our customers,
do we predictthey will default on a
loan or they will notdefault on a loan?This is very similar to the
binary logistic regressionI'm going to show you
in the demonstration.But we're going to be making
a prediction about somethingslightly different.All right, let's get right
into my demonstration.OK, let's go ahead and get
started in the demonstration.Now you'll notice I went
ahead and turned off my video.And that way, I don't cover
up anything in the interface,so that you can see all
the important features.As you can see, I've already
logged into SAS Drive.And now I'm ready to begin my
binary logistic regression.So I'm going to go to the
upper left hand corner menu.We call that sometimes
the hamburger menubecause it's got the
three lines on it,looks a little bit
like a hamburger.And I'm going to go into
Explore and Visualize.And basically, this opens up the
SAS Visual Analytics interface.And now Visual Analytics is
going to ask me, hey, Andy,what do you want to do?Do you want to start up
with a brand new report?Or do you want to start
up with some data?Well, we're going to
start with some data.And I went ahead and loaded
our data into memory,so you and I don't
have to wait for that.We're going to be using
this VS_BANK table.So this is data that has to do
with a banking campaign, where,over a six month period,
there gathered information,just over a million rows of
data, about customers whomade purchases from the bank,
about different products,things like loans
and credit cards.And so that's the data that you
and I are going to be using.And we'll become a little
bit more familiar with itas we start to work
through our example.One of the things that
we're going to seeis if I scroll down here in
the list of all the measures,I have a measure called
target binary new product.This is a binary variable.It's a flag going
between zero and onethat indicates whether a
customer made a purchaseduring the campaign season.Now because it's coded
as zeros and ones,the software actually
thinks that it's a measure.But we want it to be a category.So what I'm going
to do is I'm goingto go ahead and
edit the propertiesfor our primary target variable.And we're going to
change the classificationfrom a measure to a category.And now you'll see
what it does isit pops that up into my
list of category variables.And we can also see
the cardinality here.So there are two levels
to that variable.One of the nice features
about the newest versionof SAS Visual Analytics
and SAS Visual Statisticsis that you can actually turn
on an option that says, hey,if you find any binary
variables that are measures,go ahead and automatically
convert them into categories.But I did not turn that on.So we did it manually.OK, so now I've got
a target variable.I'm going to go ahead and begin
to build my logistic regressionto help us make
predictions aboutwhether a customer is going
to make a purchase or not.I'm going to come over
to the objects pane,and I'm going to scroll all the
way down almost to the bottom.And we can see our list of
SAS Visual Statistics objects.And I'm either going to
double click or drag and drop.I tried double clicking.It seemed to work.And that's going to place my
logistic regression objectdirectly into page number one.So now we have one object
on this page in my report.I'm also going to go
ahead and take advantageof a very interesting option by
coming to this upper right handcorner menu.This menu has several
different options in it.And we're going to select one
of these interface options thatsays that I would like to
disable my auto refresh.Basically, what
that means is I wantto come in to this
particular objectand make a whole
bunch of changes,but I don't want it to refresh
for each individual changethat I make.I want to wait until I'm
done with all of my changes.And then I'll say
I'm ready to refresh.So what are we going to do?We're actually going
to assign some rolesto this particular
logistic regression.Now the response variable
or the target variablewas that variable that
I just showed you.It's that binary
flag that indicateswhether a customer made a
purchase during the campaignseason.So we're going to go
ahead and select that.Now we want to start
to specify our inputs.So what are some of
the columns that we'regoing to use to
help us determinewhether a customer
makes a purchase or not?I actually have a whole
bunch of cleaned upcontinuous variables.I have a logi_rfm1 all
the way down to logi_rfm9.So what are these rfm variables?They're recency, frequency,
monetary variables.And they're just a really
convenient way to groupa whole bunch of
continuous variables.You can see, for example,
that rfm1 is the average salesover the past three years.Whereas rfm12 is the
customer tenure in months.Now the reason that we
have logi underscoreas a preceding indicator
to each of those variablesis because they've
all been cleaned upusing a log transformation and
imputation for missing values.So we're going to go ahead and
include those in our model.And the other thing
I want to do isI want to scroll down here
to my classification effects.And we're actually going to
add in two variables here.There are two
categorical variablesavailable to us that are
information about our customer.One is the account
activity level.And the other is the
customer value level.So we're going to go ahead
and add both of those in.Now I'm ready to go
ahead and take a lookat my logistic regression.So what I'm going
to do is I'm goingto come up to the overflow menu
in the upper right hand corner.And now under
Interface Options, I'mgoing to say, hey, let's
enable that auto refresh.I'm also going to collapse this
Data Roles pane because that'sgoing to give us just a little
bit more real estate as we'reexamining what happens.Along the top here,
you'll notice that weget a name of our model.It's a logistic regression.Here's our target variable.We are modeling an event equals
a one, which is a purchase.We also have a statistic, which
tells us a little bit about howthat model is performing.This is the KS statistic.We can see how many
observations were used.I've got a total of
three panels here,a fit summary panel, a residual
plot, and a confusion matrix.I'd like to get even more
real estate on each of these.So I'm going to come
over to my Options,and I'm going to scroll down.And under the Model
Display General options,there is a Plot Layout that we
can change from Fit to Stack.And what that's going
to do is that's goingto put each of those pieces of
information on a separate tab.So now I've got my Fit
Summary tab, my Residual tab,and my Assessment.So we added a total of
12 plus two, 14 inputsinto this logistic regression.And as I scroll up
and down, you'llsee the most important
variables to this analysisare at the top.And they are these
long purple lines.And what's being plotted
here is the p-value.But it's not just the p-value.It's actually the minus
log of the p-value.So let's think about this.Inputs that are very
significant are goingto have a very small p-value.If you take the log
of a very small value,you get a very large value,
but it's going to be negative.So that's why we're doing
a negative of that value.So in other words,
a very small p-valueis going to give us a very large
positive negative log value.And so we can see customer
value level is very important.And as we scroll down,
we'll notice that we'vegot this black line here.That black line is put at 0.05%.So anything that
is less than 5%--sorry-- anything that
is greater than 5%is not going to be significant.So we can see that we have one
variable here, the last productpurchased amount, which is not
significant at the 5% level.All right, that's our
fit summary panel.It helps us see which of
these inputs are importantand which ones are not.We also have a residual plot.A residual plot for
a logistic regressionis probably most
helpful when we'retrying to spot if
we have any outliersor unusual observations.And I'm not seeing anything
here to worry about.I'm going to move over
to the assessment plot.By default, for the
assessment plot,we're going to get
a confusion matrix.And the confusion matrix,
you might remember,has those four key statistics.It has the true
positives, in other words,those particular purchasers in
the past that were identifiedas purchasers by this model.And then we also have
our true negatives,those nonpurchasers who are
identified as nonpurchasers.And then we also have the
misclassification numbersas well.If I do a right
mouse button click,I can switch over
to the lift plot,which gives me an indication
of how this model is helpful.Or in other words, what kind of
lift do we get from this model?The model is the blue line.And if I mouse over here
at the 5th percentile,you can see that the model is
giving me about a 3.7 lift.What that means
is if I were to goto the top 5th percentile of
my customers using this model,I am 3.7 times more likely to
reach a purchaser versus justpicking customers at random.So that sounds like
a pretty nice lift.I'm going to do another
right mouse button click,and I'm going to select the ROC
chart or the Receiver OperatorCharacteristic chart.This is a chart that allows
me to see how well my model isavoiding misclassifications.So this has the sensitivity
posted against 1minus the specificity.So we have 1 minus
the specificitydown here as my x-axis and
the sensitivity on my y-axis.The blue curve is my model.If my model were
to 100% correctlyidentify all the purchasers and
nonpurchasers, in other words,if there were no
misclassifications,we would have a
very big blue curve.In fact, it would fit the
outer outline of that rectanglethen I'm drawing right
now with my mouse.So in other words, on the
ROC chart, what we wantis a very full curve.So that's one way to
quantify how well my model'sperforming is that if we
have a very full curve.So one of the things
that we can dois we can plot this ROC
curve against the ROCcurve of another model.And then of course, we
would identify the modelthat had the larger ROC curve.Now sometimes when we
have multiple plots,it's a little bit
difficult to read.So one of the statistics
that I really likeis this KS statistic.The KS statistic is calculated
by finding that vertical lineor vertical distance between
what we think of as my defaultor base model, which would
be just picking customersat random, and my curve.So we want the maximum distance,
maximum vertical distancebetween those two.And we get a separation
here of about a 0.55.So that means, if we were
to compare this KS statisticagainst another KS statistic,
we would want the modelthat has the higher KS value.All right, I'm now
going to move overto the misclassification chart.And the misclassification
chart is very, very similarto the very first pane of
information that we looked at.You might remember, if I do
a right mouse button click,I can get back to
that confusion matrix.Well, these are the exact
same statistics thatare in that confusion matrix.So we can see our
true negative valuesas well as my true
positive values and thenas well as the
misclassification right here.I'm going to go to
the upper right handcorner of my logistic regression
and click the Maximize button.Because one of the
questions that I oftenget when I'm working
with students isthey really want to know, hey,
where are all the numbers?We looked at a lot of
charts of information.But where can I see
the actual numbers?And the actual numbers are
in this detailed table.So for example,
maybe you'd actuallylike to see the
parameter estimatesfor that binary logistic
regression that we created.Well, here is the y-intercept.And here are the
parameter estimatesfor our categorical
variables and allof our continuous variables.We can also see all of the
model fit statistics thathave been created that you
might want to use to helpyou evaluate this model.So we have the minus
2 log likelihood,the Akaike's criterion, the
Bayesian information criterion.So hopefully you'll find your
very favorite statistic here.All right, so those are
where all the numbers are.I'm actually going to go
ahead and restore that.Then I'm going to come
up to the Page tab.And you'll see that we also have
a little overflow or optionsmenu in the Page tab.And from here, I can actually
duplicate this particular page.So I've just taken that
logistic regressionand copied it onto
a brand new page.And the reason I did
that is because I wantto make a couple more changes.So first, I'm going to come
to our upper right hand corneroverflow menu.And from there, I'm going to
go to my Interface Options.And once again, I'm going
to select that I would liketo disable the auto refresh.And here are the changes
that I'm going to make.First, I'm going to come
over to my Data pane.And I'm going to
create a new data item.That new data item is
going to be a partition.So one of the
things that we oftenlike to do when we are
building new models to helpus make predictions
is we want to finda model that generalizes well.A model that generalizes
well is one that is not onlygoing to perform
well on the datathat we use to create
the model, but it alsoperforms well on the new data
that comes down the pipeline.So we want a model that's going
to do well in both situations.And a great way to do that
is by partitioning the data.We're actually just going to
create two partitions, wherewe are going to put 50%
into the training and 50%into the validation.I think we want 0.5.No.We want 50%.We're going to go
ahead and click on OK.And so now I've created
that partition variable,and it's available to me.So one of the changes
I'm going to make hereis I'm going to come
over to our roles.And I'm going to select
the logistic regressionobject to make it the
active item in my interface.And I'm going to scroll
down into the rolesof the logistic regression.And I'm now going to
add in that partition.So here's the new partition.And in just a
second, you're goingto be able to see how
that affects my output.But I also want to change
one other option herethat's available to us.And that is under the
logistic regression,I want to use a variable
selection method.So let's go ahead and use
the Fast Backward eliminationmethod.Fast backward is
basically a methodwhere we start with
the full model,and then we
eliminate those itemsas they are not significant.The fast backward
method actuallyuses numerical shortcuts
over the backward method.So it's a little bit
faster, and thus its name.So we don't want
to have any termsin the model that
are going to beover that 1% significant level.So we only want things
that are significant at 1%.I'm now going to come to
the upper right hand corneronce again into the overflow.And now I'm ready to
enable my auto refresh.Now we see the new
logistic regression modelthat we've built. And let me
show you a couple of changesthat you might notice.First of all, when we look
at the Fit Summary panel,because we chose to use that
fast backward eliminationmethod, you can see that
three terms, which were notsignificant at the 1%
level, were actuallyremoved from the model.So they are not
part of this model.And one of the other things
that I want you to noticeis that since we now have
a partition, if I come overto the Assessment
panel, we are nowgoing to see information for
both the training partitionand the validation partition.When we are trying to pick
a supervised model thatgeneralizes well, we'll actually
use those validation statisticsto help us pick
the correct model.The other thing I
want to show youis that if I open up that
details table, because we choseto eliminate methods and
use a fast backward methodand we chose to ask the
software to eliminate themfrom the model,
we're going to alsobe able to see the
selection summary.So you can see in what order
those terms were pulled outof the model.All right, I'm going to go
ahead and close that table.And that's it for
my demonstration.So let me just pull back to
my slides for just a secondand cover one more thing.Thanks so much for joining me
in this hands-on workshop whereyou got an opportunity to learn
just a little bit about SASVisual Statistics.If you'd like to learn more,
you can see on this slidethat we also offer
SAS training courses.So for example, you might
be interested in SAS VisualStatistics on SAS
Viya interactive modelbuilding, where
not only would youlearn how to do a
logistic regression,but we also show you things like
linear regression, clusters,and decision trees.Also, if you notice
on this slide,there is a huge selection
of free SAS tutorialsthat you can get access to that
can help you go ahead and getstarted and continue to
learn about SAS and SAS Viya.And there's also a link to
the online documentation.So thanks again for joining me.We'll see you next time."
